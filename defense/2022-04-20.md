### Title: Case-Aware Adversarial Training
* Paper ID: 2204.09398v1
* Paper URL: [http://arxiv.org/abs/2204.09398v1](http://arxiv.org/abs/2204.09398v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The neural network (NN) becomes one of the most heated type of models in
various signal processing applications. However, NNs are extremely vulnerable
to adversarial examples (AEs). To defend AEs, adversarial training (AT) is
believed to be the most effective method while due to the intensive
computation, AT is limited to be applied in most applications. In this paper,
to resolve the problem, we design a generic and efficient AT improvement
scheme, namely case-aware adversarial training (CAT). Specifically, the
intuition stems from the fact that a very limited part of informative samples
can contribute to most of model performance. Alternatively, if only the most
informative AEs are used in AT, we can lower the computation complexity of AT
significantly as maintaining the defense effect. To achieve this, CAT achieves
two breakthroughs. First, a method to estimate the information degree of
adversarial examples is proposed for AE filtering. Second, to further enrich
the information that the NN can obtain from AEs, CAT involves a weight
estimation and class-level balancing based sampling strategy to increase the
diversity of AT at each iteration. Extensive experiments show that CAT is
faster than vanilla AT by up to 3x while achieving competitive defense effect.

### Title: GUARD: Graph Universal Adversarial Defense
* Paper ID: 2204.09803v1
* Paper URL: [http://arxiv.org/abs/2204.09803v1](http://arxiv.org/abs/2204.09803v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/edisonleeeee/guard](https://github.com/edisonleeeee/guard)
* Summary: Recently, graph convolutional networks (GCNs) have shown to be vulnerable to
small adversarial perturbations, which becomes a severe threat and largely
limits their applications in security-critical scenarios. To mitigate such a
threat, considerable research efforts have been devoted to increasing the
robustness of GCNs against adversarial attacks. However, current approaches for
defense are typically designed for the whole graph and consider the global
performance, posing challenges in protecting important local nodes from
stronger adversarial targeted attacks. In this work, we present a simple yet
effective method, named \textbf{\underline{G}}raph
\textbf{\underline{U}}niversal
\textbf{\underline{A}}dve\textbf{\underline{R}}sarial
\textbf{\underline{D}}efense (GUARD). Unlike previous works, GUARD protects
each individual node from attacks with a universal defensive patch, which is
generated once and can be applied to any node (node-agnostic) in a graph.
Extensive experiments on four benchmark datasets demonstrate that our method
significantly improves robustness for several established GCNs against multiple
adversarial attacks and outperforms existing adversarial defense methods by
large margins. Our code is publicly available at
https://github.com/EdisonLeeeee/GUARD.

### Title: Decoherence: From Interpretation to Experiment
* Paper ID: 2204.09755v1
* Paper URL: [http://arxiv.org/abs/2204.09755v1](http://arxiv.org/abs/2204.09755v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: I offer a few selected reflections on the decoherence program, with an
emphasis on Zeh's role and views. First, I discuss Zeh's commitment to a
realistic interpretation of the quantum state, which he saw as necessary for a
consistent understanding of the decoherence process. I suggest that this
commitment has been more fundamental than, and prior to, his support of an
Everett-style interpretation of quantum mechanics. Seen through this lens, both
his defense of Everett and the genesis of his ideas on decoherence emerge as
consequences of his realistic view of the quantum state. Second, I give an
overview of experiments on decoherence and describe, using the study of
collisional decoherence as an example, the close interplay between experimental
advances and theoretical modeling in decoherence research.

### Title: Case-Aware Adversarial Training
* Paper ID: 2204.09398v1
* Paper URL: [http://arxiv.org/abs/2204.09398v1](http://arxiv.org/abs/2204.09398v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The neural network (NN) becomes one of the most heated type of models in
various signal processing applications. However, NNs are extremely vulnerable
to adversarial examples (AEs). To defend AEs, adversarial training (AT) is
believed to be the most effective method while due to the intensive
computation, AT is limited to be applied in most applications. In this paper,
to resolve the problem, we design a generic and efficient AT improvement
scheme, namely case-aware adversarial training (CAT). Specifically, the
intuition stems from the fact that a very limited part of informative samples
can contribute to most of model performance. Alternatively, if only the most
informative AEs are used in AT, we can lower the computation complexity of AT
significantly as maintaining the defense effect. To achieve this, CAT achieves
two breakthroughs. First, a method to estimate the information degree of
adversarial examples is proposed for AE filtering. Second, to further enrich
the information that the NN can obtain from AEs, CAT involves a weight
estimation and class-level balancing based sampling strategy to increase the
diversity of AT at each iteration. Extensive experiments show that CAT is
faster than vanilla AT by up to 3x while achieving competitive defense effect.

