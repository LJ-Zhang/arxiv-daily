### Title: Adaptive Personlization in Federated Learning for Highly Non-i.i.d. Data
* Paper ID: 2207.03448v1
* Paper URL: [http://arxiv.org/abs/2207.03448v1](http://arxiv.org/abs/2207.03448v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG']
* Code URL: null
* Summary: Federated learning (FL) is a distributed learning method that offers medical
institutes the prospect of collaboration in a global model while preserving the
privacy of their patients. Although most medical centers conduct similar
medical imaging tasks, their differences, such as specializations, number of
patients, and devices, lead to distinctive data distributions. Data
heterogeneity poses a challenge for FL and the personalization of the local
models. In this work, we investigate an adaptive hierarchical clustering method
for FL to produce intermediate semi-global models, so clients with similar data
distribution have the chance of forming a more specialized model. Our method
forms several clusters consisting of clients with the most similar data
distributions; then, each cluster continues to train separately. Inside the
cluster, we use meta-learning to improve the personalization of the
participants' models. We compare the clustering approach with classical FedAvg
and centralized training by evaluating our proposed methods on the HAM10k
dataset for skin lesion classification with extreme heterogeneous data
distribution. Our experiments demonstrate significant performance gain in
heterogeneous distribution compared to standard FL methods in classification
accuracy. Moreover, we show that the models converge faster if applied in
clusters and outperform centralized training while using only a small subset of
data.

### Title: Differentially Private Stochastic Linear Bandits: (Almost) for Free
* Paper ID: 2207.03445v1
* Paper URL: [http://arxiv.org/abs/2207.03445v1](http://arxiv.org/abs/2207.03445v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.CR']
* Code URL: null
* Summary: In this paper, we propose differentially private algorithms for the problem
of stochastic linear bandits in the central, local and shuffled models. In the
central model, we achieve almost the same regret as the optimal non-private
algorithms, which means we get privacy for free. In particular, we achieve a
regret of $\tilde{O}(\sqrt{T}+\frac{1}{\epsilon})$ matching the known lower
bound for private linear bandits, while the best previously known algorithm
achieves $\tilde{O}(\frac{1}{\epsilon}\sqrt{T})$. In the local case, we achieve
a regret of $\tilde{O}(\frac{1}{\epsilon}{\sqrt{T}})$ which matches the
non-private regret for constant $\epsilon$, but suffers a regret penalty when
$\epsilon$ is small. In the shuffled model, we also achieve regret of
$\tilde{O}(\sqrt{T}+\frac{1}{\epsilon})$ %for small $\epsilon$ as in the
central case, while the best previously known algorithm suffers a regret of
$\tilde{O}(\frac{1}{\epsilon}{T^{3/5}})$. Our numerical evaluation validates
our theoretical results.

### Title: HE-PEx: Efficient Machine Learning under Homomorphic Encryption using Pruning, Permutation and Expansion
* Paper ID: 2207.03384v1
* Paper URL: [http://arxiv.org/abs/2207.03384v1](http://arxiv.org/abs/2207.03384v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CR', 'cs.LG']
* Code URL: null
* Summary: Privacy-preserving neural network (NN) inference solutions have recently
gained significant traction with several solutions that provide different
latency-bandwidth trade-offs. Of these, many rely on homomorphic encryption
(HE), a method of performing computations over encrypted data. However, HE
operations even with state-of-the-art schemes are still considerably slow
compared to their plaintext counterparts. Pruning the parameters of a NN model
is a well-known approach to improving inference latency. However, pruning
methods that are useful in the plaintext context may lend nearly negligible
improvement in the HE case, as has also been demonstrated in recent work.
  In this work, we propose a novel set of pruning methods that reduce the
latency and memory requirement, thus bringing the effectiveness of plaintext
pruning methods to HE. Crucially, our proposal employs two key techniques, viz.
permutation and expansion of the packed model weights, that enable pruning
significantly more ciphertexts and recuperating most of the accuracy loss,
respectively. We demonstrate the advantage of our method on fully connected
layers where the weights are packed using a recently proposed packing technique
called tile tensors, which allows executing deep NN inference in a
non-interactive mode. We evaluate our methods on various autoencoder
architectures and demonstrate that for a small mean-square reconstruction loss
of 1.5*10^{-5} on MNIST, we reduce the memory requirement and latency of
HE-enabled inference by 60%.

### Title: Privacy-Preserving Synthetic Educational Data Generation
* Paper ID: 2207.03202v1
* Paper URL: [http://arxiv.org/abs/2207.03202v1](http://arxiv.org/abs/2207.03202v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CY', 'cs.AI', 'cs.CR', 'cs.LG']
* Code URL: [https://github.com/akulen/privgen](https://github.com/akulen/privgen)
* Summary: Institutions collect massive learning traces but they may not disclose it for
privacy issues. Synthetic data generation opens new opportunities for research
in education. In this paper we present a generative model for educational data
that can preserve the privacy of participants, and an evaluation framework for
comparing synthetic data generators. We show how naive pseudonymization can
lead to re-identification threats and suggest techniques to guarantee privacy.
We evaluate our method on existing massive educational open datasets.

### Title: Towards the Practical Utility of Federated Learning in the Medical Domain
* Paper ID: 2207.03075v1
* Paper URL: [http://arxiv.org/abs/2207.03075v1](http://arxiv.org/abs/2207.03075v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.AI']
* Code URL: [https://github.com/wns823/medical_federated](https://github.com/wns823/medical_federated)
* Summary: Federated learning (FL) is an active area of research. One of the most
suitable areas for adopting FL is the medical domain, where patient privacy
must be respected. Previous research, however, does not fully consider who will
most likely use FL in the medical domain. It is not the hospitals who are eager
to adopt FL, but the service providers such as IT companies who want to develop
machine learning models with real patient records. Moreover, service providers
would prefer to focus on maximizing the performance of the models at the lowest
cost possible. In this work, we propose empirical benchmarks of FL methods
considering both performance and monetary cost with three real-world datasets:
electronic health records, skin cancer images, and electrocardiogram datasets.
We also propose Federated learning with Proximal regularization eXcept local
Normalization (FedPxN), which, using a simple combination of FedProx and FedBN,
outperforms all other FL algorithms while consuming only slightly more power
than the most power efficient method.

