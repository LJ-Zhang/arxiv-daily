### Title: Continual Learning for Peer-to-Peer Federated Learning: A Study on Automated Brain Metastasis Identification
* Paper ID: 2204.13591v1
* Paper URL: [http://arxiv.org/abs/2204.13591v1](http://arxiv.org/abs/2204.13591v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Due to data privacy constraints, data sharing among multiple centers is
restricted. Continual learning, as one approach to peer-to-peer federated
learning, can promote multicenter collaboration on deep learning algorithm
development by sharing intermediate models instead of training data. This work
aims to investigate the feasibility of continual learning for multicenter
collaboration on an exemplary application of brain metastasis identification
using DeepMedic. 920 T1 MRI contrast enhanced volumes are split to simulate
multicenter collaboration scenarios. A continual learning algorithm, synaptic
intelligence (SI), is applied to preserve important model weights for training
one center after another. In a bilateral collaboration scenario, continual
learning with SI achieves a sensitivity of 0.917, and naive continual learning
without SI achieves a sensitivity of 0.906, while two models trained on
internal data solely without continual learning achieve sensitivity of 0.853
and 0.831 only. In a seven-center multilateral collaboration scenario, the
models trained on internal datasets (100 volumes each center) without continual
learning obtain a mean sensitivity value of 0.725. With single-visit continual
learning (i.e., the shared model visits each center only once during training),
the sensitivity is improved to 0.788 and 0.849 without SI and with SI,
respectively. With iterative continual learning (i.e., the shared model
revisits each center multiple times during training), the sensitivity is
further improved to 0.914, which is identical to the sensitivity using mixed
data for training. Our experiments demonstrate that continual learning can
improve brain metastasis identification performance for centers with limited
data. This study demonstrates the feasibility of applying continual learning
for peer-to-peer federated learning in multicenter collaboration.

### Title: MemFHE: End-to-End Computing with Fully Homomorphic Encryption in Memory
* Paper ID: 2204.12557v1
* Paper URL: [http://arxiv.org/abs/2204.12557v1](http://arxiv.org/abs/2204.12557v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The increasing amount of data and the growing complexity of problems has
resulted in an ever-growing reliance on cloud computing. However, many
applications, most notably in healthcare, finance or defense, demand security
and privacy which today's solutions cannot fully address. Fully homomorphic
encryption (FHE) elevates the bar of today's solutions by adding
confidentiality of data during processing. It allows computation on fully
encrypted data without the need for decryption, thus fully preserving privacy.
To enable processing encrypted data at usable levels of classic security, e.g.,
128-bit, the encryption procedure introduces noticeable data size expansion -
the ciphertext is much bigger than the native aggregate of native data types.
In this paper, we present MemFHE which is the first accelerator of both client
and server for the latest Ring-GSW (Gentry, Sahai, and Waters) based
homomorphic encryption schemes using Processing In Memory (PIM). PIM alleviates
the data movement issues with large FHE encrypted data, while providing in-situ
execution and extensive parallelism needed for FHE's polynomial operations.
While the client-PIM can homomorphically encrypt and decrypt data, the
server-PIM can process homomorphically encrypted data without decryption.
MemFHE's server-PIM is pipelined and is designed to provide flexible
bootstrapping, allowing two encryption techniques and various FHE
security-levels based on the application requirements. We evaluate MemFHE for
various security-levels and compare it with state-of-the-art CPU
implementations for Ring-GSW based FHE. MemFHE is up to 20kx (265x) faster than
CPU (GPU) for FHE arithmetic operations and provides on average 2007x higher
throughput than the state-of-the-art while implementing neural networks with
FHE.

### Title: Distances Release with Differential Privacy in Tree and Grid Graph
* Paper ID: 2204.12488v1
* Paper URL: [http://arxiv.org/abs/2204.12488v1](http://arxiv.org/abs/2204.12488v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Data about individuals may contain private and sensitive information. The
differential privacy (DP) was proposed to address the problem of protecting the
privacy of each individual while keeping useful information about a population.
Sealfon (2016) introduced a private graph model in which the graph topology is
assumed to be public while the weight information is assumed to be private.
That model can express hidden congestion patterns in a known transportation
system. In this paper, we revisit the problem of privately releasing
approximate distances between all pairs of vertices in (Sealfon 2016). Our goal
is to minimize the additive error, namely the difference between the released
distance and actual distance under private setting. We propose improved
solutions to that problem for several cases.
  For the problem of privately releasing all-pairs distances, we show that for
tree with depth $h$, we can release all-pairs distances with additive error
$O(\log^{1.5} h \cdot \log^{1.5} V)$ for fixed privacy parameter where $V$ the
number of vertices in the tree, which improves the previous error bound
$O(\log^{2.5} V)$, since the size of $h$ can be as small as $O(\log V)$. Our
result implies that a $\log V$ factor is saved, and the additive error in tree
can be smaller than the error on array/path. Additionally, for the grid graph
with arbitrary edge weights, we also propose a method to release all-pairs
distances with additive error $\tilde O(V^{3/4}) $ for fixed privacy
parameters. On the application side, many cities like Manhattan are composed of
horizontal streets and vertical avenues, which can be modeled as a grid graph.

### Title: A review of Federated Learning in Intrusion Detection Systems for IoT
* Paper ID: 2204.12443v1
* Paper URL: [http://arxiv.org/abs/2204.12443v1](http://arxiv.org/abs/2204.12443v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Intrusion detection systems are evolving into intelligent systems that
perform data analysis searching for anomalies in their environment. The
development of deep learning technologies opened the door to build more complex
and effective threat detection models. However, training those models may be
computationally infeasible in most Internet of Things devices. Current
approaches rely on powerful centralized servers that receive data from all
their parties -- violating basic privacy constraints and substantially
affecting response times and operational costs due to the huge communication
overheads. To mitigate these issues, Federated Learning emerged as a promising
approach where different agents collaboratively train a shared model, neither
exposing training data to others nor requiring a compute-intensive centralized
infrastructure. This paper focuses on the application of Federated Learning
approaches in the field of Intrusion Detection. Both technologies are described
in detail and current scientific progress is reviewed and categorized. Finally,
the paper highlights the limitations present in recent works and presents some
future directions for this technology.

### Title: Time-triggered Federated Learning over Wireless Networks
* Paper ID: 2204.12426v1
* Paper URL: [http://arxiv.org/abs/2204.12426v1](http://arxiv.org/abs/2204.12426v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The newly emerging federated learning (FL) framework offers a new way to
train machine learning models in a privacy-preserving manner. However,
traditional FL algorithms are based on an event-triggered aggregation, which
suffers from stragglers and communication overhead issues. To address these
issues, in this paper, we present a time-triggered FL algorithm (TT-Fed) over
wireless networks, which is a generalized form of classic synchronous and
asynchronous FL. Taking the constrained resource and unreliable nature of
wireless communication into account, we jointly study the user selection and
bandwidth optimization problem to minimize the FL training loss. To solve this
joint optimization problem, we provide a thorough convergence analysis for
TT-Fed. Based on the obtained analytical convergence upper bound, the
optimization problem is decomposed into tractable sub-problems with respect to
each global aggregation round, and finally solved by our proposed online search
algorithm. Simulation results show that compared to asynchronous FL (FedAsync)
and FL with asynchronous user tiers (FedAT) benchmarks, our proposed TT-Fed
algorithm improves the converged test accuracy by up to 12.5% and 5%,
respectively, under highly imbalanced and non-IID data, while substantially
reducing the communication overhead.

### Title: Federated Stochastic Primal-dual Learning with Differential Privacy
* Paper ID: 2204.12284v1
* Paper URL: [http://arxiv.org/abs/2204.12284v1](http://arxiv.org/abs/2204.12284v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated learning (FL) is a new paradigm that enables many clients to
jointly train a machine learning (ML) model under the orchestration of a
parameter server while keeping the local data not being exposed to any third
party. However, the training of FL is an interactive process between local
clients and the parameter server. Such process would cause privacy leakage
since adversaries may retrieve sensitive information by analyzing the overheard
messages. In this paper, we propose a new federated stochastic primal-dual
algorithm with differential privacy (FedSPD-DP). Compared to the existing
methods, the proposed FedSPD-DP incorporates local stochastic gradient descent
(local SGD) and partial client participation (PCP) for addressing the issues of
communication efficiency and straggler effects due to randomly accessed
clients. Our analysis shows that the data sampling strategy and PCP can enhance
the data privacy whereas the larger number of local SGD steps could increase
privacy leakage, revealing a non-trivial tradeoff between algorithm
communication efficiency and privacy protection. Specifically, we show that, by
guaranteeing $(\epsilon, \delta)$-DP for each client per communication round,
the proposed algorithm guarantees $(\mathcal{O}(q\epsilon \sqrt{p T}),
\delta)$-DP after $T$ communication rounds while maintaining an
$\mathcal{O}(1/\sqrt{pTQ})$ convergence rate for a convex and non-smooth
learning problem, where $Q$ is the number of local SGD steps, $p$ is the client
sampling probability, $q=\max_{i} q_i/\sqrt{1-q_i}$ and $q_i$ is the data
sampling probability of each client under PCP. Experiment results are presented
to evaluate the practical performance of the proposed algorithm and comparison
with state-of-the-art methods.

### Title: Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies
* Paper ID: 2204.12495v1
* Paper URL: [http://arxiv.org/abs/2204.12495v1](http://arxiv.org/abs/2204.12495v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated learning reduces the risk of information leakage, but remains
vulnerable to attacks. We investigate how several neural network design
decisions can defend against gradients inversion attacks. We show that
overlapping gradients provides numerical resistance to gradient inversion on
the highly vulnerable dense layer. Specifically, we propose to leverage
batching to maximise mixing of gradients by choosing an appropriate loss
function and drawing identical labels. We show that otherwise it is possible to
directly recover all vectors in a mini-batch without any numerical optimisation
due to the de-mixing nature of the cross entropy loss. To accurately assess
data recovery, we introduce an absolute variation distance (AVD) metric for
information leakage in images, derived from total variation. In contrast to
standard metrics, e.g. Mean Squared Error or Structural Similarity Index, AVD
offers a continuous metric for extracting information in noisy images. Finally,
our empirical results on information recovery from various inversion attacks
and training performance supports our defense strategies. These strategies are
also shown to be useful for deep convolutional neural networks such as LeNET
for image recognition. We hope that this study will help guide the development
of further strategies that achieve a trustful federation policy.

### Title: Cybertwin-enabled 6G Space-air-ground Integrated Networks: Architecture, Open Issue, and Challenges
* Paper ID: 2204.12153v1
* Paper URL: [http://arxiv.org/abs/2204.12153v1](http://arxiv.org/abs/2204.12153v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Space-air-ground integrated network (SAGIN) is considered as a core
requirement in emerging 6G networks, which integrates the terrestrial and
non-terrestrial networks to reach the full network coverage and ubiquitous
services. To envision the ubiquitous intelligence and the deep integration in
6G SAGIN, a paradigm of cybertwin-enabled 6G SAGIN is presented in this paper.
Specifically, a cybertwin-enabled SAGIN architecture is first presented, where
a novel five-dimension digital twin (DT) model is presented. Particularly,
three categories of critical technologies are presented based on the cybertwin
of SAGIN, i.e., cybertwin-based multi-source heterogeneous network integration,
cybertwin-based integrated cloud-edge-end, and cybertwin-based integrated
sensing-communication-computing. Besides, two open issues in the
cybertwin-enabled SAGIN are studied, i.e., the networking decision and
optimization and the cybertwin-enabled cross-layer privacy and security, where
the challenges are discussed and the potential solutions are directed. In
addition, a case study with federal learning is developed and open research
issues are discussed.

### Title: PP-MARL: Efficient Privacy-Preserving MARL for Cooperative Intelligence in Communication
* Paper ID: 2204.12064v1
* Paper URL: [http://arxiv.org/abs/2204.12064v1](http://arxiv.org/abs/2204.12064v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Artificial intelligence (AI) has been introduced in communication networks
and services to improve efficiency via self-optimization. Cooperative
intelligence (CI), also known as collective intelligence and collaborative
intelligence, is expected to become an integral element in next-generation
networks because it can aggregate the capabilities and intelligence of multiple
devices. However, privacy issues may intimidate, obstruct, and hinder the
deployment of CI in practice because collaboration heavily relies on data and
information sharing. Additional practical constraints in communication (e.g.,
limited bandwidth) further limit the performance of CI. To overcome these
challenges, we propose PP-MARL, an efficient privacy-preserving learning scheme
based on multi-agent reinforcement learning (MARL). We apply and evaluate our
scheme in two communication-related use cases: mobility management in
drone-assisted communication and network control with edge intelligence.
Simulation results reveal that the proposed scheme can achieve efficient and
reliable collaboration with 1.1-6 times better privacy protection and lower
overheads (e.g., 84-91% reduction in bandwidth) than state-of-the-art
approaches.

### Title: Privacy-Utility Trade-Off
* Paper ID: 2204.12057v1
* Paper URL: [http://arxiv.org/abs/2204.12057v1](http://arxiv.org/abs/2204.12057v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: In this paper, we investigate the privacy-utility trade-off (PUT) problem,
which considers the minimal privacy loss at a fixed expense of utility. Several
different kinds of privacy in the PUT problem are studied, including
differential privacy, approximate differential privacy, maximal information,
maximal leakage, Renyi differential privacy, Sibson mutual information and
mutual information. The average Hamming distance is used to measure the
distortion caused by the privacy mechanism. We consider two scenarios: global
privacy and local privacy. In the framework of global privacy framework, the
privacy-distortion function is upper-bounded by the privacy loss of a special
mechanism, and lower-bounded by the optimal privacy loss with any possible
prior input distribution. In the framework of local privacy, we generalize a
coloring method for the PUT problem.

### Title: Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks
* Paper ID: 2204.12050v1
* Paper URL: [http://arxiv.org/abs/2204.12050v1](http://arxiv.org/abs/2204.12050v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Malicious intelligent algorithms greatly threaten the security of social
users' privacy by detecting and analyzing the uploaded photos to social network
platforms. The destruction to DNNs brought by the adversarial attack sparks the
potential that adversarial examples serve as a new protection mechanism for
privacy security in social networks. However, the existing adversarial example
does not have recoverability for serving as an effective protection mechanism.
To address this issue, we propose a recoverable generative adversarial network
to generate self-recoverable adversarial examples. By modeling the adversarial
attack and recovery as a united task, our method can minimize the error of the
recovered examples while maximizing the attack ability, resulting in better
recoverability of adversarial examples. To further boost the recoverability of
these examples, we exploit a dimension reducer to optimize the distribution of
adversarial perturbation. The experimental results prove that the adversarial
examples generated by the proposed method present superior recoverability,
attack ability, and robustness on different datasets and network architectures,
which ensure its effectiveness as a protection mechanism in social networks.

### Title: One-shot Federated Learning without Server-side Training
* Paper ID: 2204.12493v1
* Paper URL: [http://arxiv.org/abs/2204.12493v1](http://arxiv.org/abs/2204.12493v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated Learning (FL) has recently made significant progress as a new
machine learning paradigm for privacy protection. Due to the high communication
cost of traditional FL, one-shot federated learning is gaining popularity as a
way to reduce communication cost between clients and the server. Most of the
existing one-shot FL methods are based on Knowledge Distillation; however,
distillation based approach requires an extra training phase and depends on
publicly available data sets. In this work, we consider a novel and challenging
setting: performing a single round of parameter aggregation on the local models
without server-side training on a public data set. In this new setting, we
propose an effective algorithm for Model Aggregation via Exploring Common
Harmonized Optima (MA-Echo), which iteratively updates the parameters of all
local models to bring them close to a common low-loss area on the loss surface,
without harming performance on their own data sets at the same time. Compared
to the existing methods, MA-Echo can work well even in extremely non-identical
data distribution settings where the support categories of each local model
have no overlapped labels with those of the others. We conduct extensive
experiments on two popular image classification data sets to compare the
proposed method with existing methods and demonstrate the effectiveness of
MA-Echo, which clearly outperforms the state-of-the-arts.

