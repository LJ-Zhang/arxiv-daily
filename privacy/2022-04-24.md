### Title: Applying Digital Twins in Metaverse: User Interface, Security and Privacy Challenges
* Paper ID: 2204.11343v1
* Paper URL: [http://arxiv.org/abs/2204.11343v1](http://arxiv.org/abs/2204.11343v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Digital Twins (DTs) are a conventional and well-known concept, proposed in
70s, that are popular in a broad spectrum of sciences, industry innovations,
and consortium alliances. However, in the last few years, the growth of digital
assets and online communications has attracted attention to DTs as highly
accurate twins of physical objects. Metaverse, as a digital world, is a concept
proposed in 1992 and has also become a popular paradigm and hot topic in public
where DTs can play critical roles. This study first presents definitions,
applications, and general challenges of DT and Metaverse. It then offers a
three-layer architecture linking the physical world to the Metaverse through a
user interface. Further, it investigates the security and privacy challenges of
using DTs in Metaverse. Finally, a conclusion, including possible solutions for
mentioned challenges and future works, will be provided.

### Title: Hardware Acceleration for Third-Generation FHE and PSI Based on It
* Paper ID: 2204.11334v1
* Paper URL: [http://arxiv.org/abs/2204.11334v1](http://arxiv.org/abs/2204.11334v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: With the expansion of cloud services, serious concerns about the privacy of
users' data arise due to the exposure of the unencrypted data to the server
during computation. Various security primitives are under investigation to
preserve privacy while evaluating private data, including Fully Homomorphic
Encryption (FHE), Private Set Intersection (PSI), and others. However, the
prohibitive processing time of these primitives hinders their practical
applications. This work proposes and implements an architecture for
accelerating third-generation FHE with Amazon Web Services (AWS) cloud FPGAs,
marking the first hardware acceleration solution for third-generation FHE. We
also introduce a novel unbalanced PSI protocol based on third-generation FHE,
optimized for the proposed hardware architecture. Several
algorithm-architecture co-optimization techniques are introduced to allow the
communication and computation costs to be independent of the Sender's set size.
The measurement results show that the proposed accelerator achieves $>21\times$
performance improvement compared to a software implementation for various
crucial subroutines of third-generation FHE and the proposed PSI.

### Title: Source-Free Domain Adaptation via Distribution Estimation
* Paper ID: 2204.11257v1
* Paper URL: [http://arxiv.org/abs/2204.11257v1](http://arxiv.org/abs/2204.11257v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Domain Adaptation aims to transfer the knowledge learned from a labeled
source domain to an unlabeled target domain whose data distributions are
different. However, the training data in source domain required by most of the
existing methods is usually unavailable in real-world applications due to
privacy preserving policies. Recently, Source-Free Domain Adaptation (SFDA) has
drawn much attention, which tries to tackle domain adaptation problem without
using source data. In this work, we propose a novel framework called SFDA-DE to
address SFDA task via source Distribution Estimation. Firstly, we produce
robust pseudo-labels for target data with spherical k-means clustering, whose
initial class centers are the weight vectors (anchors) learned by the
classifier of pretrained model. Furthermore, we propose to estimate the
class-conditioned feature distribution of source domain by exploiting target
data and corresponding anchors. Finally, we sample surrogate features from the
estimated distribution, which are then utilized to align two domains by
minimizing a contrastive adaptation loss function. Extensive experiments show
that the proposed method achieves state-of-the-art performance on multiple DA
benchmarks, and even outperforms traditional DA methods which require plenty of
source data.

