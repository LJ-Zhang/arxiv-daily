### Title: A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability
* Paper ID: 2204.08570v1
* Paper URL: [http://arxiv.org/abs/2204.08570v1](http://arxiv.org/abs/2204.08570v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Graph Neural Networks (GNNs) have made rapid developments in the recent
years. Due to their great ability in modeling graph-structured data, GNNs are
vastly used in various applications, including high-stakes scenarios such as
financial analysis, traffic predictions, and drug discovery. Despite their
great potential in benefiting humans in the real world, recent study shows that
GNNs can leak private information, are vulnerable to adversarial attacks, can
inherit and magnify societal bias from training data and lack interpretability,
which have risk of causing unintentional harm to the users and society. For
example, existing works demonstrate that attackers can fool the GNNs to give
the outcome they desire with unnoticeable perturbation on training graph. GNNs
trained on social networks may embed the discrimination in their decision
process, strengthening the undesirable societal bias. Consequently, trustworthy
GNNs in various aspects are emerging to prevent the harm from GNN models and
increase the users' trust in GNNs. In this paper, we give a comprehensive
survey of GNNs in the computational aspects of privacy, robustness, fairness,
and explainability. For each aspect, we give the taxonomy of the related
methods and formulate the general frameworks for the multiple categories of
trustworthy GNNs. We also discuss the future research directions of each aspect
and connections between these aspects to help achieve trustworthiness.

### Title: Special Session: Towards an Agile Design Methodology for Efficient, Reliable, and Secure ML Systems
* Paper ID: 2204.09514v1
* Paper URL: [http://arxiv.org/abs/2204.09514v1](http://arxiv.org/abs/2204.09514v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The real-world use cases of Machine Learning (ML) have exploded over the past
few years. However, the current computing infrastructure is insufficient to
support all real-world applications and scenarios. Apart from high efficiency
requirements, modern ML systems are expected to be highly reliable against
hardware failures as well as secure against adversarial and IP stealing
attacks. Privacy concerns are also becoming a first-order issue. This article
summarizes the main challenges in agile development of efficient, reliable and
secure ML systems, and then presents an outline of an agile design methodology
to generate efficient, reliable and secure ML systems based on user-defined
constraints and objectives.

### Title: AB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy
* Paper ID: 2204.08474v1
* Paper URL: [http://arxiv.org/abs/2204.08474v1](http://arxiv.org/abs/2204.08474v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Evaluation of keyword spotting (KWS) systems that detect keywords in speech
is a challenging task under realistic privacy constraints. The KWS is designed
to only collect data when the keyword is present, limiting the availability of
hard samples that may contain false negatives, and preventing direct estimation
of model recall from production data. Alternatively, complementary data
collected from other sources may not be fully representative of the real
application. In this work, we propose an evaluation technique which we call
AB/BA analysis. Our framework evaluates a candidate KWS model B against a
baseline model A, using cross-dataset offline decoding for relative recall
estimation, without requiring negative examples. Moreover, we propose a
formulation with assumptions that allow estimation of relative false positive
rate between models with low variance even when the number of false positives
is small. Finally, we propose to leverage machine-generated soft labels, in a
technique we call Semi-Supervised AB/BA analysis, that improves the analysis
time, privacy, and cost. Experiments with both simulation and real data show
that AB/BA analysis is successful at measuring recall improvement in
conjunction with the trade-off in relative false positive rate.

### Title: PrivateRec: Differentially Private Training and Serving for Federated News Recommendation
* Paper ID: 2204.08146v1
* Paper URL: [http://arxiv.org/abs/2204.08146v1](http://arxiv.org/abs/2204.08146v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Privacy protection is an essential issue in personalized news recommendation,
and federated learning can potentially mitigate the privacy concern by training
personalized news recommendation models over decentralized user data.For a
theoretical privacy guarantee, differential privacy is necessary. However,
applying differential privacy to federated recommendation training and serving
conventionally suffers from the unsatisfactory trade-off between privacy and
utility due to the high-dimensional characteristics of model gradients and
hidden representations. In addition, there is no formal privacy guarantee for
both training and serving in federated recommendation. In this paper, we
propose a unified federated news recommendation method for effective and
privacy-preserving model training and online serving with differential privacy
guarantees. We first clarify the notion of differential privacy over users'
behavior data for both model training and online serving in the federated
recommendation scenario. Next, we propose a privacy-preserving online serving
mechanism under this definition with differentially private user interest
decomposition. More specifically, it decomposes the high-dimensional and
privacy-sensitive user embedding into a combination of public basic vectors and
adds noise to the combination coefficients. In this way, it can avoid the
dimension curse and improve the utility by reducing the required noise
intensity for differential privacy. Besides, we design a federated
recommendation model training method with differential privacy, which can avoid
the dimension-dependent noise for large models via label permutation and
differentially private attention modules. Experiments on real-world news
recommendation datasets validate the effectiveness of our method in achieving a
good trade-off between privacy protection and utility for federated news
recommendations.

### Title: A Practical Cross-Device Federated Learning Framework over 5G Networks
* Paper ID: 2204.08134v1
* Paper URL: [http://arxiv.org/abs/2204.08134v1](http://arxiv.org/abs/2204.08134v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The concept of federated learning (FL) was first proposed by Google in 2016.
Thereafter, FL has been widely studied for the feasibility of application in
various fields due to its potential to make full use of data without
compromising the privacy. However, limited by the capacity of wireless data
transmission, the employment of federated learning on mobile devices has been
making slow progress in practical. The development and commercialization of the
5th generation (5G) mobile networks has shed some light on this. In this paper,
we analyze the challenges of existing federated learning schemes for mobile
devices and propose a novel cross-device federated learning framework, which
utilizes the anonymous communication technology and ring signature to protect
the privacy of participants while reducing the computation overhead of mobile
devices participating in FL. In addition, our scheme implements a
contribution-based incentive mechanism to encourage mobile users to participate
in FL. We also give a case study of autonomous driving. Finally, we present the
performance evaluation of the proposed scheme and discuss some open issues in
federated learning.

