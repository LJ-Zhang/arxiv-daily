### Title: sqSGD: Locally Private and Communication Efficient Federated Learning
* Paper ID: 2206.10565v1
* Paper URL: [http://arxiv.org/abs/2206.10565v1](http://arxiv.org/abs/2206.10565v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated learning (FL) is a technique that trains machine learning models
from decentralized data sources. We study FL under local notions of privacy
constraints, which provides strong protection against sensitive data
disclosures via obfuscating the data before leaving the client. We identify two
major concerns in designing practical privacy-preserving FL algorithms:
communication efficiency and high-dimensional compatibility. We then develop a
gradient-based learning algorithm called \emph{sqSGD} (selective quantized
stochastic gradient descent) that addresses both concerns. The proposed
algorithm is based on a novel privacy-preserving quantization scheme that uses
a constant number of bits per dimension per client. Then we improve the base
algorithm in three ways: first, we apply a gradient subsampling strategy that
simultaneously offers better training performance and smaller communication
costs under a fixed privacy budget. Secondly, we utilize randomized rotation as
a preprocessing step to reduce quantization error. Thirdly, an adaptive
gradient norm upper bound shrinkage strategy is adopted to improve accuracy and
stabilize training. Finally, the practicality of the proposed framework is
demonstrated on benchmark datasets. Experiment results show that sqSGD
successfully learns large models like LeNet and ResNet with local privacy
constraints. In addition, with fixed privacy and communication level, the
performance of sqSGD significantly dominates that of various baseline
algorithms.

### Title: FedHiSyn: A Hierarchical Synchronous Federated Learning Framework for Resource and Data Heterogeneity
* Paper ID: 2206.10546v1
* Paper URL: [http://arxiv.org/abs/2206.10546v1](http://arxiv.org/abs/2206.10546v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated Learning (FL) enables training a global model without sharing the
decentralized raw data stored on multiple devices to protect data privacy. Due
to the diverse capacity of the devices, FL frameworks struggle to tackle the
problems of straggler effects and outdated models. In addition, the data
heterogeneity incurs severe accuracy degradation of the global model in the FL
training process. To address aforementioned issues, we propose a hierarchical
synchronous FL framework, i.e., FedHiSyn. FedHiSyn first clusters all available
devices into a small number of categories based on their computing capacity.
After a certain interval of local training, the models trained in different
categories are simultaneously uploaded to a central server. Within a single
category, the devices communicate the local updated model weights to each other
based on a ring topology. As the efficiency of training in the ring topology
prefers devices with homogeneous resources, the classification based on the
computing capacity mitigates the impact of straggler effects. Besides, the
combination of the synchronous update of multiple categories and the device
communication within a single category help address the data heterogeneity
issue while achieving high accuracy. We evaluate the proposed framework based
on MNIST, EMNIST, CIFAR10 and CIFAR100 datasets and diverse heterogeneous
settings of devices. Experimental results show that FedHiSyn outperforms six
baseline methods, e.g., FedAvg, SCAFFOLD, and FedAT, in terms of training
accuracy and efficiency.

### Title: The Impact of Visibility on the Right to Opt-out of Sale under CCPA
* Paper ID: 2206.10545v1
* Paper URL: [http://arxiv.org/abs/2206.10545v1](http://arxiv.org/abs/2206.10545v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: The California Consumer Protection Act (CCPA) gives users the right to
opt-out of sale of their personal information, but prior work has found that
opt-out mechanisms provided under this law result in very low opt-out rates.
Privacy signals offer a solution for users who are aware of their rights and
are willing to proactively take steps to enable privacy-enhancing tools, but
this work findsthat many users are not aware of their rights under CCPA and
that current opt-out rates are very low. We therefore explore an alternative
approach to enhancing privacy under CCPA: increasing the visibility of opt-out
of sale mechanisms. For this purpose, we design and implement CCPA Opt-out
Assistant (COA), a browser extension that automatically detects when websites
sell personal information and presents users with a visible, standardized
banner that links to the opt-out of sale mechanism for the website. We conduct
an online user study with 54 participants that finds that these banners
significantly increases the rate at which users opt-out of sale of their
personal information. Participants also report less difficulty opting-out and
more satisfaction with opt-out mechanisms compared to the native mechanisms
currently provided by websites. Our results suggest that effective privacy
regulation depends on imposing clear, enforceable visibility standards, and
that CCPA's requirements for opt-out of sale mechanisms fall short.

### Title: QuantFace: Towards Lightweight Face Recognition by Synthetic Data Low-bit Quantization
* Paper ID: 2206.10526v1
* Paper URL: [http://arxiv.org/abs/2206.10526v1](http://arxiv.org/abs/2206.10526v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Deep learning-based face recognition models follow the common trend in deep
neural networks by utilizing full-precision floating-point networks with high
computational costs. Deploying such networks in use-cases constrained by
computational requirements is often infeasible due to the large memory required
by the full-precision model. Previous compact face recognition approaches
proposed to design special compact architectures and train them from scratch
using real training data, which may not be available in a real-world scenario
due to privacy concerns. We present in this work the QuantFace solution based
on low-bit precision format model quantization. QuantFace reduces the required
computational cost of the existing face recognition models without the need for
designing a particular architecture or accessing real training data. QuantFace
introduces privacy-friendly synthetic face data to the quantization process to
mitigate potential privacy concerns and issues related to the accessibility to
real training data. Through extensive evaluation experiments on seven
benchmarks and four network architectures, we demonstrate that QuantFace can
successfully reduce the model size up to 5x while maintaining, to a large
degree, the verification performance of the full-precision model without
accessing real training datasets.

### Title: Three-way optimization of privacy and utility of location data
* Paper ID: 2206.10525v1
* Paper URL: [http://arxiv.org/abs/2206.10525v1](http://arxiv.org/abs/2206.10525v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: With the recent bloom of data and the drive towards an information-based
society, the urge of and the advancements in data analytics is surging like
never before. And with this, the risks of privacy violation of various kinds
are also increasing manifold. Most of the methods to mitigate the privacy risks
for location data resort to adding some noise to the location, like the planar
Laplace used to achieve geo-indistinguishability. However, the noise should be
calibrated carefully, taking into account the implications for utility, because
it is far from ideal for the service providers to completely lose the utility
of the collected data succumbing to the privacy requirements of the users.
Similarly, the quality of service for the users should be optimized with their
personalized needs of privacy protection used to shield their sensitive
information. In this paper, we address this age-old battle between privacy and
utility from three ends: privacy of the users' data, the quality of service
(QoS) received by them in exchange for sharing their privatized data, and the
statistical utility of the privatized data for the service providers who wish
to perform various kinds of analysis and research on the data collected from
the users. We propose a method to produce a geo-indistinguishable
location-privacy mechanism that advances to optimize simultaneously between the
level of privacy attained, the QoS, and the statistical utility achieved by the
obfuscated data. We illustrate the soundness of this three-way privacy-utility
optimization mechanism both analytically and with experiments. Apart from the
novelty of the proposed method, this work is aimed to engender an analytical
perspective to bridge between geo-indistinguishable location-privacy, QoS, and
statistical utilities used in standard data analytics, from an information
theoretical, probabilistic, and statistical perspective.

### Title: SFace: Privacy-friendly and Accurate Face Recognition using Synthetic Data
* Paper ID: 2206.10520v1
* Paper URL: [http://arxiv.org/abs/2206.10520v1](http://arxiv.org/abs/2206.10520v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Recent deep face recognition models proposed in the literature utilized
large-scale public datasets such as MS-Celeb-1M and VGGFace2 for training very
deep neural networks, achieving state-of-the-art performance on mainstream
benchmarks. Recently, many of these datasets, e.g., MS-Celeb-1M and VGGFace2,
are retracted due to credible privacy and ethical concerns. This motivates this
work to propose and investigate the feasibility of using a privacy-friendly
synthetically generated face dataset to train face recognition models. Towards
this end, we utilize a class-conditional generative adversarial network to
generate class-labeled synthetic face images, namely SFace. To address the
privacy aspect of using such data to train a face recognition model, we provide
extensive evaluation experiments on the identity relation between the synthetic
dataset and the original authentic dataset used to train the generative model.
Our reported evaluation proved that associating an identity of the authentic
dataset to one with the same class label in the synthetic dataset is hardly
possible. We also propose to train face recognition on our privacy-friendly
dataset, SFace, using three different learning strategies, multi-class
classification, label-free knowledge transfer, and combined learning of
multi-class classification and knowledge transfer. The reported evaluation
results on five authentic face benchmarks demonstrated that the
privacy-friendly synthetic dataset has high potential to be used for training
face recognition models, achieving, for example, a verification accuracy of
91.87\% on LFW using multi-class classification and 99.13\% using the combined
learning strategy.

### Title: Securing the Future Internet of Things with Post-Quantum Cryptography
* Paper ID: 2206.10473v1
* Paper URL: [http://arxiv.org/abs/2206.10473v1](http://arxiv.org/abs/2206.10473v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Traditional and lightweight cryptography primitives and protocols are
insecure against quantum attacks. Thus, a real-time application using
traditional or lightweight cryptography primitives and protocols does not
ensure full-proof security. Post-quantum Cryptography is important for the
Internet of Things (IoT) due to its security against Quantum attacks. This
paper offers a broad literature analysis of post-quantum cryptography for IoT
networks, including the challenges and research directions to adopt in
real-time applications. The work draws focus towards post-quantum cryptosystems
that are useful for resource-constraint devices. Further, those quantum attacks
are surveyed, which may occur over traditional and lightweight cryptographic
primitives.

### Title: The Privacy Onion Effect: Memorization is Relative
* Paper ID: 2206.10469v1
* Paper URL: [http://arxiv.org/abs/2206.10469v1](http://arxiv.org/abs/2206.10469v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Machine learning models trained on private datasets have been shown to leak
their private data. While recent work has found that the average data point is
rarely leaked, the outlier samples are frequently subject to memorization and,
consequently, privacy leakage. We demonstrate and analyse an Onion Effect of
memorization: removing the "layer" of outlier points that are most vulnerable
to a privacy attack exposes a new layer of previously-safe points to the same
attack. We perform several experiments to study this effect, and understand why
it occurs. The existence of this effect has various consequences. For example,
it suggests that proposals to defend against memorization without training with
rigorous privacy guarantees are unlikely to be effective. Further, it suggests
that privacy-enhancing technologies such as machine unlearning could actually
harm the privacy of other users.

### Title: An Overview of Privacy-enhancing Technologies in Biometric Recognition
* Paper ID: 2206.10465v1
* Paper URL: [http://arxiv.org/abs/2206.10465v1](http://arxiv.org/abs/2206.10465v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Privacy-enhancing technologies are technologies that implement fundamental
data protection principles. With respect to biometric recognition, different
types of privacy-enhancing technologies have been introduced for protecting
stored biometric data which are generally classified as sensitive. In this
regard, various taxonomies and conceptual categorizations have been proposed
and standardization activities have been carried out. However, these efforts
have mainly been devoted to certain sub-categories of privacy-enhancing
technologies and therefore lack generalization. This work provides an overview
of concepts of privacy-enhancing technologies for biometrics in a unified
framework. Key aspects and differences between existing concepts are
highlighted in detail at each processing step. Fundamental properties and
limitations of existing approaches are discussed and related to data protection
techniques and principles. Moreover, scenarios and methods for the assessment
of privacy-enhancing technologies for biometrics are presented. This paper is
meant as a point of entry to the field of biometric data protection and is
directed towards experienced researchers as well as non-experts.

### Title: Model Joins: Enabling Analytics Over Joins of Absent Big Tables
* Paper ID: 2206.10434v1
* Paper URL: [http://arxiv.org/abs/2206.10434v1](http://arxiv.org/abs/2206.10434v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: This work is motivated by two key facts. First, it is highly desirable to be
able to learn and perform knowledge discovery and analytics (LKD) tasks without
the need to access raw-data tables. This may be due to organizations finding it
increasingly frustrating and costly to manage and maintain ever-growing tables,
or for privacy reasons. Hence, compact models can be developed from the raw
data and used instead of the tables. Second, oftentimes, LKD tasks are to be
performed on a (potentially very large) table which is itself the result of
joining separate (potentially very large) relational tables. But how can one do
this, when the individual to-be-joined tables are absent? Here, we pose the
following fundamental questions: Q1: How can one "join models" of
(absent/deleted) tables or "join models with other tables" in a way that
enables LKD as if it were performed on the join of the actual raw tables? Q2:
What are appropriate models to use per table? Q3: As the model join would be an
approximation of the actual data join, how can one evaluate the quality of the
model join result? This work puts forth a framework, Model Join, addressing
these challenges. The framework integrates and joins the per-table models of
the absent tables and generates a uniform and independent sample that is a
high-quality approximation of a uniform and independent sample of the actual
raw-data join. The approximation stems from the models, but not from the Model
Join framework. The sample obtained by the Model Join can be used to perform
LKD downstream tasks, such as approximate query processing, classification,
clustering, regression, association rule mining, visualization, and so on. To
our knowledge, this is the first work with this agenda and solutions. Detailed
experiments with TPC-DS data and synthetic data showcase Model Join's
usefulness.

### Title: WrapperFL: A Model Agnostic Plug-in for Industrial Federated Learning
* Paper ID: 2206.10407v1
* Paper URL: [http://arxiv.org/abs/2206.10407v1](http://arxiv.org/abs/2206.10407v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated learning, as a privacy-preserving collaborative machine learning
paradigm, has been gaining more and more attention in the industry. With the
huge rise in demand, there have been many federated learning platforms that
allow federated participants to set up and build a federated model from
scratch. However, exiting platforms are highly intrusive, complicated, and hard
to integrate with built machine learning models. For many real-world businesses
that already have mature serving models, existing federated learning platforms
have high entry barriers and development costs. This paper presents a simple
yet practical federated learning plug-in inspired by ensemble learning, dubbed
WrapperFL, allowing participants to build/join a federated system with existing
models at minimal costs. The WrapperFL works in a plug-and-play way by simply
attaching to the input and output interfaces of an existing model, without the
need of re-development, significantly reducing the overhead of manpower and
resources. We verify our proposed method on diverse tasks under heterogeneous
data distributions and heterogeneous models. The experimental results
demonstrate that WrapperFL can be successfully applied to a wide range of
applications under practical settings and improves the local model with
federated learning at a low cost.

### Title: An Energy and Carbon Footprint Analysis of Distributed and Federated Learning
* Paper ID: 2206.10380v1
* Paper URL: [http://arxiv.org/abs/2206.10380v1](http://arxiv.org/abs/2206.10380v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Classical and centralized Artificial Intelligence (AI) methods require moving
data from producers (sensors, machines) to energy hungry data centers, raising
environmental concerns due to computational and communication resource demands,
while violating privacy. Emerging alternatives to mitigate such high energy
costs propose to efficiently distribute, or federate, the learning tasks across
devices, which are typically low-power. This paper proposes a novel framework
for the analysis of energy and carbon footprints in distributed and federated
learning (FL). The proposed framework quantifies both the energy footprints and
the carbon equivalent emissions for vanilla FL methods and consensus-based
fully decentralized approaches. We discuss optimal bounds and operational
points that support green FL designs and underpin their sustainability
assessment. Two case studies from emerging 5G industry verticals are analyzed:
these quantify the environmental footprints of continual and reinforcement
learning setups, where the training process is repeated periodically for
continuous improvements. For all cases, sustainability of distributed learning
relies on the fulfillment of specific requirements on communication efficiency
and learner population size. Energy and test accuracy should be also traded off
considering the model and the data footprints for the targeted industrial
applications.

### Title: Personalized Subgraph Federated Learning
* Paper ID: 2206.10206v1
* Paper URL: [http://arxiv.org/abs/2206.10206v1](http://arxiv.org/abs/2206.10206v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In real-world scenarios, subgraphs of a larger global graph may be
distributed across multiple devices or institutions, and only locally
accessible due to privacy restrictions, although there may be links between
them. Recently proposed subgraph Federated Learning (FL) methods deal with
those missing links across private local subgraphs while distributively
training Graph Neural Networks (GNNs) on them. However, they have overlooked
the inevitable heterogeneity among subgraphs, caused by subgraphs comprising
different parts of a global graph. For example, a subgraph may belong to one of
the communities within the larger global graph. A naive subgraph FL in such a
case will collapse incompatible knowledge from local GNN models trained on
heterogeneous graph distributions. To overcome such a limitation, we introduce
a new subgraph FL problem, personalized subgraph FL, which focuses on the joint
improvement of the interrelated local GNN models rather than learning a single
global GNN model, and propose a novel framework, FEDerated Personalized
sUBgraph learning (FED-PUB), to tackle it. A crucial challenge in personalized
subgraph FL is that the server does not know which subgraph each client has.
FED-PUB thus utilizes functional embeddings of the local GNNs using random
graphs as inputs to compute similarities between them, and use them to perform
weighted averaging for server-side aggregation. Further, it learns a
personalized sparse mask at each client to select and update only the
subgraph-relevant subset of the aggregated parameters. We validate FED-PUB for
its subgraph FL performance on six datasets, considering both non-overlapping
and overlapping subgraphs, on which ours largely outperforms relevant
baselines.

### Title: Federated Reinforcement Learning: Linear Speedup Under Markovian Sampling
* Paper ID: 2206.10185v1
* Paper URL: [http://arxiv.org/abs/2206.10185v1](http://arxiv.org/abs/2206.10185v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Since reinforcement learning algorithms are notoriously data-intensive, the
task of sampling observations from the environment is usually split across
multiple agents. However, transferring these observations from the agents to a
central location can be prohibitively expensive in terms of the communication
cost, and it can also compromise the privacy of each agent's local behavior
policy. In this paper, we consider a federated reinforcement learning framework
where multiple agents collaboratively learn a global model, without sharing
their individual data and policies. Each agent maintains a local copy of the
model and updates it using locally sampled data. Although having N agents
enables the sampling of N times more data, it is not clear if it leads to
proportional convergence speedup. We propose federated versions of on-policy
TD, off-policy TD and Q-learning, and analyze their convergence. For all these
algorithms, to the best of our knowledge, we are the first to consider
Markovian noise and multiple local updates, and prove a linear convergence
speedup with respect to the number of agents. To obtain these results, we show
that federated TD and Q-learning are special cases of a general framework for
federated stochastic approximation with Markovian noise, and we leverage this
framework to provide a unified convergence analysis that applies to all the
algorithms.

### Title: Open-Source Framework for Encrypted Internet and Malicious Traffic Classification
* Paper ID: 2206.10144v1
* Paper URL: [http://arxiv.org/abs/2206.10144v1](http://arxiv.org/abs/2206.10144v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Internet traffic classification plays a key role in network visibility,
Quality of Services (QoS), intrusion detection, Quality of Experience (QoE) and
traffic-trend analyses. In order to improve privacy, integrity,
confidentiality, and protocol obfuscation, the current traffic is based on
encryption protocols, e.g., SSL/TLS. With the increased use of Machine-Learning
(ML) and Deep-Learning (DL) models in the literature, comparison between
different models and methods has become cumbersome and difficult due to a lack
of a standardized framework. In this paper, we propose an open-source
framework, named OSF-EIMTC, which can provide the full pipeline of the learning
process. From the well-known datasets to extracting new and well-known
features, it provides implementations of well-known ML and DL models (from the
traffic classification literature) as well as evaluations. Such a framework can
facilitate research in traffic classification domains, so that it will be more
repeatable, reproducible, easier to execute, and will allow a more accurate
comparison of well-known and novel features and models. As part of our
framework evaluation, we demonstrate a variety of cases where the framework can
be of use, utilizing multiple datasets, models, and feature sets. We show
analyses of publicly available datasets and invite the community to participate
in our open challenges using the OSF-EIMTC.

