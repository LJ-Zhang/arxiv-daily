# privacy

## 04-20

### Title: The 2020 Census Disclosure Avoidance System TopDown Algorithm
* Paper ID: 2204.08986v1
* Paper URL: [http://arxiv.org/abs/2204.08986v1](http://arxiv.org/abs/2204.08986v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The Census TopDown Algorithm (TDA) is a disclosure avoidance system using
differential privacy for privacy-loss accounting. The algorithm ingests the
final, edited version of the 2020 Census data and the final tabulation
geographic definitions. The algorithm then creates noisy versions of key
queries on the data, referred to as measurements, using zero-Concentrated
Differential Privacy. Another key aspect of the TDA are invariants, statistics
that the Census Bureau has determined, as matter of policy, to exclude from the
privacy-loss accounting. The TDA post-processes the measurements together with
the invariants to produce a Microdata Detail File (MDF) that contains one
record for each person and one record for each housing unit enumerated in the
2020 Census. The MDF is passed to the 2020 Census tabulation system to produce
the 2020 Census Redistricting Data (P.L. 94-171) Summary File. This paper
describes the mathematics and testing of the TDA for this purpose.

### Title: Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies
* Paper ID: 2204.08952v1
* Paper URL: [http://arxiv.org/abs/2204.08952v1](http://arxiv.org/abs/2204.08952v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Prior studies in privacy policies frame the question answering (QA) tasks as
identifying the most relevant text segment or a list of sentences from the
policy document for a user query. However, annotating such a dataset is
challenging as it requires specific domain expertise (e.g., law academics).
Even if we manage a small-scale one, a bottleneck that remains is that the
labeled data are heavily imbalanced (only a few segments are relevant)
--limiting the gain in this domain. Therefore, in this paper, we develop a
novel data augmentation framework based on ensembling retriever models that
captures the relevant text segments from unlabeled policy documents and expand
the positive examples in the training set. In addition, to improve the
diversity and quality of the augmented data, we leverage multiple pre-trained
language models (LMs) and cascaded them with noise reduction oracles. Using our
augmented data on the PrivacyQA benchmark, we elevate the existing baseline by
a large margin (10\% F1) and achieve a new state-of-the-art F1 score of 50\%.
Our ablation studies provide further insights into the effectiveness of our
approach.

### Title: Invertible Mask Network for Face Privacy-Preserving
* Paper ID: 2204.08895v1
* Paper URL: [http://arxiv.org/abs/2204.08895v1](http://arxiv.org/abs/2204.08895v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Face privacy-preserving is one of the hotspots that arises dramatic interests
of research. However, the existing face privacy-preserving methods aim at
causing the missing of semantic information of face and cannot preserve the
reusability of original facial information. To achieve the naturalness of the
processed face and the recoverability of the original protected face, this
paper proposes face privacy-preserving method based on Invertible "Mask"
Network (IMN). In IMN, we introduce a Mask-net to generate "Mask" face firstly.
Then, put the "Mask" face onto the protected face and generate the masked face,
in which the masked face is indistinguishable from "Mask" face. Finally, "Mask"
face can be put off from the masked face and obtain the recovered face to the
authorized users, in which the recovered face is visually indistinguishable
from the protected face. The experimental results show that the proposed method
can not only effectively protect the privacy of the protected face, but also
almost perfectly recover the protected face from the masked face.

### Title: CoFHEE: A Co-processor for Fully Homomorphic Encryption Execution
* Paper ID: 2204.08742v1
* Paper URL: [http://arxiv.org/abs/2204.08742v1](http://arxiv.org/abs/2204.08742v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The migration of computation to the cloud has raised privacy concerns as
sensitive data becomes vulnerable to attacks since they need to be decrypted
for processing. Fully Homomorphic Encryption (FHE) mitigates this issue as it
enables meaningful computations to be performed directly on encrypted data.
Nevertheless, FHE is orders of magnitude slower than unencrypted computation,
which hinders its practicality and adoption. Therefore, improving FHE
performance is essential for its real world deployment. In this paper, we
present a year-long effort to design, implement, fabricate, and post-silicon
validate a hardware accelerator for Fully Homomorphic Encryption dubbed CoFHEE.
With a design area of $12mm^2$, CoFHEE aims to improve performance of
ciphertext multiplications, the most demanding arithmetic FHE operation, by
accelerating several primitive operations on polynomials, such as polynomial
additions and subtractions, Hadamard product, and Number Theoretic Transform.
CoFHEE supports polynomial degrees of up to $n = 2^{14}$ with a maximum
coefficient sizes of 128 bits, while it is capable of performing ciphertext
multiplications entirely on chip for $n \leq 2^{13}$. CoFHEE is fabricated in
55nm CMOS technology and achieves 250 MHz with our custom-built low-power
digital PLL design. In addition, our chip includes two communication interfaces
to the host machine: UART and SPI. This manuscript presents all steps and
design techniques in the ASIC development process, ranging from RTL design to
fabrication and validation. We evaluate our chip with performance and power
experiments and compare it against state-of-the-art software implementations
and other ASIC designs. Developed RTL files are available in an open-source
repository.

### Title: Toward Understanding the Use of Centralized Exchanges for Decentralized Cryptocurrency
* Paper ID: 2204.08664v1
* Paper URL: [http://arxiv.org/abs/2204.08664v1](http://arxiv.org/abs/2204.08664v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Cryptocurrency has been extensively studied as a decentralized financial
technology built on blockchain. However, there is a lack of understanding of
user experience with cryptocurrency exchanges, the main means for novice users
to interact with cryptocurrency. We conduct a qualitative study to provide a
panoramic view of user experience and security perception of exchanges. All 15
Chinese participants mainly use centralized exchanges (CEX) instead of
decentralized exchanges (DEX) to trade decentralized cryptocurrency, which is
paradoxical. A closer examination reveals that CEXes provide better usability
and charge lower transaction fee than DEXes. Country-specific security
perceptions are observed. Though DEXes provide better anonymity and privacy
protection, and are free of governmental regulation, these are not necessary
features for many participants. Based on the findings, we propose design
implications to make cryptocurrency trading more decentralized.

### Title: Poisons that are learned faster are more effective
* Paper ID: 2204.08615v1
* Paper URL: [http://arxiv.org/abs/2204.08615v1](http://arxiv.org/abs/2204.08615v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Imperceptible poisoning attacks on entire datasets have recently been touted
as methods for protecting data privacy. However, among a number of defenses
preventing the practical use of these techniques, early-stopping stands out as
a simple, yet effective defense. To gauge poisons' vulnerability to
early-stopping, we benchmark error-minimizing, error-maximizing, and synthetic
poisons in terms of peak test accuracy over 100 epochs and make a number of
surprising observations. First, we find that poisons that reach a low training
loss faster have lower peak test accuracy. Second, we find that a current
state-of-the-art error-maximizing poison is 7 times less effective when poison
training is stopped at epoch 8. Third, we find that stronger, more transferable
adversarial attacks do not make stronger poisons. We advocate for evaluating
poisons in terms of peak test accuracy.

### Title: A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability
* Paper ID: 2204.08570v1
* Paper URL: [http://arxiv.org/abs/2204.08570v1](http://arxiv.org/abs/2204.08570v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Graph Neural Networks (GNNs) have made rapid developments in the recent
years. Due to their great ability in modeling graph-structured data, GNNs are
vastly used in various applications, including high-stakes scenarios such as
financial analysis, traffic predictions, and drug discovery. Despite their
great potential in benefiting humans in the real world, recent study shows that
GNNs can leak private information, are vulnerable to adversarial attacks, can
inherit and magnify societal bias from training data and lack interpretability,
which have risk of causing unintentional harm to the users and society. For
example, existing works demonstrate that attackers can fool the GNNs to give
the outcome they desire with unnoticeable perturbation on training graph. GNNs
trained on social networks may embed the discrimination in their decision
process, strengthening the undesirable societal bias. Consequently, trustworthy
GNNs in various aspects are emerging to prevent the harm from GNN models and
increase the users' trust in GNNs. In this paper, we give a comprehensive
survey of GNNs in the computational aspects of privacy, robustness, fairness,
and explainability. For each aspect, we give the taxonomy of the related
methods and formulate the general frameworks for the multiple categories of
trustworthy GNNs. We also discuss the future research directions of each aspect
and connections between these aspects to help achieve trustworthiness.

### Title: AB/BA analysis: A framework for estimating keyword spotting recall improvement while maintaining audio privacy
* Paper ID: 2204.08474v1
* Paper URL: [http://arxiv.org/abs/2204.08474v1](http://arxiv.org/abs/2204.08474v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Evaluation of keyword spotting (KWS) systems that detect keywords in speech
is a challenging task under realistic privacy constraints. The KWS is designed
to only collect data when the keyword is present, limiting the availability of
hard samples that may contain false negatives, and preventing direct estimation
of model recall from production data. Alternatively, complementary data
collected from other sources may not be fully representative of the real
application. In this work, we propose an evaluation technique which we call
AB/BA analysis. Our framework evaluates a candidate KWS model B against a
baseline model A, using cross-dataset offline decoding for relative recall
estimation, without requiring negative examples. Moreover, we propose a
formulation with assumptions that allow estimation of relative false positive
rate between models with low variance even when the number of false positives
is small. Finally, we propose to leverage machine-generated soft labels, in a
technique we call Semi-Supervised AB/BA analysis, that improves the analysis
time, privacy, and cost. Experiments with both simulation and real data show
that AB/BA analysis is successful at measuring recall improvement in
conjunction with the trade-off in relative false positive rate.

### Title: PrivateRec: Differentially Private Training and Serving for Federated News Recommendation
* Paper ID: 2204.08146v1
* Paper URL: [http://arxiv.org/abs/2204.08146v1](http://arxiv.org/abs/2204.08146v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Privacy protection is an essential issue in personalized news recommendation,
and federated learning can potentially mitigate the privacy concern by training
personalized news recommendation models over decentralized user data.For a
theoretical privacy guarantee, differential privacy is necessary. However,
applying differential privacy to federated recommendation training and serving
conventionally suffers from the unsatisfactory trade-off between privacy and
utility due to the high-dimensional characteristics of model gradients and
hidden representations. In addition, there is no formal privacy guarantee for
both training and serving in federated recommendation. In this paper, we
propose a unified federated news recommendation method for effective and
privacy-preserving model training and online serving with differential privacy
guarantees. We first clarify the notion of differential privacy over users'
behavior data for both model training and online serving in the federated
recommendation scenario. Next, we propose a privacy-preserving online serving
mechanism under this definition with differentially private user interest
decomposition. More specifically, it decomposes the high-dimensional and
privacy-sensitive user embedding into a combination of public basic vectors and
adds noise to the combination coefficients. In this way, it can avoid the
dimension curse and improve the utility by reducing the required noise
intensity for differential privacy. Besides, we design a federated
recommendation model training method with differential privacy, which can avoid
the dimension-dependent noise for large models via label permutation and
differentially private attention modules. Experiments on real-world news
recommendation datasets validate the effectiveness of our method in achieving a
good trade-off between privacy protection and utility for federated news
recommendations.

### Title: A Practical Cross-Device Federated Learning Framework over 5G Networks
* Paper ID: 2204.08134v1
* Paper URL: [http://arxiv.org/abs/2204.08134v1](http://arxiv.org/abs/2204.08134v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The concept of federated learning (FL) was first proposed by Google in 2016.
Thereafter, FL has been widely studied for the feasibility of application in
various fields due to its potential to make full use of data without
compromising the privacy. However, limited by the capacity of wireless data
transmission, the employment of federated learning on mobile devices has been
making slow progress in practical. The development and commercialization of the
5th generation (5G) mobile networks has shed some light on this. In this paper,
we analyze the challenges of existing federated learning schemes for mobile
devices and propose a novel cross-device federated learning framework, which
utilizes the anonymous communication technology and ring signature to protect
the privacy of participants while reducing the computation overhead of mobile
devices participating in FL. In addition, our scheme implements a
contribution-based incentive mechanism to encourage mobile users to participate
in FL. We also give a case study of autonomous driving. Finally, we present the
performance evaluation of the proposed scheme and discuss some open issues in
federated learning.

