### Title: Edge-PRUNE: Flexible Distributed Deep Learning Inference
* Paper ID: 2204.12947v1
* Paper URL: [http://arxiv.org/abs/2204.12947v1](http://arxiv.org/abs/2204.12947v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Collaborative deep learning inference between low-resource endpoint devices
and edge servers has received significant research interest in the last few
years. Such computation partitioning can help reducing endpoint device energy
consumption and improve latency, but equally importantly also contributes to
privacy-preserving of sensitive data. This paper describes Edge-PRUNE, a
flexible but light-weight computation framework for distributing machine
learning inference between edge servers and one or more client devices.
Compared to previous approaches, Edge-PRUNE is based on a formal dataflow
computing model, and is agnostic towards machine learning training frameworks,
offering at the same time wide support for leveraging deep learning
accelerators such as embedded GPUs. The experimental section of the paper
demonstrates the use and performance of Edge-PRUNE by image classification and
object tracking applications on two heterogeneous endpoint devices and an edge
server, over wireless and physical connections. Endpoint device inference time
for SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by
collaborative inference.

### Title: Spending Privacy Budget Fairly and Wisely
* Paper ID: 2204.12903v1
* Paper URL: [http://arxiv.org/abs/2204.12903v1](http://arxiv.org/abs/2204.12903v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Differentially private (DP) synthetic data generation is a practical method
for improving access to data as a means to encourage productive partnerships.
One issue inherent to DP is that the "privacy budget" is generally "spent"
evenly across features in the data set. This leads to good statistical parity
with the real data, but can undervalue the conditional probabilities and
marginals that are critical for predictive quality of synthetic data. Further,
loss of predictive quality may be non-uniform across the data set, with subsets
that correspond to minority groups potentially suffering a higher loss.
  In this paper, we develop ensemble methods that distribute the privacy budget
"wisely" to maximize predictive accuracy of models trained on DP data, and
"fairly" to bound potential disparities in accuracy across groups and reduce
inequality. Our methods are based on the insights that feature importance can
inform how privacy budget is allocated, and, further, that per-group feature
importance and fairness-related performance objectives can be incorporated in
the allocation. These insights make our methods tunable to social contexts,
allowing data owners to produce balanced synthetic data for predictive
analysis.

