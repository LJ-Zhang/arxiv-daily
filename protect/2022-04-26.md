### Title: Absolute neutrino mass scale and dark matter stability from flavour symmetry
* Paper ID: 2204.12517v1
* Paper URL: [http://arxiv.org/abs/2204.12517v1](http://arxiv.org/abs/2204.12517v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: We explore a simple but extremely predictive extension of the scotogenic
model. We promote the scotogenic symmetry $\mathbb{Z}_2$ to the flavour
non-Abelian symmetry $\Sigma(81)$, which can also automatically protect dark
matter stability. In addition, $\Sigma(81)$ leads to striking predictions in
the lepton sector: only Inverted Ordering is realised, the absolute neutrino
mass scale is predicted to be $m_\text{lightest} \approx 7.5 \times 10^{-4}$ eV
and the Majorana phases are correlated in such a way that $|m_{ee}| \approx
0.018$ eV. The model also leads to a strong correlation between the solar
mixing angle $\theta_{12}$ and $\delta_{CP}$, which may be falsified by the
next generation of neutrino oscillation experiments. The setup is minimal in
the sense that no additional symmetries or flavons are required.

### Title: Distances Release with Differential Privacy in Tree and Grid Graph
* Paper ID: 2204.12488v1
* Paper URL: [http://arxiv.org/abs/2204.12488v1](http://arxiv.org/abs/2204.12488v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Data about individuals may contain private and sensitive information. The
differential privacy (DP) was proposed to address the problem of protecting the
privacy of each individual while keeping useful information about a population.
Sealfon (2016) introduced a private graph model in which the graph topology is
assumed to be public while the weight information is assumed to be private.
That model can express hidden congestion patterns in a known transportation
system. In this paper, we revisit the problem of privately releasing
approximate distances between all pairs of vertices in (Sealfon 2016). Our goal
is to minimize the additive error, namely the difference between the released
distance and actual distance under private setting. We propose improved
solutions to that problem for several cases.
  For the problem of privately releasing all-pairs distances, we show that for
tree with depth $h$, we can release all-pairs distances with additive error
$O(\log^{1.5} h \cdot \log^{1.5} V)$ for fixed privacy parameter where $V$ the
number of vertices in the tree, which improves the previous error bound
$O(\log^{2.5} V)$, since the size of $h$ can be as small as $O(\log V)$. Our
result implies that a $\log V$ factor is saved, and the additive error in tree
can be smaller than the error on array/path. Additionally, for the grid graph
with arbitrary edge weights, we also propose a method to release all-pairs
distances with additive error $\tilde O(V^{3/4}) $ for fixed privacy
parameters. On the application side, many cities like Manhattan are composed of
horizontal streets and vertical avenues, which can be modeled as a grid graph.

### Title: Poisoning Deep Learning based Recommender Model in Federated Learning Scenarios
* Paper ID: 2204.13594v1
* Paper URL: [http://arxiv.org/abs/2204.13594v1](http://arxiv.org/abs/2204.13594v1)
* Updated Date: 2022-04-26
* Code URL: [https://github.com/rdz98/poisonfeddlrs](https://github.com/rdz98/poisonfeddlrs)
* Summary: Various attack methods against recommender systems have been proposed in the
past years, and the security issues of recommender systems have drawn
considerable attention. Traditional attacks attempt to make target items
recommended to as many users as possible by poisoning the training data.
Benifiting from the feature of protecting users' private data, federated
recommendation can effectively defend such attacks. Therefore, quite a few
works have devoted themselves to developing federated recommender systems. For
proving current federated recommendation is still vulnerable, in this work we
probe to design attack approaches targeting deep learning based recommender
models in federated learning scenarios. Specifically, our attacks generate
poisoned gradients for manipulated malicious users to upload based on two
strategies (i.e., random approximation and hard user mining). Extensive
experiments show that our well-designed attacks can effectively poison the
target models, and the attack effectiveness sets the state-of-the-art.

### Title: Restricted Black-box Adversarial Attack Against DeepFake Face Swapping
* Paper ID: 2204.12347v1
* Paper URL: [http://arxiv.org/abs/2204.12347v1](http://arxiv.org/abs/2204.12347v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: DeepFake face swapping presents a significant threat to online security and
social media, which can replace the source face in an arbitrary photo/video
with the target face of an entirely different person. In order to prevent this
fraud, some researchers have begun to study the adversarial methods against
DeepFake or face manipulation. However, existing works focus on the white-box
setting or the black-box setting driven by abundant queries, which severely
limits the practical application of these methods. To tackle this problem, we
introduce a practical adversarial attack that does not require any queries to
the facial image forgery model. Our method is built on a substitute model
persuing for face reconstruction and then transfers adversarial examples from
the substitute model directly to inaccessible black-box DeepFake models.
Specially, we propose the Transferable Cycle Adversary Generative Adversarial
Network (TCA-GAN) to construct the adversarial perturbation for disrupting
unknown DeepFake systems. We also present a novel post-regularization module
for enhancing the transferability of generated adversarial examples. To
comprehensively measure the effectiveness of our approaches, we construct a
challenging benchmark of DeepFake adversarial attacks for future development.
Extensive experiments impressively show that the proposed adversarial attack
method makes the visual quality of DeepFake face images plummet so that they
are easier to be detected by humans and algorithms. Moreover, we demonstrate
that the proposed algorithm can be generalized to offer face image protection
against various face translation methods.

### Title: Federated Stochastic Primal-dual Learning with Differential Privacy
* Paper ID: 2204.12284v1
* Paper URL: [http://arxiv.org/abs/2204.12284v1](http://arxiv.org/abs/2204.12284v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated learning (FL) is a new paradigm that enables many clients to
jointly train a machine learning (ML) model under the orchestration of a
parameter server while keeping the local data not being exposed to any third
party. However, the training of FL is an interactive process between local
clients and the parameter server. Such process would cause privacy leakage
since adversaries may retrieve sensitive information by analyzing the overheard
messages. In this paper, we propose a new federated stochastic primal-dual
algorithm with differential privacy (FedSPD-DP). Compared to the existing
methods, the proposed FedSPD-DP incorporates local stochastic gradient descent
(local SGD) and partial client participation (PCP) for addressing the issues of
communication efficiency and straggler effects due to randomly accessed
clients. Our analysis shows that the data sampling strategy and PCP can enhance
the data privacy whereas the larger number of local SGD steps could increase
privacy leakage, revealing a non-trivial tradeoff between algorithm
communication efficiency and privacy protection. Specifically, we show that, by
guaranteeing $(\epsilon, \delta)$-DP for each client per communication round,
the proposed algorithm guarantees $(\mathcal{O}(q\epsilon \sqrt{p T}),
\delta)$-DP after $T$ communication rounds while maintaining an
$\mathcal{O}(1/\sqrt{pTQ})$ convergence rate for a convex and non-smooth
learning problem, where $Q$ is the number of local SGD steps, $p$ is the client
sampling probability, $q=\max_{i} q_i/\sqrt{1-q_i}$ and $q_i$ is the data
sampling probability of each client under PCP. Experiment results are presented
to evaluate the practical performance of the proposed algorithm and comparison
with state-of-the-art methods.

### Title: Accelerating Fully Homomorphic Encryption by Bridging Modular and Bit-Level Arithmetic
* Paper ID: 2204.12201v1
* Paper URL: [http://arxiv.org/abs/2204.12201v1](http://arxiv.org/abs/2204.12201v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The dramatic increase of data breaches in modern computing platforms has
emphasized that access control is not sufficient to protect sensitive user
data. Recent advances in cryptography allow end-to-end processing of encrypted
data without the need for decryption using Fully Homomorphic Encryption (FHE).
Such computation however, is still orders of magnitude slower than direct
(unencrypted) computation. Depending on the underlying cryptographic scheme,
FHE schemes can work natively either at bit-level using Boolean circuits, or
over integers using modular arithmetic. Operations on integers are limited to
addition/subtraction and multiplication. On the other hand, bit-level
arithmetic is much more comprehensive allowing more operations, such as
comparison and division. While modular arithmetic can emulate bit-level
computation, there is a significant cost in performance. In this work, we
propose a novel method, dubbed \emph{bridging}, that blends faster and
restricted modular computation with slower and comprehensive bit-level
computation, making them both usable within the same application and with the
same cryptographic scheme instantiation. We introduce and open source C++ types
representing the two distinct arithmetic modes, offering the possibility to
convert from one to the other. Experimental results show that bridging modular
and bit-level arithmetic computation can lead to 1-2 orders of magnitude
performance improvement for tested synthetic benchmarks, as well as two
real-world FHE applications: A URL denylisting case study, and a genotype
imputation application. Bridging performance enhancement comes from two
factors: 1) Reduced number of operations (especially ciphertext
multiplications), and 2) Arithmetic circuits with smaller multiplicative depth,
allowing more efficient encryption parameters with smaller polynomial degrees.

### Title: Lipids and lipid-mixtures in boundary layers: from hydration lubrication to osteoarthritis
* Paper ID: 2204.12116v1
* Paper URL: [http://arxiv.org/abs/2204.12116v1](http://arxiv.org/abs/2204.12116v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The hydration layer surrounding the phosphocholine headgroups of
single-component phosphatidylcholine (PC) lipids, or of lipid-mixtures,
assembled at an interface greatly modifies the interfacial properties and
interactions. As water molecules within the hydration layer are held tightly by
the headgroup but are nonetheless very fluid upon shear, the boundary lipid
layers, exposing the highly hydrated headgroup arrays, can provide efficient
boundary lubrication when sliding against an opposing surface, at
physiologically-high contact pressures. Additionally, any free lipids in the
surrounding liquid can heal defects which may form during sliding on the
boundary PC layer. Similar boundary lipid layers contribute to the lubricating,
pressure-bearing, and wear-protection functions of healthy articular joints.
This review presents a survey of the relationship between the molecular
composition of the interfacial complex and the lubrication behavior of the
lipid-based boundary layers, which could be beneficial for designing boundary
lubricants for intra-articular injection for the treatment of early OA.

### Title: PP-MARL: Efficient Privacy-Preserving MARL for Cooperative Intelligence in Communication
* Paper ID: 2204.12064v1
* Paper URL: [http://arxiv.org/abs/2204.12064v1](http://arxiv.org/abs/2204.12064v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Artificial intelligence (AI) has been introduced in communication networks
and services to improve efficiency via self-optimization. Cooperative
intelligence (CI), also known as collective intelligence and collaborative
intelligence, is expected to become an integral element in next-generation
networks because it can aggregate the capabilities and intelligence of multiple
devices. However, privacy issues may intimidate, obstruct, and hinder the
deployment of CI in practice because collaboration heavily relies on data and
information sharing. Additional practical constraints in communication (e.g.,
limited bandwidth) further limit the performance of CI. To overcome these
challenges, we propose PP-MARL, an efficient privacy-preserving learning scheme
based on multi-agent reinforcement learning (MARL). We apply and evaluate our
scheme in two communication-related use cases: mobility management in
drone-assisted communication and network control with edge intelligence.
Simulation results reveal that the proposed scheme can achieve efficient and
reliable collaboration with 1.1-6 times better privacy protection and lower
overheads (e.g., 84-91% reduction in bandwidth) than state-of-the-art
approaches.

### Title: Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks
* Paper ID: 2204.12050v1
* Paper URL: [http://arxiv.org/abs/2204.12050v1](http://arxiv.org/abs/2204.12050v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Malicious intelligent algorithms greatly threaten the security of social
users' privacy by detecting and analyzing the uploaded photos to social network
platforms. The destruction to DNNs brought by the adversarial attack sparks the
potential that adversarial examples serve as a new protection mechanism for
privacy security in social networks. However, the existing adversarial example
does not have recoverability for serving as an effective protection mechanism.
To address this issue, we propose a recoverable generative adversarial network
to generate self-recoverable adversarial examples. By modeling the adversarial
attack and recovery as a united task, our method can minimize the error of the
recovered examples while maximizing the attack ability, resulting in better
recoverability of adversarial examples. To further boost the recoverability of
these examples, we exploit a dimension reducer to optimize the distribution of
adversarial perturbation. The experimental results prove that the adversarial
examples generated by the proposed method present superior recoverability,
attack ability, and robustness on different datasets and network architectures,
which ensure its effectiveness as a protection mechanism in social networks.

### Title: On Routing, Wavelength, Network Coding Assignment and Protection Configuration Problem in Optical-processing-enabled Networks
* Paper ID: 2204.12027v1
* Paper URL: [http://arxiv.org/abs/2204.12027v1](http://arxiv.org/abs/2204.12027v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: In optical-processing-enabled network, transitional lightpaths crossing the
same node could be optically encoded to each other to achieve greater spectral
efficiency. In this context, we present a new research problem, entitled,
routing, wavelength, network coding assignment and protection configuration
(RWNCA-PC) arisen in exploiting photonic network coding (NC) for dedicated path
protection in wavelength division multiplexing (WDM) networks with an extra
degree of freedom in the selection of protection triggering mechanism, that is,
network-side and client-side, tailoring to each connection. In order to
maximize the NC benefits, we thus provide a weighted multi-objective
optimization model for solving RWNCA-PC problem so as to minimize the
wavelength count as the strictly prioritized goal and the redundant resources
measured by the number of client-side connections as the secondary objective.
Numerical results on the realistic COST239 network reveal that a saving of up
to $25\%$ wavelength resources could be achieved thanks to the optimal use of
NC compared to the non-coding designs and among coding-aware designs, the use
of mixed protection configurations would be spectrally more efficient than the
design with only network-side protection scheme. Our proposal yields the
highest spectrum efficiency compared to all reference designs and moreover,
features an average saving of more than $40\%$ transponder count compared with
its single objective counterpart.

### Title: One-shot Federated Learning without Server-side Training
* Paper ID: 2204.12493v1
* Paper URL: [http://arxiv.org/abs/2204.12493v1](http://arxiv.org/abs/2204.12493v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated Learning (FL) has recently made significant progress as a new
machine learning paradigm for privacy protection. Due to the high communication
cost of traditional FL, one-shot federated learning is gaining popularity as a
way to reduce communication cost between clients and the server. Most of the
existing one-shot FL methods are based on Knowledge Distillation; however,
distillation based approach requires an extra training phase and depends on
publicly available data sets. In this work, we consider a novel and challenging
setting: performing a single round of parameter aggregation on the local models
without server-side training on a public data set. In this new setting, we
propose an effective algorithm for Model Aggregation via Exploring Common
Harmonized Optima (MA-Echo), which iteratively updates the parameters of all
local models to bring them close to a common low-loss area on the loss surface,
without harming performance on their own data sets at the same time. Compared
to the existing methods, MA-Echo can work well even in extremely non-identical
data distribution settings where the support categories of each local model
have no overlapped labels with those of the others. We conduct extensive
experiments on two popular image classification data sets to compare the
proposed method with existing methods and demonstrate the effectiveness of
MA-Echo, which clearly outperforms the state-of-the-arts.

