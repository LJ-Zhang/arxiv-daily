### Title: SynWMD: Syntax-aware Word Mover's Distance for Sentence Similarity Evaluation
* Paper ID: 2206.10029v1
* Paper URL: [http://arxiv.org/abs/2206.10029v1](http://arxiv.org/abs/2206.10029v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Word Mover's Distance (WMD) computes the distance between words and models
text similarity with the moving cost between words in two text sequences. Yet,
it does not offer good performance in sentence similarity evaluation since it
does not incorporate word importance and fails to take inherent contextual and
structural information in a sentence into account. An improved WMD method using
the syntactic parse tree, called Syntax-aware Word Mover's Distance (SynWMD),
is proposed to address these two shortcomings in this work. First, a weighted
graph is built upon the word co-occurrence statistics extracted from the
syntactic parse trees of sentences. The importance of each word is inferred
from graph connectivities. Second, the local syntactic parsing structure of
words is considered in computing the distance between words. To demonstrate the
effectiveness of the proposed SynWMD, we conduct experiments on 6 textual
semantic similarity (STS) datasets and 4 sentence classification datasets.
Experimental results show that SynWMD achieves state-of-the-art performance on
STS tasks. It also outperforms other WMD-based methods on sentence
classification tasks.

### Title: Deep Partial Least Squares for Empirical Asset Pricing
* Paper ID: 2206.10014v1
* Paper URL: [http://arxiv.org/abs/2206.10014v1](http://arxiv.org/abs/2206.10014v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We use deep partial least squares (DPLS) to estimate an asset pricing model
for individual stock returns that exploits conditioning information in a
flexible and dynamic way while attributing excess returns to a small set of
statistical risk factors. The novel contribution is to resolve the non-linear
factor structure, thus advancing the current paradigm of deep learning in
empirical asset pricing which uses linear stochastic discount factors under an
assumption of Gaussian asset returns and factors. This non-linear factor
structure is extracted by using projected least squares to jointly project firm
characteristics and asset returns on to a subspace of latent factors and using
deep learning to learn the non-linear map from the factor loadings to the asset
returns. The result of capturing this non-linear risk factor structure is to
characterize anomalies in asset returns by both linear risk factor exposure and
interaction effects. Thus the well known ability of deep learning to capture
outliers, shed lights on the role of convexity and higher order terms in the
latent factor structure on the factor risk premia. On the empirical side, we
implement our DPLS factor models and exhibit superior performance to LASSO and
plain vanilla deep learning models. Furthermore, our network training times are
significantly reduced due to the more parsimonious architecture of DPLS.
Specifically, using 3290 assets in the Russell 1000 index over a period of
December 1989 to January 2018, we assess our DPLS factor model and generate
information ratios that are approximately 1.2x greater than deep learning. DPLS
explains variation and pricing errors and identifies the most prominent latent
factors and firm characteristics.

### Title: Event-Case Correlation for Process Mining using Probabilistic Optimization
* Paper ID: 2206.10009v1
* Paper URL: [http://arxiv.org/abs/2206.10009v1](http://arxiv.org/abs/2206.10009v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Process mining supports the analysis of the actual behavior and performance
of business processes using event logs. % such as, e.g., sales transactions
recorded by an ERP system. An essential requirement is that every event in the
log must be associated with a unique case identifier (e.g., the order ID of an
order-to-cash process). In reality, however, this case identifier may not
always be present, especially when logs are acquired from different systems or
extracted from non-process-aware information systems. In such settings, the
event log needs to be pre-processed by grouping events into cases -- an
operation known as event correlation. Existing techniques for correlating
events have worked with assumptions to make the problem tractable: some assume
the generative processes to be acyclic, while others require heuristic
information or user input. Moreover, %these techniques' primary assumption is
that they abstract the log to activities and timestamps, and miss the
opportunity to use data attributes. % In this paper, we lift these assumptions
and propose a new technique called EC-SA-Data based on probabilistic
optimization. The technique takes as inputs a sequence of timestamped events
(the log without case IDs), a process model describing the underlying business
process, and constraints over the event attributes. Our approach returns an
event log in which every event is associated with a case identifier. The
technique allows users to incorporate rules on process knowledge and data
constraints flexibly. The approach minimizes the misalignment between the
generated log and the input process model, maximizes the support of the given
data constraints over the correlated log, and the variance between activity
durations across cases. Our experiments with various real-life datasets show
the advantages of our approach over the state of the art.

### Title: Nanoscale mineralogy and organic structure in Orgueil (CI) and EET 92042 (CR) carbonaceous chondrites studied with AFM-IR spectroscopy
* Paper ID: 2206.09953v1
* Paper URL: [http://arxiv.org/abs/2206.09953v1](http://arxiv.org/abs/2206.09953v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Meteorite matrices from primitive chondrites are an interplay of ingredients
at the sub-micron scale, which requires analytical techniques with the
nanometer spatial resolution to decipher the composition of individual
components in their petrographic context. Infrared spectroscopy is an effective
method that enables to probe of vibrations at the molecule-atomic scale of
organic and inorganic compounds but is often limited to a few micrometers in
spatial resolution. To efficiently distinguish spectral signatures of the
different constituents, we apply here nano-IR spectroscopy (AFM-IR), based on
the combination of infrared and atomic force microscopy, having a spatial
resolution beyond the diffraction limits. Our study aims to characterize two
chosen meteorite samples to investigate primitive material in terms of bulk
chemistry (the CI chondrite Orgueil) and organic composition (the CR chondrite
EET 92042). We confirm that this technique allows unmixing the IR signatures of
organics and minerals to assess the variability of organic structure within
these samples. We report an investigation of the impact of the widely used
chemical HF/HCl (Hydrogen Fluoride/Hydrochloric) extraction on the nature of
refractory organics (Insoluble Organic Matter, IOM) and provide insights on the
mineralogy of meteorites matrices from these two samples by comparing to
reference (extra)terrestrial materials. These findings are discussed with a
perspective toward understanding the impact of post-accretional aqueous
alteration and thermal metamorphism on the composition of chondrites. Last, we
highlight that the heterogeneity of organic matter within meteoritic materials
extends down to the nanoscale, and by comparison with IOMs, oxygenated chemical
groups are not affected by acid extractions.

### Title: StrayCats II: An Updated Catalog of NuSTAR Stray Light Observations
* Paper ID: 2206.09930v1
* Paper URL: [http://arxiv.org/abs/2206.09930v1](http://arxiv.org/abs/2206.09930v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We present an updated catalog of StrayCats (a catalog of NuSTAR stray light
observations of X-ray sources) that includes nearly 18 additional months of
observations. StrayCats v2 has an added 53 sequence IDs, 106 rows, and 3 new
identified stray light (SL) sources in comparison to the original catalog. The
total catalog now has 489 unique sequence IDs, 862 entries, and 83 confirmed
StrayCats sources. Additionally, we provide new resources for the community to
gauge the utility and spectral state of the source in a given observation. We
have created long term light curves for each identified SL source using MAXI
and Swift/BAT data when available. Further, source extraction regions for 632
identified SL observations were created and are available to the public. In
this paper we present an overview of the updated catalog and new resources for
each identified StrayCats SL source.

### Title: Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world
* Paper ID: 2206.09889v1
* Paper URL: [http://arxiv.org/abs/2206.09889v1](http://arxiv.org/abs/2206.09889v1)
* Updated Date: 2022-06-20
* Code URL: [https://github.com/facebookresearch/nocturne](https://github.com/facebookresearch/nocturne)
* Summary: We introduce \textit{Nocturne}, a new 2D driving simulator for investigating
multi-agent coordination under partial observability. The focus of Nocturne is
to enable research into inference and theory of mind in real-world multi-agent
settings without the computational overhead of computer vision and feature
extraction from images. Agents in this simulator only observe an obstructed
view of the scene, mimicking human visual sensing constraints. Unlike existing
benchmarks that are bottlenecked by rendering human-like observations directly
using a camera input, Nocturne uses efficient intersection methods to compute a
vectorized set of visible features in a C++ back-end, allowing the simulator to
run at $2000+$ steps-per-second. Using open-source trajectory and map data, we
construct a simulator to load and replay arbitrary trajectories and scenes from
real-world driving data. Using this environment, we benchmark
reinforcement-learning and imitation-learning agents and demonstrate that the
agents are quite far from human-level coordination ability and deviate
significantly from the expert trajectories.

### Title: DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment
* Paper ID: 2206.09853v1
* Paper URL: [http://arxiv.org/abs/2206.09853v1](http://arxiv.org/abs/2206.09853v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The temporal relationships between frames and their influences on video
quality assessment (VQA) are still under-studied in existing works. These
relationships lead to two important types of effects for video quality.
Firstly, some temporal variations (such as shaking, flicker, and abrupt scene
transitions) are causing temporal distortions and lead to extra quality
degradations, while other variations (e.g. those related to meaningful
happenings) do not. Secondly, the human visual system often has different
attention to frames with different contents, resulting in their different
importance to the overall video quality. Based on prominent time-series
modeling ability of transformers, we propose a novel and effective
transformer-based VQA method to tackle these two issues. To better
differentiate temporal variations and thus capture the temporal distortions, we
design a transformer-based Spatial-Temporal Distortion Extraction (STDE)
module. To tackle with temporal quality attention, we propose the
encoder-decoder-like temporal content transformer (TCT). We also introduce the
temporal sampling on features to reduce the input length for the TCT, so as to
improve the learning effectiveness and efficiency of this module. Consisting of
the STDE and the TCT, the proposed Temporal Distortion-Content Transformers for
Video Quality Assessment (DisCoVQA) reaches state-of-the-art performance on
several VQA benchmarks without any extra pre-training datasets and up to 10%
better generalization ability than existing methods. We also conduct extensive
ablation experiments to prove the effectiveness of each part in our proposed
model, and provide visualizations to prove that the proposed modules achieve
our intention on modeling these temporal issues. We will publish our codes and
pretrained weights later.

### Title: Actively Learning Deep Neural Networks with Uncertainty Sampling Based on Sum-Product Networks
* Paper ID: 2206.09798v1
* Paper URL: [http://arxiv.org/abs/2206.09798v1](http://arxiv.org/abs/2206.09798v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Active learning is popular approach for reducing the amount of data in
training deep neural network model. Its success hinges on the choice of an
effective acquisition function, which ranks not yet labeled data points
according to their expected informativeness. In uncertainty sampling, the
uncertainty that the current model has about a point's class label is the main
criterion for this type of ranking. This paper proposes a new approach to
uncertainty sampling in training a Convolutional Neural Network (CNN). The main
idea is to use feature representation extracted extracted by the CNN as data
for training a Sum-Product Network (SPN). Since SPNs are typically used for
estimating the distribution of a dataset, they are well suited to the task of
estimating class probabilities that can be used directly by standard
acquisition functions such as max entropy and variational ratio. Moreover, we
enhance these acquisition functions by weights calculated with the help of the
SPN model; these weights make the acquisition function more sensitive to the
diversity of conceivable class labels for data points. The effectiveness of our
method is demonstrated in an experimental study on the MNIST, Fashion-MNIST and
CIFAR-10 datasets, where we compare it to the state-of-the-art methods MC
Dropout and Bayesian Batch.

### Title: Real-time Full-stack Traffic Scene Perception for Autonomous Driving with Roadside Cameras
* Paper ID: 2206.09770v1
* Paper URL: [http://arxiv.org/abs/2206.09770v1](http://arxiv.org/abs/2206.09770v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We propose a novel and pragmatic framework for traffic scene perception with
roadside cameras. The proposed framework covers a full-stack of roadside
perception pipeline for infrastructure-assisted autonomous driving, including
object detection, object localization, object tracking, and multi-camera
information fusion. Unlike previous vision-based perception frameworks rely
upon depth offset or 3D annotation at training, we adopt a modular decoupling
design and introduce a landmark-based 3D localization method, where the
detection and localization can be well decoupled so that the model can be
easily trained based on only 2D annotations. The proposed framework applies to
either optical or thermal cameras with pinhole or fish-eye lenses. Our
framework is deployed at a two-lane roundabout located at Ellsworth Rd. and
State St., Ann Arbor, MI, USA, providing 7x24 real-time traffic flow monitoring
and high-precision vehicle trajectory extraction. The whole system runs
efficiently on a low-power edge computing device with all-component end-to-end
delay of less than 20ms.

### Title: Quantitative CT texture-based method to predict diagnosis and prognosis of fibrosing interstitial lung disease patterns
* Paper ID: 2206.09766v1
* Paper URL: [http://arxiv.org/abs/2206.09766v1](http://arxiv.org/abs/2206.09766v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Purpose: To utilize high-resolution quantitative CT (QCT) imaging features
for prediction of diagnosis and prognosis in fibrosing interstitial lung
diseases (ILD). Approach: 40 ILD patients (20 usual interstitial pneumonia
(UIP), 20 non-UIP pattern ILD) were classified by expert consensus of 2
radiologists and followed for 7 years. Clinical variables were recorded.
Following segmentation of the lung field, a total of 26 texture features were
extracted using a lattice-based approach (TM model). The TM model was compared
with previously histogram-based model (HM) for their abilities to classify UIP
vs non-UIP. For prognostic assessment, survival analysis was performed
comparing the expert diagnostic labels versus TM metrics. Results: In the
classification analysis, the TM model outperformed the HM method with AUC of
0.70. While survival curves of UIP vs non-UIP expert labels in Cox regression
analysis were not statistically different, TM QCT features allowed
statistically significant partition of the cohort. Conclusions: TM model
outperformed HM model in distinguishing UIP from non-UIP patterns. Most
importantly, TM allows for partitioning of the cohort into distinct survival
groups, whereas expert UIP vs non-UIP labeling does not. QCT TM models may
improve diagnosis of ILD and offer more accurate prognostication, better
guiding patient management.

### Title: A Comparative Study on Application of Class-Imbalance Learning for Severity Prediction of Adverse Events Following Immunization
* Paper ID: 2206.09752v1
* Paper URL: [http://arxiv.org/abs/2206.09752v1](http://arxiv.org/abs/2206.09752v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In collaboration with the Liaoning CDC, China, we propose a prediction system
to predict the subsequent hospitalization of children with adverse reactions
based on data on adverse events following immunization. We extracted multiple
features from the data, and selected "hospitalization or not" as the target for
classification. Since the data are imbalanced, we used various class-imbalance
learning methods for training and improved the RUSBoost algorithm. Experimental
results show that the improved RUSBoost has the highest Area Under the ROC
Curve on the target among these algorithms. Additionally, we compared these
class-imbalance learning methods with some common machine learning algorithms.
We combined the improved RUSBoost with dynamic web resource development
techniques to build an evaluation system with information entry and vaccination
response prediction capabilities for relevant medical practitioners.

### Title: Three-Generation Super No-Scale Models in Heterotic Superstrings
* Paper ID: 2206.09732v1
* Paper URL: [http://arxiv.org/abs/2206.09732v1](http://arxiv.org/abs/2206.09732v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We derive the conditions for the one-loop contribution to the cosmological
constant to be exponentially suppressed in a class of heterotic string
compactifications with three generations of chiral matter, where supersymmetry
is spontaneously broken \`a la Scherk-Schwarz. Using techniques of partial
unfolding based on the Hecke congruence subgroup $\Gamma_0(2)$ of the modular
group to extract the leading asymptotics, we show that the super no-scale
condition $n_B=n_F$ between the degeneracies of massless bosons $n_B$ and
fermions $n_F$ in the full string spectrum is necessary but not sufficient, and
needs to be supplemented by additional conditions which we identify. We use
these results to construct three-generation Pati-Salam models with interesting
phenomenological characteristics.

### Title: Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers
* Paper ID: 2206.09731v1
* Paper URL: [http://arxiv.org/abs/2206.09731v1](http://arxiv.org/abs/2206.09731v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Semantic segmentation necessitates approaches that learn high-level
characteristics while dealing with enormous amounts of data. Convolutional
neural networks (CNNs) can learn unique and adaptive features to achieve this
aim. However, due to the large size and high spatial resolution of remote
sensing images, these networks cannot analyze an entire scene efficiently.
Recently, deep transformers have proven their capability to record global
interactions between different objects in the image. In this paper, we propose
a new segmentation model that combines convolutional neural networks with
transformers, and show that this mixture of local and global feature extraction
techniques provides significant advantages in remote sensing segmentation. In
addition, the proposed model includes two fusion layers that are designed to
represent multi-modal inputs and output of the network efficiently. The input
fusion layer extracts feature maps summarizing the relationship between image
content and elevation maps (DSM). The output fusion layer uses a novel
multi-task segmentation strategy where class labels are identified using
class-specific feature extraction layers and loss functions. Finally, a
fast-marching method is used to convert all unidentified class labels to their
closest known neighbors. Our results demonstrate that the proposed methodology
improves segmentation accuracy compared to state-of-the-art techniques.

### Title: Superconducting routing platform for control and read-out of spin qubits
* Paper ID: 2206.09727v1
* Paper URL: [http://arxiv.org/abs/2206.09727v1](http://arxiv.org/abs/2206.09727v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: To reach large-scale quantum computing, three-dimensional integration of
scalable qubit arrays and their control electronics in multi-chip assemblies is
promising. Within these assemblies, the use of superconducting
interconnections, as routing layers, offers interesting perspective in term of
(1) thermal management to protect the qubits from control electronics
self-heating, (2) passive device performance with significant increase of
quality factors and (3) density rise of low and high frequency signals thanks
to minimal dispersion. We report on the fabrication, using 200 mm silicon wafer
technologies, of a multi-layer routing platform designed for the hybridation of
spin qubit and control electronics chips. A routing level couples the qubits
and the control circuits through one layer of Al0.995Cu0.005 and
superconducting layers of TiN, Nb or NbN, connected between them by W-based
vias. Wafer-level parametric tests at 300 K validate the yield of these
technologies and low temperature electrical measurements in cryostat are used
to extract the superconducting properties of the routing layers. Preliminary
low temperature radio-frequency characterizations of superconducting passive
elements, embedded in these routing levels, are presented.

### Title: Technical Report: Combining knowledge from Transfer Learning during training and Wide Resnets
* Paper ID: 2206.09697v1
* Paper URL: [http://arxiv.org/abs/2206.09697v1](http://arxiv.org/abs/2206.09697v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In this report, we combine the idea of Wide ResNets and transfer learning to
optimize the architecture of deep neural networks. The first improvement of the
architecture is the use of all layers as information source for the last layer.
This idea comes from transfer learning, which uses networks pre-trained on
other data and extracts different levels of the network as input for the new
task. The second improvement is the use of deeper layers instead of deeper
sequences of blocks. This idea comes from Wide ResNets. Using both
optimizations, both high data augmentation and standard data augmentation can
produce better results for different models.
  Link:
https://github.com/wolfgangfuhl/PublicationStuff/tree/master/TechnicalReport1/Supp

### Title: Time-varying Metallic Interfaces
* Paper ID: 2206.09684v1
* Paper URL: [http://arxiv.org/abs/2206.09684v1](http://arxiv.org/abs/2206.09684v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: This paper presents an analytical framework for the analysis of time-varying
metallic metamaterials. Concretely, we particularize the study to
time-modulated metallic interfaces embedded between two different semi-infinite
media that are illuminated by monochromatic plane waves of frequency
$\omega_0$. The formulation is based on a Floquet-Bloch modal expansion, which
takes into account the time periodicity of the structure ($T_s = 2\pi /
\omega_s)$, and integral-equation techniques. It allows to extract the
reflection/transmission coefficients as well as to derive nontrivial features
about the dynamic response and dispersion curves of time-modulated screens. In
addition, the proposed formulation has an associated equivalent circuit that
gives physical insight to the diffraction phenomenon. Finally, some analytical
results are presented to validate the present framework. A good agreement is
observed with numerical computations provided by a self-implemented
finite-difference time-domain (FDTD) method. Interestingly, the present results
suggest that time-modulated metallic screens can be used either as pulsed
sources (when $\omega_s \ll \omega_0$) or as beamformers (when $\omega_s \sim
\omega_0$) to redirect energy in specific regions of space.

### Title: EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL
* Paper ID: 2206.09674v1
* Paper URL: [http://arxiv.org/abs/2206.09674v1](http://arxiv.org/abs/2206.09674v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Reinforcement learning (RL) in long horizon and sparse reward tasks is
notoriously difficult and requires a lot of training steps. A standard solution
to speed up the process is to leverage additional reward signals, shaping it to
better guide the learning process. In the context of language-conditioned RL,
the abstraction and generalisation properties of the language input provide
opportunities for more efficient ways of shaping the reward. In this paper, we
leverage this idea and propose an automated reward shaping method where the
agent extracts auxiliary objectives from the general language goal. These
auxiliary objectives use a question generation (QG) and question answering (QA)
system: they consist of questions leading the agent to try to reconstruct
partial information about the global goal using its own trajectory. When it
succeeds, it receives an intrinsic reward proportional to its confidence in its
answer. This incentivizes the agent to generate trajectories which
unambiguously explain various aspects of the general language goal. Our
experimental study shows that this approach, which does not require engineer
intervention to design the auxiliary objectives, improves sample efficiency by
effectively directing exploration.

### Title: MSANet: Multi-Similarity and Attention Guidance for Boosting Few-Shot Segmentation
* Paper ID: 2206.09667v1
* Paper URL: [http://arxiv.org/abs/2206.09667v1](http://arxiv.org/abs/2206.09667v1)
* Updated Date: 2022-06-20
* Code URL: [https://github.com/AIVResearch/MSANet](https://github.com/AIVResearch/MSANet)
* Summary: Few-shot segmentation aims to segment unseen-class objects given only a
handful of densely labeled samples. Prototype learning, where the support
feature yields a singleor several prototypes by averaging global and local
object information, has been widely used in FSS. However, utilizing only
prototype vectors may be insufficient to represent the features for all
training data. To extract abundant features and make more precise predictions,
we propose a Multi-Similarity and Attention Network (MSANet) including two
novel modules, a multi-similarity module and an attention module. The
multi-similarity module exploits multiple feature-maps of support images and
query images to estimate accurate semantic relationships. The attention module
instructs the network to concentrate on class-relevant information. The network
is tested on standard FSS datasets, PASCAL-5i 1-shot, PASCAL-5i 5-shot,
COCO-20i 1-shot, and COCO-20i 5-shot. The MSANet with the backbone of
ResNet-101 achieves the state-of-the-art performance for all 4-benchmark
datasets with mean intersection over union (mIoU) of 69.13%, 73.99%, 51.09%,
56.80%, respectively. Code is available at
https://github.com/AIVResearch/MSANet

### Title: An improved method for estimating the velocity field of coronal propagating disturbances
* Paper ID: 2206.09640v1
* Paper URL: [http://arxiv.org/abs/2206.09640v1](http://arxiv.org/abs/2206.09640v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The solar corona is host to a continuous flow of propagating disturbances
(PD). These are continuous and ubiquitous across broad regions of the corona,
including the quiet Sun. The aim of this paper is to present an improved,
efficient method to create velocity vector field maps, based on the direction
and magnitude of the PD as observed in time series of extreme ultraviolet (EUV)
images. The method is presented here for use with the Atmospheric Imaging
Assembly (AIA)/Solar Dynamics Observatory (SDO) EUV channels, and takes as
input \app2 hours of images at the highest 12s cadence. Data from a region near
disk center is extracted, and a process called time normalization applied to
the co-aligned data. Following noise reduction using \atrous\ decomposition,
the PD are effectively revealed. A modified Lucas Kanade algorithm is then used
to map the velocity field. The method described here runs comfortably on a
desktop computer in a few minutes, and offers an order of magnitude improvement
in efficiency compared to a previous implementation. Applied to a region of the
quiet Sun, we find that the velocity field describes a mosaic of cells of
coherent outwardly-diverging PD flows, of typical size 50 to 100\arcsec\ (36 to
72Mm). The flows originate from points and narrow corridors in the cell
centres, and end in the narrow boundaries between cells. Visual comparison with
ultraviolet AIA images shows that the flow sources are correlated with the
bright photospheric supergranular network boundaries. Assuming that the PD
follow the local magnetic field, the velocity flow field is a proxy for the
plane-of-sky distribution of the coronal magnetic field, and therefore the maps
offer a unique insight into the topology of the corona. These are particularly
valuable for quiet Sun regions where the appearance of structures in EUV images
is hard to interpret.

### Title: Occultation Portal: a web-based platform for data collection and analysis of stellar occultations
* Paper ID: 2206.09615v1
* Paper URL: [http://arxiv.org/abs/2206.09615v1](http://arxiv.org/abs/2206.09615v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Recording a stellar occultation is one powerful method that gives direct
information about the physical properties of the occulting solar system object.
In order to obtain reliable and accurate results, simultaneous observations
from different locations across-track of the projected path are of great
importance. However, organising all the observing stations, aggregating, and
analysing the data is time-consuming and not that easy. We have developed a web
portal named Occultation Portal (OP) to manage all those occultation
observation campaigns from a central server. With this portal, the instrumental
and observational information of all observers participating in a stellar
occultation campaign and the concerned data are archived systematically in a
standard format. The researchers can then visualise the archived data on an
event basis. The investigators can also extract the light curve for each
data-set with the added reduction pipeline to the portal base. This paper
describes in detail the portal structure and the developed features.

### Title: NLOS Ranging Mitigation with Neural Network Model for UWB Localization
* Paper ID: 2206.09607v1
* Paper URL: [http://arxiv.org/abs/2206.09607v1](http://arxiv.org/abs/2206.09607v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Localization of robots is vital for navigation and path planning, such as in
cases where a map of the environment is needed. Ultra-Wideband (UWB) for indoor
location systems has been gaining popularity over the years with the
introduction of low-cost UWB modules providing centimetre-level accuracy.
However, in the presence of obstacles in the environment, Non-Line-Of-Sight
(NLOS) measurements from the UWB will produce inaccurate results. As low-cost
UWB devices do not provide channel information, we propose an approach to
decide if a measurement is within Line-Of-Sight (LOS) or not by using some
signal strength information provided by low-cost UWB modules through a Neural
Network (NN) model. The result of this model is the probability of a ranging
measurement being LOS which was used for localization through the
Weighted-Least-Square (WLS) method. Our approach improves localization accuracy
by 16.93% on the lobby testing data and 27.97% on the corridor testing data
using the NN model trained with all extracted inputs from the office training
data.

### Title: Eliminating The Impossible, Whatever Remains Must Be True
* Paper ID: 2206.09551v1
* Paper URL: [http://arxiv.org/abs/2206.09551v1](http://arxiv.org/abs/2206.09551v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The rise of AI methods to make predictions and decisions has led to a
pressing need for more explainable artificial intelligence (XAI) methods. One
common approach for XAI is to produce a post-hoc explanation, explaining why a
black box ML model made a certain prediction. Formal approaches to post-hoc
explanations provide succinct reasons for why a prediction was made, as well as
why not another prediction was made. But these approaches assume that features
are independent and uniformly distributed. While this means that "why"
explanations are correct, they may be longer than required. It also means the
"why not" explanations may be suspect as the counterexamples they rely on may
not be meaningful. In this paper, we show how one can apply background
knowledge to give more succinct "why" formal explanations, that are presumably
easier to interpret by humans, and give more accurate "why not" explanations.
Furthermore, we also show how to use existing rule induction techniques to
efficiently extract background information from a dataset, and also how to
report which background information was used to make an explanation, allowing a
human to examine it if they doubt the correctness of the explanation.

### Title: Extracting Fast and Slow: User-Action Embedding with Inter-temporal Information
* Paper ID: 2206.09535v1
* Paper URL: [http://arxiv.org/abs/2206.09535v1](http://arxiv.org/abs/2206.09535v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: With the recent development of technology, data on detailed human temporal
behaviors has become available. Many methods have been proposed to mine those
human dynamic behavior data and revealed valuable insights for research and
businesses. However, most methods analyze only sequence of actions and do not
study the inter-temporal information such as the time intervals between actions
in a holistic manner. While actions and action time intervals are
interdependent, it is challenging to integrate them because they have different
natures: time and action. To overcome this challenge, we propose a unified
method that analyzes user actions with intertemporal information (time
interval). We simultaneously embed the user's action sequence and its time
intervals to obtain a low-dimensional representation of the action along with
intertemporal information. The paper demonstrates that the proposed method
enables us to characterize user actions in terms of temporal context, using
three real-world data sets. This paper demonstrates that explicit modeling of
action sequences and inter-temporal user behavior information enable successful
interpretable analysis.

### Title: Hands-on Wireless Sensing with Wi-Fi: A Tutorial
* Paper ID: 2206.09532v1
* Paper URL: [http://arxiv.org/abs/2206.09532v1](http://arxiv.org/abs/2206.09532v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: With the rapid development of wireless communication technology, wireless
access points (AP) and internet of things (IoT) devices have been widely
deployed in our surroundings. Various types of wireless signals (e.g., Wi-Fi,
LoRa, LTE) are filling out our living and working spaces. Previous researches
reveal the fact that radio waves are modulated by the spatial structure during
the propagation process (e.g., reflection, diffraction, and scattering) and
superimposed on the receiver. This observation allows us to reconstruct the
surrounding environment based on received wireless signals, called "wireless
sensing". Wireless sensing is an emerging technology that enables a wide range
of applications, such as gesture recognition for human-computer interaction,
vital signs monitoring for health care, and intrusion detection for security
management. Compared with other sensing paradigms, such as vision-based and
IMU-based sensing, wireless sensing solutions have unique advantages such as
high coverage, pervasiveness, low cost, and robustness under adverse light and
texture scenarios. Besides, wireless sensing solutions are generally
lightweight in terms of both computation overhead and device size. This
tutorial takes Wi-Fi sensing as an example. It introduces both the theoretical
principles and the code implementation of data collection, signal processing,
features extraction, and model design. In addition, this tutorial highlights
state-of-the-art deep learning models (e.g., CNN, RNN, and adversarial learning
models) and their applications in wireless sensing systems. We hope this
tutorial will help people in other research fields to break into wireless
sensing research and learn more about its theories, designs, and implementation
skills, promoting prosperity in the wireless sensing research field.

### Title: Temporal Link Prediction via Adjusted Sigmoid Function and 2-Simplex Sructure
* Paper ID: 2206.09529v1
* Paper URL: [http://arxiv.org/abs/2206.09529v1](http://arxiv.org/abs/2206.09529v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Temporal network link prediction is an important task in the field of network
science, and has a wide range of applications in practical scenarios. Revealing
the evolutionary mechanism of the network is essential for link prediction, and
how to effectively utilize the historical information for temporal links and
efficiently extract the high-order patterns of network structure remains a
vital challenge. To address these issues, in this paper, we propose a novel
temporal link prediction model with adjusted sigmoid function and 2-simplex
structure (TLPSS). The adjusted sigmoid decay mode takes the active, decay and
stable states of edges into account, which properly fits the life cycle of
information. Moreover, the latent matrix sequence is introduced, which is
composed of simplex high-order structure, to enhance the performance of link
prediction method since it is highly feasible in sparse network. Combining the
life cycle of information and simplex high-order structure, the overall
performance of TLPSS is achieved by satisfying the consistency of temporal and
structural information in dynamic networks. Experimental results on six
real-world datasets demonstrate the effectiveness of TLPSS, and our proposed
model improves the performance of link prediction by an average of 15% compared
to other baseline methods.

### Title: Fully-strange tetraquark states with the exotic quantum number $J^{PC} = 4^{+-}$
* Paper ID: 2206.09517v1
* Paper URL: [http://arxiv.org/abs/2206.09517v1](http://arxiv.org/abs/2206.09517v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We study the fully-strange tetraquark states with the exotic quantum number
$J^{PC} = 4^{+-}$. We systematically construct all the diquark-antidiquark
interpolating currents, and apply the method of QCD sum rules to calculate both
the diagonal and off-diagonal correlation functions. The obtained results are
used to construct three mixing currents that are nearly non-correlated. We use
one mixing current to extract the mass of the lowest-lying state to be
$2.85^{+0.19}_{-0.22}$ GeV. We apply the Fierz rearrangement to transform this
mixing current to be the combination of three meson-meson currents. The
obtained Fierz identity suggests that this lowest-lying state dominantly decays
into the $P$-wave $\phi(1020) f_2^\prime(1525)$ channel.

