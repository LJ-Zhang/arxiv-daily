### Title: TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation
* Paper ID: 2207.03450v1
* Paper URL: [http://arxiv.org/abs/2207.03450v1](http://arxiv.org/abs/2207.03450v1)
* Updated Date: 2022-07-07
* Categories: ['eess.IV', 'cs.CV']
* Code URL: [https://github.com/huanglizi/tfcns](https://github.com/huanglizi/tfcns)
* Summary: Medical image segmentation is one of the most fundamental tasks concerning
medical information analysis. Various solutions have been proposed so far,
including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc.
However, high-precision medical image segmentation remains a highly challenging
task due to the existence of inherent magnification and distortion in medical
images as well as the presence of lesions with similar density to normal
tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional
denseNets) to tackle the problem by introducing ResLinear-Transformer
(RL-Transformer) and Convolutional Linear Attention Block (CLAB) to
FC-DenseNet. TFCNs is not only able to utilize more latent information from the
CT images for feature extraction, but also can capture and disseminate semantic
features and filter non-semantic features more effectively through the CLAB
module. Our experimental results show that TFCNs can achieve state-of-the-art
performance with dice scores of 83.72\% on the Synapse dataset. In addition, we
evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public
datasets. The Python code will be made publicly available on
https://github.com/HUANGLIZI/TFCNs.

### Title: SC2EGSet: StarCraft II Esport Replay and Game-state Dataset
* Paper ID: 2207.03428v1
* Paper URL: [http://arxiv.org/abs/2207.03428v1](http://arxiv.org/abs/2207.03428v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.AI', 'stat.ML']
* Code URL: [https://github.com/Kaszanas/SC2EGSet_Dataset](https://github.com/Kaszanas/SC2EGSet_Dataset)
* Summary: As a relatively new form of sport, esports offers unparalleled data
availability. Despite the vast amounts of data that are generated by game
engines, it can be challenging to extract them and verify their integrity for
the purposes of practical and scientific use.
  Our work aims to open esports to a broader scientific community by supplying
raw and pre-processed files from StarCraft II esports tournaments. These files
can be used in statistical and machine learning modeling tasks and related to
various laboratory-based measurements (e.g., behavioral tests, brain imaging).
We have gathered publicly available game-engine generated "replays" of
tournament matches and performed data extraction and cleanup using a low-level
application programming interface (API) parser library.
  Additionally, we open-sourced and published all the custom tools that were
developed in the process of creating our dataset. These tools include PyTorch
and PyTorch Lightning API abstractions to load and model the data.
  Our dataset contains replays from major and premiere StarCraft II tournaments
since 2016. To prepare the dataset, we processed 55 tournament "replaypacks"
that contained 17930 files with game-state information. Based on initial
investigation of available StarCraft II datasets, we observed that our dataset
is the largest publicly available source of StarCraft II esports data upon its
publication.
  Analysis of the extracted data holds promise for further Artificial
Intelligence (AI), Machine Learning (ML), psychological, Human-Computer
Interaction (HCI), and sports-related studies in a variety of supervised and
self-supervised tasks.

### Title: Multimodal Feature Extraction for Memes Sentiment Classification
* Paper ID: 2207.03317v1
* Paper URL: [http://arxiv.org/abs/2207.03317v1](http://arxiv.org/abs/2207.03317v1)
* Updated Date: 2022-07-07
* Categories: ['cs.AI']
* Code URL: null
* Summary: In this study, we propose feature extraction for multimodal meme
classification using Deep Learning approaches. A meme is usually a photo or
video with text shared by the young generation on social media platforms that
expresses a culturally relevant idea. Since they are an efficient way to
express emotions and feelings, a good classifier that can classify the
sentiment behind the meme is important. To make the learning process more
efficient, reduce the likelihood of overfitting, and improve the
generalizability of the model, one needs a good approach for joint feature
extraction from all modalities. In this work, we proposed to use different
multimodal neural network approaches for multimodal feature extraction and use
the extracted features to train a classifier to identify the sentiment in a
meme.

### Title: Multimodal E-Commerce Product Classification Using Hierarchical Fusion
* Paper ID: 2207.03305v1
* Paper URL: [http://arxiv.org/abs/2207.03305v1](http://arxiv.org/abs/2207.03305v1)
* Updated Date: 2022-07-07
* Categories: ['cs.AI', 'cs.CE']
* Code URL: null
* Summary: In this work, we present a multi-modal model for commercial product
classification, that combines features extracted by multiple neural network
models from textual (CamemBERT and FlauBERT) and visual data (SE-ResNeXt-50),
using simple fusion techniques. The proposed method significantly outperformed
the unimodal models' performance and the reported performance of similar models
on our specific task. We did experiments with multiple fusing techniques and
found, that the best performing technique to combine the individual embedding
of the unimodal network is based on combining concatenation and averaging the
feature vectors. Each modality complemented the shortcomings of the other
modalities, demonstrating that increasing the number of modalities can be an
effective method for improving the performance of multi-label and multimodal
classification problems.

### Title: Vessel-following model for inland waterways based on deep reinforcement learning
* Paper ID: 2207.03257v1
* Paper URL: [http://arxiv.org/abs/2207.03257v1](http://arxiv.org/abs/2207.03257v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CE', 'cs.LG']
* Code URL: null
* Summary: While deep reinforcement learning (RL) has been increasingly applied in
designing car-following models in the last years, this study aims at
investigating the feasibility of RL-based vehicle-following for complex vehicle
dynamics and strong environmental disturbances. As a use case, we developed an
inland waterways vessel-following model based on realistic vessel dynamics,
which considers environmental influences, such as varying stream velocity and
river profile. We extracted natural vessel behavior from anonymized AIS data to
formulate a reward function that reflects a realistic driving style next to
comfortable and safe navigation. Aiming at high generalization capabilities, we
propose an RL training environment that uses stochastic processes to model
leading trajectory and river dynamics. To validate the trained model, we
defined different scenarios that have not been seen in training, including
realistic vessel-following on the Middle Rhine. Our model demonstrated safe and
comfortable driving in all scenarios, proving excellent generalization
abilities. Furthermore, traffic oscillations could effectively be dampened by
deploying the trained model on a sequence of following vessels.

### Title: Part-of-Speech Tagging of Odia Language Using statistical and Deep Learning-Based Approaches
* Paper ID: 2207.03256v1
* Paper URL: [http://arxiv.org/abs/2207.03256v1](http://arxiv.org/abs/2207.03256v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CL']
* Code URL: null
* Summary: Automatic Part-of-speech (POS) tagging is a preprocessing step of many
natural language processing (NLP) tasks such as name entity recognition (NER),
speech processing, information extraction, word sense disambiguation, and
machine translation. It has already gained a promising result in English and
European languages, but in Indian languages, particularly in Odia language, it
is not yet well explored because of the lack of supporting tools, resources,
and morphological richness of language. Unfortunately, we were unable to locate
an open source POS tagger for Odia, and only a handful of attempts have been
made to develop POS taggers for Odia language. The main contribution of this
research work is to present a conditional random field (CRF) and deep
learning-based approaches (CNN and Bidirectional Long Short-Term Memory) to
develop Odia part-of-speech tagger. We used a publicly accessible corpus and
the dataset is annotated with the Bureau of Indian Standards (BIS) tagset.
However, most of the languages around the globe have used the dataset annotated
with Universal Dependencies (UD) tagset. Hence, to maintain uniformity Odia
dataset should use the same tagset. So we have constructed a simple mapping
from BIS tagset to UD tagset. We experimented with various feature set inputs
to the CRF model, observed the impact of constructed feature set. The deep
learning-based model includes Bi-LSTM network, CNN network, CRF layer,
character sequence information, and pre-trained word vector. Character sequence
information was extracted by using convolutional neural network (CNN) and
Bi-LSTM network. Six different combinations of neural sequence labelling models
are implemented, and their performance measures are investigated. It has been
observed that Bi-LSTM model with character sequence feature and pre-trained
word vector achieved a significant state-of-the-art result.

### Title: Entropy-Based Feature Extraction For Real-Time Semantic Segmentation
* Paper ID: 2207.03233v1
* Paper URL: [http://arxiv.org/abs/2207.03233v1](http://arxiv.org/abs/2207.03233v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV', 'cs.AI']
* Code URL: null
* Summary: This paper introduces an efficient patch-based computational module, coined
Entropy-based Patch Encoder (EPE) module, for resource-constrained semantic
segmentation. The EPE module consists of three lightweight fully-convolutional
encoders, each extracting features from image patches with a different amount
of entropy. Patches with high entropy are being processed by the encoder with
the largest number of parameters, patches with moderate entropy are processed
by the encoder with a moderate number of parameters, and patches with low
entropy are processed by the smallest encoder. The intuition behind the module
is the following: as patches with high entropy contain more information, they
need an encoder with more parameters, unlike low entropy patches, which can be
processed using a small encoder. Consequently, processing part of the patches
via the smaller encoder can significantly reduce the computational cost of the
module. Experiments show that EPE can boost the performance of existing
real-time semantic segmentation models with a slight increase in the
computational cost. Specifically, EPE increases the mIOU performance of DFANet
A by 0.9% with only 1.2% increase in the number of parameters and the mIOU
performance of EDANet by 1% with 10% increase of the model parameters.

### Title: Dual Stream Computer-Generated Image Detection Network Based On Channel Joint And Softpool
* Paper ID: 2207.03205v1
* Paper URL: [http://arxiv.org/abs/2207.03205v1](http://arxiv.org/abs/2207.03205v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV']
* Code URL: [https://github.com/zoie-ui/cg-detection](https://github.com/zoie-ui/cg-detection)
* Summary: With the development of computer graphics technology, the images synthesized
by computer software become more and more closer to the photographs. While
computer graphics technology brings us a grand visual feast in the field of
games and movies, it may also be utilized by someone with bad intentions to
guide public opinions and cause political crisis or social unrest. Therefore,
how to distinguish the computer-generated graphics (CG) from the photographs
(PG) has become an important topic in the field of digital image forensics.
This paper proposes a dual stream convolutional neural network based on channel
joint and softpool. The proposed network architecture includes a residual
module for extracting image noise information and a joint channel information
extraction module for capturing the shallow semantic information of image. In
addition, we also design a residual structure to enhance feature extraction and
reduce the loss of information in residual flow. The joint channel information
extraction module can obtain the shallow semantic information of the input
image which can be used as the information supplement block of the residual
module. The whole network uses SoftPool to reduce the information loss of
down-sampling for image. Finally, we fuse the two flows to get the
classification results. Experiments on SPL2018 and DsTok show that the proposed
method outperforms existing methods, especially on the DsTok dataset. For
example, the performance of our model surpasses the state-of-the-art by a large
margin of 3%.

### Title: Self-Supervised Learning of Music-Dance Representation through Explicit-Implicit Rhythm Synchronization
* Paper ID: 2207.03190v1
* Paper URL: [http://arxiv.org/abs/2207.03190v1](http://arxiv.org/abs/2207.03190v1)
* Updated Date: 2022-07-07
* Categories: ['cs.SD', 'cs.CV', 'cs.MM', 'eess.AS']
* Code URL: null
* Summary: Although audio-visual representation has been proved to be applicable in many
downstream tasks, the representation of dancing videos, which is more specific
and always accompanied by music with complex auditory contents, remains
challenging and uninvestigated. Considering the intrinsic alignment between the
cadent movement of dancer and music rhythm, we introduce MuDaR, a novel
Music-Dance Representation learning framework to perform the synchronization of
music and dance rhythms both in explicit and implicit ways. Specifically, we
derive the dance rhythms based on visual appearance and motion cues inspired by
the music rhythm analysis. Then the visual rhythms are temporally aligned with
the music counterparts, which are extracted by the amplitude of sound
intensity. Meanwhile, we exploit the implicit coherence of rhythms implied in
audio and visual streams by contrastive learning. The model learns the joint
embedding by predicting the temporal consistency between audio-visual pairs.
The music-dance representation, together with the capability of detecting audio
and visual rhythms, can further be applied to three downstream tasks: (a) dance
classification, (b) music-dance retrieval, and (c) music-dance retargeting.
Extensive experiments demonstrate that our proposed framework outperforms other
self-supervised methods by a large margin.

### Title: Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions
* Paper ID: 2207.03182v1
* Paper URL: [http://arxiv.org/abs/2207.03182v1](http://arxiv.org/abs/2207.03182v1)
* Updated Date: 2022-07-07
* Categories: ['stat.ME', 'cs.CV', 'math.PR', 'math.ST', 'stat.AP', 'stat.TH']
* Code URL: null
* Summary: Atmospheric motion vectors (AMVs) extracted from satellite imagery are the
only wind observations with good global coverage. They are important features
for feeding numerical weather prediction (NWP) models. Several Bayesian models
have been proposed to estimate AMVs. Although critical for correct assimilation
into NWP models, very few methods provide a thorough characterization of the
estimation errors. The difficulty of estimating errors stems from the
specificity of the posterior distribution, which is both very high dimensional,
and highly ill-conditioned due to a singular likelihood, which becomes critical
in particular in the case of missing data (unobserved pixels). This work
studies the evaluation of the expected error of AMVs using gradient-based
Markov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose
a tempering strategy, which amounts to sampling a local approximation of the
joint posterior distribution of AMVs and image variables in the neighborhood of
a point estimate. In addition, we provide efficient preconditioning with the
covariance related to the prior family itself (fractional Brownian motion),
with possibly different hyper-parameters. From a theoretical point of view, we
show that under regularity assumptions, the family of tempered posterior
distributions converges in distribution as temperature decreases to an
{optimal} Gaussian approximation at a point estimate given by the Maximum A
Posteriori (MAP) log-density. From an empirical perspective, we evaluate the
proposed approach based on some quantitative Bayesian evaluation criteria. Our
numerical simulations performed on synthetic and real meteorological data
reveal a significant gain in terms of accuracy of the AMV point estimates and
of their associated expected error estimates, but also a substantial
acceleration in the convergence speed of the MCMC algorithms.

### Title: EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2022: Team HNU-FPV Technical Report
* Paper ID: 2207.03095v1
* Paper URL: [http://arxiv.org/abs/2207.03095v1](http://arxiv.org/abs/2207.03095v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV']
* Code URL: null
* Summary: In this report, we present the technical details of our submission to the
2022 EPIC-Kitchens Unsupervised Domain Adaptation (UDA) Challenge. Existing UDA
methods align the global features extracted from the whole video clips across
the source and target domains but suffer from the spatial redundancy of feature
matching in video recognition. Motivated by the observation that in most cases
a small image region in each video frame can be informative enough for the
action recognition task, we propose to exploit informative image regions to
perform efficient domain alignment. Specifically, we first use lightweight CNNs
to extract the global information of the input two-stream video frames and
select the informative image patches by a differentiable interpolation-based
selection strategy. Then the global information from videos frames and local
information from image patches are processed by an existing video adaptation
method, i.e., TA3N, in order to perform feature alignment for the source domain
and the target domain. Our method (without model ensemble) ranks 4th among this
year's teams on the test set of EPIC-KITCHENS-100.

### Title: Word Embedding for Social Sciences: An Interdisciplinary Survey
* Paper ID: 2207.03086v1
* Paper URL: [http://arxiv.org/abs/2207.03086v1](http://arxiv.org/abs/2207.03086v1)
* Updated Date: 2022-07-07
* Categories: ['cs.AI']
* Code URL: null
* Summary: To extract essential information from complex data, computer scientists have
been developing machine learning models that learn low-dimensional
representation mode. From such advances in machine learning research, not only
computer scientists but also social scientists have benefited and advanced
their research because human behavior or social phenomena lies in complex data.
To document this emerging trend, we survey the recent studies that apply word
embedding techniques to human behavior mining, building a taxonomy to
illustrate the methods and procedures used in the surveyed papers and highlight
the recent emerging trends applying word embedding models to non-textual human
behavior data. This survey conducts a simple experiment to warn that common
similarity measurements used in the literature could yield different results
even if they return consistent results at an aggregate level.

### Title: DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning
* Paper ID: 2207.03081v1
* Paper URL: [http://arxiv.org/abs/2207.03081v1](http://arxiv.org/abs/2207.03081v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'eess.IV']
* Code URL: null
* Summary: In this paper, we propose a multi-objective camera ISP framework that
utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist
of network-based and conventional ISP tools. The proposed DRL-based camera ISP
framework iteratively selects a proper tool from the toolbox and applies it to
the image to maximize a given vision task-specific reward function. For this
purpose, we implement total 51 ISP tools that include exposure correction,
color-and-tone correction, white balance, sharpening, denoising, and the
others. We also propose an efficient DRL network architecture that can extract
the various aspects of an image and make a rigid mapping relationship between
images and a large number of actions. Our proposed DRL-based ISP framework
effectively improves the image quality according to each vision task such as
RAW-to-RGB image restoration, 2D object detection, and monocular depth
estimation.

### Title: A Large Scale Search Dataset for Unbiased Learning to Rank
* Paper ID: 2207.03051v1
* Paper URL: [http://arxiv.org/abs/2207.03051v1](http://arxiv.org/abs/2207.03051v1)
* Updated Date: 2022-07-07
* Categories: ['cs.AI']
* Code URL: [https://github.com/chuxiaokai/baidu_ultr_dataset](https://github.com/chuxiaokai/baidu_ultr_dataset)
* Summary: The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset.

### Title: Dual-Stream Transformer for Generic Event Boundary Captioning
* Paper ID: 2207.03038v1
* Paper URL: [http://arxiv.org/abs/2207.03038v1](http://arxiv.org/abs/2207.03038v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV', 'cs.CL']
* Code URL: [https://github.com/gx77/dual-stream-transformer-for-generic-event-boundary-captioning](https://github.com/gx77/dual-stream-transformer-for-generic-event-boundary-captioning)
* Summary: This paper describes our champion solution for the CVPR2022 Generic Event
Boundary Captioning (GEBC) competition. GEBC requires the captioning model to
have a comprehension of instantaneous status changes around the given video
boundary, which makes it much more challenging than conventional video
captioning task. In this paper, a Dual-Stream Transformer with improvements on
both video content encoding and captions generation is proposed: (1) We utilize
three pre-trained models to extract the video features from different
granularities. Moreover, we exploit the types of boundary as hints to help the
model generate captions. (2) We particularly design an model, termed as
Dual-Stream Transformer, to learn discriminative representations for boundary
captioning. (3) Towards generating content-relevant and human-like captions, we
improve the description quality by designing a word-level ensemble strategy.
The promising results on the GEBC test split demonstrate the efficacy of our
proposed model.

