### Title: Thermal Friction as a Solution to the Hubble and Large-Scale Structure Tensions
* Paper ID: 2204.09133v1
* Paper URL: [http://arxiv.org/abs/2204.09133v1](http://arxiv.org/abs/2204.09133v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Thermal friction offers a promising solution to the Hubble and the
large-scale structure (LSS) tensions. This additional friction acts on a scalar
field in the early universe and extracts its energy density into dark
radiation, the cumulative effect being similar to that of an early dark energy
(EDE) scenario. The dark radiation automatically redshifts at the minimal
necessary rate to improve the Hubble tension. On the other hand, the addition
of extra radiation to the Universe can improve the LSS tension. We explore this
model in light of cosmic microwave background (CMB), baryon acoustic
oscillation and supernova data, including the SH0ES $H_0$ measurement and the
Dark Energy Survey Y1 data release in our analysis. Our results indicate a
preference for the regime where the scalar field converts to dark radiation at
very high redshifts, asymptoting effectively to an extra self-interacting
radiation species rather than an EDE-like injection. In this limit, thermal
friction can ease both the Hubble and the LSS tensions, but not resolve them.
We find the source of this preference to be the incompatibility of the CMB data
with the linear density perturbations of the dark radiation when injected at
redshifts close to matter-radiation equality.

### Title: A Unified Approach for Multi-Scale Synchronous Correlation Search in Big Time Series -- Full Version
* Paper ID: 2204.09131v1
* Paper URL: [http://arxiv.org/abs/2204.09131v1](http://arxiv.org/abs/2204.09131v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The wide deployment of IoT sensors has enabled the collection of very big
time series across different domains, from which advanced analytics can be
performed to find unknown relationships, most importantly the correlations
between them. However, current approaches for correlation search on time series
are limited to only a single temporal scale and simple types of relations, and
cannot handle noise effectively. This paper presents the integrated SYnchronous
COrrelation Search (iSYCOS) framework to find multi-scale correlations in big
time series. Specifically, iSYCOS integrates top-down and bottom-up approaches
into a single auto-configured framework capable of efficiently extracting
complex window-based correlations from big time series using mutual information
(MI). Moreover, iSYCOS includes a novel MI-based theory to identify noise in
the data, and is used to perform pruning to improve iSYCOS performance.
Besides, we design a distributed version of iSYCOS that can scale out in a
Spark cluster to handle big time series. Our extensive experimental evaluation
on synthetic and real-world datasets shows that iSYCOS can auto-configure on a
given dataset to find complex multi-scale correlations. The pruning and
optimisations can improve iSYCOS performance up to an order of magnitude, and
the distributed iSYCOS can scale out linearly on a computing cluster.

### Title: Sintel: A Machine Learning Framework to Extract Insights from Signals
* Paper ID: 2204.09108v1
* Paper URL: [http://arxiv.org/abs/2204.09108v1](http://arxiv.org/abs/2204.09108v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/sarahmish/sintel-paper](https://github.com/sarahmish/sintel-paper)
* Summary: The detection of anomalies in time series data is a critical task with many
monitoring applications. Existing systems often fail to encompass an end-to-end
detection process, to facilitate comparative analysis of various anomaly
detection methods, or to incorporate human knowledge to refine output. This
precludes current methods from being used in real-world settings by
practitioners who are not ML experts. In this paper, we introduce Sintel, a
machine learning framework for end-to-end time series tasks such as anomaly
detection. The framework uses state-of-the-art approaches to support all steps
of the anomaly detection process. Sintel logs the entire anomaly detection
journey, providing detailed documentation of anomalies over time. It enables
users to analyze signals, compare methods, and investigate anomalies through an
interactive visualization tool, where they can annotate, modify, create, and
remove events. Using these annotations, the framework leverages human knowledge
to improve the anomaly detection pipeline. We demonstrate the usability,
efficiency, and effectiveness of Sintel through a series of experiments on
three public time series datasets, as well as one real-world use case involving
spacecraft experts tasked with anomaly analysis tasks. Sintel's framework,
code, and datasets are open-sourced at https://github.com/sintel-dev/.

### Title: Constraining spontaneous black hole scalarization in scalar-tensor-Gauss-Bonnet theories with current gravitational-wave data
* Paper ID: 2204.09038v1
* Paper URL: [http://arxiv.org/abs/2204.09038v1](http://arxiv.org/abs/2204.09038v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We examine the constraining power of current gravitational-wave data on
scalar-tensor-Gauss-Bonnet theories that allow for the spontaneous
scalarization of black holes. In the fiducial model that we consider, a slowly
rotating black hole must scalarize if its size is comparable to the new length
scale $\lambda$ that the theory introduces, although rapidly rotating black
holes of any mass are effectively indistinguishable from their counterparts in
general relativity. With this in mind, we use the gravitational-wave event
GW190814$\,\unicode{x2014}\,$whose primary black hole has a spin that is
bounded to be small, and whose signal shows no evidence of a scalarized
primary$\,\unicode{x2014}\,$to rule out a narrow region of the parameter space.
In particular, we find that values of ${\lambda \in [56, 96]~M_\odot}$ are
strongly disfavored with a Bayes factor of $0.1$ or less. We also include a
second event, GW151226, in our analysis to illustrate what information can be
extracted when the spins of both components are poorly measured.

### Title: Per-clip adaptive Lagrangian multiplier optimisation with low-resolution proxies
* Paper ID: 2204.08966v1
* Paper URL: [http://arxiv.org/abs/2204.08966v1](http://arxiv.org/abs/2204.08966v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: This work focuses on reducing the computational cost of repeated video
encodes by using a lower resolution clip as a proxy. Features extracted from
the low resolution clip are used to learn an optimal lagrange multiplier for
rate control on the original resolution clip. In addition to reducing the
computational cost and encode time by using lower resolution clips, we also
investigate the use of older, but faster codecs such as H.264 to create
proxies. This work shows that the computational load is reduced by 22 times
using 144p proxies. Our tests are based on the YouTube UGC dataset, hence our
results are based on a practical instance of the adaptive bitrate encoding
problem. Further improvements are possible, by optimising the placement and
sparsity of operating points required for the rate distortion curves.

### Title: Adaptive measurement filter: efficient strategy for optimal estimation of quantum Markov chains
* Paper ID: 2204.08964v1
* Paper URL: [http://arxiv.org/abs/2204.08964v1](http://arxiv.org/abs/2204.08964v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Continuous-time measurements are instrumental for a multitude of tasks in
quantum engineering and quantum control, including the estimation of dynamical
parameters of open quantum systems monitored through the environment. However,
such measurements do not extract the maximum amount of information available in
the output state, so finding alternative optimal measurement strategies is a
major open problem.
  In this paper we solve this problem in the setting of discrete-time
input-output quantum Markov chains. We present an efficient algorithm for
optimal estimation of one-dimensional dynamical parameters which consists of an
iterative procedure for updating a `measurement filter' operator and
determining successive measurement bases for the output units. A key ingredient
of the scheme is the use of a coherent quantum absorber as a way to
post-process the output after the interaction with the system. This is designed
adaptively such that the joint system and absorber stationary state is pure at
a reference parameter value. The scheme offers an exciting prospect for optimal
continuous-time adaptive measurements, but more work is needed to find
realistic practical implementations.

### Title: Seculator: A Fast and Secure Neural Processing Unit
* Paper ID: 2204.08951v1
* Paper URL: [http://arxiv.org/abs/2204.08951v1](http://arxiv.org/abs/2204.08951v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Securing deep neural networks (DNNs) is a problem of significant interest
since an ML model incorporates high-quality intellectual property, features of
data sets painstakingly collated by mechanical turks, and novel methods of
training on large cluster computers. Sadly, attacks to extract model parameters
are on the rise, and thus designers are being forced to create architectures
for securing such models. State-of-the-art proposals in this field take the
deterministic memory access patterns of such networks into cognizance (albeit
partially), group a set of memory blocks into a tile, and maintain state at the
level of tiles (to reduce storage space). For providing integrity guarantees
(tamper avoidance), they don't propose any significant optimizations, and still
maintain block-level state.
  We observe that it is possible to exploit the deterministic memory access
patterns of DNNs even further, and maintain state information for only the
current tile and current layer, which may comprise a large number of tiles.
This reduces the storage space, reduces the number of memory accesses,
increases performance, and simplifies the design without sacrificing any
security guarantees. The key techniques in our proposed accelerator
architecture, Seculator, are to encode memory access patterns to create a small
HW-based tile version number generator for a given layer, and to store
layer-level MACs. We completely eliminate the need for having a MAC cache and a
tile version number store (as used in related work). We show that using
intelligently-designed mathematical operations, these structures are not
required. By reducing such overheads, we show a speedup of 16% over the closest
competing work.

### Title: Global-and-Local Collaborative Learning for Co-Salient Object Detection
* Paper ID: 2204.08917v1
* Paper URL: [http://arxiv.org/abs/2204.08917v1](http://arxiv.org/abs/2204.08917v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The goal of co-salient object detection (CoSOD) is to discover salient
objects that commonly appear in a query group containing two or more relevant
images. Therefore, how to effectively extract inter-image correspondence is
crucial for the CoSOD task. In this paper, we propose a global-and-local
collaborative learning architecture, which includes a global correspondence
modeling (GCM) and a local correspondence modeling (LCM) to capture
comprehensive inter-image corresponding relationship among different images
from the global and local perspectives. Firstly, we treat different images as
different time slices and use 3D convolution to integrate all intra features
intuitively, which can more fully extract the global group semantics. Secondly,
we design a pairwise correlation transformation (PCT) to explore similarity
correspondence between pairwise images and combine the multiple local pairwise
correspondences to generate the local inter-image relationship. Thirdly, the
inter-image relationships of the GCM and LCM are integrated through a
global-and-local correspondence aggregation (GLA) module to explore more
comprehensive inter-image collaboration cues. Finally, the intra- and
inter-features are adaptively integrated by an intra-and-inter weighting fusion
(AEWF) module to learn co-saliency features and predict the co-saliency map.
The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets,
demonstrating that our model trained on a small dataset (about 3k images) still
outperforms eleven state-of-the-art competitors trained on some large datasets
(about 8k-200k images).

### Title: Self-Calibrated Efficient Transformer for Lightweight Super-Resolution
* Paper ID: 2204.08913v1
* Paper URL: [http://arxiv.org/abs/2204.08913v1](http://arxiv.org/abs/2204.08913v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/alexzou14/scet](https://github.com/alexzou14/scet)
* Summary: Recently, deep learning has been successfully applied to the single-image
super-resolution (SISR) with remarkable performance. However, most existing
methods focus on building a more complex network with a large number of layers,
which can entail heavy computational costs and memory storage. To address this
problem, we present a lightweight Self-Calibrated Efficient Transformer (SCET)
network to solve this problem. The architecture of SCET mainly consists of the
self-calibrated module and efficient transformer block, where the
self-calibrated module adopts the pixel attention mechanism to extract image
features effectively. To further exploit the contextual information from
features, we employ an efficient transformer to help the network obtain similar
features over long distances and thus recover sufficient texture details. We
provide comprehensive results on different settings of the overall network. Our
proposed method achieves more remarkable performance than baseline methods. The
source code and pre-trained models are available at
https://github.com/AlexZou14/SCET.

### Title: Adaptable Semantic Compression and Resource Allocation for Task-Oriented Communications
* Paper ID: 2204.08910v1
* Paper URL: [http://arxiv.org/abs/2204.08910v1](http://arxiv.org/abs/2204.08910v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Task-oriented communication is a new paradigm that aims at providing
efficient connectivity for accomplishing intelligent tasks rather than the
reception of every transmitted bit. In this paper, a deep learning-based
task-oriented communication architecture is proposed where the user extracts,
compresses and transmits semantics in an end-to-end (E2E) manner. Furthermore,
an approach is proposed to compress the semantics according to their importance
relevant to the task, namely, adaptable semantic compression (ASC). Assuming a
delay-intolerant system, supporting multiple users indicates a problem that
executing with the higher compression ratio requires fewer channel resources
but leads to the distortion of semantics, while executing with the lower
compression ratio requires more channel resources and thus may lead to a
transmission failure due to delay constraint. To solve the problem, both
compression ratio and resource allocation are optimized for the task-oriented
communication system to maximize the success probability of tasks.
Specifically, due to the nonconvexity of the problem, we propose a compression
ratio and resource allocation (CRRA) algorithm by separating the problem into
two subproblems and solving iteratively to obtain the convergent solution.
Furthermore, considering the scenarios where users have various service levels,
a compression ratio, resource allocation, and user selection (CRRAUS) algorithm
is proposed to deal with the problem. In CRRAUS, users are adaptively selected
to complete the corresponding intelligent tasks based on branch and bound
method at the expense of higher algorithm complexity compared with CRRA.
Simulation results show that the proposed CRRA and CRRAUS algorithms can obtain
at least 15% and 10% success gains over baseline algorithms, respectively.

### Title: Distributional Transform Based Information Reconciliation
* Paper ID: 2204.08891v1
* Paper URL: [http://arxiv.org/abs/2204.08891v1](http://arxiv.org/abs/2204.08891v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In this paper we present an information reconciliation protocol designed for
Continuous-Variable QKD using the Distributional Transform. By combining tools
from copula and information theories, we present a method for extracting
independent symmetric Bernoulli bits for Gaussian modulated CVQKD protocols,
which we called the Distributional Transform Expansion (DTE). We derived the
expressions for the maximum reconciliation efficiency for both homodyne and
heterodyne measurement, which, for the last one, efficiency greater than 0.9 is
achievable at signal to noise ratio lower than -3.6 dB

### Title: Cross-Lingual Phrase Retrieval
* Paper ID: 2204.08887v1
* Paper URL: [http://arxiv.org/abs/2204.08887v1](http://arxiv.org/abs/2204.08887v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Cross-lingual retrieval aims to retrieve relevant text across languages.
Current methods typically achieve cross-lingual retrieval by learning
language-agnostic text representations in word or sentence level. However, how
to learn phrase representations for cross-lingual phrase retrieval is still an
open problem. In this paper, we propose XPR, a cross-lingual phrase retriever
that extracts phrase representations from unlabeled example sentences.
Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which
contains 65K bilingual phrase pairs and 4.2M example sentences in 8
English-centric language pairs. Experimental results show that XPR outperforms
state-of-the-art baselines which utilize word-level or sentence-level
representations. XPR also shows impressive zero-shot transferability that
enables the model to perform retrieval in an unseen language pair during
training. Our dataset, code, and trained models are publicly available at
www.github.com/cwszz/XPR/.

### Title: Antipatterns in Software Classification Taxonomies
* Paper ID: 2204.08880v1
* Paper URL: [http://arxiv.org/abs/2204.08880v1](http://arxiv.org/abs/2204.08880v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Empirical results in software engineering have long started to show that
findings are unlikely to be applicable to all software systems, or any domain:
results need to be evaluated in specified contexts, and limited to the type of
systems that they were extracted from. This is a known issue, and requires the
establishment of a classification of software types.
  This paper makes two contributions: the first is to evaluate the quality of
the current software classifications landscape. The second is to perform a case
study showing how to create a classification of software types using a curated
set of software systems.
  Our contributions show that existing, and very likely even new,
classification attempts are deemed to fail for one or more issues, that we
named as the `antipatterns' of software classification tasks. We collected 7 of
these antipatterns that emerge from both our case study, and the existing
classifications.
  These antipatterns represent recurring issues in a classification, so we
discuss practical ways to help researchers avoid these pitfalls. It becomes
clear that classification attempts must also face the daunting task of
formulating a taxonomy of software types, with the objective of establishing a
hierarchy of categories in a classification.

### Title: Energy-Efficient Tree-Based EEG Artifact Detection
* Paper ID: 2204.09577v1
* Paper URL: [http://arxiv.org/abs/2204.09577v1](http://arxiv.org/abs/2204.09577v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In the context of epilepsy monitoring, EEG artifacts are often mistaken for
seizures due to their morphological similarity in both amplitude and frequency,
making seizure detection systems susceptible to higher false alarm rates. In
this work we present the implementation of an artifact detection algorithm
based on a minimal number of EEG channels on a parallel ultra-low-power (PULP)
embedded platform. The analyses are based on the TUH EEG Artifact Corpus
dataset and focus on the temporal electrodes. First, we extract optimal feature
models in the frequency domain using an automated machine learning framework,
achieving a 93.95% accuracy, with a 0.838 F1 score for a 4 temporal EEG channel
setup. The achieved accuracy levels surpass state-of-the-art by nearly 20%.
Then, these algorithms are parallelized and optimized for a PULP platform,
achieving a 5.21 times improvement of energy-efficient compared to
state-of-the-art low-power implementations of artifact detection frameworks.
Combining this model with a low-power seizure detection algorithm would allow
for 300h of continuous monitoring on a 300 mAh battery in a wearable form
factor and power budget. These results pave the way for implementing
affordable, wearable, long-term epilepsy monitoring solutions with low
false-positive rates and high sensitivity, meeting both patients' and
caregivers' requirements.

### Title: C(X) determines X -- an inherent theory
* Paper ID: 2204.08833v1
* Paper URL: [http://arxiv.org/abs/2204.08833v1](http://arxiv.org/abs/2204.08833v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: One of the fundamental problem in rings of continuous function is to extract
those spaces for which C(X) determines X, that is to investigate X and Y such
that C(X) isomorphic with C(Y ) implies X homeomorphic with Y . The development
started back from Tychono? who first pointed out inevitability of Tychono?
space in this category of problem. Later S.Banach and M. Stone proved
independently with slight variance, that if X is compact Hausdor? space, C(X)
also determine X. Their works were maximally extended by E. Hewitt by
introducing realcompact spaces and later Melvin Henriksen and Biswajit Mitra
solved the problem for locally compact and nearly realcompact spaces. In this
paper we tried to develop an inherent theory of this problem to cover up all
the works in the literature introducing a notion so called P-compact spaces.

### Title: SmartSales: Sales Script Extraction and Analysis from Sales Chatlog
* Paper ID: 2204.08811v1
* Paper URL: [http://arxiv.org/abs/2204.08811v1](http://arxiv.org/abs/2204.08811v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In modern sales applications, automatic script extraction and management
greatly decrease the need for human labor to collect the winning sales scripts,
which largely boost the success rate for sales and can be shared across the
sales teams. In this work, we present the SmartSales system to serve both the
sales representatives and managers to attain the sales insights from the
large-scale sales chatlog. SmartSales consists of three modules: 1) Customer
frequently asked questions (FAQ) extraction aims to enrich the FAQ knowledge
base by harvesting high quality customer question-answer pairs from the
chatlog. 2) Customer objection response assists the salespeople to figure out
the typical customer objections and corresponding winning sales scripts, as
well as search for proper sales responses for a certain customer objection. 3)
Sales manager dashboard helps sales managers to monitor whether a specific
sales representative or team follows the sales standard operating procedures
(SOP). The proposed prototype system is empowered by the state-of-the-art
conversational intelligence techniques and has been running on the Tencent
Cloud to serve the sales teams from several different areas.

### Title: Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation
* Paper ID: 2204.08797v1
* Paper URL: [http://arxiv.org/abs/2204.08797v1](http://arxiv.org/abs/2204.08797v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/zhanglingming1/tsgcnet](https://github.com/zhanglingming1/tsgcnet)
* Summary: Precise segmentation of teeth from intra-oral scanner images is an essential
task in computer-aided orthodontic surgical planning. The state-of-the-art deep
learning-based methods often simply concatenate the raw geometric attributes
(i.e., coordinates and normal vectors) of mesh cells to train a single-stream
network for automatic intra-oral scanner image segmentation. However, since
different raw attributes reveal completely different geometric information, the
naive concatenation of different raw attributes at the (low-level) input stage
may bring unnecessary confusion in describing and differentiating between mesh
cells, thus hampering the learning of high-level geometric representations for
the segmentation task. To address this issue, we design a two-stream graph
convolutional network (i.e., TSGCN), which can effectively handle inter-view
confusion between different raw attributes to more effectively fuse their
complementary information and learn discriminative multi-view geometric
representations. Specifically, our TSGCN adopts two input-specific
graph-learning streams to extract complementary high-level geometric
representations from coordinates and normal vectors, respectively. Then, these
single-view representations are further fused by a self-attention module to
adaptively balance the contributions of different views in learning more
discriminative multi-view representations for accurate and fully automatic
tooth segmentation. We have evaluated our TSGCN on a real-patient dataset of
dental (mesh) models acquired by 3D intraoral scanners. Experimental results
show that our TSGCN significantly outperforms state-of-the-art methods in 3D
tooth (surface) segmentation. Github:
https://github.com/ZhangLingMing1/TSGCNet.

### Title: Enhancing CTR Prediction with Context-Aware Feature Representation Learning
* Paper ID: 2204.08758v1
* Paper URL: [http://arxiv.org/abs/2204.08758v1](http://arxiv.org/abs/2204.08758v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/frnetnetwork/frnet](https://github.com/frnetnetwork/frnet)
* Summary: CTR prediction has been widely used in the real world. Many methods model
feature interaction to improve their performance. However, most methods only
learn a fixed representation for each feature without considering the varying
importance of each feature under different contexts, resulting in inferior
performance. Recently, several methods tried to learn vector-level weights for
feature representations to address the fixed representation issue. However,
they only produce linear transformations to refine the fixed feature
representations, which are still not flexible enough to capture the varying
importance of each feature under different contexts. In this paper, we propose
a novel module named Feature Refinement Network (FRNet), which learns
context-aware feature representations at bit-level for each feature in
different contexts. FRNet consists of two key components: 1) Information
Extraction Unit (IEU), which captures contextual information and cross-feature
relationships to guide context-aware feature refinement; and 2) Complementary
Selection Gate (CSGate), which adaptively integrates the original and
complementary feature representations learned in IEU with bit-level weights.
Notably, FRNet is orthogonal to existing CTR methods and thus can be applied in
many existing methods to boost their performance. Comprehensive experiments are
conducted to verify the effectiveness, efficiency, and compatibility of FRNet.

### Title: Near K-Edge Photoionization and Photoabsorption of Singly, Doubly, and Triply Charged Silicon Ions
* Paper ID: 2204.08750v1
* Paper URL: [http://arxiv.org/abs/2204.08750v1](http://arxiv.org/abs/2204.08750v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Experimental and theoretical results are presented for double, triple, and
quadruple photoionization of Si$^+$ and Si$^{2+}$ ions and for double
photoionization of Si$^{3+}$ ions by a single photon. The experiments employed
the photon-ion merged-beams technique at a synchrotron light source. The
experimental photon-energy range 1835--1900 eV comprises resonances associated
with the excitation of a $1s$ electron to higher subshells and subsequent
autoionization. Energies, widths, and strengths of these resonances are
extracted from high-resolution photoionization measurements, and the core-hole
lifetime of K-shell ionized neutral silicon is inferred. In addition,
theoretical cross sections for photoabsorption and multiple photoionization
were obtained from large-scale Multi-Configuration Dirac-Hartree-Fock (MCDHF)
calculations. The present calculations agree with the experiment much better
than previously published theoretical results. The importance of an accurate
energy calibration of laboratory data is pointed out. The present benchmark
results are particularly useful for discriminating between silicon absorption
in the gaseous and in the solid component (dust grains) of the interstellar
medium.

### Title: Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition
* Paper ID: 2204.08747v1
* Paper URL: [http://arxiv.org/abs/2204.08747v1](http://arxiv.org/abs/2204.08747v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Sign language is a beautiful visual language and is also the primary language
used by speaking and hearing-impaired people. However, sign language has many
complex expressions, which are difficult for the public to understand and
master. Sign language recognition algorithms will significantly facilitate
communication between hearing-impaired people and normal people. Traditional
continuous sign language recognition often uses a sequence learning method
based on Convolutional Neural Network (CNN) and Long Short-Term Memory Network
(LSTM). These methods can only learn spatial and temporal features separately,
which cannot learn the complex spatial-temporal features of sign language. LSTM
is also difficult to learn long-term dependencies. To alleviate these problems,
this paper proposes a multi-view spatial-temporal continuous sign language
recognition network. The network consists of three parts. The first part is a
Multi-View Spatial-Temporal Feature Extractor Network (MSTN), which can
directly extract the spatial-temporal features of RGB and skeleton data; the
second is a sign language encoder network based on Transformer, which can learn
long-term dependencies; the third is a Connectionist Temporal Classification
(CTC) decoder network, which is used to predict the whole meaning of the
continuous sign language. Our algorithm is tested on two public sign language
datasets SLR-100 and PHOENIX-Weather 2014T (RWTH). As a result, our method
achieves excellent performance on both datasets. The word error rate on the
SLR-100 dataset is 1.9%, and the word error rate on the RWTHPHOENIX-Weather
dataset is 22.8%.

### Title: Scaling relations of z~0.25-1.5 galaxies in various environments from the morpho-kinematic analysis of the MAGIC sample
* Paper ID: 2204.08724v1
* Paper URL: [http://arxiv.org/abs/2204.08724v1](http://arxiv.org/abs/2204.08724v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The evolution of galaxies is influenced by many physical processes which may
vary depending on their environment. We combine Hubble Space Telescope (HST)
and Multi-Unit Spectroscopic Explorer (MUSE) data of galaxies at 0.25<z<1.5 to
probe the impact of environment on the size-mass relation, the Main Sequence
(MS) and the Tully-Fisher relation (TFR).
  We perform a morpho-kinematic modelling of 593 [Oii] emitters in various
environments in the COSMOS area from the MUSE-gAlaxy Groups In Cosmos (MAGIC)
survey. The HST F814W images are modelled with a bulge-disk decomposition to
estimate their bulge-disk ratio, effective radius and disk inclination. We use
the [Oii]{\lambda}{\lambda}3727, 3729 doublet to extract the ionised gas
kinematic maps from the MUSE cubes, and we model them for a sample of 146 [Oii]
emitters, with bulge and disk components constrained from morphology and a dark
matter halo.
  We find an offset of 0.03 dex on the size-mass relation zero point between
the field and the large structure subsamples, with a richness threshold of N=10
to separate between small and large structures, and of 0.06 dex with N=20.
Similarly, we find a 0.1 dex difference on the MS with N=10 and 0.15 dex with
N=20. These results suggest that galaxies in massive structures are smaller by
14% and have star formation rates reduced by a factor of 1.3-1.5 with respect
to field galaxies at z=0.7. Finally, we do not find any impact of the
environment on the TFR, except when using N=20 with an offset of 0.04 dex. We
discard the effect of quenching for the largest structures that would lead to
an offset in the opposite direction. We find that, at z=0.7, if quenching
impacts the mass budget of galaxies in structures, these galaxies would have
been affected quite recently, for roughly 0.7-1.5 Gyr. This result holds when
including the gas mass, but vanishes once we include the asymmetric drift
correction.

### Title: NAFSSR: Stereo Image Super-Resolution Using NAFNet
* Paper ID: 2204.08714v1
* Paper URL: [http://arxiv.org/abs/2204.08714v1](http://arxiv.org/abs/2204.08714v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/megvii-research/NAFNet](https://github.com/megvii-research/NAFNet)
* Summary: Stereo image super-resolution aims at enhancing the quality of
super-resolution results by utilizing the complementary information provided by
binocular systems. To obtain reasonable performance, most methods focus on
finely designing modules, loss functions, and etc. to exploit information from
another viewpoint. This has the side effect of increasing system complexity,
making it difficult for researchers to evaluate new ideas and compare methods.
This paper inherits a strong and simple image restoration model, NAFNet, for
single-view feature extraction and extends it by adding cross attention modules
to fuse features between views to adapt to binocular scenarios. The proposed
baseline for stereo image super-resolution is noted as NAFSSR. Furthermore,
training/testing strategies are proposed to fully exploit the performance of
NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In
particular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,
KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place
in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models
will be released at https://github.com/megvii-research/NAFNet.

### Title: Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing
* Paper ID: 2204.08707v1
* Paper URL: [http://arxiv.org/abs/2204.08707v1](http://arxiv.org/abs/2204.08707v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in remote sensing (RS). In this paper,
we focus our attention on cross-modal text-image retrieval, where queries from
one modality (e.g., text) can be matched to archive entries from another (e.g.,
image). Most of the existing cross-modal text-image retrieval systems in RS
require a high number of labeled training samples and also do not allow fast
and memory-efficient retrieval. These issues limit the applicability of the
existing cross-modal retrieval systems for large-scale applications in RS. To
address this problem, in this paper we introduce a novel unsupervised
cross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.
To this end, the proposed DUCH is made up of two main modules: 1) feature
extraction module, which extracts deep representations of two modalities; 2)
hashing module that learns to generate cross-modal binary hash codes from the
extracted representations. We introduce a novel multi-objective loss function
including: i) contrastive objectives that enable similarity preservation in
intra- and inter-modal similarities; ii) an adversarial objective that is
enforced across two modalities for cross-modal representation consistency; and
iii) binarization objectives for generating hash codes. Experimental results
show that the proposed DUCH outperforms state-of-the-art methods. Our code is
publicly available at https://git.tu-berlin.de/rsim/duch.

### Title: Multi-shot Solution Prediction for Combinatorial Optimization
* Paper ID: 2204.08700v1
* Paper URL: [http://arxiv.org/abs/2204.08700v1](http://arxiv.org/abs/2204.08700v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: This paper aims to predict optimal solutions for combinatorial optimization
problems (COPs) via machine learning (ML). To find high-quality solutions
efficiently, existing methods use a ML model to predict the optimal solution
and use the ML prediction to guide the search. Prediction of the optimal
solution to sufficient accuracy is critical, however it is challenging due to
the high complexity of COPs. Nevertheless, these existing methods are
single-shot, i.e., predicting the optimal solution only for once. This paper
proposes a framework that enables a ML model to predict the optimal solution in
multiple shots, namely multi-shot solution prediction (MSSP), which can improve
the quality of a ML prediction by harnessing feedback from search.
Specifically, we employ a set of statistical measures as features, to extract
useful information from feasible solutions found by the search method and
inform the ML model as to which value a decision variable is likely to take in
high-quality solutions. Our experiments on three NP-hard COPs show that MSSP
improves the quality of a ML prediction substantially and achieves competitive
results as compared with other search methods in terms of solution quality.
Furthermore, we demonstrate that MSSP can be used as a pricing heuristic for
column generation, to boost a branch-and-price algorithm for solving the graph
coloring problem.

### Title: A generalized echo squeezing protocol with near-Heisenberg limit sensitivity and strong robustness against excess noise and variation in squeezing parameter
* Paper ID: 2204.08681v1
* Paper URL: [http://arxiv.org/abs/2204.08681v1](http://arxiv.org/abs/2204.08681v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We present a generalized echo squeezing protocol (GESP) as a generalization
of the Schr\"odinger cat state protocol (SCSP) with the value of the squeezing
parameter being an arbitrary number rather than pi/2. We show analytically that
over a broad range of the squeezing parameter the sensitivity reaches the
Heisenberg limit within a factor of root-2. For a large number of particles, N,
this plateau interval is almost the whole range from zero to pi/2, and the
sensitivity is independent of the parity of N. Therefore, it is possible to
operate a sensor over wide interval of the squeezing parameter without changing
the sensitivity. This is to be contrasted with the conventional echo squeezing
protocol (CESP) which only works for a very small interval of the squeezing
parameter. We also show that, in contrast to the CESP, the sensitivity of the
GESP is close to the quantum Cram\'er-Rao bound over the whole range of the
squeezing parameter, indicating that the phase shift information is
near-optimally extracted. We find that the enhancement in sensitivity in the
case of the GESP is due to a combination of two parameters: the phase
magnification (PMF) and the noise amplification factor (NAF). As the value of
the squeezing parameter increases, both PMF and NAF increase, while keeping the
ratio of PMF/NAF essentially constant, yielding a net enhancement of
sensitivity that at the Heisenberg limit within a factor of root-2 over the
whole plateau interval. An important consequence of this behavior is that the
robustness of the GESP against excess noise easily exceeds that of the CESP for
a broad range of values of the squeezing parameter. As such, in the context of
an experimental study, it should be possible to achieve a net enhancement in
sensitivity higher than that for the CESP, under typical conditions where the
excess noise exceeds the unsqueezed quantum projection noise.

### Title: The Electromagnetic Characteristics of the Tianlai Cylindrical Pathfinder Array
* Paper ID: 2204.08632v1
* Paper URL: [http://arxiv.org/abs/2204.08632v1](http://arxiv.org/abs/2204.08632v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: A great challenge for 21 cm intensity mapping experiments is the strong
foreground radiation which is orders of magnitude brighter than the 21cm
signal. Removal of the foreground takes advantage of the fact that its
frequency spectrum is smooth while the redshifted 21cm signal spectrum is
stochastic. However, a complication is the non-smoothness of the instrument
response. This paper describes the electromagnetic simulation of the Tianlai
cylinder array, a pathfinder for 21 cm intensity mapping experiments. Due to
the vast scales involved, a direct simulation requires large amount of
computing resources. We have made the simulation practical by using a
combination of methods: first simulate a single feed, then an array of feed
units, finally with the feed array and a cylindrical reflector together, to
obtain the response for a single cylinder. We studied its radiation pattern,
bandpass response and the effects of mutual coupling between feed units, and
compared the results with observation. Many features seen in the measurement
result are well reproduced in the simulation, especially the oscillatory
features which are associated with the standing waves on the reflector. The
mutual coupling between feed units is quantified with S-parameters, which
decrease as the distance between the two feeds increases. Based on the
simulated S-parameters, we estimate the correlated noise which has been seen in
the visibility data, the results show very good agreement with the data in both
magnitude and frequency structures. These results provide useful insights on
the problem of 21cm signal extraction for real instruments.

### Title: A Subject-Independent Brain-Computer Interface Framework Based on Supervised Autoencoder
* Paper ID: 2204.08626v1
* Paper URL: [http://arxiv.org/abs/2204.08626v1](http://arxiv.org/abs/2204.08626v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: A calibration procedure is required in motor imagery-based brain-computer
interface (MI-BCI) to tune the system for new users. This procedure is
time-consuming and prevents na\"ive users from using the system immediately.
Developing a subject-independent MI-BCI system to reduce the calibration phase
is still challenging due to the subject-dependent characteristics of the MI
signals. Many algorithms based on machine learning and deep learning have been
developed to extract high-level features from the MI signals to improve the
subject-to-subject generalization of a BCI system. However, these methods are
based on supervised learning and extract features useful for discriminating
various MI signals. Hence, these approaches cannot find the common underlying
patterns in the MI signals and their generalization level is limited. This
paper proposes a subject-independent MI-BCI based on a supervised autoencoder
(SAE) to circumvent the calibration phase. The suggested framework is validated
on dataset 2a from BCI competition IV. The simulation results show that our
SISAE model outperforms the conventional and widely used BCI algorithms, common
spatial and filter bank common spatial patterns, in terms of the mean Kappa
value, in eight out of nine subjects.

