### Title: Fairness and Bias in Robot Learning
* Paper ID: 2207.03444v1
* Paper URL: [http://arxiv.org/abs/2207.03444v1](http://arxiv.org/abs/2207.03444v1)
* Updated Date: 2022-07-07
* Categories: ['cs.RO', 'cs.AI', 'cs.CV', 'cs.CY', 'cs.LG']
* Code URL: null
* Summary: Machine learning has significantly enhanced the abilities of robots, enabling
them to perform a wide range of tasks in human environments and adapt to our
uncertain real world. Recent works in various domains of machine learning have
highlighted the importance of accounting for fairness to ensure that these
algorithms do not reproduce human biases and consequently lead to
discriminatory outcomes. With robot learning systems increasingly performing
more and more tasks in our everyday lives, it is crucial to understand the
influence of such biases to prevent unintended behavior toward certain groups
of people. In this work, we present the first survey on fairness in robot
learning from an interdisciplinary perspective spanning technical, ethical, and
legal challenges. We propose a taxonomy for sources of bias and the resulting
types of discrimination due to them. Using examples from different robot
learning domains, we examine scenarios of unfair outcomes and strategies to
mitigate them. We present early advances in the field by covering different
fairness definitions, ethical and legal considerations, and methods for fair
robot learning. With this work, we aim at paving the road for groundbreaking
developments in fair robot learning.

### Title: Group Fairness in Adaptive Submodular Maximization
* Paper ID: 2207.03364v1
* Paper URL: [http://arxiv.org/abs/2207.03364v1](http://arxiv.org/abs/2207.03364v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.DS']
* Code URL: null
* Summary: In this paper, we study the classic submodular maximization problem subject
to a group fairness constraint under both non-adaptive and adaptive settings.
It has been shown that the utility function of many machine learning
applications, including data summarization, influence maximization in social
networks, and personalized recommendation, satisfies the property of
submodularity. Hence, maximizing a submodular function subject to various
constraints can be found at the heart of many of those applications. On a high
level, submodular maximization aims to select a group of most representative
items (e.g., data points). However, the design of most existing algorithms does
not incorporate the fairness constraint, leading to under- or
over-representation some particular groups. This motivates us to study the fair
submodular maximization problem, where we aim to select a group of items to
maximize a (possibly non-monotone) submodular utility function subject to a
group fairness constraint. To this end, we develop the first constant-factor
approximation algorithm for this problem. The design of our algorithm is robust
enough to be extended to solving the submodular maximization problem under a
more complicated adaptive setting. Moreover, we further extend our study to
incorporating a global cardinality constraint.

### Title: Calibrate to Interpret
* Paper ID: 2207.03324v1
* Paper URL: [http://arxiv.org/abs/2207.03324v1](http://arxiv.org/abs/2207.03324v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'I.2.6; I.2.10; I.4.8; I.5.2']
* Code URL: [https://github.com/euranova/calibrate_to_interpret](https://github.com/euranova/calibrate_to_interpret)
* Summary: Trustworthy machine learning is driving a large number of ML community works
in order to improve ML acceptance and adoption. The main aspect of trustworthy
machine learning are the followings: fairness, uncertainty, robustness,
explainability and formal guaranties. Each of these individual domains gains
the ML community interest, visible by the number of related publications.
However few works tackle the interconnection between these fields. In this
paper we show a first link between uncertainty and explainability, by studying
the relation between calibration and interpretation. As the calibration of a
given model changes the way it scores samples, and interpretation approaches
often rely on these scores, it seems safe to assume that the
confidence-calibration of a model interacts with our ability to interpret such
model. In this paper, we show, in the context of networks trained on image
classification tasks, to what extent interpretations are sensitive to
confidence-calibration. It leads us to suggest a simple practice to improve the
interpretation outcomes: Calibrate to Interpret.

### Title: A Comprehensive Empirical Study of Bias Mitigation Methods for Software Fairness
* Paper ID: 2207.03277v1
* Paper URL: [http://arxiv.org/abs/2207.03277v1](http://arxiv.org/abs/2207.03277v1)
* Updated Date: 2022-07-07
* Categories: ['cs.SE', 'cs.AI']
* Code URL: null
* Summary: Software bias is an increasingly important operational concern for software
engineers. We present a large-scale, comprehensive empirical evaluation of 17
representative bias mitigation methods, evaluated with 12 Machine Learning (ML)
performance metrics, 4 fairness metrics, and 24 types of fairness-performance
trade-off assessment, applied to 8 widely-adopted benchmark software
decision/prediction tasks. The empirical coverage is comprehensive, covering
the largest numbers of bias mitigation methods, evaluation metrics, and
fairness-performance trade-off measures compared to previous work on this
important operational software characteristic. We find that (1) the bias
mitigation methods significantly decrease the values reported by all ML
performance metrics (including those not considered in previous work) in a
large proportion of the scenarios studied (42%~75% according to different ML
performance metrics); (2) the bias mitigation methods achieve fairness
improvement in only approximately 50% over all scenarios and metrics (ranging
between 29%~59% according to the metric used to asses bias/fairness); (3) the
bias mitigation methods have a poor fairness-performance trade-off or even lead
to decreases in both fairness and ML performance in 37% of the scenarios; (4)
the effectiveness of the bias mitigation methods depends on tasks, models, and
fairness and ML performance metrics, and there is no 'silver bullet' bias
mitigation method demonstrated to be effective for all scenarios studied. The
best bias mitigation method that we find outperforms other methods in only 29%
of the scenarios. We have made publicly available the scripts and data used in
this study in order to allow for future replication and extension of our work.

