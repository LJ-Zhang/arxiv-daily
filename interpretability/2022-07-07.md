### Title: Machine Learning to Predict Aerodynamic Stall
* Paper ID: 2207.03424v1
* Paper URL: [http://arxiv.org/abs/2207.03424v1](http://arxiv.org/abs/2207.03424v1)
* Updated Date: 2022-07-07
* Categories: ['physics.flu-dyn', 'cs.LG']
* Code URL: null
* Summary: A convolutional autoencoder is trained using a database of airfoil
aerodynamic simulations and assessed in terms of overall accuracy and
interpretability. The goal is to predict the stall and to investigate the
ability of the autoencoder to distinguish between the linear and non-linear
response of the airfoil pressure distribution to changes in the angle of
attack. After a sensitivity analysis on the learning infrastructure, we
investigate the latent space identified by the autoencoder targeting extreme
compression rates, i.e. very low-dimensional reconstructions. We also propose a
strategy to use the decoder to generate new synthetic airfoil geometries and
aerodynamic solutions by interpolation and extrapolation in the latent
representation learned by the autoencoder.

### Title: VecGAN: Image-to-Image Translation with Interpretable Latent Directions
* Paper ID: 2207.03411v1
* Paper URL: [http://arxiv.org/abs/2207.03411v1](http://arxiv.org/abs/2207.03411v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV', 'cs.AI', 'cs.LG']
* Code URL: null
* Summary: We propose VecGAN, an image-to-image translation framework for facial
attribute editing with interpretable latent directions. Facial attribute
editing task faces the challenges of precise attribute editing with
controllable strength and preservation of the other attributes of an image. For
this goal, we design the attribute editing by latent space factorization and
for each attribute, we learn a linear direction that is orthogonal to the
others. The other component is the controllable strength of the change, a
scalar value. In our framework, this scalar can be either sampled or encoded
from a reference image by projection. Our work is inspired by the latent space
factorization works of fixed pretrained GANs. However, while those models
cannot be trained end-to-end and struggle to edit encoded images precisely,
VecGAN is end-to-end trained for image translation task and successful at
editing an attribute while preserving the others. Our extensive experiments
show that VecGAN achieves significant improvements over state-of-the-arts for
both local and global edits.

### Title: Calibrate to Interpret
* Paper ID: 2207.03324v1
* Paper URL: [http://arxiv.org/abs/2207.03324v1](http://arxiv.org/abs/2207.03324v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'I.2.6; I.2.10; I.4.8; I.5.2']
* Code URL: [https://github.com/euranova/calibrate_to_interpret](https://github.com/euranova/calibrate_to_interpret)
* Summary: Trustworthy machine learning is driving a large number of ML community works
in order to improve ML acceptance and adoption. The main aspect of trustworthy
machine learning are the followings: fairness, uncertainty, robustness,
explainability and formal guaranties. Each of these individual domains gains
the ML community interest, visible by the number of related publications.
However few works tackle the interconnection between these fields. In this
paper we show a first link between uncertainty and explainability, by studying
the relation between calibration and interpretation. As the calibration of a
given model changes the way it scores samples, and interpretation approaches
often rely on these scores, it seems safe to assume that the
confidence-calibration of a model interacts with our ability to interpret such
model. In this paper, we show, in the context of networks trained on image
classification tasks, to what extent interpretations are sensitive to
confidence-calibration. It leads us to suggest a simple practice to improve the
interpretation outcomes: Calibrate to Interpret.

### Title: A unified interpretable intelligent learning diagnosis framework for smart education
* Paper ID: 2207.03122v1
* Paper URL: [http://arxiv.org/abs/2207.03122v1](http://arxiv.org/abs/2207.03122v1)
* Updated Date: 2022-07-07
* Categories: ['cs.AI', 'cs.CY']
* Code URL: [https://github.com/ccnuzfw/ldm-id-hmi](https://github.com/ccnuzfw/ldm-id-hmi)
* Summary: Intelligent learning diagnosis is a critical engine of smart education, which
aims to estimate learners' current knowledge mastery status and predict their
future learning performance. The significant challenge with traditional
learning diagnosis methods is the inability to balance diagnostic accuracy and
interpretability. To settle the above problem, the proposed unified
interpretable intelligent learning diagnosis framework, which benefits from the
powerful representation learning ability of deep learning and the
interpretability of psychometric, achieves good performance of learning
prediction and provides interpretability from three aspects: cognitive
parameters, learner-resource response network, and weights of self-attention
mechanism. Within the proposed framework, this paper proposes a two-channel
learning diagnosis mechanism LDM-ID as well as a three-channel learning
diagnosis mechanism LDM-HMI. Experiments on two real-world datasets and a
simulation dataset show that our method has higher accuracy in predicting
learners' performances compared with the state-of-the-art models, and can
provide valuable educational interpretabilities for applications such as
precise learning resource recommendation and personalized learning tutoring in
smart education.

### Title: Equivariant Representation Learning via Class-Pose Decomposition
* Paper ID: 2207.03116v1
* Paper URL: [http://arxiv.org/abs/2207.03116v1](http://arxiv.org/abs/2207.03116v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'math.GR']
* Code URL: [https://github.com/equivariant-ml/equivariant-representation-learning](https://github.com/equivariant-ml/equivariant-representation-learning)
* Summary: We introduce a general method for learning representations that are
equivariant to symmetries of data. The central idea is to to decompose the
latent space in an invariant factor and the symmetry group itself. The
components semantically correspond to intrinsic data classes and poses
respectively. The learner is self-supervised and infers these semantics based
on relative symmetry information. The approach is motivated by theoretical
results from group theory and guarantees representations that are lossless,
interpretable and disentangled. We empirically investigate the approach via
experiments involving datasets with a variety of symmetries. Results show that
our representations capture the geometry of data and outperform other
equivariant representation learning frameworks.

### Title: An Additive Instance-Wise Approach to Multi-class Model Interpretation
* Paper ID: 2207.03113v1
* Paper URL: [http://arxiv.org/abs/2207.03113v1](http://arxiv.org/abs/2207.03113v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.AI']
* Code URL: [https://github.com/isvy08/aim](https://github.com/isvy08/aim)
* Summary: Interpretable machine learning offers insights into what factors drive a
certain prediction of a black-box system and whether to trust it for
high-stakes decisions or large-scale deployment. Existing methods mainly focus
on selecting explanatory input features, which follow either locally additive
or instance-wise approaches. Additive models use heuristically sampled
perturbations to learn instance-specific explainers sequentially. The process
is thus inefficient and susceptible to poorly-conditioned samples. Meanwhile,
instance-wise techniques directly learn local sampling distributions and can
leverage global information from other inputs. However, they can only interpret
single-class predictions and suffer from inconsistency across different
settings, due to a strict reliance on a pre-defined number of features
selected. This work exploits the strengths of both methods and proposes a
global framework for learning local explanations simultaneously for multiple
target classes. We also propose an adaptive inference strategy to determine the
optimal number of features for a specific instance. Our model explainer
significantly outperforms additive and instance-wise counterparts on
faithfulness while achieves high level of brevity on various data sets and
black-box model architectures.

