<h2>diffusion</h2>
<h3>Title: Text-image Alignment for Diffusion-based Perception. (arXiv:2310.00031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00031">http://arxiv.org/abs/2310.00031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00031] Text-image Alignment for Diffusion-based Perception](http://arxiv.org/abs/2310.00031) #diffusion</code></li>
<li>Summary: <p>Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving.
</p></li>
</ul>
<h3>Title: Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks. (arXiv:2310.00076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00076">http://arxiv.org/abs/2310.00076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00076] Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks](http://arxiv.org/abs/2310.00076) #diffusion</code></li>
<li>Summary: <p>In light of recent advancements in generative AI models, it has become
essential to distinguish genuine content from AI-generated one to prevent the
malicious usage of fake materials as authentic ones and vice versa. Various
techniques have been introduced for identifying AI-generated images, with
watermarking emerging as a promising approach. In this paper, we analyze the
robustness of various AI-image detectors including watermarking and
classifier-based deepfake detectors. For watermarking methods that introduce
subtle image perturbations (i.e., low perturbation budget methods), we reveal a
fundamental trade-off between the evasion error rate (i.e., the fraction of
watermarked images detected as non-watermarked ones) and the spoofing error
rate (i.e., the fraction of non-watermarked images detected as watermarked
ones) upon an application of a diffusion purification attack. In this regime,
we also empirically show that diffusion purification effectively removes
watermarks with minimal changes to images. For high perturbation watermarking
methods where notable changes are applied to images, the diffusion purification
attack is not effective. In this case, we develop a model substitution
adversarial attack that can successfully remove watermarks. Moreover, we show
that watermarking methods are vulnerable to spoofing attacks where the attacker
aims to have real images (potentially obscene) identified as watermarked ones,
damaging the reputation of the developers. In particular, by just having
black-box access to the watermarking method, we show that one can generate a
watermarked noise image which can be added to the real images to have them
falsely flagged as watermarked ones. Finally, we extend our theory to
characterize a fundamental trade-off between the robustness and reliability of
classifier-based deep fake detectors and demonstrate it through experiments.
</p></li>
</ul>
<h3>Title: Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00096">http://arxiv.org/abs/2310.00096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00096] Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation](http://arxiv.org/abs/2310.00096) #diffusion</code></li>
<li>Summary: <p>Diffusion models showcased strong capabilities in image synthesis, being used
in many computer vision tasks with great success. To this end, we propose to
explore a new use case, namely to copy black-box classification models without
having access to the original training data, the architecture, and the weights
of the model, \ie~the model is only exposed through an inference API. More
specifically, we can only observe the (soft or hard) labels for some image
samples passed as input to the model. Furthermore, we consider an additional
constraint limiting the number of model calls, mostly focusing our research on
few-call model stealing. In order to solve the model extraction task given the
applied restrictions, we propose the following framework. As training data, we
create a synthetic data set (called proxy data set) by leveraging the ability
of diffusion models to generate realistic and diverse images. Given a maximum
number of allowed API calls, we pass the respective number of samples through
the black-box model to collect labels. Finally, we distill the knowledge of the
black-box teacher (attacked model) into a student model (copy of the attacked
model), harnessing both labeled and unlabeled data generated by the diffusion
model. We employ a novel active self-paced learning framework to make the most
of the proxy data during distillation. Our empirical results on two data sets
confirm the superiority of our framework over two state-of-the-art methods in
the few-call model extraction scenario.
</p></li>
</ul>
<h3>Title: FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery. (arXiv:2310.00106v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00106">http://arxiv.org/abs/2310.00106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00106] FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery](http://arxiv.org/abs/2310.00106) #diffusion</code></li>
<li>Summary: <p>Our study introduces a new image-to-video generator called FashionFlow. By
utilising a diffusion model, we are able to create short videos from still
images. Our approach involves developing and connecting relevant components
with the diffusion model, which sets our work apart. The components include the
use of pseudo-3D convolutional layers to generate videos efficiently. VAE and
CLIP encoders capture vital characteristics from still images to influence the
diffusion model. Our research demonstrates a successful synthesis of fashion
videos featuring models posing from various angles, showcasing the fit and
appearance of the garment. Our findings hold great promise for improving and
enhancing the shopping experience for the online fashion industry.
</p></li>
</ul>
<h3>Title: Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00224">http://arxiv.org/abs/2310.00224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00224] Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis](http://arxiv.org/abs/2310.00224) #diffusion</code></li>
<li>Summary: <p>Conditional generative models typically demand large annotated training sets
to achieve high-quality synthesis. As a result, there has been significant
interest in designing models that perform plug-and-play generation, i.e., to
use a predefined or pretrained model, which is not explicitly trained on the
generative task, to guide the generative process (e.g., using language).
However, such guidance is typically useful only towards synthesizing high-level
semantics rather than editing fine-grained details as in image-to-image
translation tasks. To this end, and capitalizing on the powerful fine-grained
generative control offered by the recent diffusion-based generative models, we
introduce Steered Diffusion, a generalized framework for photorealistic
zero-shot conditional image generation using a diffusion model trained for
unconditional generation. The key idea is to steer the image generation of the
diffusion model at inference time via designing a loss using a pre-trained
inverse model that characterizes the conditional task. This loss modulates the
sampling trajectory of the diffusion process. Our framework allows for easy
incorporation of multiple conditions during inference. We present experiments
using steered diffusion on several tasks including inpainting, colorization,
text-guided semantic editing, and image super-resolution. Our results
demonstrate clear qualitative and quantitative improvements over
state-of-the-art diffusion-based plug-and-play models while adding negligible
additional computational cost.
</p></li>
</ul>
<h3>Title: On the Counting of Involutory MDS Matrices. (arXiv:2310.00090v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00090">http://arxiv.org/abs/2310.00090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00090] On the Counting of Involutory MDS Matrices](http://arxiv.org/abs/2310.00090) #diffusion</code></li>
<li>Summary: <p>The optimal branch number of MDS matrices has established their prominence in
the design of diffusion layers for various block ciphers and hash functions.
Consequently, several matrix structures have been proposed for designing MDS
matrices, including Hadamard and circulant matrices. In this paper, we first
provide the count of Hadamard MDS matrices of order $4$ over the field
$\mathbb{F}<em>{2^r}$. Subsequently, we present the counts of order $2$ MDS
matrices and order $2$ involutory MDS matrices over the field
$\mathbb{F}</em>{2^r}$. Finally, leveraging these counts of order $2$ matrices, we
derive an upper bound for the number of all involutory MDS matrices of order
$4$ over $\mathbb{F}_{2^r}$.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic Objects. (arXiv:2310.00011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00011">http://arxiv.org/abs/2310.00011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00011] Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic Objects](http://arxiv.org/abs/2310.00011) #self-supervised</code></li>
<li>Summary: <p>Significant attention has been attracted to deep learning-based depth
estimates. Dynamic objects become the most hard problems in
inter-frame-supervised depth estimates due to the uncertainty in adjacent
frames. Thus, integrating optical flow information with depth estimation is a
feasible solution, as the optical flow is an essential motion representation.
In this work, we construct a joint inter-frame-supervised depth and optical
flow estimation framework, which predicts depths in various motions by
minimizing pixel wrap errors in bilateral photometric re-projections and
optical vectors. For motion segmentation, we adaptively segment the preliminary
estimated optical flow map with large areas of connectivity. In self-supervised
depth estimation, different motion regions are predicted independently and then
composite into a complete depth. Further, the pose and depth estimations
re-synthesize the optical flow maps, serving to compute reconstruction errors
with the preliminary predictions. Our proposed joint depth and optical flow
estimation outperforms existing depth estimators on the KITTI Depth dataset,
both with and without Cityscapes pretraining. Additionally, our optical flow
results demonstrate competitive performance on the KITTI Flow 2015 dataset.
</p></li>
</ul>
<h3>Title: Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding. (arXiv:2310.00022v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00022">http://arxiv.org/abs/2310.00022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00022] Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding](http://arxiv.org/abs/2310.00022) #self-supervised</code></li>
<li>Summary: <p>Learning representations through self-supervision on a large-scale, unlabeled
dataset has proven to be highly effective for understanding diverse images,
such as those used in remote sensing image analysis. However, remote sensing
images often have complex and densely populated scenes, with multiple land
objects and no clear foreground objects. This intrinsic property can lead to
false positive pairs in contrastive learning, or missing contextual information
in reconstructive learning, which can limit the effectiveness of existing
self-supervised learning methods. To address these problems, we propose a
prompt-enhanced self-supervised representation learning method that uses a
simple yet efficient pre-training pipeline. Our approach involves utilizing
original image patches as a reconstructive prompt template, and designing a
prompt-enhanced generative branch that provides contextual information through
semantic consistency constraints. We collected a dataset of over 1.28 million
remote sensing images that is comparable to the popular ImageNet dataset, but
without specific temporal or geographical constraints. Our experiments show
that our method outperforms fully supervised learning models and
state-of-the-art self-supervised learning methods on various downstream tasks,
including land cover classification, semantic segmentation, object detection,
and instance segmentation. These results demonstrate that our approach learns
impressive remote sensing representations with high generalization and
transferability.
</p></li>
</ul>
<h3>Title: LSOR: Longitudinally-Consistent Self-Organized Representation Learning. (arXiv:2310.00213v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00213">http://arxiv.org/abs/2310.00213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00213] LSOR: Longitudinally-Consistent Self-Organized Representation Learning](http://arxiv.org/abs/2310.00213) #self-supervised</code></li>
<li>Summary: <p>Interpretability is a key issue when applying deep learning models to
longitudinal brain MRIs. One way to address this issue is by visualizing the
high-dimensional latent spaces generated by deep learning via self-organizing
maps (SOM). SOM separates the latent space into clusters and then maps the
cluster centers to a discrete (typically 2D) grid preserving the
high-dimensional relationship between clusters. However, learning SOM in a
high-dimensional latent space tends to be unstable, especially in a
self-supervision setting. Furthermore, the learned SOM grid does not
necessarily capture clinically interesting information, such as brain age. To
resolve these issues, we propose the first self-supervised SOM approach that
derives a high-dimensional, interpretable representation stratified by brain
age solely based on longitudinal brain MRIs (i.e., without demographic or
cognitive information). Called Longitudinally-consistent Self-Organized
Representation learning (LSOR), the method is stable during training as it
relies on soft clustering (vs. the hard cluster assignments used by existing
SOM). Furthermore, our approach generates a latent space stratified according
to brain age by aligning trajectories inferred from longitudinal MRIs to the
reference vector associated with the corresponding SOM cluster. When applied to
longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632), LSOR generates an interpretable latent space and achieves comparable or
higher accuracy than the state-of-the-art representations with respect to the
downstream tasks of classification (static vs. progressive mild cognitive
impairment) and regression (determining ADAS-Cog score of all subjects). The
code is available at
https://github.com/ouyangjiahong/longitudinal-som-single-modality.
</p></li>
</ul>
<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00158">http://arxiv.org/abs/2310.00158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00158] Feedback-guided Data Synthesis for Imbalanced Classification](http://arxiv.org/abs/2310.00158) #generative</code></li>
<li>Summary: <p>Current status quo in machine learning is to use static datasets of real
images for training, which often come from long-tailed distributions. With the
recent advances in generative models, researchers have started augmenting these
static datasets with synthetic data, reporting moderate performance
improvements on classification tasks. We hypothesize that these performance
gains are limited by the lack of feedback from the classifier to the generative
model, which would promote the usefulness of the generated samples to improve
the classifier's performance. In this work, we introduce a framework for
augmenting static datasets with useful synthetic samples, which leverages
one-shot feedback from the classifier to drive the sampling of the generative
model. In order for the framework to be effective, we find that the samples
must be close to the support of the real data of the task at hand, and be
sufficiently diverse. We validate three feedback criteria on a long-tailed
dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On
ImageNet-LT, we achieve state-of-the-art results, with over 4 percent
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts of
over 5 percent in worst group accuracy. With these results, our framework paves
the path towards effectively leveraging state-of-the-art text-to-image models
as data sources that can be queried to improve downstream applications.
</p></li>
</ul>
<h3>Title: Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00105">http://arxiv.org/abs/2310.00105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00105] Latent Space Symmetry Discovery](http://arxiv.org/abs/2310.00105) #generative</code></li>
<li>Summary: <p>Equivariant neural networks require explicit knowledge of the symmetry group.
Automatic symmetry discovery methods aim to relax this constraint and learn
invariance and equivariance from data. However, existing symmetry discovery
methods are limited to linear symmetries in their search space and cannot
handle the complexity of symmetries in real-world, often high-dimensional data.
We propose a novel generative model, Latent LieGAN (LaLiGAN), which can
discover nonlinear symmetries from data. It learns a mapping from data to a
latent space where the symmetries become linear and simultaneously discovers
symmetries in the latent space. Theoretically, we show that our method can
express any nonlinear symmetry under certain conditions. Experimentally, our
method can capture the intrinsic symmetry in high-dimensional observations,
which results in a well-structured latent space that is useful for other
downstream tasks. We demonstrate the use cases for LaLiGAN in improving
equation discovery and long-term forecasting for various dynamical systems.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks. (arXiv:2310.00144v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00144">http://arxiv.org/abs/2310.00144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00144] Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks](http://arxiv.org/abs/2310.00144) #anomaly</code></li>
<li>Summary: <p>The rapid evolution of the Ethereum network necessitates sophisticated
techniques to ensure its robustness against potential threats and to maintain
transparency. While Graph Neural Networks (GNNs) have pioneered anomaly
detection in such platforms, capturing the intricacies of both spatial and
temporal transactional patterns has remained a challenge. This study presents a
fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW)
enhanced by probabilistic sampling to bridge this gap. Our approach, unlike
traditional GCNs, leverages the strengths of TRW to discern complex temporal
sequences in Ethereum transactions, thereby providing a more nuanced
transaction anomaly detection mechanism. Preliminary evaluations demonstrate
that our TRW-GCN framework substantially advances the performance metrics over
conventional GCNs in detecting anomalies and transaction bursts. This research
not only underscores the potential of temporal cues in Ethereum transactional
data but also offers a scalable and effective methodology for ensuring the
security and transparency of decentralized platforms. By harnessing both
spatial relationships and time-based transactional sequences as node features,
our model introduces an additional layer of granularity, making the detection
process more robust and less prone to false positives. This work lays the
foundation for future research aimed at optimizing and enhancing the
transparency of blockchain technologies, and serves as a testament to the
significance of considering both time and space dimensions in the ever-evolving
landscape of the decentralized platforms.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00149">http://arxiv.org/abs/2310.00149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00149] One for All: Towards Training One Graph Model for All Classification Tasks](http://arxiv.org/abs/2310.00149) #in-context</code></li>
<li>Summary: <p>Designing a single model that addresses multiple tasks has been a
long-standing objective in artificial intelligence. Recently, large language
models have demonstrated exceptional capability in integrating and solving
different tasks within the language domain. However, a unified model for
various tasks on graphs remains underexplored, primarily due to the challenges
unique to the graph learning domain. First, graph data from different areas
carry distinct attributes and follow different distributions. Such discrepancy
makes it hard to represent graphs in a single representation space. Second,
tasks on graphs diversify into node, link, and graph tasks, requiring distinct
embedding strategies. Finally, an appropriate graph prompting paradigm for
in-context learning is unclear. Striving to handle all the aforementioned
challenges, we propose One for All (OFA), the first general framework that can
use a single graph model to address the above challenges. Specifically, OFA
proposes text-attributed graphs to unify different graph data by describing
nodes and edges with natural language and uses language models to encode the
diverse and possibly cross-domain text attributes to feature vectors in the
same embedding space. Furthermore, OFA introduces the concept of
nodes-of-interest to standardize different tasks with a single task
representation. For in-context learning on graphs, OFA introduces a novel graph
prompting paradigm that appends prompting substructures to the input graph,
which enables it to address varied tasks without fine-tuning. We train the OFA
model using graph data from multiple domains (including citation networks,
molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its
ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs
well across different tasks, making it the first general-purpose graph
classification model across domains.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm. (arXiv:2310.00178v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00178">http://arxiv.org/abs/2310.00178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00178] Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm](http://arxiv.org/abs/2310.00178) #memory</code></li>
<li>Summary: <p>Contextual biasing refers to the problem of biasing the automatic speech
recognition (ASR) systems towards rare entities that are relevant to the
specific user or application scenarios. We propose algorithms for contextual
biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During
beam search, we boost the score of a token extension if it extends matching
into a set of biasing phrases. Our method simulates the classical approaches
often implemented in the weighted finite state transducer (WFST) framework, but
avoids the FST language altogether, with careful considerations on memory
footprint and efficiency on tensor processing units (TPUs) by vectorization.
Without introducing additional model parameters, our method achieves
significant word error rate (WER) reductions on biasing test sets by itself,
and yields further performance gain when combined with a model-based biasing
method.
</p></li>
</ul>
<h3>Title: LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00035">http://arxiv.org/abs/2310.00035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00035] LoRA ensembles for large language model fine-tuning](http://arxiv.org/abs/2310.00035) #memory</code></li>
<li>Summary: <p>Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.
</p></li>
</ul>
<h3>Title: Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs. (arXiv:2310.00120v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00120">http://arxiv.org/abs/2310.00120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00120] Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs](http://arxiv.org/abs/2310.00120) #memory</code></li>
<li>Summary: <p>Memory complexity and data scarcity have so far prohibited learning solution
operators of partial differential equations (PDEs) at high resolutions. We
address these limitations by introducing a new data efficient and highly
parallelizable operator learning approach with reduced memory requirement and
better generalization, called multi-grid tensorized neural operator (MG-TFNO).
MG-TFNO scales to large resolutions by leveraging local and global structures
of full-scale, real-world phenomena, through a decomposition of both the input
domain and the operator's parameter space. Our contributions are threefold: i)
we enable parallelization over input samples with a novel multi-grid-based
domain decomposition, ii) we represent the parameters of the model in a
high-order latent subspace of the Fourier domain, through a global tensor
factorization, resulting in an extreme reduction in the number of parameters
and improved generalization, and iii) we propose architectural improvements to
the backbone FNO. Our approach can be used in any operator learning setting. We
demonstrate superior performance on the turbulent Navier-Stokes equations where
we achieve less than half the error with over 150x compression. The
tensorization combined with the domain decomposition, yields over 150x
reduction in the number of parameters and 7x reduction in the domain size
without losses in accuracy, while slightly enabling parallelism.
</p></li>
</ul>
<h3>Title: Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00154">http://arxiv.org/abs/2310.00154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00154] Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers](http://arxiv.org/abs/2310.00154) #memory</code></li>
<li>Summary: <p>Continual learning is inherently a constrained learning problem. The goal is
to learn a predictor under a \emph{no-forgetting} requirement. Although several
prior studies formulate it as such, they do not solve the constrained problem
explicitly. In this work, we show that it is both possible and beneficial to
undertake the constrained optimization problem directly. To do this, we
leverage recent results in constrained learning through Lagrangian duality. We
focus on memory-based methods, where a small subset of samples from previous
tasks can be stored in a replay buffer. In this setting, we analyze two
versions of the continual learning problem: a coarse approach with constraints
at the task level and a fine approach with constraints at the sample level. We
show that dual variables indicate the sensitivity of the optimal value with
respect to constraint perturbations. We then leverage this result to partition
the buffer in the coarse approach, allocating more resources to harder tasks,
and to populate the buffer in the fine approach, including only impactful
samples. We derive sub-optimality bounds, and empirically corroborate our
theoretical results in various continual learning benchmarks. We also discuss
the limitations of these methods with respect to the amount of memory available
and the number of constraints involved in the optimization problem.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes. (arXiv:2310.00196v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00196">http://arxiv.org/abs/2310.00196</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00196] The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes](http://arxiv.org/abs/2310.00196) #few-shot</code></li>
<li>Summary: <p>Sign language recognition and translation technologies have the potential to
increase access and inclusion of deaf signing communities, but research
progress is bottlenecked by a lack of representative data. We introduce a new
resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The
Benchmark is the current largest of its kind, consisting of over 84k videos of
isolated sign productions from deaf ASL signers who gave informed consent and
received compensation. Human experts aligned these videos with other sign
language resources including ASL-LEX, SignBank, and ASL Citizen, enabling
useful expansions for sign and phonological feature recognition. We present a
suite of experiments which make use of the linguistic information in ASL-LEX,
evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated
sign recognition (ISR). We use an SL-GCN model to show that the phonological
features are recognizable with 85% accuracy, and that they are effective as an
auxiliary target to ISR. Learning to recognize phonological features alongside
gloss results in a 6% improvement for few-shot ISR accuracy and a 2%
improvement for ISR accuracy overall. Instructions for downloading the data can
be found at https://github.com/leekezar/SemLex.
</p></li>
</ul>
<h3>Title: Self-Specialization: Uncovering Latent Expertise within Large Language Models. (arXiv:2310.00160v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00160">http://arxiv.org/abs/2310.00160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00160] Self-Specialization: Uncovering Latent Expertise within Large Language Models](http://arxiv.org/abs/2310.00160) #few-shot</code></li>
<li>Summary: <p>Recent works have demonstrated the effectiveness of self-alignment in which a
large language model is, by itself, aligned to follow general instructions
through the automatic generation of instructional data using a handful of
human-written seeds. Instead of general alignment, in this work, we focus on
self-alignment for expert domain specialization (e.g., biomedicine),
discovering it to be very effective for improving zero-shot and few-shot
performance in target domains of interest. As a preliminary, we first present
the benchmark results of existing aligned models within a specialized domain,
which reveals the marginal effect that "generic" instruction-following training
has on downstream expert domains' performance. To remedy this, we explore
self-specialization that leverages domain-specific unlabelled data and a few
labeled seeds for the self-alignment process. When augmented with retrieval to
reduce hallucination and enhance concurrency of the alignment,
self-specialization offers an effective (and efficient) way of "carving out" an
expert model out of a "generalist", pre-trained LLM where different domains of
expertise are originally combined in a form of "superposition". Our
experimental results on a biomedical domain show that our self-specialized
model (30B) outperforms its base model, MPT-30B by a large margin and even
surpasses larger popular models based on LLaMA-65B, highlighting its potential
and practicality for specialization, especially considering its efficiency in
terms of data and parameters.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-10-03]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
