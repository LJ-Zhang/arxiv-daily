<h2>diffusion</h2>
<h3>Title: Diffusion Model as Representation Learner. (arXiv:2308.10916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10916">http://arxiv.org/abs/2308.10916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10916] Diffusion Model as Representation Learner](http://arxiv.org/abs/2308.10916) #diffusion</code></li>
<li>Summary: <p>Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive
results on various generative tasks.Despite its promises, the learned
representations of pre-trained DPMs, however, have not been fully understood.
In this paper, we conduct an in-depth investigation of the representation power
of DPMs, and propose a novel knowledge transfer method that leverages the
knowledge acquired by generative DPMs for recognition tasks. Our study begins
by examining the feature space of DPMs, revealing that DPMs are inherently
denoising autoencoders that balance the representation learning with
regularizing model capacity. To this end, we introduce a novel knowledge
transfer paradigm named RepFusion. Our paradigm extracts representations at
different time steps from off-the-shelf DPMs and dynamically employs them as
supervision for student networks, in which the optimal time is determined
through reinforcement learning. We evaluate our approach on several image
classification, semantic segmentation, and landmark detection benchmarks, and
demonstrate that it outperforms state-of-the-art methods. Our results uncover
the potential of DPMs as a powerful tool for representation learning and
provide insights into the usefulness of generative models beyond sample
generation. The code is available at
\url{https://github.com/Adamdad/Repfusion}.
</p></li>
</ul>
<h3>Title: DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment. (arXiv:2308.11206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11206">http://arxiv.org/abs/2308.11206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11206] DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment](http://arxiv.org/abs/2308.11206) #diffusion</code></li>
<li>Summary: <p>Cross-modal garment synthesis and manipulation will significantly benefit the
way fashion designers generate garments and modify their designs via flexible
linguistic interfaces.Current approaches follow the general text-to-image
paradigm and mine cross-modal relations via simple cross-attention modules,
neglecting the structural correspondence between visual and textual
representations in the fashion design domain. In this work, we instead
introduce DiffCloth, a diffusion-based pipeline for cross-modal garment
synthesis and manipulation, which empowers diffusion models with flexible
compositionality in the fashion domain by structurally aligning the cross-modal
semantics. Specifically, we formulate the part-level cross-modal alignment as a
bipartite matching problem between the linguistic Attribute-Phrases (AP) and
the visual garment parts which are obtained via constituency parsing and
semantic segmentation, respectively. To mitigate the issue of attribute
confusion, we further propose a semantic-bundled cross-attention to preserve
the spatial structure similarities between the attention maps of attribute
adjectives and part nouns in each AP. Moreover, DiffCloth allows for
manipulation of the generated results by simply replacing APs in the text
prompts. The manipulation-irrelevant regions are recognized by blended masks
obtained from the bundled attention maps of the APs and kept unchanged.
Extensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth
both yields state-of-the-art garment synthesis results by leveraging the
inherent structural information and supports flexible manipulation with region
consistency.
</p></li>
</ul>
<h3>Title: MatFuse: Controllable Material Generation with Diffusion Models. (arXiv:2308.11408v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11408">http://arxiv.org/abs/2308.11408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11408] MatFuse: Controllable Material Generation with Diffusion Models](http://arxiv.org/abs/2308.11408) #diffusion</code></li>
<li>Summary: <p>Creating high quality and realistic materials in computer graphics is a
challenging and time-consuming task, which requires great expertise. In this
paper, we present MatFuse, a novel unified approach that harnesses the
generative power of diffusion models (DM) to simplify the creation of SVBRDF
maps. Our DM-based pipeline integrates multiple sources of conditioning, such
as color palettes, sketches, and pictures, enabling fine-grained control and
flexibility in material synthesis. This design allows for the combination of
diverse information sources (e.g., sketch + image embedding), enhancing
creative possibilities in line with the principle of compositionality. We
demonstrate the generative capabilities of the proposed method under various
conditioning settings; on the SVBRDF estimation task, we show that our method
yields performance comparable to state-of-the-art approaches, both
qualitatively and quantitatively.
</p></li>
</ul>
<h3>Title: SDeMorph: Towards Better Facial De-morphing from Single Morph. (arXiv:2308.11442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11442">http://arxiv.org/abs/2308.11442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11442] SDeMorph: Towards Better Facial De-morphing from Single Morph](http://arxiv.org/abs/2308.11442) #diffusion</code></li>
<li>Summary: <p>Face Recognition Systems (FRS) are vulnerable to morph attacks. A face morph
is created by combining multiple identities with the intention to fool FRS and
making it match the morph with multiple identities. Current Morph Attack
Detection (MAD) can detect the morph but are unable to recover the identities
used to create the morph with satisfactory outcomes. Existing work in
de-morphing is mostly reference-based, i.e. they require the availability of
one identity to recover the other. Sudipta et al. \cite{ref9} proposed a
reference-free de-morphing technique but the visual realism of outputs produced
were feeble. In this work, we propose SDeMorph (Stably Diffused De-morpher), a
novel de-morphing method that is reference-free and recovers the identities of
bona fides. Our method produces feature-rich outputs that are of significantly
high quality in terms of definition and facial fidelity. Our method utilizes
Denoising Diffusion Probabilistic Models (DDPM) by destroying the input morphed
signal and then reconstructing it back using a branched-UNet. Experiments on
ASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectiveness
of the proposed method.
</p></li>
</ul>
<h3>Title: IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. (arXiv:2308.11473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11473">http://arxiv.org/abs/2308.11473</a></li>
<li>Code URL: <a href="https://github.com/buaacyw/it3d-text-to-3d">https://github.com/buaacyw/it3d-text-to-3d</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11473] IT3D: Improved Text-to-3D Generation with Explicit View Synthesis](http://arxiv.org/abs/2308.11473) #diffusion</code></li>
<li>Summary: <p>Recent strides in Text-to-3D techniques have been propelled by distilling
knowledge from powerful large text-to-image diffusion models (LDMs).
Nonetheless, existing Text-to-3D approaches often grapple with challenges such
as over-saturation, inadequate detailing, and unrealistic outputs. This study
presents a novel strategy that leverages explicitly synthesized multi-view
images to address these issues. Our approach involves the utilization of
image-to-image pipelines, empowered by LDMs, to generate posed high-quality
images based on the renderings of coarse 3D models. Although the generated
images mostly alleviate the aforementioned issues, challenges such as view
inconsistency and significant content variance persist due to the inherent
generative nature of large diffusion models, posing extensive difficulties in
leveraging these images effectively. To overcome this hurdle, we advocate
integrating a discriminator alongside a novel Diffusion-GAN dual training
strategy to guide the training of 3D models. For the incorporated
discriminator, the synthesized multi-view images are considered real data,
while the renderings of the optimized 3D models function as fake data. We
conduct a comprehensive set of experiments that demonstrate the effectiveness
of our method over baseline approaches.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations. (arXiv:2308.11021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11021">http://arxiv.org/abs/2308.11021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11021] Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations](http://arxiv.org/abs/2308.11021) #self-supervised</code></li>
<li>Summary: <p>There are many ways of interpreting the world and they are highly
interdependent. We exploit such complex dependencies and introduce a powerful
multi-task hypergraph, in which every node is a task and different paths
through the hypergraph reaching a given task become unsupervised teachers, by
forming ensembles that learn to generate reliable pseudolabels for that task.
Each hyperedge is part of an ensemble teacher for a given task and it is also a
student of the self-supervised hypergraph system. We apply our model to one of
the most important problems of our times, that of Earth Observation, which is
highly multi-task and it often suffers from missing ground-truth data. By
performing extensive experiments on the NASA NEO Dataset, spanning a period of
22 years, we demonstrate the value of our multi-task semi-supervised approach,
by consistent improvements over strong baselines and recent work. We also show
that the hypergraph can adapt unsupervised to gradual data distribution shifts
and reliably recover, through its multi-task self-supervision process, the
missing data for several observational layers for up to seven years.
</p></li>
</ul>
<h3>Title: TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection. (arXiv:2308.11072v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11072">http://arxiv.org/abs/2308.11072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11072] TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection](http://arxiv.org/abs/2308.11072) #self-supervised</code></li>
<li>Summary: <p>Video anomaly detection (VAD) without human monitoring is a complex computer
vision task that can have a positive impact on society if implemented
successfully. While recent advances have made significant progress in solving
this task, most existing approaches overlook a critical real-world concern:
privacy. With the increasing popularity of artificial intelligence
technologies, it becomes crucial to implement proper AI ethics into their
development. Privacy leakage in VAD allows models to pick up and amplify
unnecessary biases related to people's personal information, which may lead to
undesirable decision making. In this paper, we propose TeD-SPAD, a
privacy-aware video anomaly detection framework that destroys visual private
information in a self-supervised manner. In particular, we propose the use of a
temporally-distinct triplet loss to promote temporally discriminative features,
which complements current weakly-supervised VAD methods. Using TeD-SPAD, we
achieve a positive trade-off between privacy protection and utility anomaly
detection performance on three popular weakly supervised VAD datasets:
UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model
reduces private attribute prediction by 32.25% while only reducing frame-level
ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page:
https://joefioresi718.github.io/TeD-SPAD_webpage/
</p></li>
</ul>
<h3>Title: Exploring Unsupervised Cell Recognition with Prior Self-activation Maps. (arXiv:2308.11144v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>Code URL: <a href="https://github.com/cpystan/psm">https://github.com/cpystan/psm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11144] Exploring Unsupervised Cell Recognition with Prior Self-activation Maps](http://arxiv.org/abs/2308.11144) #self-supervised</code></li>
<li>Summary: <p>The success of supervised deep learning models on cell recognition tasks
relies on detailed annotations. Many previous works have managed to reduce the
dependency on labels. However, considering the large number of cells contained
in a patch, costly and inefficient labeling is still inevitable. To this end,
we explored label-free methods for cell recognition. Prior self-activation maps
(PSM) are proposed to generate pseudo masks as training targets. To be
specific, an activation network is trained with self-supervised learning. The
gradient information in the shallow layers of the network is aggregated to
generate prior self-activation maps. Afterward, a semantic clustering module is
then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo
masks for downstream tasks. We evaluated our method on two histological
datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection).
Compared with other fully-supervised and weakly-supervised methods, our method
can achieve competitive performance without any manual annotations. Our simple
but effective framework can also achieve multi-class cell detection which can
not be done by existing unsupervised methods. The results show the potential of
PSMs that might inspire other research to deal with the hunger for labels in
medical area.
</p></li>
</ul>
<h3>Title: ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data. (arXiv:2308.11194v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11194">http://arxiv.org/abs/2308.11194</a></li>
<li>Code URL: <a href="https://github.com/stanfordmimi/villa">https://github.com/stanfordmimi/villa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11194] ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data](http://arxiv.org/abs/2308.11194) #self-supervised</code></li>
<li>Summary: <p>Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained
on datasets consisting of image-caption pairs obtained from the web. However,
real-world multimodal datasets, such as healthcare data, are significantly more
complex: each image (e.g. X-ray) is often paired with text (e.g. physician
report) that describes many distinct attributes occurring in fine-grained
regions of the image. We refer to these samples as exhibiting high pairwise
complexity, since each image-text pair can be decomposed into a large number of
region-attribute pairings. The extent to which VLMs can capture fine-grained
relationships between image regions and textual attributes when trained on such
data has not been previously evaluated. The first key contribution of this work
is to demonstrate through systematic evaluations that as the pairwise
complexity of the training dataset increases, standard VLMs struggle to learn
region-attribute relationships, exhibiting performance degradations of up to
37% on retrieval tasks. In order to address this issue, we introduce ViLLA as
our second key contribution. ViLLA, which is trained to capture fine-grained
region-attribute relationships from complex datasets, involves two components:
(a) a lightweight, self-supervised mapping model to decompose image-text
samples into region-attribute pairs, and (b) a contrastive VLM to learn
representations from generated region-attribute pairs. We demonstrate with
experiments across four domains (synthetic, product, medical, and natural
images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks,
such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP
points on LVIS) and retrieval (up to 14.2 R-Precision points).
</p></li>
</ul>
<h3>Title: LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training. (arXiv:2308.11239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11239">http://arxiv.org/abs/2308.11239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11239] LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training](http://arxiv.org/abs/2308.11239) #self-supervised</code></li>
<li>Summary: <p>Learning object segmentation in image and video datasets without human
supervision is a challenging problem. Humans easily identify moving salient
objects in videos using the gestalt principle of common fate, which suggests
that what moves together belongs together. Building upon this idea, we propose
a self-supervised object discovery approach that leverages motion and
appearance information to produce high-quality object segmentation masks.
Specifically, we redesign the traditional graph cut on images to include motion
information in a linear combination with appearance information to produce edge
weights. Remarkably, this step produces object segmentation masks comparable to
the current state-of-the-art on multiple benchmarks. To further improve
performance, we bootstrap a segmentation network trained on these preliminary
masks as pseudo-ground truths to learn from its own outputs via self-training.
We demonstrate the effectiveness of our approach, named LOCATE, on multiple
standard video object segmentation, image saliency detection, and object
segmentation benchmarks, achieving results on par with and, in many cases
surpassing state-of-the-art methods. We also demonstrate the transferability of
our approach to novel domains through a qualitative study on in-the-wild
images. Additionally, we present extensive ablation analysis to support our
design choices and highlight the contribution of each component of our proposed
method.
</p></li>
</ul>
<h3>Title: Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding. (arXiv:2308.11448v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11448">http://arxiv.org/abs/2308.11448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11448] Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding](http://arxiv.org/abs/2308.11448) #self-supervised</code></li>
<li>Summary: <p>Self-supervised pretraining (SSP) has emerged as a popular technique in
machine learning, enabling the extraction of meaningful feature representations
without labelled data. In the realm of computer vision, pretrained vision
transformers (ViTs) have played a pivotal role in advancing transfer learning.
Nonetheless, the escalating cost of finetuning these large models has posed a
challenge due to the explosion of model size. This study endeavours to evaluate
the effectiveness of pure self-supervised learning (SSL) techniques in computer
vision tasks, obviating the need for finetuning, with the intention of
emulating human-like capabilities in generalisation and recognition of unseen
objects. To this end, we propose an evaluation protocol for zero-shot
segmentation based on a prompting patch. Given a point on the target object as
a prompt, the algorithm calculates the similarity map between the selected
patch and other patches, upon that, a simple thresholding is applied to segment
the target. Another evaluation is intra-object and inter-object similarity to
gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation
from prompting and discriminatory abilities of SSP led to the design of a
simple SSP approach, termed MMC. This approaches combines Masked image
modelling for encouraging similarity of local features, Momentum based
self-distillation for transferring semantics from global to local features, and
global Contrast for promoting semantics of global features, to enhance
discriminative representations of SSP ViTs. Consequently, our proposed method
significantly reduces the overlap of intra-object and inter-object
similarities, thereby facilitating effective object segmentation within an
image. Our experiments reveal that MMC delivers top-tier results in zero-shot
semantic segmentation across various datasets.
</p></li>
</ul>
<h3>Title: GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning. (arXiv:2308.11605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11605">http://arxiv.org/abs/2308.11605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11605] GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning](http://arxiv.org/abs/2308.11605) #self-supervised</code></li>
<li>Summary: <p>Large-scale foundation models, such as CLIP, have demonstrated remarkable
success in visual recognition tasks by embedding images in a semantically rich
space. Self-supervised learning (SSL) has also shown promise in improving
visual recognition by learning invariant features. However, the combination of
CLIP with SSL is found to face challenges due to the multi-task framework that
blends CLIP's contrastive loss and SSL's loss, including difficulties with loss
weighting and inconsistency among different views of images in CLIP's output
space. To overcome these challenges, we propose a prompt learning-based model
called GOPro, which is a unified framework that ensures similarity between
various augmented views of input images in a shared image-text embedding space,
using a pair of learnable image and text projectors atop CLIP, to promote
invariance and generalizability. To automatically learn such prompts, we
leverage the visual content and style primitives extracted from pre-trained
CLIP and adapt them to the target task. In addition to CLIP's cross-domain
contrastive loss, we introduce a visual contrastive loss and a novel prompt
consistency loss, considering the different views of the images. GOPro is
trained end-to-end on all three loss objectives, combining the strengths of
CLIP and SSL in a principled manner. Empirical evaluations demonstrate that
GOPro outperforms the state-of-the-art prompting techniques on three
challenging domain generalization tasks across multiple benchmarks by a
significant margin. Our code is available at
https://github.com/mainaksingha01/GOPro.
</p></li>
</ul>
<h3>Title: SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11596">http://arxiv.org/abs/2308.11596</a></li>
<li>Code URL: <a href="https://github.com/facebookresearch/seamless_communication">https://github.com/facebookresearch/seamless_communication</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11596] SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation](http://arxiv.org/abs/2308.11596) #self-supervised</code></li>
<li>Summary: <p>What does it take to create the Babel Fish, a tool that can help individuals
translate speech between any two languages? While recent breakthroughs in
text-based models have pushed machine translation coverage beyond 200
languages, unified speech-to-speech translation models have yet to achieve
similar strides. More specifically, conventional speech-to-speech translation
systems rely on cascaded systems that perform translation progressively,
putting high-performing unified systems out of reach. To address these gaps, we
introduce SeamlessM4T, a single model that supports speech-to-speech
translation, speech-to-text translation, text-to-speech translation,
text-to-text translation, and automatic speech recognition for up to 100
languages. To build this, we used 1 million hours of open speech audio data to
learn self-supervised speech representations with w2v-BERT 2.0. Subsequently,
we created a multimodal corpus of automatically aligned speech translations.
Filtered and combined with human-labeled and pseudo-labeled data, we developed
the first multilingual system capable of translating from and into English for
both speech and text. On FLEURS, SeamlessM4T sets a new standard for
translations into multiple target languages, achieving an improvement of 20%
BLEU over the previous SOTA in direct speech-to-text translation. Compared to
strong cascaded models, SeamlessM4T improves the quality of into-English
translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in
speech-to-speech. Tested for robustness, our system performs better against
background noises and speaker variations in speech-to-text tasks compared to
the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and
added toxicity to assess translation safety. Finally, all contributions in this
work are open-sourced at this https
https://github.com/facebookresearch/seamless_communication.
</p></li>
</ul>
<h3>Title: A Survey on Self-Supervised Representation Learning. (arXiv:2308.11455v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11455">http://arxiv.org/abs/2308.11455</a></li>
<li>Code URL: <a href="https://github.com/microsoft/esvit">https://github.com/microsoft/esvit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11455] A Survey on Self-Supervised Representation Learning](http://arxiv.org/abs/2308.11455) #self-supervised</code></li>
<li>Summary: <p>Learning meaningful representations is at the heart of many tasks in the
field of modern machine learning. Recently, a lot of methods were introduced
that allow learning of image representations without supervision. These
representations can then be used in downstream tasks like classification or
object detection. The quality of these representations is close to supervised
learning, while no labeled images are needed. This survey paper provides a
comprehensive review of these methods in a unified notation, points out
similarities and differences of these methods, and proposes a taxonomy which
sets these methods in relation to each other. Furthermore, our survey
summarizes the most-recent experimental results reported in the literature in
form of a meta-study. Our survey is intended as a starting point for
researchers and practitioners who want to dive into the field of representation
learning.
</p></li>
</ul>
<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10997] SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models](http://arxiv.org/abs/2308.10997) #generative</code></li>
<li>Summary: <p>Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running inference multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
This method is shown to work in conjunction with the recently proposed Muse
model. The MRF encodes the compatibility among image tokens at different
spatial locations and enables us to significantly reduce the required number of
Muse prediction steps. Inference with the MRF is significantly cheaper, and its
parameters can be quickly learned through back-propagation by modeling MRF
inference as a differentiable neural-network layer. Our full model, SPEGTI,
uses this proposed MRF model to speed up Muse by 1.5X with no loss in output
image quality.
</p></li>
</ul>
<h3>Title: TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. (arXiv:2308.11421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11421] TurboViT: Generating Fast Vision Transformers via Generative Architecture Search](http://arxiv.org/abs/2308.11421) #generative</code></li>
<li>Summary: <p>Vision transformers have shown unprecedented levels of performance in
tackling various visual perception tasks in recent years. However, the
architectural and computational complexity of such network architectures have
made them challenging to deploy in real-world applications with
high-throughput, low-memory requirements. As such, there has been significant
research recently on the design of efficient vision transformer architectures.
In this study, we explore the generation of fast vision transformer
architecture designs via generative architecture search (GAS) to achieve a
strong balance between accuracy and architectural and computational efficiency.
Through this generative architecture search process, we create TurboViT, a
highly efficient hierarchical vision transformer architecture design that is
generated around mask unit attention and Q-pooling design patterns. The
resulting TurboViT architecture design achieves significantly lower
architectural computational complexity (>2.47$\times$ smaller than FasterViT-0
while achieving same accuracy) and computational complexity (>3.4$\times$ fewer
FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other
state-of-the-art efficient vision transformer network architecture designs
within a similar range of accuracy on the ImageNet-1K dataset. Furthermore,
TurboViT demonstrated strong inference latency and throughput in both
low-latency and batch processing scenarios (>3.21$\times$ lower latency and
>3.18$\times$ higher throughput compared to FasterViT-0 for low-latency
scenario). These promising results demonstrate the efficacy of leveraging
generative architecture search for generating efficient transformer
architecture designs for high-throughput scenarios.
</p></li>
</ul>
<h3>Title: Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>Code URL: <a href="https://github.com/servicenow/broad-openood">https://github.com/servicenow/broad-openood</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11480] Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection](http://arxiv.org/abs/2308.11480) #generative</code></li>
<li>Summary: <p>Improving the reliability of deployed machine learning systems often involves
developing methods to detect out-of-distribution (OOD) inputs. However,
existing research often narrowly focuses on samples from classes that are
absent from the training set, neglecting other types of plausible distribution
shifts. This limitation reduces the applicability of these methods in
real-world scenarios, where systems encounter a wide variety of anomalous
inputs. In this study, we categorize five distinct types of distribution shifts
and critically evaluate the performance of recent OOD detection methods on each
of them. We publicly release our benchmark under the name BROAD (Benchmarking
Resilience Over Anomaly Diversity). Our findings reveal that while these
methods excel in detecting unknown classes, their performance is inconsistent
when encountering other types of distribution shifts. In other words, they only
reliably detect unexpected inputs that they have been specifically designed to
expect. As a first step toward broad OOD detection, we learn a generative model
of existing detection scores with a Gaussian mixture. By doing so, we present
an ensemble approach that offers a more consistent and comprehensive solution
for broad OOD detection, demonstrating superior performance compared to
existing methods. Our code to download BROAD and reproduce our experiments is
publicly available.
</p></li>
</ul>
<h3>Title: Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11584">http://arxiv.org/abs/2308.11584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11584] Building Emotional Support Chatbots in the Era of LLMs](http://arxiv.org/abs/2308.11584) #generative</code></li>
<li>Summary: <p>The integration of emotional support into various conversational scenarios
presents profound societal benefits, such as social interactions, mental health
counseling, and customer service. However, there are unsolved challenges that
hinder real-world applications in this field, including limited data
availability and the absence of well-accepted model training paradigms. This
work endeavors to navigate these challenges by harnessing the capabilities of
Large Language Models (LLMs). We introduce an innovative methodology that
synthesizes human insights with the computational prowess of LLMs to curate an
extensive emotional support dialogue dataset. Our approach is initiated with a
meticulously designed set of dialogues spanning diverse scenarios as generative
seeds. By utilizing the in-context learning potential of ChatGPT, we
recursively generate an ExTensible Emotional Support dialogue dataset, named
ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,
examining the impact of diverse training strategies, ultimately yielding an LLM
meticulously optimized for emotional support interactions. An exhaustive
assessment of the resultant model showcases its proficiency in offering
emotional support, marking a pivotal step in the realm of emotional support
bots and paving the way for subsequent research and implementations.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. (arXiv:2308.11119v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11119">http://arxiv.org/abs/2308.11119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11119] Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection](http://arxiv.org/abs/2308.11119) #anomaly</code></li>
<li>Summary: <p>This paper presents a novel method that leverages a visual-language model,
CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have
been put towards developing anomaly detectors due to their potential industrial
applications. Considering the difficulty in acquiring various anomalous samples
for training, most existing methods train models with only normal samples and
measure discrepancies from the distribution of normal samples during inference,
which requires training a model for each object category. The problem of this
inefficient training requirement has been tackled by designing a CLIP-based
anomaly detector that applies prompt-guided classification to each part of an
image in a sliding window manner. However, the method still suffers from the
labor of careful prompt ensembling with known object categories. To overcome
the issues above, we propose leveraging CLIP as a data source for training. Our
method generates text embeddings with the text encoder in CLIP with typical
prompts that include words of normal and anomaly. In addition to these words,
we insert several randomly generated words into prompts, which enables the
encoder to generate a diverse set of normal and anomalous samples. Using the
generated embeddings as training data, a feed-forward neural network learns to
extract features of normal and anomaly from CLIP's embeddings, and as a result,
a category-agnostic anomaly detector can be obtained without any training
images. Experimental results demonstrate that our method achieves
state-of-the-art performance without laborious prompt ensembling in zero-shot
setups.
</p></li>
</ul>
<h3>Title: A novel analysis of utility in privacy pipelines, using Kronecker products and quantitative information flow. (arXiv:2308.11110v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11110">http://arxiv.org/abs/2308.11110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11110] A novel analysis of utility in privacy pipelines, using Kronecker products and quantitative information flow](http://arxiv.org/abs/2308.11110) #anomaly</code></li>
<li>Summary: <p>We combine Kronecker products, and quantitative information flow, to give a
novel formal analysis for the fine-grained verification of utility in complex
privacy pipelines. The combination explains a surprising anomaly in the
behaviour of utility of privacy-preserving pipelines -- that sometimes a
reduction in privacy results also in a decrease in utility. We use the standard
measure of utility for Bayesian analysis, introduced by Ghosh at al., to
produce tractable and rigorous proofs of the fine-grained statistical behaviour
leading to the anomaly. More generally, we offer the prospect of
formal-analysis tools for utility that complement extant formal analyses of
privacy. We demonstrate our results on a number of common privacy-preserving
designs.
</p></li>
</ul>
<h3>Title: Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10918">http://arxiv.org/abs/2308.10918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10918] Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge](http://arxiv.org/abs/2308.10918) #anomaly</code></li>
<li>Summary: <p>Graph anomaly detection has attracted considerable attention in recent years.
This paper introduces a novel approach that leverages metapath-based
semi-supervised learning, addressing the limitations of previous methods. We
present a new framework, Metapath-based Semi-supervised Anomaly Detection
(MSAD), incorporating GCN layers in both the encoder and decoder to efficiently
propagate context information between abnormal and normal nodes. The design of
metapath-based context information and a specifically crafted anomaly community
enhance the process of learning differences in structures and attributes, both
globally and locally. Through a comprehensive set of experiments conducted on
seven real-world networks, this paper demonstrates the superiority of the MSAD
method compared to state-of-the-art techniques. The promising results of this
study pave the way for future investigations, focusing on the optimization and
analysis of metapath patterns to further enhance the effectiveness of anomaly
detection on attributed networks.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11578">http://arxiv.org/abs/2308.11578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11578] Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models](http://arxiv.org/abs/2308.11578) #in-context</code></li>
<li>Summary: <p>After the inception of emotion recognition or affective computing, it has
increasingly become an active research topic due to its broad applications.
Over the past couple of decades, emotion recognition models have gradually
migrated from statistically shallow models to neural network-based deep models,
which can significantly boost the performance of emotion recognition models and
consistently achieve the best results on different benchmarks. Therefore, in
recent years, deep models have always been considered the first option for
emotion recognition. However, the debut of large language models (LLMs), such
as ChatGPT, has remarkably astonished the world due to their emerged
capabilities of zero/few-shot learning, in-context learning, chain-of-thought,
and others that are never shown in previous deep models. In the present paper,
we comprehensively investigate how the LLMs perform in emotion recognition in
terms of diverse aspects, including in-context learning, few-short learning,
accuracy, generalisation, and explanation. Moreover, we offer some insights and
pose other potential challenges, hoping to ignite broader discussions about
enhancing emotion recognition in the new era of advanced and generalised large
models.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging. (arXiv:2308.11443v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11443">http://arxiv.org/abs/2308.11443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11443] Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging](http://arxiv.org/abs/2308.11443) #memory</code></li>
<li>Summary: <p>Fast Adversarial Training (FAT) not only improves the model robustness but
also reduces the training cost of standard adversarial training. However, fast
adversarial training often suffers from Catastrophic Overfitting (CO), which
results in poor robustness performance. Catastrophic Overfitting describes the
phenomenon of a sudden and significant decrease in robust accuracy during the
training of fast adversarial training. Many effective techniques have been
developed to prevent Catastrophic Overfitting and improve the model robustness
from different perspectives. However, these techniques adopt inconsistent
training settings and require different training costs, i.e, training time and
memory costs, leading to unfair comparisons. In this paper, we conduct a
comprehensive study of over 10 fast adversarial training methods in terms of
adversarial robustness and training costs. We revisit the effectiveness and
efficiency of fast adversarial training techniques in preventing Catastrophic
Overfitting from the perspective of model local nonlinearity and propose an
effective Lipschitz regularization method for fast adversarial training.
Furthermore, we explore the effect of data augmentation and weight averaging in
fast adversarial training and propose a simple yet effective auto weight
averaging method to improve robustness further. By assembling these techniques,
we propose a FGSM-based fast adversarial training method equipped with
Lipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW.
Experimental evaluations on four benchmark databases demonstrate the
superiority of the proposed method over state-of-the-art fast adversarial
training methods and the advanced standard adversarial training methods.
</p></li>
</ul>
<h3>Title: Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10905">http://arxiv.org/abs/2308.10905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10905] Analyzing Quantization in TVM](http://arxiv.org/abs/2308.10905) #memory</code></li>
<li>Summary: <p>There has been many papers in academic literature on quantizing weight
tensors in deep learning models to reduce inference latency and memory
footprint. TVM also has the ability to quantize weights and support low-bit
computations. Although quantization is typically expected to improve inference
time, in TVM, the performance of 8-bit quantization does not meet the
expectations. Typically, when applying 8-bit quantization to a deep learning
model, it is usually expected to achieve around 50% of the full-precision
inference time. However, in this particular case, not only does the quantized
version fail to achieve the desired performance boost, but it actually performs
worse, resulting in an inference time that is about 2 times as slow as the
non-quantized version. In this project, we thoroughly investigate the reasons
behind the underperformance and assess the compatibility and optimization
opportunities of 8-bit quantization in TVM. We discuss the optimization of two
different types of tasks: computation-bound and memory-bound, and provide a
detailed comparison of various optimization techniques in TVM. Through the
identification of performance issues, we have successfully improved
quantization by addressing a bug in graph building. Furthermore, we analyze
multiple optimization strategies to achieve the optimal quantization result.
The best experiment achieves 163.88% improvement compared with the TVM compiled
baseline in inference time for the compute-bound task and 194.98% for the
memory-bound task.
</p></li>
</ul>
<h3>Title: SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11200">http://arxiv.org/abs/2308.11200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11200] SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting](http://arxiv.org/abs/2308.11200) #memory</code></li>
<li>Summary: <p>RNN-based methods have faced challenges in the Long-term Time Series
Forecasting (LTSF) domain when dealing with excessively long look-back windows
and forecast horizons. Consequently, the dominance in this domain has shifted
towards Transformer, MLP, and CNN approaches. The substantial number of
recurrent iterations are the fundamental reasons behind the limitations of RNNs
in LTSF. To address these issues, we propose two novel strategies to reduce the
number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and
Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies,
namely SegRNN, significantly reduce the required recurrent iterations for LTSF,
resulting in notable improvements in forecast accuracy and inference speed.
Extensive experiments demonstrate that SegRNN not only outperforms SOTA
Transformer-based models but also reduces runtime and memory usage by more than
78%. These achievements provide strong evidence that RNNs continue to excel in
LTSF tasks and encourage further exploration of this domain with more RNN-based
approaches. The source code is coming soon.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models. (arXiv:2308.11186v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11186">http://arxiv.org/abs/2308.11186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11186] Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](http://arxiv.org/abs/2308.11186) #few-shot</code></li>
<li>Summary: <p>Pre-trained vision-language models, e.g., CLIP, working with manually
designed prompts have demonstrated great capacity of transfer learning.
Recently, learnable prompts achieve state-of-the-art performance, which however
are prone to overfit to seen classes, failing to generalize to unseen classes.
In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for
vision-language models. Our approach takes inspiration from human intelligence
in which external knowledge is usually incorporated into recognizing novel
categories of objects. Specifically, we design two complementary types of
knowledge-aware prompts for the text encoder to leverage the distinctive
characteristics of category-related external knowledge. The discrete prompt
extracts the key information from descriptions of an object category, and the
learned continuous prompt captures overall contexts. We further design an
adaptation head for the visual encoder to aggregate salient attentive visual
cues, which establishes discriminative and task-aware visual representations.
We conduct extensive experiments on 11 widely-used benchmark datasets and the
results verify the effectiveness in few-shot image classification, especially
in generalizing to unseen categories. Compared with the state-of-the-art CoCoOp
method, KAPT exhibits favorable performance and achieves an absolute gain of
3.22% on new classes and 2.57% in terms of harmonic mean.
</p></li>
</ul>
<h3>Title: Masked Cross-image Encoding for Few-shot Segmentation. (arXiv:2308.11201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11201">http://arxiv.org/abs/2308.11201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11201] Masked Cross-image Encoding for Few-shot Segmentation](http://arxiv.org/abs/2308.11201) #few-shot</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) is a dense prediction task that aims to infer the
pixel-wise labels of unseen classes using only a limited number of annotated
images. The key challenge in FSS is to classify the labels of query pixels
using class prototypes learned from the few labeled support exemplars. Prior
approaches to FSS have typically focused on learning class-wise descriptors
independently from support images, thereby ignoring the rich contextual
information and mutual dependencies among support-query features. To address
this limitation, we propose a joint learning method termed Masked Cross-Image
Encoding (MCE), which is designed to capture common visual properties that
describe object details and to learn bidirectional inter-image dependencies
that enhance feature interaction. MCE is more than a visual representation
enrichment module; it also considers cross-image mutual dependencies and
implicit guidance. Experiments on FSS benchmarks PASCAL-$5^i$ and COCO-$20^i$
demonstrate the advanced meta-learning ability of the proposed method.
</p></li>
</ul>
<h3>Title: DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10959">http://arxiv.org/abs/2308.10959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10959] DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering](http://arxiv.org/abs/2308.10959) #few-shot</code></li>
<li>Summary: <p>In this paper, we propose Docprompt for document question answering tasks
with powerful zero-shot and few-shot performance. We proposed a novel weakly
supervised data generation method, a novel multl-stage training method and a
novel understanding model &amp; generation model ensemble method. Experiment
results show that the Docprompt model after continue pretrain significantly
outperforms the existing strong baseline models on document question answering
tasks. This method greatly improves the delivery efficiency and model
performance of document question answering customer projects, reducing
annotation costs and labor costs. Our demo can be found at
https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
</p></li>
</ul>
<h3>Title: Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>Code URL: <a href="https://github.com/lab-v2/diversity_measures">https://github.com/lab-v2/diversity_measures</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11189] Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries](http://arxiv.org/abs/2308.11189) #few-shot</code></li>
<li>Summary: <p>Error prediction in large language models often relies on domain-specific
information. In this paper, we present measures for quantification of error in
the response of a large language model based on the diversity of responses to a
given prompt - hence independent of the underlying application. We describe how
three such measures - based on entropy, Gini impurity, and centroid distance -
can be employed. We perform a suite of experiments on multiple datasets and
temperature settings to demonstrate that these measures strongly correlate with
the probability of failure. Additionally, we present empirical results
demonstrating how these measures can be applied to few-shot prompting,
chain-of-thought reasoning, and error detection.
</p></li>
</ul>
<h3>Title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11483] Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions](http://arxiv.org/abs/2308.11483) #few-shot</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
various NLP tasks. However, previous works have shown these models are
sensitive towards prompt wording, and few-shot demonstrations and their order,
posing challenges to fair assessment of these models. As these models become
more powerful, it becomes imperative to understand and address these
limitations. In this paper, we focus on LLMs robustness on the task of
multiple-choice questions -- commonly adopted task to study reasoning and
fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs
towards the order of options in multiple-choice questions, we demonstrate a
considerable performance gap of approximately 13% to 75% in LLMs on different
benchmarks, when answer options are reordered, even when using demonstrations
in a few-shot setting. Through a detailed analysis, we conjecture that this
sensitivity arises when LLMs are uncertain about the prediction between the
top-2/3 choices, and specific options placements may favor certain prediction
between those top choices depending on the question caused by positional bias.
We also identify patterns in top-2 choices that amplify or mitigate the model's
bias toward option placement. We found that for amplifying bias, the optimal
strategy involves positioning the top two choices as the first and last
options. Conversely, to mitigate bias, we recommend placing these choices among
the adjacent options. To validate our conjecture, we conduct various
experiments and adopt two approaches to calibrate LLMs' predictions, leading to
up to 8 percentage points improvement across different models and benchmarks.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-23]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
