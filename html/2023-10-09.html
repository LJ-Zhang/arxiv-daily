<h2>diffusion</h2>
<h3>Title: Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model. (arXiv:2310.03893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03893">http://arxiv.org/abs/2310.03893</a></li>
<li>Code URL: <a href="https://github.com/cagladbahadir/dpm-for-mitotic-figures">https://github.com/cagladbahadir/dpm-for-mitotic-figures</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03893] Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model](http://arxiv.org/abs/2310.03893) #diffusion</code></li>
<li>Summary: <p>Mitotic figure detection in histology images is a hard-to-define, yet
clinically significant task, where labels are generated with pathologist
interpretations and where there is no ``gold-standard'' independent
ground-truth. However, it is well-established that these interpretation based
labels are often unreliable, in part, due to differences in expertise levels
and human subjectivity. In this paper, our goal is to shed light on the
inherent uncertainty of mitosis labels and characterize the mitotic figure
classification task in a human interpretable manner. We train a probabilistic
diffusion model to synthesize patches of cell nuclei for a given mitosis label
condition. Using this model, we can then generate a sequence of synthetic
images that correspond to the same nucleus transitioning into the mitotic
state. This allows us to identify different image features associated with
mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity
and high contrast between the nucleus and the cell body. Our approach offers a
new tool for pathologists to interpret and communicate the features driving the
decision to recognize a mitotic figure.
</p></li>
</ul>
<h3>Title: VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for Single Modality Labeled Visible-Infrared Person Re-identification. (arXiv:2310.04122v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04122">http://arxiv.org/abs/2310.04122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04122] VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for Single Modality Labeled Visible-Infrared Person Re-identification](http://arxiv.org/abs/2310.04122) #diffusion</code></li>
<li>Summary: <p>Visible-Infrared person re-identification (VI-ReID) in real-world scenarios
poses a significant challenge due to the high cost of cross-modality data
annotation. Different sensing cameras, such as RGB/IR cameras for good/poor
lighting conditions, make it costly and error-prone to identify the same person
across modalities. To overcome this, we explore the use of single-modality
labeled data for the VI-ReID task, which is more cost-effective and practical.
By labeling pedestrians in only one modality (e.g., visible images) and
retrieving in another modality (e.g., infrared images), we aim to create a
training set containing both originally labeled and modality-translated data
using unpaired image-to-image translation techniques. In this paper, we propose
VI-Diff, a diffusion model that effectively addresses the task of
Visible-Infrared person image translation. Through comprehensive experiments,
we demonstrate that VI-Diff outperforms existing diffusion and GAN models,
making it a promising solution for VI-ReID with single-modality labeled data.
Our approach can be a promising solution to the VI-ReID task with
single-modality labeled data and serves as a good starting point for future
study. Code will be available.
</p></li>
</ul>
<h3>Title: Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. (arXiv:2310.04378v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04378">http://arxiv.org/abs/2310.04378</a></li>
<li>Code URL: <a href="https://github.com/luosiallen/latent-consistency-model">https://github.com/luosiallen/latent-consistency-model</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04378] Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference](http://arxiv.org/abs/2310.04378) #diffusion</code></li>
<li>Summary: <p>Latent Diffusion models (LDMs) have achieved remarkable results in
synthesizing high-resolution images. However, the iterative sampling process is
computationally intensive and leads to slow generation. Inspired by Consistency
Models (song et al.), we propose Latent Consistency Models (LCMs), enabling
swift inference with minimal steps on any pre-trained LDMs, including Stable
Diffusion (rombach et al). Viewing the guided reverse diffusion process as
solving an augmented probability flow ODE (PF-ODE), LCMs are designed to
directly predict the solution of such ODE in latent space, mitigating the need
for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently
distilled from pre-trained classifier-free guided diffusion models, a
high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.
Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method
that is tailored for fine-tuning LCMs on customized image datasets. Evaluation
on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve
state-of-the-art text-to-image generation performance with few-step inference.
Project Page: https://latent-consistency-models.github.io/
</p></li>
</ul>
<h3>Title: CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis. (arXiv:2310.04414v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04414">http://arxiv.org/abs/2310.04414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04414] CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis](http://arxiv.org/abs/2310.04414) #diffusion</code></li>
<li>Summary: <p>Analyzing model performance in various unseen environments is a critical
research problem in the machine learning community. To study this problem, it
is important to construct a testbed with out-of-distribution test sets that
have broad coverage of environmental discrepancies. However, existing testbeds
typically either have a small number of domains or are synthesized by image
corruptions, hindering algorithm design that demonstrates real-world
effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of
180 datasets collected by prompting image search engines and diffusion models
in various ways. Generally sized between 300 and 8,000 images, the datasets
contain natural images, cartoons, certain colors, or objects that do not
naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen
the understanding of two generalization tasks: domain generalization and model
accuracy prediction in various out-of-distribution environments. We conduct
extensive benchmarking and comparison experiments and show that CIFAR-10-W
offers new and interesting insights inherent to these tasks. We also discuss
other fields that would benefit from CIFAR-10-W.
</p></li>
</ul>
<h3>Title: Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04041">http://arxiv.org/abs/2310.04041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04041] Observation-Guided Diffusion Probabilistic Models](http://arxiv.org/abs/2310.04041) #diffusion</code></li>
<li>Summary: <p>We propose a novel diffusion model called observation-guided diffusion
probabilistic model (OGDM), which effectively addresses the trade-off between
quality control and fast sampling. Our approach reestablishes the training
objective by integrating the guidance of the observation process with the
Markov chain in a principled way. This is achieved by introducing an additional
loss term derived from the observation based on the conditional discriminator
on noise level, which employs Bernoulli distribution indicating whether its
input lies on the (noisy) real manifold or not. This strategy allows us to
optimize the more accurate negative log-likelihood induced in the inference
stage especially when the number of function evaluations is limited. The
proposed training method is also advantageous even when incorporated only into
the fine-tuning process, and it is compatible with various fast inference
strategies since our method yields better denoising networks using the exactly
same inference procedure without incurring extra computational cost. We
demonstrate the effectiveness of the proposed training algorithm using diverse
inference methods on strong diffusion model baselines.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer. (arXiv:2310.04099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04099">http://arxiv.org/abs/2310.04099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04099] ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer](http://arxiv.org/abs/2310.04099) #self-supervised</code></li>
<li>Summary: <p>Visual place recognition (VPR) is a highly challenging task that has a wide
range of applications, including robot navigation and self-driving vehicles.
VPR is particularly difficult due to the presence of duplicate regions and the
lack of attention to small objects in complex scenes, resulting in recognition
deviations. In this paper, we present ClusVPR, a novel approach that tackles
the specific issues of redundant information in duplicate regions and
representations of small objects. Different from existing methods that rely on
Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR
introduces a unique paradigm called Clustering-based Weighted Transformer
Network (CWTNet). CWTNet leverages the power of clustering-based weighted
feature maps and integrates global dependencies to effectively address visual
deviations encountered in large-scale VPR problems. We also introduce the
optimized-VLAD (OptLAD) layer that significantly reduces the number of
parameters and enhances model efficiency. This layer is specifically designed
to aggregate the information obtained from scale-wise image patches.
Additionally, our pyramid self-supervised strategy focuses on extracting
representative and diverse information from scale-wise image patches instead of
entire images, which is crucial for capturing representative and diverse
information in VPR. Extensive experiments on four VPR datasets show our model's
superior performance compared to existing models while being less complex.
</p></li>
</ul>
<h3>Title: Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning. (arXiv:2310.04148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04148">http://arxiv.org/abs/2310.04148</a></li>
<li>Code URL: <a href="https://github.com/ydchen0806/dbmim">https://github.com/ydchen0806/dbmim</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04148] Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2310.04148) #self-supervised</code></li>
<li>Summary: <p>The performance of existing supervised neuron segmentation methods is highly
dependent on the number of accurate annotations, especially when applied to
large scale electron microscopy (EM) data. By extracting semantic information
from unlabeled data, self-supervised methods can improve the performance of
downstream tasks, among which the mask image model (MIM) has been widely used
due to its simplicity and effectiveness in recovering original information from
masked images. However, due to the high degree of structural locality in EM
images, as well as the existence of considerable noise, many voxels contain
little discriminative information, making MIM pretraining inefficient on the
neuron segmentation task. To overcome this challenge, we propose a
decision-based MIM that utilizes reinforcement learning (RL) to automatically
search for optimal image masking ratio and masking strategy. Due to the vast
exploration space, using single-agent RL for voxel prediction is impractical.
Therefore, we treat each input patch as an agent with a shared behavior policy,
allowing for multi-agent collaboration. Furthermore, this multi-agent model can
capture dependencies between voxels, which is beneficial for the downstream
segmentation task. Experiments conducted on representative EM datasets
demonstrate that our approach has a significant advantage over alternative
self-supervised methods on the task of neuron segmentation. Code is available
at \url{https://github.com/ydchen0806/dbMiM}.
</p></li>
</ul>
<h3>Title: Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03840">http://arxiv.org/abs/2310.03840</a></li>
<li>Code URL: <a href="https://github.com/ellenzhuwang/lakermap">https://github.com/ellenzhuwang/lakermap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03840] Contextualized Structural Self-supervised Learning for Ontology Matching](http://arxiv.org/abs/2310.03840) #self-supervised</code></li>
<li>Summary: <p>Ontology matching (OM) entails the identification of semantic relationships
between concepts within two or more knowledge graphs (KGs) and serves as a
critical step in integrating KGs from various sources. Recent advancements in
deep OM models have harnessed the power of transformer-based language models
and the advantages of knowledge graph embedding. Nevertheless, these OM models
still face persistent challenges, such as a lack of reference alignments,
runtime latency, and unexplored different graph structures within an end-to-end
framework. In this study, we introduce a novel self-supervised learning OM
framework with input ontologies, called LaKERMap. This framework capitalizes on
the contextual and structural information of concepts by integrating implicit
knowledge into transformers. Specifically, we aim to capture multiple
structural contexts, encompassing both local and global interactions, by
employing distinct training objectives. To assess our methods, we utilize the
Bio-ML datasets and tasks. The findings from our innovative approach reveal
that LaKERMap surpasses state-of-the-art systems in terms of alignment quality
and inference time. Our models and codes are available here:
https://github.com/ellenzhuwang/lakermap.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation. (arXiv:2310.03923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03923">http://arxiv.org/abs/2310.03923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03923] Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation](http://arxiv.org/abs/2310.03923) #foundation model</code></li>
<li>Summary: <p>Precise 3D environmental mapping is pivotal in robotics. Existing methods
often rely on predefined concepts during training or are time-intensive when
generating semantic maps. This paper presents Open-Fusion, a groundbreaking
approach for real-time open-vocabulary 3D mapping and queryable scene
representation using RGB-D data. Open-Fusion harnesses the power of a
pre-trained vision-language foundation model (VLFM) for open-set semantic
comprehension and employs the Truncated Signed Distance Function (TSDF) for
swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based
embeddings and their associated confidence maps. These are then integrated with
3D knowledge from TSDF using an enhanced Hungarian-based feature-matching
mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D
segmentation for open-vocabulary without necessitating additional 3D training.
Benchmark tests on the ScanNet dataset against leading zero-shot methods
highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the
strengths of region-based VLFM and TSDF, facilitating real-time 3D scene
comprehension that includes object concepts and open-world semantics. We
encourage the readers to view the demos on our project page:
https://uark-aicv.github.io/OpenFusion
</p></li>
</ul>
<h3>Title: DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions. (arXiv:2310.04181v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04181">http://arxiv.org/abs/2310.04181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04181] DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions](http://arxiv.org/abs/2310.04181) #foundation model</code></li>
<li>Summary: <p>Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
</p></li>
</ul>
<h3>Title: PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction. (arXiv:2310.03777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03777">http://arxiv.org/abs/2310.03777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03777] PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction](http://arxiv.org/abs/2310.03777) #foundation model</code></li>
<li>Summary: <p>In this paper, we introduce strategies for developing private Key Information
Extraction (KIE) systems by leveraging large pretrained document foundation
models in conjunction with differential privacy (DP), federated learning (FL),
and Differentially Private Federated Learning (DP-FL). Through extensive
experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts,
XFUND, and DOCILE), we demonstrate that large document foundation models can be
effectively fine-tuned for the KIE task under private settings to achieve
adequate performance while maintaining strong privacy guarantees. Moreover, by
thoroughly analyzing the impact of various training and model parameters on
model performance, we propose simple yet effective guidelines for achieving an
optimal privacy-utility trade-off for the KIE task under global DP. Finally, we
introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling
global DP from a standalone context to a multi-client federated environment. We
conduct a comprehensive evaluation of the algorithm across various client and
privacy settings, and demonstrate its capability to achieve comparable
performance and privacy guarantees to standalone DP, even when accommodating an
increasing number of participating clients. Overall, our study offers valuable
insights into the development of private KIE systems, and highlights the
potential of document foundation models for privacy-preserved Document AI
applications. To the best of authors' knowledge, this is the first work that
explores privacy preserved document KIE using document foundation models.
</p></li>
</ul>
<h3>Title: Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection. (arXiv:2310.04358v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04358">http://arxiv.org/abs/2310.04358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04358] Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection](http://arxiv.org/abs/2310.04358) #foundation model</code></li>
<li>Summary: <p>The detection of Alzheimer's disease (AD) from spontaneous speech has
attracted increasing attention while the sparsity of training data remains an
important issue. This paper handles the issue by knowledge transfer,
specifically from both speech-generic and depression-specific knowledge. The
paper first studies sequential knowledge transfer from generic foundation
models pretrained on large amounts of speech and text data. A block-wise
analysis is performed for AD diagnosis based on the representations extracted
from different intermediate blocks of different foundation models. Apart from
the knowledge from speech-generic representations, this paper also proposes to
simultaneously transfer the knowledge from a speech depression detection task
based on the high comorbidity rates of depression and AD. A parallel knowledge
transfer framework is studied that jointly learns the information shared
between these two tasks. Experimental results show that the proposed method
improves AD and depression detection, and produces a state-of-the-art F1 score
of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.
</p></li>
</ul>
<h3>Title: Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03916">http://arxiv.org/abs/2310.03916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03916] Toward a Foundation Model for Time Series Data](http://arxiv.org/abs/2310.03916) #foundation model</code></li>
<li>Summary: <p>A foundation model is a machine learning model trained on a large and diverse
set of data, typically using self-supervised learning-based pre-training
techniques, that can be adapted to various downstream tasks. However, current
research on time series pre-training has mostly focused on models pre-trained
solely on data from a single domain, resulting in a lack of knowledge about
other types of time series. However, current research on time series
pre-training has predominantly focused on models trained exclusively on data
from a single domain. As a result, these models possess domain-specific
knowledge that may not be easily transferable to time series from other
domains. In this paper, we aim to develop an effective time series foundation
model by leveraging unlabeled samples from multiple domains. To achieve this,
we repurposed the publicly available UCR Archive and evaluated four existing
self-supervised learning-based pre-training methods, along with a novel method,
on the datasets. We tested these methods using four popular neural network
architectures for time series to understand how the pre-training methods
interact with different network designs. Our experimental results show that
pre-training improves downstream classification tasks by enhancing the
convergence of the fine-tuning process. Furthermore, we found that the proposed
pre-training method, when combined with the Transformer model, outperforms the
alternatives.
</p></li>
</ul>
<h3>Title: Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04292">http://arxiv.org/abs/2310.04292</a></li>
<li>Code URL: <a href="https://github.com/datamol-io/graphium">https://github.com/datamol-io/graphium</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04292] Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets](http://arxiv.org/abs/2310.04292) #foundation model</code></li>
<li>Summary: <p>Recently, pre-trained foundation models have enabled significant advancements
in multiple fields. In molecular machine learning, however, where datasets are
often hand-curated, and hence typically small, the lack of datasets with
labeled features, and codebases to manage those datasets, has hindered the
development of foundation models. In this work, we present seven novel datasets
categorized by size into three distinct categories: ToyMix, LargeMix and
UltraLarge. These datasets push the boundaries in both the scale and the
diversity of supervised labels for molecular learning. They cover nearly 100
million molecules and over 3000 sparsely defined tasks, totaling more than 13
billion individual labels of both quantum and biological nature. In comparison,
our datasets contain 300 times more data points than the widely used OGB-LSC
PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In
addition, to support the development of foundational models based on our
proposed datasets, we present the Graphium graph machine learning library which
simplifies the process of building and training molecular machine learning
models for multi-task and multi-level molecular datasets. Finally, we present a
range of baseline results as a starting point of multi-task and multi-level
training on these datasets. Empirically, we observe that performance on
low-resource biological datasets show improvement by also training on large
amounts of quantum data. This indicates that there may be potential in
multi-task and multi-level training of a foundation model and fine-tuning it to
resource-constrained downstream tasks.
</p></li>
</ul>
<h3>Title: On the Embedding Collapse when Scaling up Recommendation Models. (arXiv:2310.04400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04400">http://arxiv.org/abs/2310.04400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04400] On the Embedding Collapse when Scaling up Recommendation Models](http://arxiv.org/abs/2310.04400) #foundation model</code></li>
<li>Summary: <p>Recent advances in deep foundation models have led to a promising trend of
developing large recommendation models to leverage vast amounts of available
data. However, we experiment to scale up existing recommendation models and
observe that the enlarged models do not improve satisfactorily. In this
context, we investigate the embedding layers of enlarged models and identify a
phenomenon of embedding collapse, which ultimately hinders scalability, wherein
the embedding matrix tends to reside in a low-dimensional subspace. Through
empirical and theoretical analysis, we demonstrate that the feature interaction
module specific to recommendation models has a two-sided effect. On the one
hand, the interaction restricts embedding learning when interacting with
collapsed embeddings, exacerbating the collapse issue. On the other hand,
feature interaction is crucial in mitigating the fitting of spurious features,
thereby improving scalability. Based on this analysis, we propose a simple yet
effective multi-embedding design incorporating embedding-set-specific
interaction modules to capture diverse patterns and reduce collapse. Extensive
experiments demonstrate that this proposed design provides consistent
scalability for various recommendation models.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03890">http://arxiv.org/abs/2310.03890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03890] Accelerated Neural Network Training with Rooted Logistic Objectives](http://arxiv.org/abs/2310.03890) #generative</code></li>
<li>Summary: <p>Many neural networks deployed in the real world scenarios are trained using
cross entropy based loss functions. From the optimization perspective, it is
known that the behavior of first order methods such as gradient descent
crucially depend on the separability of datasets. In fact, even in the most
simplest case of binary classification, the rate of convergence depends on two
factors: (1) condition number of data matrix, and (2) separability of the
dataset. With no further pre-processing techniques such as
over-parametrization, data augmentation etc., separability is an intrinsic
quantity of the data distribution under consideration. We focus on the
landscape design of the logistic function and derive a novel sequence of {\em
strictly} convex functions that are at least as strict as logistic loss. The
minimizers of these functions coincide with those of the minimum norm solution
wherever possible. The strict convexity of the derived function can be extended
to finetune state-of-the-art models and applications. In empirical experimental
analysis, we apply our proposed rooted logistic objective to multiple deep
models, e.g., fully-connected neural networks and transformers, on various of
classification benchmarks. Our results illustrate that training with rooted
loss function is converged faster and gains performance improvements.
Furthermore, we illustrate applications of our novel rooted loss function in
generative modeling based downstream applications, such as finetuning StyleGAN
model with the rooted loss. The code implementing our losses and models can be
found here for open source software development purposes:
https://anonymous.4open.science/r/rooted_loss.
</p></li>
</ul>
<h3>Title: A Deeply Supervised Semantic Segmentation Method Based on GAN. (arXiv:2310.04081v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04081">http://arxiv.org/abs/2310.04081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04081] A Deeply Supervised Semantic Segmentation Method Based on GAN](http://arxiv.org/abs/2310.04081) #generative</code></li>
<li>Summary: <p>In recent years, the field of intelligent transportation has witnessed rapid
advancements, driven by the increasing demand for automation and efficiency in
transportation systems. Traffic safety, one of the tasks integral to
intelligent transport systems, requires accurately identifying and locating
various road elements, such as road cracks, lanes, and traffic signs. Semantic
segmentation plays a pivotal role in achieving this task, as it enables the
partition of images into meaningful regions with accurate boundaries. In this
study, we propose an improved semantic segmentation model that combines the
strengths of adversarial learning with state-of-the-art semantic segmentation
techniques. The proposed model integrates a generative adversarial network
(GAN) framework into the traditional semantic segmentation model, enhancing the
model's performance in capturing complex and subtle features in transportation
images. The effectiveness of our approach is demonstrated by a significant
boost in performance on the road crack dataset compared to the existing
methods, \textit{i.e.,} SEGAN. This improvement can be attributed to the
synergistic effect of adversarial learning and semantic segmentation, which
leads to a more refined and accurate representation of road structures and
conditions. The enhanced model not only contributes to better detection of road
cracks but also to a wide range of applications in intelligent transportation,
such as traffic sign recognition, vehicle detection, and lane segmentation.
</p></li>
</ul>
<h3>Title: Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04285">http://arxiv.org/abs/2310.04285</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04285] Assessing Robustness via Score-Based Adversarial Image Generation](http://arxiv.org/abs/2310.04285) #generative</code></li>
<li>Summary: <p>Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantic-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate adversarial examples beyond
$\ell_p$-norm constraints, so-called unrestricted adversarial examples,
overcoming their limitations. Unlike traditional methods, ScoreAG maintains the
core semantics of images while generating realistic adversarial examples,
either by transforming existing images or synthesizing new ones entirely from
scratch. We further exploit the generative capability of ScoreAG to purify
images, empirically enhancing the robustness of classifiers. Our extensive
empirical evaluation demonstrates that ScoreAG matches the performance of
state-of-the-art attacks and defenses across multiple benchmarks. This work
highlights the importance of investigating adversarial examples bounded by
semantics rather than $\ell_p$-norm constraints. ScoreAG represents an
important step towards more encompassing robustness assessments.
</p></li>
</ul>
<h3>Title: Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models. (arXiv:2310.04039v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04039">http://arxiv.org/abs/2310.04039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04039] Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models](http://arxiv.org/abs/2310.04039) #generative</code></li>
<li>Summary: <p>Recent advancements in Large Language Models (LLMs) have demonstrated
impressive capabilities across a range of natural language processing tasks,
especially in reasoning, a cornerstone for achieving Artificial General
Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate
the inferential abilities of these models in real-world scenarios. To address
this gap, a new form of Question-Answering (QA) task, termed Reasoning with
Redundant Information Provided (RRIP), is introduced. The study designed a
modified version of the grade school math 8K (GSM-8K) dataset which has several
variants focusing on different attributes of redundant information. This
investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative
pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on
traditional QA tasks against the RRIP tasks. Findings indicate that while these
models achieved moderate success on standard QA benchmarks, their performance
notably declines when assessed on RRIP tasks. The study not only highlights the
limitations of current LLMs in handling redundant information but also suggests
that future training of these models should focus on incorporating redundant
information into the training data to increase the performance on RRIP tasks.
</p></li>
</ul>
<h3>Title: Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04363">http://arxiv.org/abs/2310.04363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04363] Amortizing intractable inference in large language models](http://arxiv.org/abs/2310.04363) #generative</code></li>
<li>Summary: <p>Autoregressive large language models (LLMs) compress knowledge from their
training data through next-token conditional distributions. This limits
tractable querying of this knowledge to start-to-end autoregressive sampling.
However, many tasks of interest -- including sequence continuation, infilling,
and other forms of constrained generation -- involve sampling from intractable
posterior distributions. We address this limitation by using amortized Bayesian
inference to sample from these intractable posteriors. Such amortization is
algorithmically achieved by fine-tuning LLMs via diversity-seeking
reinforcement learning algorithms: generative flow networks (GFlowNets). We
empirically demonstrate that this distribution-matching paradigm of LLM
fine-tuning can serve as an effective alternative to maximum-likelihood
training and reward-maximizing policy optimization. As an important
application, we interpret chain-of-thought reasoning as a latent variable
modeling problem and demonstrate that our approach enables data-efficient
adaptation of LLMs to tasks that require multi-step rationalization and tool
use.
</p></li>
</ul>
<h3>Title: Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization. (arXiv:2310.03898v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03898">http://arxiv.org/abs/2310.03898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03898] Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization](http://arxiv.org/abs/2310.03898) #generative</code></li>
<li>Summary: <p>Learning new tasks accumulatively without forgetting remains a critical
challenge in continual learning. Generative experience replay addresses this
challenge by synthesizing pseudo-data points for past learned tasks and later
replaying them for concurrent training along with the new tasks' data.
Generative replay is the best strategy for continual learning under a strict
class-incremental setting when certain constraints need to be met: (i) constant
model size, (ii) no pre-training dataset, and (iii) no memory buffer for
storing past tasks' data. Inspired by the biological nervous system mechanisms,
we introduce a time-aware regularization method to dynamically fine-tune the
three training objective terms used for generative replay: supervised learning,
latent regularization, and data reconstruction. Experimental results on major
benchmarks indicate that our method pushes the limit of brain-inspired
continual learners under such strict settings, improves memory retention, and
increases the average performance over continually arriving tasks.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking. (arXiv:2310.04010v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04010">http://arxiv.org/abs/2310.04010</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04010] Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking](http://arxiv.org/abs/2310.04010) #anomaly</code></li>
<li>Summary: <p>Anomaly detection (AD) in surface inspection is an essential yet challenging
task in manufacturing due to the quantity imbalance problem of scarce abnormal
data. To overcome the above, a reconstruction encoder-decoder (ED) such as
autoencoder or U-Net which is trained with only anomaly-free samples is widely
adopted, in the hope that unseen abnormals should yield a larger reconstruction
error than normal. Over the past years, researches on self-supervised
reconstruction-by-inpainting have been reported. They mask out suspected
defective regions for inpainting in order to make them invisible to the
reconstruction ED to deliberately cause inaccurate reconstruction for
abnormals. However, their limitation is multiple random masking to cover the
whole input image due to defective regions not being known in advance. We
propose a novel reconstruction-by-inpainting method dubbed Excision and
Recovery (EAR) that features single deterministic masking. For this, we exploit
a pre-trained spatial attention model to predict potential suspected defective
regions that should be masked out. We also employ a variant of U-Net as our ED
to further limit the reconstruction ability of the U-Net model for abnormals,
in which skip connections of different layers can be selectively disabled. In
the training phase, all the skip connections are switched on to fully take the
benefits from the U-Net architecture. In contrast, for inferencing, we only
keep deeper skip connections with shallower connections off. We validate the
effectiveness of EAR using an MNIST pre-trained attention for a commonly used
surface AD dataset, KolektorSDD2. The experimental results show that EAR
achieves both better AD performance and higher throughput than state-of-the-art
methods. We expect that the proposed EAR model can be widely adopted as
training and inference strategies for AD purposes.
</p></li>
</ul>
<h3>Title: Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04055">http://arxiv.org/abs/2310.04055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04055] Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning](http://arxiv.org/abs/2310.04055) #anomaly</code></li>
<li>Summary: <p>Federated learning (FL) systems are vulnerable to malicious clients that
submit poisoned local models to achieve their adversarial goals, such as
preventing the convergence of the global model or inducing the global model to
misclassify some data. Many existing defense mechanisms are impractical in
real-world FL systems, as they require prior knowledge of the number of
malicious clients or rely on re-weighting or modifying submissions. This is
because adversaries typically do not announce their intentions before
attacking, and re-weighting might change aggregation results even in the
absence of attacks. To address these challenges in real FL systems, this paper
introduces a cutting-edge anomaly detection approach with the following
features: i) Detecting the occurrence of attacks and performing defense
operations only when attacks happen; ii) Upon the occurrence of an attack,
further detecting the malicious client models and eliminating them without
harming the benign ones; iii) Ensuring honest execution of defense mechanisms
at the server by leveraging a zero-knowledge proof mechanism. We validate the
superior performance of the proposed approach with extensive experiments.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. (arXiv:2310.04408v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04408">http://arxiv.org/abs/2310.04408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04408] RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation](http://arxiv.org/abs/2310.04408) #in-context</code></li>
<li>Summary: <p>Retrieving documents and prepending them in-context at inference time
improves performance of language model (LMs) on a wide range of tasks. However,
these documents, often spanning hundreds of words, make inference substantially
more expensive. We propose compressing the retrieved documents into textual
summaries prior to in-context integration. This not only reduces the
computational costs but also relieves the burden of LMs to identify relevant
information in long retrieved documents. We present two compressors -- an
extractive compressor which selects useful sentences from retrieved documents
and an abstractive compressor which generates summaries by synthesizing
information from multiple documents. Both compressors are trained to improve
LMs' performance on end tasks when the generated summaries are prepended to the
LMs' input, while keeping the summary concise.If the retrieved documents are
irrelevant to the input or offer no additional information to LM, our
compressor can return an empty string, implementing selective augmentation.We
evaluate our approach on language modeling task and open domain question
answering task. We achieve a compression rate of as low as 6% with minimal loss
in performance for both tasks, significantly outperforming the off-the-shelf
summarization models. We show that our compressors trained for one LM can
transfer to other LMs on the language modeling task and provide summaries
largely faithful to the retrieved documents.
</p></li>
</ul>
<h3>Title: A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04353">http://arxiv.org/abs/2310.04353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04353] A Language-Agent Approach to Formal Theorem-Proving](http://arxiv.org/abs/2310.04353) #in-context</code></li>
<li>Summary: <p>Language agents, which use a large language model (LLM) capable of in-context
learning to interact with an external environment, have recently emerged as a
promising approach to control tasks. We present the first language-agent
approach to formal theorem-proving. Our method, COPRA, uses a high-capacity,
black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.
During the search, the policy can select proof tactics and retrieve lemmas and
definitions from an external database. Each selected tactic is executed in the
underlying proof framework, and the execution feedback is used to build the
prompt for the next policy invocation. The search also tracks selected
information from its history and uses it to reduce hallucinations and
unnecessary LLM queries.
</p></li>
</ul>
<p>We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks
from the Compcert project. On these benchmarks, COPRA is significantly better
than one-shot invocations of GPT-4, as well as state-of-the-art models
fine-tuned on proof data, at finding correct proofs quickly.
</p>

<h2>memory</h2>
<h3>Title: Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03915">http://arxiv.org/abs/2310.03915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03915] Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control](http://arxiv.org/abs/2310.03915) #memory</code></li>
<li>Summary: <p>Developing autonomous agents that can interact with changing environments is
an open challenge in machine learning. Robustness is particularly important in
these settings as agents are often fit offline on expert demonstrations but
deployed online where they must generalize to the closed feedback loop within
the environment. In this work, we explore the application of recurrent neural
networks to tasks of this nature and understand how a parameterization of their
recurrent connectivity influences robustness in closed-loop settings.
Specifically, we represent the recurrent connectivity as a function of rank and
sparsity and show both theoretically and empirically that modulating these two
variables has desirable effects on network dynamics. The proposed low-rank,
sparse connectivity induces an interpretable prior on the network that proves
to be most amenable for a class of models known as closed-form continuous-time
neural networks (CfCs). We find that CfCs with fewer parameters can outperform
their full-rank, fully-connected counterparts in the online setting under
distribution shift. This yields memory-efficient and robust agents while
opening a new perspective on how we can modulate network dynamics through
connectivity.
</p></li>
</ul>
<h3>Title: Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04128">http://arxiv.org/abs/2310.04128</a></li>
<li>Code URL: <a href="https://github.com/proroklab/ffm">https://github.com/proroklab/ffm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04128] Reinforcement Learning with Fast and Forgetful Memory](http://arxiv.org/abs/2310.04128) #memory</code></li>
<li>Summary: <p>Nearly all real world tasks are inherently partially observable,
necessitating the use of memory in Reinforcement Learning (RL). Most model-free
approaches summarize the trajectory into a latent Markov state using memory
models borrowed from Supervised Learning (SL), even though RL tends to exhibit
different training and efficiency characteristics. Addressing this discrepancy,
we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model
designed specifically for RL. Our approach constrains the model search space
via strong structural priors inspired by computational psychology. It is a
drop-in replacement for recurrent neural networks (RNNs) in recurrent RL
algorithms, achieving greater reward than RNNs across various recurrent
benchmarks and algorithms without changing any hyperparameters. Moreover, Fast
and Forgetful Memory exhibits training speeds two orders of magnitude faster
than RNNs, attributed to its logarithmic time and linear space complexity. Our
implementation is available at https://github.com/proroklab/ffm.
</p></li>
</ul>
<h3>Title: Program Synthesis with Best-First Bottom-Up Search. (arXiv:2310.04327v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04327">http://arxiv.org/abs/2310.04327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04327] Program Synthesis with Best-First Bottom-Up Search](http://arxiv.org/abs/2310.04327) #memory</code></li>
<li>Summary: <p>Cost-guided bottom-up search (BUS) algorithms use a cost function to guide
the search to solve program synthesis tasks. In this paper, we show that
current state-of-the-art cost-guided BUS algorithms suffer from a common
problem: they can lose useful information given by the model and fail to
perform the search in a best-first order according to a cost function. We
introduce a novel best-first bottom-up search algorithm, which we call Bee
Search, that does not suffer information loss and is able to perform
cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search
performs best-first search with respect to the generation of programs, i.e., it
does not even create in memory programs that are more expensive than the
solution program. It attains best-first ordering with respect to generation by
performing a search in an abstract space of program costs. We also introduce a
new cost function that better uses the information provided by an existing cost
model. Empirical results on string manipulation and bit-vector tasks show that
Bee Search can outperform existing cost-guided BUS approaches when employing
more complex domain-specific languages (DSLs); Bee Search and previous
approaches perform equally well with simpler DSLs. Furthermore, our new cost
function with Bee Search outperforms previous cost functions on string
manipulation tasks.
</p></li>
</ul>
<h3>Title: Saliency-Guided Hidden Associative Replay for Continual Learning. (arXiv:2310.04334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.04334">http://arxiv.org/abs/2310.04334</a></li>
<li>Code URL: <a href="https://github.com/baithebest/sharc">https://github.com/baithebest/sharc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.04334] Saliency-Guided Hidden Associative Replay for Continual Learning](http://arxiv.org/abs/2310.04334) #memory</code></li>
<li>Summary: <p>Continual Learning is a burgeoning domain in next-generation AI, focusing on
training neural networks over a sequence of tasks akin to human learning. While
CL provides an edge over traditional supervised learning, its central challenge
remains to counteract catastrophic forgetting and ensure the retention of prior
tasks during subsequent learning. Amongst various strategies to tackle this,
replay based methods have emerged as preeminent, echoing biological memory
mechanisms. However, these methods are memory intensive, often preserving
entire data samples, an approach inconsistent with humans selective memory
retention of salient experiences. While some recent works have explored the
storage of only significant portions of data in episodic memory, the inherent
nature of partial data necessitates innovative retrieval mechanisms. Current
solutions, like inpainting, approximate full data reconstruction from partial
cues, a method that diverges from genuine human memory processes. Addressing
these nuances, this paper presents the Saliency Guided Hidden Associative
Replay for Continual Learning. This novel framework synergizes associative
memory with replay-based strategies. SHARC primarily archives salient data
segments via sparse memory encoding. Importantly, by harnessing associative
memory paradigms, it introduces a content focused memory retrieval mechanism,
promising swift and near-perfect recall, bringing CL a step closer to authentic
human memory processes. Extensive experimental results demonstrate the
effectiveness of our proposed method for various continual learning tasks.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks. (arXiv:2310.03843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03843">http://arxiv.org/abs/2310.03843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03843] Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks](http://arxiv.org/abs/2310.03843) #few-shot</code></li>
<li>Summary: <p>Transferring a pretrained model to a downstream task can be as easy as
conducting linear probing with target data, that is, training a linear
classifier upon frozen features extracted from the pretrained model. As there
may exist significant gaps between pretraining and downstream datasets, one may
ask whether all dimensions of the pretrained features are useful for a given
downstream task. We show that, for linear probing, the pretrained features can
be extremely redundant when the downstream data is scarce, or few-shot. For
some cases such as 5-way 1-shot tasks, using only 1\% of the most important
feature dimensions is able to recover the performance achieved by using the
full representation. Interestingly, most dimensions are redundant only under
few-shot settings and gradually become useful when the number of shots
increases, suggesting that feature redundancy may be the key to characterizing
the "few-shot" nature of few-shot transfer problems. We give a theoretical
understanding of this phenomenon and show how dimensions with high variance and
small distance between class centroids can serve as confounding factors that
severely disturb classification results under few-shot settings. As an attempt
at solving this problem, we find that the redundant features are difficult to
identify accurately with a small number of training samples, but we can instead
adjust feature magnitude with a soft mask based on estimated feature
importance. We show that this method can generally improve few-shot transfer
performance across various pretrained models and downstream datasets.
</p></li>
</ul>
<h3>Title: CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03981">http://arxiv.org/abs/2310.03981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03981] CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation](http://arxiv.org/abs/2310.03981) #few-shot</code></li>
<li>Summary: <p>While pre-training on object detection tasks, such as Common Objects in
Contexts (COCO) [1], could significantly boost the performance of cell
segmentation, it still consumes on massive fine-annotated cell images [2] with
bounding boxes, masks, and cell types for every cell in every image, to
fine-tune the pre-trained model. To lower the cost of annotation, this work
considers the problem of pre-training DNN models for few-shot cell
segmentation, where massive unlabeled cell images are available but only a
small proportion is annotated. Hereby, we propose Cross-domain Unsupervised
Pre-training, namely CUPre, transferring the capability of object detection and
instance segmentation for common visual objects (learned from COCO) to the
visual domain of cells using unlabeled images. Given a standard COCO
pre-trained network with backbone, neck, and head modules, CUPre adopts an
alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in
every iteration of pre-training, AMT2 first trains the backbone with cell
images from multiple cell datasets via unsupervised momentum contrastive
learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets
via instance segmentation. After pre-training, CUPre fine-tunes the whole model
on the cell segmentation task using a few annotated images. We carry out
extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4]
datasets in few-shot instance segmentation settings. The experiment shows that
CUPre can outperform existing pre-training methods, achieving the highest
average precision (AP) for few-shot cell segmentation and detection.
</p></li>
</ul>
<h3>Title: RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03912">http://arxiv.org/abs/2310.03912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03912] RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels](http://arxiv.org/abs/2310.03912) #few-shot</code></li>
<li>Summary: <p>Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has
proven to be an invaluable technique for efficient, high-dimensional, black-box
optimization, a critical problem inherent to many applications such as
industrial design and scientific computing. Recent contributions have
introduced reinforcement learning (RL) to improve the optimization performance
on both single function optimization and \textit{few-shot} multi-objective
optimization. However, even few-shot techniques fail to exploit similarities
shared between closely related objectives. In this paper, we combine recent
developments in Deep Kernel Learning (DKL) and attention-based Transformer
models to improve the modeling powers of GP surrogates with meta-learning. We
propose a novel method for improving meta-learning BO surrogates by
incorporating attention mechanisms into DKL, empowering the surrogates to adapt
to contextual information gathered during the BO process. We combine this
Transformer Deep Kernel with a learned acquisition function trained with
continuous Soft Actor-Critic Reinforcement Learning to aid in exploration. This
Reinforced Transformer Deep Kernel (RTDK-BO) approach yields state-of-the-art
results in continuous high-dimensional optimization problems.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-10-09]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
