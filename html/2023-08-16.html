<h2>diffusion</h2>
<h3>Title: U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07421">http://arxiv.org/abs/2308.07421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07421] U-Turn Diffusion](http://arxiv.org/abs/2308.07421) #diffusion</code></li>
<li>Summary: <p>We present a comprehensive examination of score-based diffusion models of AI
for generating synthetic images. These models hinge upon a dynamic auxiliary
time mechanism driven by stochastic differential equations, wherein the score
function is acquired from input images. Our investigation unveils a criterion
for evaluating efficiency of the score-based diffusion models: the power of the
generative process depends on the ability to de-construct fast correlations
during the reverse/de-noising phase. To improve the quality of the produced
synthetic images, we introduce an approach coined "U-Turn Diffusion". The
U-Turn Diffusion technique starts with the standard forward diffusion process,
albeit with a condensed duration compared to conventional settings.
Subsequently, we execute the standard reverse dynamics, initialized with the
concluding configuration from the forward process. This U-Turn Diffusion
procedure, combining forward, U-turn, and reverse processes, creates a
synthetic image approximating an independent and identically distributed
(i.i.d.) sample from the probability distribution implicitly described via
input samples. To analyze relevant time scales we employ various analytical
tools, including auto-correlation analysis, weighted norm of the score-function
analysis, and Kolmogorov-Smirnov Gaussianity test. The tools guide us to
establishing that the Kernel Intersection Distance, a metric comparing the
quality of synthetic samples with real data samples, is minimized at the
optimal U-turn time.
</p></li>
</ul>
<h3>Title: UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. (arXiv:2308.07428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07428">http://arxiv.org/abs/2308.07428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07428] UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity](http://arxiv.org/abs/2308.07428) #diffusion</code></li>
<li>Summary: <p>Image reconstruction and captioning from brain activity evoked by visual
stimuli allow researchers to further understand the connection between the
human brain and the visual perception system. While deep generative models have
recently been employed in this field, reconstructing realistic captions and
images with both low-level details and high semantic fidelity is still a
challenging problem. In this work, we propose UniBrain: Unify Image
Reconstruction and Captioning All in One Diffusion Model from Human Brain
Activity. For the first time, we unify image reconstruction and captioning from
visual-evoked functional magnetic resonance imaging (fMRI) through a latent
diffusion model termed Versatile Diffusion. Specifically, we transform fMRI
voxels into text and image latent for low-level information and guide the
backward diffusion process through fMRI-based image and text conditions derived
from CLIP to generate realistic captions and images. UniBrain outperforms
current methods both qualitatively and quantitatively in terms of image
reconstruction and reports image captioning results for the first time on the
Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and
functional region-of-interest (ROI) analysis further exhibit the superiority of
UniBrain and provide comprehensive insight for visual-evoked brain decoding.
</p></li>
</ul>
<h3>Title: SGDiff: A Style Guided Diffusion Model for Fashion Synthesis. (arXiv:2308.07605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07605">http://arxiv.org/abs/2308.07605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07605] SGDiff: A Style Guided Diffusion Model for Fashion Synthesis](http://arxiv.org/abs/2308.07605) #diffusion</code></li>
<li>Summary: <p>This paper reports on the development of \textbf{a novel style guided
diffusion model (SGDiff)} which overcomes certain weaknesses inherent in
existing models for image synthesis. The proposed SGDiff combines image
modality with a pretrained text-to-image diffusion model to facilitate creative
fashion image synthesis. It addresses the limitations of text-to-image
diffusion models by incorporating supplementary style guidance, substantially
reducing training costs, and overcoming the difficulties of controlling
synthesized styles with text-only inputs. This paper also introduces a new
dataset -- SG-Fashion, specifically designed for fashion image synthesis
applications, offering high-resolution images and an extensive range of garment
categories. By means of comprehensive ablation study, we examine the
application of classifier-free guidance to a variety of conditions and validate
the effectiveness of the proposed model for generating fashion images of the
desired categories, product attributes, and styles. The contributions of this
paper include a novel classifier-free guidance method for multi-modal feature
fusion, a comprehensive dataset for fashion image synthesis application, a
thorough investigation on conditioned text-to-image synthesis, and valuable
insights for future research in the text-to-image synthesis domain. The code
and dataset are available at: \url{https://github.com/taited/SGDiff}.
</p></li>
</ul>
<h3>Title: Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement. (arXiv:2308.07652v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07652">http://arxiv.org/abs/2308.07652</a></li>
<li>Code URL: <a href="https://github.com/ballerin/v1diffusion">https://github.com/ballerin/v1diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07652] Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement](http://arxiv.org/abs/2308.07652) #diffusion</code></li>
<li>Summary: <p>Equipping the rototranslation group $SE(2)$ with a sub-Riemannian structure
inspired by the visual cortex V1, we propose algorithms for image inpainting
and enhancement based on hypoelliptic diffusion. We innovate on previous
implementations of the methods by Citti, Sarti and Boscain et al., by proposing
an alternative that prevents fading and capable of producing sharper results in
a procedure that we call WaxOn-WaxOff. We also exploit the sub-Riemannian
structure to define a completely new unsharp using $SE(2)$, analogous of the
classical unsharp filter for 2D image processing, with applications to image
enhancement. We demonstrate our method on blood vessels enhancement in retinal
scans.
</p></li>
</ul>
<h3>Title: Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training. (arXiv:2308.07665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07665">http://arxiv.org/abs/2308.07665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07665] Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training](http://arxiv.org/abs/2308.07665) #diffusion</code></li>
<li>Summary: <p>Exemplar-based sketch-to-photo synthesis allows users to generate
photo-realistic images based on sketches. Recently, diffusion-based methods
have achieved impressive performance on image generation tasks, enabling
highly-flexible control through text-driven generation or energy functions.
However, generating photo-realistic images with color and texture from sketch
images remains challenging for diffusion models. Sketches typically consist of
only a few strokes, with most regions left blank, making it difficult for
diffusion-based methods to produce photo-realistic images. In this work, we
propose a two-stage method named ``Inversion-by-Inversion" for exemplar-based
sketch-to-photo synthesis. This approach includes shape-enhancing inversion and
full-control inversion. During the shape-enhancing inversion process, an
uncolored photo is generated with the guidance of a shape-energy function. This
step is essential to ensure control over the shape of the generated photo. In
the full-control inversion process, we propose an appearance-energy function to
control the color and texture of the final generated photo.Importantly, our
Inversion-by-Inversion pipeline is training-free and can accept different types
of exemplars for color and texture control. We conducted extensive experiments
to evaluate our proposed method, and the results demonstrate its effectiveness.
</p></li>
</ul>
<h3>Title: DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>Code URL: <a href="https://github.com/cure-lab/diffguard">https://github.com/cure-lab/diffguard</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07687] DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models](http://arxiv.org/abs/2308.07687) #diffusion</code></li>
<li>Summary: <p>Given a classifier, the inherent property of semantic Out-of-Distribution
(OOD) samples is that their contents differ from all legal classes in terms of
semantics, namely semantic mismatch. There is a recent work that directly
applies it to OOD detection, which employs a conditional Generative Adversarial
Network (cGAN) to enlarge semantic mismatch in the image space. While achieving
remarkable OOD detection performance on small datasets, it is not applicable to
ImageNet-scale datasets due to the difficulty in training cGANs with both input
images and labels as conditions. As diffusion models are much easier to train
and amenable to various conditions compared to cGANs, in this work, we propose
to directly use pre-trained diffusion models for semantic mismatch-guided OOD
detection, named DiffGuard. Specifically, given an OOD input image and the
predicted label from the classifier, we try to enlarge the semantic difference
between the reconstructed OOD image under these conditions and the original
input image. We also present several test-time techniques to further strengthen
such differences. Experimental results show that DiffGuard is effective on both
Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily
combined with existing OOD detection techniques to achieve state-of-the-art OOD
detection results.
</p></li>
</ul>
<h3>Title: Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model. (arXiv:2308.07749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07749">http://arxiv.org/abs/2308.07749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07749] Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model](http://arxiv.org/abs/2308.07749) #diffusion</code></li>
<li>Summary: <p>The rising demand for creating lifelike avatars in the digital realm has led
to an increased need for generating high-quality human videos guided by textual
descriptions and poses. We propose Dancing Avatar, designed to fabricate human
motion videos driven by poses and textual cues. Our approach employs a
pretrained T2I diffusion model to generate each video frame in an
autoregressive fashion. The crux of innovation lies in our adept utilization of
the T2I diffusion model for producing video frames successively while
preserving contextual relevance. We surmount the hurdles posed by maintaining
human character and clothing consistency across varying poses, along with
upholding the background's continuity amidst diverse human movements. To ensure
consistent human appearances across the entire video, we devise an intra-frame
alignment module. This module assimilates text-guided synthesized human
character knowledge into the pretrained T2I diffusion model, synergizing
insights from ChatGPT. For preserving background continuity, we put forth a
background alignment pipeline, amalgamating insights from segment anything and
image inpainting techniques. Furthermore, we propose an inter-frame alignment
module that draws inspiration from an auto-regressive pipeline to augment
temporal consistency between adjacent frames, where the preceding frame guides
the synthesis process of the current frame. Comparisons with state-of-the-art
methods demonstrate that Dancing Avatar exhibits the capacity to generate human
videos with markedly superior quality, both in terms of human and background
fidelity, as well as temporal coherence compared to existing state-of-the-art
approaches.
</p></li>
</ul>
<h3>Title: CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction. (arXiv:2308.07837v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07837">http://arxiv.org/abs/2308.07837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07837] CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction](http://arxiv.org/abs/2308.07837) #diffusion</code></li>
<li>Summary: <p>In this paper, we present a novel shape reconstruction method leveraging
diffusion model to generate 3D sparse point cloud for the object captured in a
single RGB image. Recent methods typically leverage global embedding or local
projection-based features as the condition to guide the diffusion model.
However, such strategies fail to consistently align the denoised point cloud
with the given image, leading to unstable conditioning and inferior
performance. In this paper, we present CCD-3DR, which exploits a novel centered
diffusion probabilistic model for consistent local feature conditioning. We
constrain the noise and sampled point cloud from the diffusion model into a
subspace where the point cloud center remains unchanged during the forward
diffusion process and reverse process. The stable point cloud center further
serves as an anchor to align each point with its corresponding local
projection-based features. Extensive experiments on synthetic benchmark
ShapeNet-R2N2 demonstrate that CCD-3DR outperforms all competitors by a large
margin, with over 40% improvement. We also provide results on real-world
dataset Pix3D to thoroughly demonstrate the potential of CCD-3DR in real-world
applications. Codes will be released soon
</p></li>
</ul>
<h3>Title: StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models. (arXiv:2308.07863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07863">http://arxiv.org/abs/2308.07863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07863] StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models](http://arxiv.org/abs/2308.07863) #diffusion</code></li>
<li>Summary: <p>Content and style (C-S) disentanglement is a fundamental problem and critical
challenge of style transfer. Existing approaches based on explicit definitions
(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable
nor easy to control, resulting in entangled representations and less satisfying
results. In this paper, we propose a new C-S disentangled framework for style
transfer without using previous assumptions. The key insight is to explicitly
extract the content information and implicitly learn the complementary style
information, yielding interpretable and controllable C-S disentanglement and
style transfer. A simple yet effective CLIP-based style disentanglement loss
coordinated with a style reconstruction prior is introduced to disentangle C-S
in the CLIP image space. By further leveraging the powerful style removal and
generative ability of diffusion models, our framework achieves superior results
than state of the art and flexible C-S disentanglement and trade-off control.
Our work provides new insights into the C-S disentanglement in style transfer
and demonstrates the potential of diffusion models for learning
well-disentangled C-S characteristics.
</p></li>
</ul>
<h3>Title: Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides. (arXiv:2308.07441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07441] Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides](http://arxiv.org/abs/2308.07441) #diffusion</code></li>
<li>Summary: <p>Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have
recognized acute and chronic health and environmental effects. Machine learning
(ML) methods have significantly enhanced our capacity to predict NOx
concentrations at ground-level with high spatiotemporal resolution but may
suffer from high estimation bias since they lack physical and chemical
knowledge about air pollution dynamics. Chemical transport models (CTMs)
leverage this knowledge; however, accurate predictions of ground-level
concentrations typically necessitate extensive post-calibration. Here, we
present a physics-informed deep learning framework that encodes
advection-diffusion mechanisms and fluid dynamics constraints to jointly
predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures
fine-scale transport of NO2 and NOx, generates robust spatial extrapolation,
and provides explicit uncertainty estimation. The framework fuses
knowledge-driven physicochemical principles of CTMs with the predictive power
of ML for air quality exposure, health, and policy applications. Our approach
offers significant improvements over purely data-driven ML methods and has
unprecedented bias reduction in joint NO2 and NOx prediction.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects. (arXiv:2308.07391v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07391">http://arxiv.org/abs/2308.07391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07391] PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects](http://arxiv.org/abs/2308.07391) #self-supervised</code></li>
<li>Summary: <p>We address the task of simultaneous part-level reconstruction and motion
parameter estimation for articulated objects. Given two sets of multi-view
images of an object in two static articulation states, we decouple the movable
part from the static part and reconstruct shape and appearance while predicting
the motion parameters. To tackle this problem, we present PARIS: a
self-supervised, end-to-end architecture that learns part-level implicit shape
and appearance models and optimizes motion parameters jointly without any 3D
supervision, motion, or semantic annotation. Our experiments show that our
method generalizes better across object categories, and outperforms baselines
and prior work that are given 3D point clouds as input. Our approach improves
reconstruction relative to state-of-the-art baselines with a Chamfer-L1
distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and
achieves 5% error rate for motion estimation across 10 object categories.
</p></li>
</ul>
<p>Video summary at: https://youtu.be/tDSrROPCgUc
</p>

<h3>Title: Semantify: Simplifying the Control of 3D Morphable Models using CLIP. (arXiv:2308.07415v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07415">http://arxiv.org/abs/2308.07415</a></li>
<li>Code URL: <a href="https://github.com/Omergral/Semantify">https://github.com/Omergral/Semantify</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07415] Semantify: Simplifying the Control of 3D Morphable Models using CLIP](http://arxiv.org/abs/2308.07415) #self-supervised</code></li>
<li>Summary: <p>We present Semantify: a self-supervised method that utilizes the semantic
power of CLIP language-vision foundation model to simplify the control of 3D
morphable models. Given a parametric model, training data is created by
randomly sampling the model's parameters, creating various shapes and rendering
them. The similarity between the output images and a set of word descriptors is
calculated in CLIP's latent space. Our key idea is first to choose a small set
of semantically meaningful and disentangled descriptors that characterize the
3DMM, and then learn a non-linear mapping from scores across this set to the
parametric coefficients of the given 3DMM. The non-linear mapping is defined by
training a neural network without a human-in-the-loop. We present results on
numerous 3DMMs: body shape models, face shape and expression models, as well as
animal shapes. We demonstrate how our method defines a simple slider interface
for intuitive modeling, and show how the mapping can be used to instantly fit a
3D parametric body shape to in-the-wild images.
</p></li>
</ul>
<h3>Title: Multi-view 3D Face Reconstruction Based on Flame. (arXiv:2308.07551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07551">http://arxiv.org/abs/2308.07551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07551] Multi-view 3D Face Reconstruction Based on Flame](http://arxiv.org/abs/2308.07551) #self-supervised</code></li>
<li>Summary: <p>At present, face 3D reconstruction has broad application prospects in various
fields, but the research on it is still in the development stage. In this
paper, we hope to achieve better face 3D reconstruction quality by combining
multi-view training framework with face parametric model Flame, propose a
multi-view training and testing model MFNet (Multi-view Flame Network). We
build a self-supervised training framework and implement constraints such as
multi-view optical flow loss function and face landmark loss, and finally
obtain a complete MFNet. We propose innovative implementations of multi-view
optical flow loss and the covisible mask. We test our model on AFLW and
facescape datasets and also take pictures of our faces to reconstruct 3D faces
while simulating actual scenarios as much as possible, which achieves good
results. Our work mainly addresses the problem of combining parametric models
of faces with multi-view face 3D reconstruction and explores the implementation
of a Flame based multi-view training and testing framework for contributing to
the field of face 3D reconstruction.
</p></li>
</ul>
<h3>Title: Self-supervised Hypergraphs for Learning Multiple World Interpretations. (arXiv:2308.07615v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07615">http://arxiv.org/abs/2308.07615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07615] Self-supervised Hypergraphs for Learning Multiple World Interpretations](http://arxiv.org/abs/2308.07615) #self-supervised</code></li>
<li>Summary: <p>We present a method for learning multiple scene representations given a small
labeled set, by exploiting the relationships between such representations in
the form of a multi-task hypergraph. We also show how we can use the hypergraph
to improve a powerful pretrained VisTransformer model without any additional
labeled data. In our hypergraph, each node is an interpretation layer (e.g.,
depth or segmentation) of the scene. Within each hyperedge, one or several
input nodes predict the layer at the output node. Thus, each node could be an
input node in some hyperedges and an output node in others. In this way,
multiple paths can reach the same node, to form ensembles from which we obtain
robust pseudolabels, which allow self-supervised learning in the hypergraph. We
test different ensemble models and different types of hyperedges and show
superior performance to other multi-task graph models in the field. We also
introduce Dronescapes, a large video dataset captured with UAVs in different
complex real-world scenes, with multiple representations, suitable for
multi-task learning.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval. (arXiv:2308.07648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07648">http://arxiv.org/abs/2308.07648</a></li>
<li>Code URL: <a href="https://github.com/bladewaltz1/promptswitch">https://github.com/bladewaltz1/promptswitch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07648] Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval](http://arxiv.org/abs/2308.07648) #foundation model</code></li>
<li>Summary: <p>In text-video retrieval, recent works have benefited from the powerful
learning capabilities of pre-trained text-image foundation models (e.g., CLIP)
by adapting them to the video domain. A critical problem for them is how to
effectively capture the rich semantics inside the video using the image encoder
of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal
modeling techniques to fuse the text information into video frame
representations, which, however, incurs severe efficiency issues in large-scale
retrieval systems as the video representations must be recomputed online for
every text query. In this paper, we discard this problematic cross-modal fusion
process and aim to learn semantically-enhanced representations purely from the
video, so that the video representations can be computed offline and reused for
different texts. Concretely, we first introduce a spatial-temporal "Prompt
Cube" into the CLIP image encoder and iteratively switch it within the encoder
layers to efficiently incorporate the global video semantics into frame
representations. We then propose to apply an auxiliary video captioning
objective to train the frame representations, which facilitates the learning of
detailed video semantics by providing fine-grained guidance in the semantic
space. With a naive temporal fusion strategy (i.e., mean-pooling) on the
enhanced frame representations, we obtain state-of-the-art performances on
three benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.
</p></li>
</ul>
<h3>Title: A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision. (arXiv:2308.07898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07898">http://arxiv.org/abs/2308.07898</a></li>
<li>Code URL: <a href="https://github.com/jusiro/flair">https://github.com/jusiro/flair</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07898] A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision](http://arxiv.org/abs/2308.07898) #foundation model</code></li>
<li>Summary: <p>Foundation vision-language models are currently transforming computer vision,
and are on the rise in medical imaging fueled by their very promising
generalization capabilities. However, the initial attempts to transfer this new
paradigm to medical imaging have shown less impressive performances than those
observed in other domains, due to the significant domain shift and the complex,
expert domain knowledge inherent to medical-imaging tasks. Motivated by the
need for domain-expert foundation models, we present FLAIR, a pre-trained
vision-language model for universal retinal fundus image understanding. To this
end, we compiled 37 open-access, mostly categorical fundus imaging datasets
from various sources, with up to 97 different target conditions and 284,660
images. We integrate the expert's domain knowledge in the form of descriptive
textual prompts, during both pre-training and zero-shot inference, enhancing
the less-informative categorical supervision of the data. Such a textual
expert's knowledge, which we compiled from the relevant clinical literature and
community standards, describes the fine-grained features of the pathologies as
well as the hierarchies and dependencies between them. We report comprehensive
evaluations, which illustrate the benefit of integrating expert knowledge and
the strong generalization capabilities of FLAIR under difficult scenarios with
domain shifts or unseen categories. When adapted with a lightweight linear
probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the
few-shot regimes. Interestingly, FLAIR outperforms by a large margin more
generalist, larger-scale image-language models, which emphasizes the potential
of embedding experts' domain knowledge and the limitations of generalist models
in medical imaging.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation. (arXiv:2308.07528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07528">http://arxiv.org/abs/2308.07528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07528] Confidence Contours: Uncertainty-Aware Annotation for Medical Semantic Segmentation](http://arxiv.org/abs/2308.07528) #generative</code></li>
<li>Summary: <p>Medical image segmentation modeling is a high-stakes task where understanding
of uncertainty is crucial for addressing visual ambiguity. Prior work has
developed segmentation models utilizing probabilistic or generative mechanisms
to infer uncertainty from labels where annotators draw a singular boundary.
However, as these annotations cannot represent an individual annotator's
uncertainty, models trained on them produce uncertainty maps that are difficult
to interpret. We propose a novel segmentation representation, Confidence
Contours, which uses high- and low-confidence ``contours'' to capture
uncertainty directly, and develop a novel annotation system for collecting
contours. We conduct an evaluation on the Lung Image Dataset Consortium (LIDC)
and a synthetic dataset. From an annotation study with 30 participants, results
show that Confidence Contours provide high representative capacity without
considerably higher annotator effort. We also find that general-purpose
segmentation models can learn Confidence Contours at the same performance level
as standard singular annotations. Finally, from interviews with 5 medical
experts, we find that Confidence Contour maps are more interpretable than
Bayesian maps due to representation of structural uncertainty.
</p></li>
</ul>
<h3>Title: Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders. (arXiv:2308.07407v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07407">http://arxiv.org/abs/2308.07407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07407] Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders](http://arxiv.org/abs/2308.07407) #generative</code></li>
<li>Summary: <p>In collaboration with Postpartum Support International (PSI), a non-profit
organization dedicated to supporting caregivers with postpartum mood and
anxiety disorders, we developed three chatbots to provide context-specific
empathetic support to postpartum caregivers, leveraging both rule-based and
generative models. We present and evaluate the performance of our chatbots
using both machine-based metrics and human-based questionnaires. Overall, our
rule-based model achieves the best performance, with outputs that are close to
ground truth reference and contain the highest levels of empathy. Human users
prefer the rule-based chatbot over the generative chatbot for its
context-specific and human-like replies. Our generative chatbot also produced
empathetic responses and was described by human users as engaging. However,
limitations in the training dataset often result in confusing or nonsensical
responses. We conclude by discussing practical benefits of rule-based vs.
generative models for supporting individuals with mental health challenges. In
light of the recent surge of ChatGPT and BARD, we also discuss the
possibilities and pitfalls of large language models for digital mental
healthcare.
</p></li>
</ul>
<h3>Title: Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07462">http://arxiv.org/abs/2308.07462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07462] Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans](http://arxiv.org/abs/2308.07462) #generative</code></li>
<li>Summary: <p>The introduction of Artificial Intelligence (AI) generative language models
such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has
triggered a revolution that can transform how text is generated. This has many
implications, for example, as AI-generated text becomes a significant fraction
of the text in many disciplines, would this have an effect on the language
capabilities of readers and also on the training of newer AI tools? Would it
affect the evolution of languages? Focusing on one specific aspect of the
language: words; will the use of tools such as ChatGPT increase or reduce the
vocabulary used or the lexical richness (understood as the number of different
words used in a written or oral production) when writing a given text? This has
implications for words, as those not included in AI-generated content will tend
to be less and less popular and may eventually be lost. In this work, we
perform an initial comparison of the vocabulary and lexical richness of ChatGPT
and humans when performing the same tasks. In more detail, two datasets
containing the answers to different types of questions answered by ChatGPT and
humans are used, and the analysis shows that ChatGPT tends to use fewer
distinct words and lower lexical richness than humans. These results are very
preliminary and additional datasets and ChatGPT configurations have to be
evaluated to extract more general conclusions. Therefore, further research is
needed to understand how the use of ChatGPT and more broadly generative AI
tools will affect the vocabulary and lexical richness in different types of
text and languages.
</p></li>
</ul>
<h3>Title: Informed Named Entity Recognition Decoding for Generative Language Models. (arXiv:2308.07791v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07791] Informed Named Entity Recognition Decoding for Generative Language Models](http://arxiv.org/abs/2308.07791) #generative</code></li>
<li>Summary: <p>Ever-larger language models with ever-increasing capabilities are by now
well-established text processing tools. Alas, information extraction tasks such
as named entity recognition are still largely unaffected by this progress as
they are primarily based on the previous generation of encoder-only transformer
models. Here, we propose a simple yet effective approach, Informed Named Entity
Recognition Decoding (iNERD), which treats named entity recognition as a
generative process. It leverages the language understanding capabilities of
recent generative models in a future-proof manner and employs an informed
decoding scheme incorporating the restricted nature of information extraction
into open-ended text generation, improving performance and eliminating any risk
of hallucinations. We coarse-tune our model on a merged named entity corpus to
strengthen its performance, evaluate five generative language models on eight
named entity recognition datasets, and achieve remarkable results, especially
in an environment with an unknown entity class set, demonstrating the
adaptability of the approach.
</p></li>
</ul>
<h3>Title: Generating Personas for Games with Multimodal Adversarial Imitation Learning. (arXiv:2308.07598v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07598] Generating Personas for Games with Multimodal Adversarial Imitation Learning](http://arxiv.org/abs/2308.07598) #generative</code></li>
<li>Summary: <p>Reinforcement learning has been widely successful in producing agents capable
of playing games at a human level. However, this requires complex reward
engineering, and the agent's resulting policy is often unpredictable. Going
beyond reinforcement learning is necessary to model a wide range of human
playstyles, which can be difficult to represent with a reward function. This
paper presents a novel imitation learning approach to generate multiple persona
policies for playtesting. Multimodal Generative Adversarial Imitation Learning
(MultiGAIL) uses an auxiliary input parameter to learn distinct personas using
a single-agent model. MultiGAIL is based on generative adversarial imitation
learning and uses multiple discriminators as reward models, inferring the
environment reward by comparing the agent and distinct expert policies. The
reward from each discriminator is weighted according to the auxiliary input.
Our experimental analysis demonstrates the effectiveness of our technique in
two environments with continuous and discrete action spaces.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Future Video Prediction from a Single Frame for Video Anomaly Detection. (arXiv:2308.07783v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07783">http://arxiv.org/abs/2308.07783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07783] Future Video Prediction from a Single Frame for Video Anomaly Detection](http://arxiv.org/abs/2308.07783) #anomaly</code></li>
<li>Summary: <p>Video anomaly detection (VAD) is an important but challenging task in
computer vision. The main challenge rises due to the rarity of training samples
to model all anomaly cases. Hence, semi-supervised anomaly detection methods
have gotten more attention, since they focus on modeling normals and they
detect anomalies by measuring the deviations from normal patterns. Despite
impressive advances of these methods in modeling normal motion and appearance,
long-term motion modeling has not been effectively explored so far. Inspired by
the abilities of the future frame prediction proxy-task, we introduce the task
of future video prediction from a single frame, as a novel proxy-task for video
anomaly detection. This proxy-task alleviates the challenges of previous
methods in learning longer motion patterns. Moreover, we replace the initial
and future raw frames with their corresponding semantic segmentation map, which
not only makes the method aware of object class but also makes the prediction
task less complex for the model. Extensive experiments on the benchmark
datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the
method and the superiority of its performance compared to SOTA prediction-based
VAD methods.
</p></li>
</ul>
<h3>Title: ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition. (arXiv:2308.07815v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07815">http://arxiv.org/abs/2308.07815</a></li>
<li>Code URL: <a href="https://github.com/cool-xuan/imbalanced_sam">https://github.com/cool-xuan/imbalanced_sam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07815] ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition](http://arxiv.org/abs/2308.07815) #anomaly</code></li>
<li>Summary: <p>Class imbalance is a common challenge in real-world recognition tasks, where
the majority of classes have few samples, also known as tail classes. We
address this challenge with the perspective of generalization and empirically
find that the promising Sharpness-Aware Minimization (SAM) fails to address
generalization issues under the class-imbalanced setting. Through investigating
this specific type of task, we identify that its generalization bottleneck
primarily lies in the severe overfitting for tail classes with limited training
data. To overcome this bottleneck, we leverage class priors to restrict the
generalization scope of the class-agnostic SAM and propose a class-aware
smoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With the
guidance of class priors, our ImbSAM specifically improves generalization
targeting tail classes. We also verify the efficacy of ImbSAM on two
prototypical applications of class-imbalanced recognition: long-tailed
classification and semi-supervised anomaly detection, where our ImbSAM
demonstrates remarkable performance improvements for tail classes and anomaly.
Our code implementation is available at
https://github.com/cool-xuan/Imbalanced_SAM.
</p></li>
</ul>
<h3>Title: A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection. (arXiv:2308.07774v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07774] A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection](http://arxiv.org/abs/2308.07774) #anomaly</code></li>
<li>Summary: <p>A key component of many graph neural networks (GNNs) is the pooling
operation, which seeks to reduce the size of a graph while preserving important
structural information. However, most existing graph pooling strategies rely on
an assignment matrix obtained by employing a GNN layer, which is characterized
by trainable parameters, often leading to significant computational complexity
and a lack of interpretability in the pooling process. In this paper, we
propose an unsupervised graph encoder-decoder model to detect abnormal nodes
from graphs by learning an anomaly scoring function to rank nodes based on
their degree of abnormality. In the encoding stage, we design a novel pooling
mechanism, named LCPool, which leverages locality-constrained linear coding for
feature encoding to find a cluster assignment matrix by solving a least-squares
optimization problem with a locality regularization term. By enforcing locality
constraints during the coding process, LCPool is designed to be free from
learnable parameters, capable of efficiently handling large graphs, and can
effectively generate a coarser graph representation while retaining the most
significant structural characteristics of the graph. In the decoding stage, we
propose an unpooling operation, called LCUnpool, to reconstruct both the
structure and nodal features of the original graph. We conduct empirical
evaluations of our method on six benchmark datasets using several evaluation
metrics, and the results demonstrate its superiority over state-of-the-art
anomaly detection approaches.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: Link-Context Learning for Multimodal LLMs. (arXiv:2308.07891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07891">http://arxiv.org/abs/2308.07891</a></li>
<li>Code URL: <a href="https://github.com/isekai-portal/Link-Context-Learning">https://github.com/isekai-portal/Link-Context-Learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07891] Link-Context Learning for Multimodal LLMs](http://arxiv.org/abs/2308.07891) #in-context</code></li>
<li>Summary: <p>The ability to learn from context with novel concepts, and deliver
appropriate responses are essential in human conversations. Despite current
Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being
trained on mega-scale datasets, recognizing unseen images or understanding
novel concepts in a training-free manner remains a challenge. In-Context
Learning (ICL) explores training-free few-shot learning, where models are
encouraged to ``learn to learn" from limited tasks and generalize to unseen
tasks. In this work, we propose link-context learning (LCL), which emphasizes
"reasoning from cause and effect" to augment the learning capabilities of
MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal
relationship between the support set and the query set. By providing
demonstrations with causal links, LCL guides the model to discern not only the
analogy but also the underlying causal associations between data points, which
empowers MLLMs to recognize unseen images and understand novel concepts more
effectively. To facilitate the evaluation of this novel approach, we introduce
the ISEKAI dataset, comprising exclusively of unseen generated image-label
pairs designed for link-context learning. Extensive experiments show that our
LCL-MLLM exhibits strong link-context learning capabilities to novel concepts
over vanilla MLLMs. Code and data will be released at
https://github.com/isekai-portal/Link-Context-Learning.
</p></li>
</ul>
<h3>Title: RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07922">http://arxiv.org/abs/2308.07922</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07922] RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models](http://arxiv.org/abs/2308.07922) #in-context</code></li>
<li>Summary: <p>In this paper, we investigate the in-context learning ability of
retrieval-augmented encoder-decoder language models. We first conduct a
comprehensive analysis of the state-of-the-art ATLAS model and identify its
limitations in in-context learning, primarily due to a mismatch between
pretraining and testing, as well as a restricted context length. To address
these issues, we propose RAVEN, a model that combines retrieval-augmented
masked language modeling and prefix language modeling. We further introduce
Fusion-in-Context Learning to enhance the few-shot performance by enabling the
model to leverage more in-context examples without requiring additional
training or model modifications. Through extensive experiments, we demonstrate
that RAVEN significantly outperforms ATLAS and achieves results comparable to
the most advanced language models in certain scenarios, despite having
substantially fewer parameters. Our work underscores the potential of
retrieval-augmented encoder-decoder language models for in-context learning and
encourages further research in this direction.
</p></li>
</ul>
<h3>Title: Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models. (arXiv:2308.07847v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07847">http://arxiv.org/abs/2308.07847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07847] Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models](http://arxiv.org/abs/2308.07847) #in-context</code></li>
<li>Summary: <p>Large Language Models (LLMs) have led to significant improvements in many
tasks across various domains, such as code interpretation, response generation,
and ambiguity handling. These LLMs, however, when upgrading, primarily
prioritize enhancing user experience while neglecting security, privacy, and
safety implications. Consequently, unintended vulnerabilities or biases can be
introduced. Previous studies have predominantly focused on specific versions of
the models and disregard the potential emergence of new attack vectors
targeting the updated versions. Through the lens of adversarial examples within
the in-context learning framework, this longitudinal study addresses this gap
by conducting a comprehensive assessment of the robustness of successive
versions of LLMs, vis-`a-vis GPT-3.5. We conduct extensive experiments to
analyze and understand the impact of the robustness in two distinct learning
categories: zero-shot learning and few-shot learning. Our findings indicate
that, in comparison to earlier versions of LLMs, the updated versions do not
exhibit the anticipated level of robustness against adversarial attacks. In
addition, our study emphasizes the increased effectiveness of synergized
adversarial queries in most zero-shot learning and few-shot learning cases. We
hope that our study can lead to a more refined assessment of the robustness of
LLMs over time and provide valuable insights of these models for both
developers and users.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07439">http://arxiv.org/abs/2308.07439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07439] Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks](http://arxiv.org/abs/2308.07439) #memory</code></li>
<li>Summary: <p>Accurate prediction of vehicle trajectories is vital for advanced driver
assistance systems and autonomous vehicles. Existing methods mainly rely on
generic trajectory predictions derived from large datasets, overlooking the
personalized driving patterns of individual drivers. To address this gap, we
propose an approach for interaction-aware personalized vehicle trajectory
prediction that incorporates temporal graph neural networks. Our method
utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to
model the spatio-temporal interactions between target vehicles and their
surrounding traffic. To personalize the predictions, we establish a pipeline
that leverages transfer learning: the model is initially pre-trained on a
large-scale trajectory dataset and then fine-tuned for each driver using their
specific driving data. We employ human-in-the-loop simulation to collect
personalized naturalistic driving trajectories and corresponding surrounding
vehicle trajectories. Experimental results demonstrate the superior performance
of our personalized GCN-LSTM model, particularly for longer prediction
horizons, compared to its generic counterpart. Moreover, the personalized model
outperforms individual models created without pre-training, emphasizing the
significance of pre-training on a large dataset to avoid overfitting. By
incorporating personalization, our approach enhances trajectory prediction
accuracy.
</p></li>
</ul>
<h3>Title: Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07575] Story Visualization by Online Text Augmentation with Context Memory](http://arxiv.org/abs/2308.07575) #memory</code></li>
<li>Summary: <p>Story visualization (SV) is a challenging text-to-image generation task for
the difficulty of not only rendering visual details from the text descriptions
but also encoding a long-term context across multiple sentences. While prior
efforts mostly focus on generating a semantically relevant image for each
sentence, encoding a context spread across the given paragraph to generate
contextually convincing images (e.g., with a correct character or with a proper
background of the scene) remains a challenge. To this end, we propose a novel
memory architecture for the Bi-directional Transformers with an online text
augmentation that generates multiple pseudo-descriptions as supplementary
supervision during training, for better generalization to the language
variation at inference. In extensive experiments on the two popular SV
benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method
significantly outperforms the state of the arts in various evaluation metrics
including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with
similar or less computational complexity.
</p></li>
</ul>
<h3>Title: AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model. (arXiv:2308.07593v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07593">http://arxiv.org/abs/2308.07593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07593] AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model](http://arxiv.org/abs/2308.07593) #memory</code></li>
<li>Summary: <p>Visual Speech Recognition (VSR) is the task of predicting spoken words from
silent lip movements. VSR is regarded as a challenging task because of the
insufficient information on lip movements. In this paper, we propose an Audio
Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement
the insufficient speech information of visual modality by using audio modality.
Different from the previous methods, the proposed AKVSR 1) utilizes rich audio
knowledge encoded by a large-scale pretrained audio model, 2) saves the
linguistic information of audio knowledge in compact audio memory by discarding
the non-linguistic information from the audio through quantization, and 3)
includes Audio Bridging Module which can find the best-matched audio features
from the compact audio memory, which makes our training possible without audio
inputs, once after the compact audio memory is composed. We validate the
effectiveness of the proposed method through extensive experiments, and achieve
new state-of-the-art performances on the widely-used datasets, LRS2 and LRS3.
</p></li>
</ul>
<h3>Title: Memory-and-Anticipation Transformer for Online Action Understanding. (arXiv:2308.07893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07893">http://arxiv.org/abs/2308.07893</a></li>
<li>Code URL: <a href="https://github.com/echo0125/memory-and-anticipation-transformer">https://github.com/echo0125/memory-and-anticipation-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07893] Memory-and-Anticipation Transformer for Online Action Understanding](http://arxiv.org/abs/2308.07893) #memory</code></li>
<li>Summary: <p>Most existing forecasting systems are memory-based methods, which attempt to
mimic human forecasting ability by employing various memory mechanisms and have
progressed in temporal modeling for memory dependency. Nevertheless, an obvious
weakness of this paradigm is that it can only model limited historical
dependence and can not transcend the past. In this paper, we rethink the
temporal dependence of event evolution and propose a novel
memory-anticipation-based paradigm to model an entire temporal structure,
including the past, present, and future. Based on this idea, we present
Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based
approach, to address the online action detection and anticipation tasks. In
addition, owing to the inherent superiority of MAT, it can process online
action detection and anticipation tasks in a unified manner. The proposed MAT
model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and
EPIC-Kitchens-100, for online action detection and anticipation tasks, and it
significantly outperforms all existing methods. Code is available at
https://github.com/Echo0125/Memory-and-Anticipation-Transformer.
</p></li>
</ul>
<h3>Title: Helping Hands: An Object-Aware Ego-Centric Video Recognition Model. (arXiv:2308.07918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07918">http://arxiv.org/abs/2308.07918</a></li>
<li>Code URL: <a href="https://github.com/chuhanxx/helping_hand_for_egocentric_videos">https://github.com/chuhanxx/helping_hand_for_egocentric_videos</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07918] Helping Hands: An Object-Aware Ego-Centric Video Recognition Model](http://arxiv.org/abs/2308.07918) #memory</code></li>
<li>Summary: <p>We introduce an object-aware decoder for improving the performance of
spatio-temporal representations on ego-centric videos. The key idea is to
enhance object-awareness during training by tasking the model to predict hand
positions, object positions, and the semantic label of the objects using paired
captions when available. At inference time the model only requires RGB frames
as inputs, and is able to track and ground objects (although it has not been
trained explicitly for this). We demonstrate the performance of the
object-aware representations learnt by our model, by: (i) evaluating it for
strong transfer, i.e. through zero-shot testing, on a number of downstream
video-text retrieval and classification benchmarks; and (ii) by using the
representations learned as input for long-term video understanding tasks (e.g.
Episodic Memory in Ego4D). In all cases the performance improves over the state
of the art -- even compared to networks trained with far larger batch sizes. We
also show that by using noisy image-level detection as pseudo-labels in
training, the model learns to provide better bounding boxes using video
consistency, as well as grounding the words in the associated text
descriptions. Overall, we show that the model can act as a drop-in replacement
for an ego-centric video model to improve performance through visual-text
grounding.
</p></li>
</ul>
<h3>Title: Attention Is Not All You Need Anymore. (arXiv:2308.07661v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07661] Attention Is Not All You Need Anymore](http://arxiv.org/abs/2308.07661) #memory</code></li>
<li>Summary: <p>In recent years, the popular Transformer architecture has achieved great
success in many application areas, including natural language processing and
computer vision. Many existing works aim to reduce the computational and memory
complexity of the self-attention mechanism in the Transformer by trading off
performance. However, performance is key for the continuing success of the
Transformer. In this paper, a drop-in replacement for the self-attention
mechanism in the Transformer, called the Extractor, is proposed. Experimental
results show that replacing the self-attention mechanism with the Extractor
improves the performance of the Transformer. Furthermore, the proposed
Extractor has the potential to run faster than the self-attention since it has
a much shorter critical path of computation. Additionally, the sequence
prediction problem in the context of text generation is formulated using
variable-length discrete-time Markov chains, and the Transformer is reviewed
based on our understanding.
</p></li>
</ul>
<h3>Title: White-Box Adversarial Attacks on Deep Learning-Based Radio Frequency Fingerprint Identification. (arXiv:2308.07433v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07433">http://arxiv.org/abs/2308.07433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07433] White-Box Adversarial Attacks on Deep Learning-Based Radio Frequency Fingerprint Identification](http://arxiv.org/abs/2308.07433) #memory</code></li>
<li>Summary: <p>Radio frequency fingerprint identification (RFFI) is an emerging technique
for the lightweight authentication of wireless Internet of things (IoT)
devices. RFFI exploits unique hardware impairments as device identifiers, and
deep learning is widely deployed as the feature extractor and classifier for
RFFI. However, deep learning is vulnerable to adversarial attacks, where
adversarial examples are generated by adding perturbation to clean data for
causing the classifier to make wrong predictions. Deep learning-based RFFI has
been shown to be vulnerable to such attacks, however, there is currently no
exploration of effective adversarial attacks against a diversity of RFFI
classifiers. In this paper, we report on investigations into white-box attacks
(non-targeted and targeted) using two approaches, namely the fast gradient sign
method (FGSM) and projected gradient descent (PGD). A LoRa testbed was built
and real datasets were collected. These adversarial examples have been
experimentally demonstrated to be effective against convolutional neural
networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent
units (GRU).
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Improved Region Proposal Network for Enhanced Few-Shot Object Detection. (arXiv:2308.07535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07535">http://arxiv.org/abs/2308.07535</a></li>
<li>Code URL: <a href="https://github.com/zshanggu/htrpn">https://github.com/zshanggu/htrpn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07535] Improved Region Proposal Network for Enhanced Few-Shot Object Detection](http://arxiv.org/abs/2308.07535) #few-shot</code></li>
<li>Summary: <p>Despite significant success of deep learning in object detection tasks, the
standard training of deep neural networks requires access to a substantial
quantity of annotated images across all classes. Data annotation is an arduous
and time-consuming endeavor, particularly when dealing with infrequent objects.
Few-shot object detection (FSOD) methods have emerged as a solution to the
limitations of classic object detection approaches based on deep learning. FSOD
methods demonstrate remarkable performance by achieving robust object detection
using a significantly smaller amount of training data. A challenge for FSOD is
that instances from novel classes that do not belong to the fixed set of
training classes appear in the background and the base model may pick them up
as potential objects. These objects behave similarly to label noise because
they are classified as one of the training dataset classes, leading to FSOD
performance degradation. We develop a semi-supervised algorithm to detect and
then utilize these unlabeled novel objects as positive samples during the FSOD
training stage to improve FSOD performance. Specifically, we develop a
hierarchical ternary classification region proposal network (HTRPN) to localize
the potential unlabeled novel objects and assign them new objectness labels to
distinguish these objects from the base training dataset classes. Our improved
hierarchical sampling strategy for the region proposal network (RPN) also
boosts the perception ability of the object detection model for large objects.
We test our approach and COCO and PASCAL VOC baselines that are commonly used
in FSOD literature. Our experimental results indicate that our method is
effective and outperforms the existing state-of-the-art (SOTA) FSOD methods.
Our implementation is provided as a supplement to support reproducibility of
the results.
</p></li>
</ul>
<h3>Title: Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond. (arXiv:2308.07539v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07539">http://arxiv.org/abs/2308.07539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07539] Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond](http://arxiv.org/abs/2308.07539) #few-shot</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) aims to segment the novel classes with a few
annotated images. Due to CLIP's advantages of aligning visual and textual
information, the integration of CLIP can enhance the generalization ability of
FSS model. However, even with the CLIP model, the existing CLIP-based FSS
methods are still subject to the biased prediction towards base classes, which
is caused by the class-specific feature level interactions. To solve this
issue, we propose a visual and textual Prior Guided Mask Assemble Network
(PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the
bias, and formulates diverse tasks into a unified manner by assembling the
prior through affinity. Specifically, the class-relevant textual and visual
features are first transformed to class-agnostic prior in the form of
probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including
multiple General Assemble Units (GAUs) is introduced. It considers diverse and
plug-and-play interactions, such as visual-textual, inter- and intra-image,
training-free, and high-order ones. Lastly, to ensure the class-agnostic
ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed
to flexibly exploit the assembled masks and low-level features, without relying
on any class-specific information. It achieves new state-of-the-art results in
the FSS task, with mIoU of $77.6$ on $\text{PASCAL-}5^i$ and $59.4$ on
$\text{COCO-}20^i$ in 1-shot scenario. Beyond this, we show that without extra
re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS,
co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot
segmentation framework.
</p></li>
</ul>
<h3>Title: Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation. (arXiv:2308.07624v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.07624">http://arxiv.org/abs/2308.07624</a></li>
<li>Code URL: <a href="https://github.com/peteryyzhang/few-shot-self-prompt-sam">https://github.com/peteryyzhang/few-shot-self-prompt-sam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.07624] Self-Prompting Large Vision Models for Few-Shot Medical Image Segmentation](http://arxiv.org/abs/2308.07624) #few-shot</code></li>
<li>Summary: <p>Recent advancements in large foundation models have shown promising potential
in the medical industry due to their flexible prompting capability. One such
model, the Segment Anything Model (SAM), a prompt-driven segmentation model,
has shown remarkable performance improvements, surpassing state-of-the-art
approaches in medical image segmentation. However, existing methods primarily
rely on tuning strategies that require extensive data or prior prompts tailored
to the specific task, making it particularly challenging when only a limited
number of data samples are available. In this paper, we propose a novel
perspective on self-prompting in medical vision applications. Specifically, we
harness the embedding space of SAM to prompt itself through a simple yet
effective linear pixel-wise classifier. By preserving the encoding capabilities
of the large model, the contextual information from its decoder, and leveraging
its interactive promptability, we achieve competitive results on multiple
datasets (i.e. improvement of more than 15% compared to fine-tuning the mask
decoder using a few images).
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-16]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
