<h2>diffusion</h2>
<h3>Title: Resolution Chromatography of Diffusion Models. (arXiv:2401.10247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10247">http://arxiv.org/abs/2401.10247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10247] Resolution Chromatography of Diffusion Models](http://arxiv.org/abs/2401.10247) #diffusion</code></li>
<li>Summary: <p>Diffusion models generate high-resolution images through iterative stochastic
processes. In particular, the denoising method is one of the most popular
approaches that predicts the noise in samples and denoises it at each time
step. It has been commonly observed that the resolution of generated samples
changes over time, starting off blurry and coarse, and becoming sharper and
finer. In this paper, we introduce "resolution chromatography" that indicates
the signal generation rate of each resolution, which is very helpful concept to
mathematically explain this coarse-to-fine behavior in generation process, to
understand the role of noise schedule, and to design time-dependent modulation.
Using resolution chromatography, we determine which resolution level becomes
dominant at a specific time step, and experimentally verify our theory with
text-to-image diffusion models. We also propose some direct applications
utilizing the concept: upscaling pre-trained models to higher resolutions and
time-dependent prompt composing. Our theory not only enables a better
understanding of numerous pre-existing techniques for manipulating image
generation, but also suggests the potential for designing better noise
schedules.
</p></li>
</ul>
<h3>Title: Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution. (arXiv:2401.10404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10404">http://arxiv.org/abs/2401.10404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10404] Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution](http://arxiv.org/abs/2401.10404) #diffusion</code></li>
<li>Summary: <p>We propose an efficient diffusion-based text-to-video super-resolution (SR)
tuning approach that leverages the readily learned capacity of pixel level
image diffusion model to capture spatial information for video generation. To
accomplish this goal, we design an efficient architecture by inflating the
weightings of the text-to-image SR model into our video generation framework.
Additionally, we incorporate a temporal adapter to ensure temporal coherence
across video frames. We investigate different tuning approaches based on our
inflated architecture and report trade-offs between computational costs and
super-resolution quality. Empirical evaluation, both quantitative and
qualitative, on the Shutterstock video dataset, demonstrates that our approach
is able to perform text-to-video SR generation with good visual quality and
temporal consistency. To evaluate temporal coherence, we also present
visualizations in video format in
https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .
</p></li>
</ul>
<h3>Title: Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion. (arXiv:2401.10786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10786">http://arxiv.org/abs/2401.10786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10786] Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion](http://arxiv.org/abs/2401.10786) #diffusion</code></li>
<li>Summary: <p>Directly generating scenes from satellite imagery offers exciting
possibilities for integration into applications like games and map services.
However, challenges arise from significant view changes and scene scale.
Previous efforts mainly focused on image or video generation, lacking
exploration into the adaptability of scene generation for arbitrary views.
Existing 3D generation works either operate at the object level or are
difficult to utilize the geometry obtained from satellite imagery. To overcome
these limitations, we propose a novel architecture for direct 3D scene
generation by introducing diffusion models into 3D sparse representations and
combining them with neural rendering techniques. Specifically, our approach
generates texture colors at the point level for a given geometry using a 3D
diffusion model first, which is then transformed into a scene representation in
a feed-forward manner. The representation can be utilized to render arbitrary
views which would excel in both single-frame quality and inter-frame
consistency. Experiments in two city-scale datasets show that our model
demonstrates proficiency in generating photo-realistic street-view image
sequences and cross-view urban scenes from satellite imagery.
</p></li>
</ul>
<h3>Title: ActAnywhere: Subject-Aware Video Background Generation. (arXiv:2401.10822v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10822">http://arxiv.org/abs/2401.10822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10822] ActAnywhere: Subject-Aware Video Background Generation](http://arxiv.org/abs/2401.10822) #diffusion</code></li>
<li>Summary: <p>Generating video background that tailors to foreground subject motion is an
important problem for the movie industry and visual effects community. This
task involves synthesizing background that aligns with the motion and
appearance of the foreground subject, while also complies with the artist's
creative intention. We introduce ActAnywhere, a generative model that automates
this process which traditionally requires tedious manual efforts. Our model
leverages the power of large-scale video diffusion models, and is specifically
tailored for this task. ActAnywhere takes a sequence of foreground subject
segmentation as input and an image that describes the desired scene as
condition, to produce a coherent video with realistic foreground-background
interactions while adhering to the condition frame. We train our model on a
large-scale dataset of human-scene interaction videos. Extensive evaluations
demonstrate the superior performance of our model, significantly outperforming
baselines. Moreover, we show that ActAnywhere generalizes to diverse
out-of-distribution samples, including non-human subjects. Please visit our
project webpage at https://actanywhere.github.io.
</p></li>
</ul>
<h3>Title: Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10889">http://arxiv.org/abs/2401.10889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10889] Synthesizing Moving People with 3D Control](http://arxiv.org/abs/2401.10889) #diffusion</code></li>
<li>Summary: <p>In this paper, we present a diffusion model-based framework for animating
people from a single image for a given target 3D motion sequence. Our approach
has two core components: a) learning priors about invisible parts of the human
body and clothing, and b) rendering novel body poses with proper clothing and
texture. For the first part, we learn an in-filling diffusion model to
hallucinate unseen parts of a person given a single image. We train this model
on texture map space, which makes it more sample-efficient since it is
invariant to pose and viewpoint. Second, we develop a diffusion-based rendering
pipeline, which is controlled by 3D human poses. This produces realistic
renderings of novel poses of the person, including clothing, hair, and
plausible in-filling of unseen regions. This disentangled approach allows our
method to generate a sequence of images that are faithful to the target motion
in the 3D pose and, to the input image in terms of visual similarity. In
addition to that, the 3D control allows various synthetic camera trajectories
to render a person. Our experiments show that our method is resilient in
generating prolonged motions and varied challenging and complex poses compared
to prior methods. Please check our website for more details:
https://boyiliee.github.io/3DHM.github.io/.
</p></li>
</ul>
<h3>Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10700">http://arxiv.org/abs/2401.10700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10700] Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model](http://arxiv.org/abs/2401.10700) #diffusion</code></li>
<li>Summary: <p>Safe offline RL is a promising way to bypass risky online interactions
towards safe policy learning. Most existing methods only enforce soft
constraints, i.e., constraining safety violations in expectation below
thresholds predetermined. This can lead to potentially unsafe outcomes, thus
unacceptable in safety-critical scenarios. An alternative is to enforce the
hard constraint of zero violation. However, this can be challenging in offline
setting, as it needs to strike the right balance among three highly intricate
and correlated aspects: safety constraint satisfaction, reward maximization,
and behavior regularization imposed by offline datasets. Interestingly, we
discover that via reachability analysis of safe-control theory, the hard safety
constraint can be equivalently translated to identifying the largest feasible
region given the offline dataset. This seamlessly converts the original trilogy
problem to a feasibility-dependent objective, i.e., maximizing reward value
within the feasible region while minimizing safety risks in the infeasible
region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline
RL), which allows safety constraint adherence, reward maximization, and offline
policy learning to be realized via three decoupled processes, while offering
strong safety performance and stability. In FISOR, the optimal policy for the
translated optimization problem can be derived in a special form of weighted
behavior cloning. Thus, we propose a novel energy-guided diffusion model that
does not require training a complicated time-dependent classifier to extract
the policy, greatly simplifying the training. We compare FISOR against
baselines on DSRL benchmark for safe offline RL. Evaluation results show that
FISOR is the only method that can guarantee safety satisfaction in all tasks,
while achieving top returns in most tasks.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10474">http://arxiv.org/abs/2401.10474</a></li>
<li>Code URL: <a href="https://github.com/HanxunH/LDReg">https://github.com/HanxunH/LDReg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10474] LDReg: Local Dimensionality Regularized Self-Supervised Learning](http://arxiv.org/abs/2401.10474) #self-supervised</code></li>
<li>Summary: <p>Representations learned via self-supervised learning (SSL) can be susceptible
to dimensional collapse, where the learned representation subspace is of
extremely low dimensionality and thus fails to represent the full data
distribution and modalities. Dimensional collapse also known as the
"underfilling" phenomenon is one of the major causes of degraded performance on
downstream tasks. Previous work has investigated the dimensional collapse
problem of SSL at a global level. In this paper, we demonstrate that
representations can span over high dimensional space globally, but collapse
locally. To address this, we propose a method called $\textit{local
dimensionality regularization (LDReg)}$. Our formulation is based on the
derivation of the Fisher-Rao metric to compare and optimize local distance
distributions at an asymptotically small radius for each data point. By
increasing the local intrinsic dimensionality, we demonstrate through a range
of experiments that LDReg improves the representation quality of SSL. The
results also show that LDReg can regularize dimensionality at both local and
global levels.
</p></li>
</ul>
<h3>Title: 3D Shape Completion on Unseen Categories:A Weakly-supervised Approach. (arXiv:2401.10578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10578">http://arxiv.org/abs/2401.10578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10578] 3D Shape Completion on Unseen Categories:A Weakly-supervised Approach](http://arxiv.org/abs/2401.10578) #self-supervised</code></li>
<li>Summary: <p>3D shapes captured by scanning devices are often incomplete due to occlusion.
3D shape completion methods have been explored to tackle this limitation.
However, most of these methods are only trained and tested on a subset of
categories, resulting in poor generalization to unseen categories. In this
paper, we introduce a novel weakly-supervised framework to reconstruct the
complete shapes from unseen categories. We first propose an end-to-end
prior-assisted shape learning network that leverages data from the seen
categories to infer a coarse shape. Specifically, we construct a prior bank
consisting of representative shapes from the seen categories. Then, we design a
multi-scale pattern correlation module for learning the complete shape of the
input by analyzing the correlation between local patterns within the input and
the priors at various scales. In addition, we propose a self-supervised shape
refinement model to further refine the coarse shape. Considering the shape
variability of 3D objects across categories, we construct a category-specific
prior bank to facilitate shape refinement. Then, we devise a voxel-based
partial matching loss and leverage the partial scans to drive the refinement
process. Extensive experimental results show that our approach is superior to
state-of-the-art methods by a large margin.
</p></li>
</ul>
<h3>Title: Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10831">http://arxiv.org/abs/2401.10831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10831] Understanding Video Transformers via Universal Concept Discovery](http://arxiv.org/abs/2401.10831) #self-supervised</code></li>
<li>Summary: <p>This paper studies the problem of concept-based interpretability of
transformer representations for videos. Concretely, we seek to explain the
decision-making process of video transformers based on high-level,
spatiotemporal concepts that are automatically discovered. Prior research on
concept-based interpretability has concentrated solely on image-level tasks.
Comparatively, video models deal with the added temporal dimension, increasing
complexity and posing challenges in identifying dynamic concepts over time. In
this work, we systematically address these challenges by introducing the first
Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose
an efficient approach for unsupervised identification of units of video
transformer representations - concepts, and ranking their importance to the
output of a model. The resulting concepts are highly interpretable, revealing
spatio-temporal reasoning mechanisms and object-centric representations in
unstructured video models. Performing this analysis jointly over a diverse set
of supervised and self-supervised representations, we discover that some of
these mechanism are universal in video transformers. Finally, we demonstrate
that VTCDcan be used to improve model performance for fine-grained tasks.
</p></li>
</ul>
<h3>Title: Data-driven grapheme-to-phoneme representations for a lexicon-free text-to-speech. (arXiv:2401.10465v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10465">http://arxiv.org/abs/2401.10465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10465] Data-driven grapheme-to-phoneme representations for a lexicon-free text-to-speech](http://arxiv.org/abs/2401.10465) #self-supervised</code></li>
<li>Summary: <p>Grapheme-to-Phoneme (G2P) is an essential first step in any modern,
high-quality Text-to-Speech (TTS) system. Most of the current G2P systems rely
on carefully hand-crafted lexicons developed by experts. This poses a two-fold
problem. Firstly, the lexicons are generated using a fixed phoneme set,
usually, ARPABET or IPA, which might not be the most optimal way to represent
phonemes for all languages. Secondly, the man-hours required to produce such an
expert lexicon are very high. In this paper, we eliminate both of these issues
by using recent advances in self-supervised learning to obtain data-driven
phoneme representations instead of fixed representations. We compare our
lexicon-free approach against strong baselines that utilize a well-crafted
lexicon. Furthermore, we show that our data-driven lexicon-free method performs
as good or even marginally better than the conventional rule-based or
lexicon-based neural G2Ps in terms of Mean Opinion Score (MOS) while using no
prior language lexicon or phoneme set, i.e. no linguistic expertise.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data. (arXiv:2401.10891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10891">http://arxiv.org/abs/2401.10891</a></li>
<li>Code URL: <a href="https://github.com/LiheYoung/Depth-Anything">https://github.com/LiheYoung/Depth-Anything</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10891] Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](http://arxiv.org/abs/2401.10891) #foundation model</code></li>
<li>Summary: <p>This work presents Depth Anything, a highly practical solution for robust
monocular depth estimation. Without pursuing novel technical modules, we aim to
build a simple yet powerful foundation model dealing with any images under any
circumstances. To this end, we scale up the dataset by designing a data engine
to collect and automatically annotate large-scale unlabeled data (~62M), which
significantly enlarges the data coverage and thus is able to reduce the
generalization error. We investigate two simple yet effective strategies that
make data scaling-up promising. First, a more challenging optimization target
is created by leveraging data augmentation tools. It compels the model to
actively seek extra visual knowledge and acquire robust representations.
Second, an auxiliary supervision is developed to enforce the model to inherit
rich semantic priors from pre-trained encoders. We evaluate its zero-shot
capabilities extensively, including six public datasets and randomly captured
photos. It demonstrates impressive generalization ability. Further, through
fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs
are set. Our better depth model also results in a better depth-conditioned
ControlNet. Our models are released at
https://github.com/LiheYoung/Depth-Anything.
</p></li>
</ul>
<h3>Title: Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats. (arXiv:2401.10375v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10375">http://arxiv.org/abs/2401.10375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10375] Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats](http://arxiv.org/abs/2401.10375) #foundation model</code></li>
<li>Summary: <p>Federated Learning (FL) addresses critical issues in machine learning related
to data privacy and security, yet suffering from data insufficiency and
imbalance under certain circumstances. The emergence of foundation models (FMs)
offers potential solutions to the limitations of existing FL frameworks, e.g.,
by generating synthetic data for model initialization. However, due to the
inherent safety concerns of FMs, integrating FMs into FL could introduce new
risks, which remains largely unexplored. To address this gap, we conduct the
first investigation on the vulnerability of FM integrated FL (FM-FL) under
adversarial threats. Based on a unified framework of FM-FL, we introduce a
novel attack strategy that exploits safety issues of FM to compromise FL client
models. Through extensive experiments with well-known models and benchmark
datasets in both image and text domains, we reveal the high susceptibility of
the FM-FL to this new threat under various FL configurations. Furthermore, we
find that existing FL defense strategies offer limited protection against this
novel attack approach. This research highlights the critical need for enhanced
security measures in FL in the era of FMs.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10299">http://arxiv.org/abs/2401.10299</a></li>
<li>Code URL: <a href="https://github.com/QQ583304953/Bridge-Flow">https://github.com/QQ583304953/Bridge-Flow</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10299] An attempt to generate new bridge types from latent space of generative flow](http://arxiv.org/abs/2401.10299) #generative</code></li>
<li>Summary: <p>Through examples of coordinate and probability transformation between
different distributions, the basic principle of normalizing flow is introduced
in a simple and concise manner. From the perspective of the distribution of
random variable function, the essence of probability transformation is
explained, and the scaling factor Jacobian determinant of probability
transformation is introduced. Treating the dataset as a sample from the
population, obtaining normalizing flow is essentially through sampling surveys
to statistically infer the numerical features of the population, and then the
loss function is established by using the maximum likelihood estimation method.
This article introduces how normalizing flow cleverly solves the two major
application challenges of high-dimensional matrix determinant calculation and
neural network reversible transformation. Using symmetric structured image
dataset of three-span beam bridge, arch bridge, cable-stayed bridge and
suspension bridge, constructing and training normalizing flow based on the Glow
API in the TensorFlow Probability library. The model can smoothly transform the
complex distribution of the bridge dataset into a standard normal distribution,
and from the obtained latent space sampling, it can generate new bridge types
that are different from the training dataset.
</p></li>
</ul>
<h3>Title: On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss. (arXiv:2401.10526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10526">http://arxiv.org/abs/2401.10526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10526] On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss](http://arxiv.org/abs/2401.10526) #generative</code></li>
<li>Summary: <p>Large-scale language-vision pre-training models, such as CLIP, have achieved
remarkable text-guided image morphing results by leveraging several
unconditional generative models. However, existing CLIP-guided image morphing
methods encounter difficulties when morphing photorealistic images.
Specifically, existing guidance fails to provide detailed explanations of the
morphing regions within the image, leading to misguidance. In this paper, we
observed that such misguidance could be effectively mitigated by simply using a
proper regularization loss. Our approach comprises two key components: 1) a
geodesic cosine similarity loss that minimizes inter-modality features (i.e.,
image and text) on a projected subspace of CLIP space, and 2) a latent
regularization loss that minimizes intra-modality features (i.e., image and
image) on the image manifold. By replacing the na\"ive directional CLIP loss in
a drop-in replacement manner, our method achieves superior morphing results on
both images and videos for various benchmarks, including CLIP-inversion.
</p></li>
</ul>
<h3>Title: Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation. (arXiv:2401.10848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10848">http://arxiv.org/abs/2401.10848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10848] Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation](http://arxiv.org/abs/2401.10848) #generative</code></li>
<li>Summary: <p>We consider the problem of source-free unsupervised category-level pose
estimation from only RGB images to a target domain without any access to source
domain data or 3D annotations during adaptation. Collecting and annotating
real-world 3D data and corresponding images is laborious, expensive, yet
unavoidable process, since even 3D pose domain adaptation methods require 3D
data in the target domain. We introduce 3DUDA, a method capable of adapting to
a nuisance-ridden target domain without 3D or depth data. Our key insight stems
from the observation that specific object subparts remain stable across
out-of-domain (OOD) scenarios, enabling strategic utilization of these
invariant subcomponents for effective model updates. We represent object
categories as simple cuboid meshes, and harness a generative model of neural
feature activations modeled at each mesh vertex learnt using differential
rendering. We focus on individual locally robust mesh vertex features and
iteratively update them based on their proximity to corresponding features in
the target domain even when the global pose is not correct. Our model is then
trained in an EM fashion, alternating between updating the vertex features and
the feature extractor. We show that our method simulates fine-tuning on a
global pseudo-labeled dataset under mild assumptions, which converges to the
target domain asymptotically. Through extensive empirical validation, including
a complex extreme UDA setup which combines real nuisances, synthetic noise, and
occlusion, we demonstrate the potency of our simple approach in addressing the
domain shift challenge and significantly improving pose estimation accuracy.
</p></li>
</ul>
<h3>Title: Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10446">http://arxiv.org/abs/2401.10446</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10446] Large Language Models are Efficient Learners of Noise-Robust Speech Recognition](http://arxiv.org/abs/2401.10446) #generative</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs) have promoted generative
error correction (GER) for automatic speech recognition (ASR), which leverages
the rich linguistic knowledge and powerful reasoning ability of LLMs to improve
recognition results. The latest work proposes a GER benchmark with HyPoradise
dataset to learn the mapping from ASR N-best hypotheses to ground-truth
transcription by efficient LLM finetuning, which shows great effectiveness but
lacks specificity on noise-robust ASR. In this work, we extend the benchmark to
noisy conditions and investigate if we can teach LLMs to perform denoising for
GER just like what robust ASR do}, where one solution is introducing noise
information as a conditioner into LLM. However, directly incorporating noise
embeddings from audio encoder could harm the LLM tuning due to cross-modality
gap. To this end, we propose to extract a language-space noise embedding from
the N-best list to represent the noise conditions of source speech, which can
promote the denoising process in GER. Furthermore, in order to enhance its
representation ability of audio noise, we design a knowledge distillation (KD)
approach via mutual information estimation to distill the real noise
information in audio embeddings to our language embedding. Experiments on
various latest LLMs demonstrate our approach achieves a new breakthrough with
up to 53.9% correction improvement in terms of word error rate while with
limited training data. Analysis shows that our language-space noise embedding
can well represent the noise conditions of source speech, under which
off-the-shelf LLMs show strong ability of language-space denoising.
</p></li>
</ul>
<h3>Title: Knowledge Fusion of Large Language Models. (arXiv:2401.10491v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10491">http://arxiv.org/abs/2401.10491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10491] Knowledge Fusion of Large Language Models](http://arxiv.org/abs/2401.10491) #generative</code></li>
<li>Summary: <p>While training large language models (LLMs) from scratch can generate models
with distinct functionalities and strengths, it comes at significant costs and
may result in redundant capabilities. Alternatively, a cost-effective and
compelling approach is to merge existing pre-trained LLMs into a more potent
model. However, due to the varying architectures of these LLMs, directly
blending their weights is impractical. In this paper, we introduce the notion
of knowledge fusion for LLMs, aimed at combining the capabilities of existing
LLMs and transferring them into a single LLM. By leveraging the generative
distributions of source LLMs, we externalize their collective knowledge and
unique strengths, thereby potentially elevating the capabilities of the target
model beyond those of any individual source LLM. We validate our approach using
three popular LLMs with different architectures--Llama-2, MPT, and
OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the
fusion of LLMs can improve the performance of the target model across a range
of capabilities such as reasoning, commonsense, and code generation. Our code,
model weights, and data are public at
\url{https://github.com/fanqiwan/FuseLLM}.
</p></li>
</ul>
<h3>Title: Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10393">http://arxiv.org/abs/2401.10393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10393] Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments](http://arxiv.org/abs/2401.10393) #generative</code></li>
<li>Summary: <p>Neural networks often suffer from catastrophic interference (CI): performance
on previously learned tasks drops off significantly when learning a new task.
This contrasts strongly with humans, who can sequentially learn new tasks
without appreciably forgetting previous tasks. Prior work has explored various
techniques for mitigating CI such as regularization, rehearsal, generative
replay, and distillation methods. The current work takes a different approach,
one guided by cognitive science research showing that in naturalistic
environments, the probability of encountering a task decreases as a power-law
of the time since it was last performed. We argue that a realistic evaluation
of techniques for the mitigation of CI should be performed in simulated
naturalistic learning environments. Thus, we evaluate the extent of mitigation
of CI when training simple rehearsal-based methods in power-law environments
similar to the ones humans face. Our work explores this novel rehearsal-based
approach for a domain-incremental task: learning permutations in the MNIST
task. We compare our rehearsal environment with other baselines to show its
efficacy in promoting continual learning. Additionally, we investigate whether
this environment shows forward facilitation, i.e., faster learning of later
tasks. Next, we explore the robustness of our learning environment to the
number of tasks, model size, and amount of data rehearsed after each task.
Notably, our results show that the performance is comparable or superior to
that of models trained using popular regularization methods and also to
rehearsals in non-power-law environments. The benefits of this training
paradigm include simplicity and the lack of a need for extra neural circuitry.
In addition, because our method is orthogonal to other methods, future research
can combine training in power-law environments with other continual learning
mechanisms.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series. (arXiv:2401.10338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10338">http://arxiv.org/abs/2401.10338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10338] MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series](http://arxiv.org/abs/2401.10338) #anomaly</code></li>
<li>Summary: <p>In large IT systems, software deployment is a crucial process in online
services as their code is regularly updated. However, a faulty code change may
degrade the target service's performance and cause cascading outages in
downstream services. Thus, software deployments should be comprehensively
monitored, and their anomalies should be detected timely. In this paper, we
study the problem of anomaly detection for deployments. We begin by identifying
the challenges unique to this anomaly detection problem, which is at
entity-level (e.g., deployments), relative to the more typical problem of
anomaly detection in multivariate time series (MTS). The unique challenges
include the heterogeneity of deployments, the low latency tolerance, the
ambiguous anomaly definition, and the limited supervision. To address them, we
propose a novel framework, semi-supervised hybrid Model for Entity-Level Online
Detection of anomalY (MELODY). MELODY first transforms the MTS of different
entities to the same feature space by an online feature extractor, then uses a
newly proposed semi-supervised deep one-class model for detecting anomalous
entities. We evaluated MELODY on real data of cloud services with 1.2M+ time
series. The relative F1 score improvement of MELODY over the state-of-the-art
methods ranges from 7.6% to 56.5%. The user evaluation suggests MELODY is
suitable for monitoring deployments in large online systems.
</p></li>
</ul>
<h3>Title: PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization. (arXiv:2401.10547v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10547">http://arxiv.org/abs/2401.10547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10547] PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization](http://arxiv.org/abs/2401.10547) #anomaly</code></li>
<li>Summary: <p>A multitude of toxic online behaviors, ranging from network attacks to
anonymous traffic and spam, have severely disrupted the smooth operation of
networks. Due to the inherent sender-receiver nature of network behaviors,
graph-based frameworks are commonly used for detecting anomalous behaviors.
However, in real-world scenarios, the boundary between normal and anomalous
behaviors tends to be ambiguous. The local heterophily of graphs interferes
with the detection, and existing methods based on nodes or edges introduce
unwanted noise into representation results, thereby impacting the effectiveness
of detection. To address these issues, we propose PhoGAD, a graph-based anomaly
detection framework. PhoGAD leverages persistent homology optimization to
clarify behavioral boundaries. Building upon this, the weights of adjacent
edges are designed to mitigate the effects of local heterophily. Subsequently,
to tackle the noise problem, we conduct a formal analysis and propose a
disentangled representation-based explicit embedding method, ultimately
achieving anomaly behavior detection. Experiments on intrusion, traffic, and
spam datasets verify that PhoGAD has surpassed the performance of
state-of-the-art (SOTA) frameworks in detection efficacy. Notably, PhoGAD
demonstrates robust detection even with diminished anomaly proportions,
highlighting its applicability to real-world scenarios. The analysis of
persistent homology demonstrates its effectiveness in capturing the topological
structure formed by normal edge features. Additionally, ablation experiments
validate the effectiveness of the innovative mechanisms integrated within
PhoGAD.
</p></li>
</ul>
<h2>in-context</h2>
<h2>memory</h2>
<h3>Title: Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10447">http://arxiv.org/abs/2401.10447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10447] Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition](http://arxiv.org/abs/2401.10447) #memory</code></li>
<li>Summary: <p>The use of low-rank adaptation (LoRA) with frozen pretrained language models
(PLMs) has become increasing popular as a mainstream, resource-efficient
modeling approach for memory-constrained hardware. In this study, we first
explore how to enhance model performance by introducing various LoRA training
strategies, achieving relative word error rate reductions of 3.50\% on the
public Librispeech dataset and of 3.67\% on an internal dataset in the
messaging domain. To further characterize the stability of LoRA-based
second-pass speech recognition models, we examine robustness against input
perturbations. These perturbations are rooted in homophone replacements and a
novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both
designed to measure the relative degradation in the performance of rescoring
models. Our experimental results indicate that while advanced variants of LoRA,
such as dynamic rank-allocated LoRA, lead to performance degradation in
$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.
This finding is in comparison to fully-tuned models and vanilla LoRA tuning
baselines, suggesting that a comprehensive selection is needed when using
LoRA-based adaptation for compute-cost savings and robust language modeling.
</p></li>
</ul>
<h3>Title: Self-training from Self-memory in Data-to-text Generation. (arXiv:2401.10567v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10567">http://arxiv.org/abs/2401.10567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10567] Self-training from Self-memory in Data-to-text Generation](http://arxiv.org/abs/2401.10567) #memory</code></li>
<li>Summary: <p>This paper introduces a novel training model, self-training from self-memory
(STSM) in data-to-text generation (DTG), allowing the model to self-train on
subsets, including self-memory as outputs inferred directly from the trained
models and/or the new data. The quality of self-memory is validated by two
models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined
conditions: (1) the appearance of all source values in the outputs of the D2T
model and (2) the ability to convert back to source data in the outputs in the
T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if
they contain all source values. Subsequently, we use the T2D model to confirm
that these outputs can capture input relationships by demonstrating their
capacity to convert text back into data. With 30% of the dataset, we can train
the D2T model with a competitive performance compared to full training in the
same setup. We experiment with our model on two datasets, E2E NLG and DART.
STSM offers the D2T model a generalization capability from its subset memory
while reducing training data volume. Ultimately, we anticipate that this paper
will contribute to continual learning solutions that adapt to new training
data, incorporating it as a form of self-memory in DTG tasks. The curated
dataset is publicly available at: https://github.com/hoangthangta/STSM.
</p></li>
</ul>
<h3>Title: Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10774">http://arxiv.org/abs/2401.10774</a></li>
<li>Code URL: <a href="https://github.com/fasterdecoding/medusa">https://github.com/fasterdecoding/medusa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10774] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](http://arxiv.org/abs/2401.10774) #memory</code></li>
<li>Summary: <p>The inference process in Large Language Models (LLMs) is often limited due to
the absence of parallelism in the auto-regressive decoding process, resulting
in most operations being restricted by the memory bandwidth of accelerators.
While methods such as speculative decoding have been suggested to address this
issue, their implementation is impeded by the challenges associated with
acquiring and maintaining a separate draft model. In this paper, we present
Medusa, an efficient method that augments LLM inference by adding extra
decoding heads to predict multiple subsequent tokens in parallel. Using a
tree-based attention mechanism, Medusa constructs multiple candidate
continuations and verifies them simultaneously in each decoding step. By
leveraging parallel processing, Medusa introduces only minimal overhead in
terms of single-step latency while substantially reducing the number of
decoding steps required.
</p></li>
</ul>
<p>We present two levels of fine-tuning procedures for Medusa to meet the needs
of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a
frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa
is fine-tuned together with the backbone LLM, enabling better prediction
accuracy of Medusa heads and higher speedup but needing a special training
recipe that preserves the backbone model's capabilities.
</p>
<p>Moreover, we propose several extensions that improve or expand the utility of
Medusa, including a self-distillation to handle situations where no training
data is available and a typical acceptance scheme to boost the acceptance rate
while maintaining generation quality. We evaluate Medusa on models of various
sizes and training procedures. Our experiments demonstrate that Medusa-1 can
achieve over 2.2x speedup without compromising generation quality, while
Medusa-2 further improves the speedup to 2.3-3.6x.
</p>

<h3>Title: Budgeted Online Model Selection and Fine-Tuning via Federated Learning. (arXiv:2401.10478v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10478">http://arxiv.org/abs/2401.10478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10478] Budgeted Online Model Selection and Fine-Tuning via Federated Learning](http://arxiv.org/abs/2401.10478) #memory</code></li>
<li>Summary: <p>Online model selection involves selecting a model from a set of candidate
models 'on the fly' to perform prediction on a stream of data. The choice of
candidate models henceforth has a crucial impact on the performance. Although
employing a larger set of candidate models naturally leads to more flexibility
in model selection, this may be infeasible in cases where prediction tasks are
performed on edge devices with limited memory. Faced with this challenge, the
present paper proposes an online federated model selection framework where a
group of learners (clients) interacts with a server with sufficient memory such
that the server stores all candidate models. However, each client only chooses
to store a subset of models that can be fit into its memory and performs its
own prediction task using one of the stored models. Furthermore, employing the
proposed algorithm, clients and the server collaborate to fine-tune models to
adapt them to a non-stationary environment. Theoretical analysis proves that
the proposed algorithm enjoys sub-linear regret with respect to the best model
in hindsight. Experiments on real datasets demonstrate the effectiveness of the
proposed algorithm.
</p></li>
</ul>
<h3>Title: Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10516">http://arxiv.org/abs/2401.10516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10516] Episodic Reinforcement Learning with Expanded State-reward Space](http://arxiv.org/abs/2401.10516) #memory</code></li>
<li>Summary: <p>Empowered by deep neural networks, deep reinforcement learning (DRL) has
demonstrated tremendous empirical successes in various domains, including
games, health care, and autonomous driving. Despite these advancements, DRL is
still identified as data-inefficient as effective policies demand vast numbers
of environmental samples. Recently, episodic control (EC)-based model-free DRL
methods enable sample efficiency by recalling past experiences from episodic
memory. However, existing EC-based methods suffer from the limitation of
potential misalignment between the state and reward spaces for neglecting the
utilization of (past) retrieval states with extensive information, which
probably causes inaccurate value estimation and degraded policy performance. To
tackle this issue, we introduce an efficient EC-based DRL framework with
expanded state-reward space, where the expanded states used as the input and
the expanded rewards used in the training both contain historical and current
information. To be specific, we reuse the historical states retrieved by EC as
part of the input states and integrate the retrieved MC-returns into the
immediate reward in each interactive transition. As a result, our method is
able to simultaneously achieve the full utilization of retrieval information
and the better evaluation of state values by a Temporal Difference (TD) loss.
Empirical results on challenging Box2d and Mujoco tasks demonstrate the
superiority of our method over a recent sibling method and common baselines.
Further, we also verify our method's effectiveness in alleviating Q-value
overestimation by additional experiments of Q-value comparison.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Name Tagging Under Domain Shift via Metric Learning for Life Sciences. (arXiv:2401.10472v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10472">http://arxiv.org/abs/2401.10472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10472] Name Tagging Under Domain Shift via Metric Learning for Life Sciences](http://arxiv.org/abs/2401.10472) #few-shot</code></li>
<li>Summary: <p>Name tagging is a key component of Information Extraction (IE), particularly
in scientific domains such as biomedicine and chemistry, where large language
models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of
transfer learning for enhancing a name tagging model trained in the biomedical
domain (the source domain) to be used in the chemical domain (the target
domain). A common practice for training such a model in a few-shot learning
setting is to pretrain the model on the labeled source data, and then, to
finetune it on a hand-full of labeled target examples. In our experiments we
observed that such a model is prone to mis-labeling the source entities, which
can often appear in the text, as the target entities. To alleviate this
problem, we propose a model to transfer the knowledge from the source domain to
the target domain, however, at the same time, to project the source entities
and target entities into separate regions of the feature space. This diminishes
the risk of mis-labeling the source entities as the target entities. Our model
consists of two stages: 1) entity grouping in the source domain, which
incorporates knowledge from annotated events to establish relations between
entities, and 2) entity discrimination in the target domain, which relies on
pseudo labeling and contrastive learning to enhance discrimination between the
entities in the two domains. We carry out our extensive experiments across
three source and three target datasets, and demonstrate that our method
outperforms the baselines, in some scenarios by 5\% absolute value.
</p></li>
</ul>
<h3>Title: FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis. (arXiv:2401.10506v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10506">http://arxiv.org/abs/2401.10506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10506] FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis](http://arxiv.org/abs/2401.10506) #few-shot</code></li>
<li>Summary: <p>Text-to-SQL, which provides zero-code interface for operating relational
databases, has gained much attention in financial analysis; because, financial
professionals may not well-skilled in SQL programming. However, until now,
there is no practical Text-to-SQL benchmark dataset for financial analysis, and
existing Text-to-SQL methods have not considered the unique characteristics of
databases in financial applications, such as commonly existing wide tables. To
address these issues, we collect a practical Text-to-SQL benchmark dataset and
propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL
framework for financial analysis. The benchmark dataset, BULL, is collected
from the practical financial analysis business of Hundsun Technologies Inc.,
including databases for fund, stock, and macro economy. Besides, the proposed
LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for
financial Text-to-SQL from the perspectives of prompt construction,
parameter-efficient fine-tuning and output calibration. Extensive experimental
results on BULL demonstrate that FinSQL achieves the state-of-the-art
Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to
36.64% performance improvement in scenarios requiring few-shot cross-database
model transfer.
</p></li>
</ul>
<h3>Title: Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.10394">http://arxiv.org/abs/2401.10394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.10394] Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels](http://arxiv.org/abs/2401.10394) #few-shot</code></li>
<li>Summary: <p>Few-shot node classification poses a significant challenge for Graph Neural
Networks (GNNs) due to insufficient supervision and potential distribution
shifts between labeled and unlabeled nodes. Self-training has emerged as a
widely popular framework to leverage the abundance of unlabeled data, which
expands the training set by assigning pseudo-labels to selected unlabeled
nodes. Efforts have been made to develop various selection strategies based on
confidence, information gain, etc. However, none of these methods takes into
account the distribution shift between the training and testing node sets. The
pseudo-labeling step may amplify this shift and even introduce new ones,
hindering the effectiveness of self-training. Therefore, in this work, we
explore the potential of explicitly bridging the distribution shift between the
expanded training set and test set during self-training. To this end, we
propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework
to identify pseudo-labeled nodes that are both informative and capable of
redeeming the distribution discrepancy and formulate it as a differentiable
optimization task. A distribution-shift-aware edge predictor is further adopted
to augment the graph and increase the model's generalizability in assigning
pseudo labels. We evaluate our proposed method on four publicly available
benchmark datasets and extensive experiments demonstrate that our framework
consistently outperforms state-of-the-art baselines.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2024-01-22]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
