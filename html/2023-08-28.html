<h2>diffusion</h2>
<h3>Title: A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions. (arXiv:2308.13142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13142">http://arxiv.org/abs/2308.13142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13142] A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions](http://arxiv.org/abs/2308.13142) #diffusion</code></li>
<li>Summary: <p>Recently, there has been significant progress in the development of large
models. Following the success of ChatGPT, numerous language models have been
introduced, demonstrating remarkable performance. Similar advancements have
also been observed in image generation models, such as Google's Imagen model,
OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive
capabilities in generating images. However, similar to large language models,
these models still encounter unresolved challenges. Fortunately, the
availability of open-source stable diffusion models and their underlying
mathematical principles has enabled the academic community to extensively
analyze the performance of current image generation models and make
improvements based on this stable diffusion framework. This survey aims to
examine the existing issues and the current solutions pertaining to image
generation models.
</p></li>
</ul>
<h3>Title: Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model. (arXiv:2308.13164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13164">http://arxiv.org/abs/2308.13164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13164] Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model](http://arxiv.org/abs/2308.13164) #diffusion</code></li>
<li>Summary: <p>In this paper, we rethink the low-light image enhancement task and propose a
physically explainable and generative diffusion model for low-light image
enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the
physical model and the generative network. Furthermore, we hope to supplement
and even deduce the information missing in the low-light image through the
generative network. Therefore, Diff-Retinex formulates the low-light image
enhancement problem into Retinex decomposition and conditional image
generation. In the Retinex decomposition, we integrate the superiority of
attention in Transformer and meticulously design a Retinex Transformer
decomposition network (TDN) to decompose the image into illumination and
reflectance maps. Then, we design multi-path generative diffusion networks to
reconstruct the normal-light Retinex probability distribution and solve the
various degradations in these components respectively, including dark
illumination, noise, color deviation, loss of scene contents, etc. Owing to
generative diffusion model, Diff-Retinex puts the restoration of low-light
subtle detail into practice. Extensive experiments conducted on real-world
low-light datasets qualitatively and quantitatively demonstrate the
effectiveness, superiority, and generalization of the proposed method.
</p></li>
</ul>
<h3>Title: EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior. (arXiv:2308.13223v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13223">http://arxiv.org/abs/2308.13223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13223] EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior](http://arxiv.org/abs/2308.13223) #diffusion</code></li>
<li>Summary: <p>While the image diffusion model has made significant strides in text-driven
3D content creation, it often falls short in accurately capturing the intended
meaning of the text prompt, particularly with respect to direction information.
This shortcoming gives rise to the Janus problem, where multi-faced 3D models
are produced with the guidance of such diffusion models. In this paper, we
present a robust pipeline for generating high-fidelity 3D content with
orthogonal-view image guidance. Specifically, we introduce a novel 2D diffusion
model that generates an image consisting of four orthogonal-view sub-images for
the given text prompt. The 3D content is then created with this diffusion
model, which enhances 3D consistency and provides strong structured semantic
priors. This addresses the infamous Janus problem and significantly promotes
generation efficiency. Additionally, we employ a progressive 3D synthesis
strategy that results in substantial improvement in the quality of the created
3D contents. Both quantitative and qualitative evaluations show that our method
demonstrates a significant improvement over previous text-to-3D techniques.
</p></li>
</ul>
<h3>Title: Distribution-Aligned Diffusion for Human Mesh Recovery. (arXiv:2308.13369v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13369">http://arxiv.org/abs/2308.13369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13369] Distribution-Aligned Diffusion for Human Mesh Recovery](http://arxiv.org/abs/2308.13369) #diffusion</code></li>
<li>Summary: <p>Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that injects input-specific distribution
information into the diffusion process, and provides useful prior knowledge to
simplify the mesh recovery task. Our method achieves state-of-the-art
performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: SurGNN: Explainable visual scene understanding and assessment of surgical skill using graph neural networks. (arXiv:2308.13073v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13073">http://arxiv.org/abs/2308.13073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13073] SurGNN: Explainable visual scene understanding and assessment of surgical skill using graph neural networks](http://arxiv.org/abs/2308.13073) #self-supervised</code></li>
<li>Summary: <p>This paper explores how graph neural networks (GNNs) can be used to enhance
visual scene understanding and surgical skill assessment. By using GNNs to
analyze the complex visual data of surgical procedures represented as graph
structures, relevant features can be extracted and surgical skill can be
predicted. Additionally, GNNs provide interpretable results, revealing the
specific actions, instruments, or anatomical structures that contribute to the
predicted skill metrics. This can be highly beneficial for surgical educators
and trainees, as it provides valuable insights into the factors that contribute
to successful surgical performance and outcomes. SurGNN proposes two concurrent
approaches -- one supervised and the other self-supervised. The paper also
briefly discusses other automated surgical skill evaluation techniques and
highlights the limitations of hand-crafted features in capturing the
intricacies of surgical expertise. We use the proposed methods to achieve
state-of-the-art results on EndoVis19, and custom datasets. The working
implementation of the code can be found at https://github.com/<redacted>.
</p></li>
</ul>
<h3>Title: Preserving Modality Structure Improves Multi-Modal Learning. (arXiv:2308.13077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13077">http://arxiv.org/abs/2308.13077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13077] Preserving Modality Structure Improves Multi-Modal Learning](http://arxiv.org/abs/2308.13077) #self-supervised</code></li>
<li>Summary: <p>Self-supervised learning on large-scale multi-modal datasets allows learning
semantically meaningful embeddings in a joint multi-modal representation space
without relying on human annotations. These joint embeddings enable zero-shot
cross-modal tasks like retrieval and classification. However, these methods
often struggle to generalize well on out-of-domain data as they ignore the
semantic structure present in modality-specific embeddings. In this context, we
propose a novel Semantic-Structure-Preserving Consistency approach to improve
generalizability by preserving the modality-specific relationships in the joint
embedding space. To capture modality-specific semantic relationships between
samples, we propose to learn multiple anchors and represent the multifaceted
relationship between samples with respect to their relationship with these
anchors. To assign multiple anchors to each sample, we propose a novel
Multi-Assignment Sinkhorn-Knopp algorithm. Our experimentation demonstrates
that our proposed approach learns semantically meaningful anchors in a
self-supervised manner. Furthermore, our evaluation on MSR-VTT and YouCook2
datasets demonstrates that our proposed multi-anchor assignment based solution
achieves state-of-the-art performance and generalizes to both inand
out-of-domain datasets. Code: https://github.com/Swetha5/Multi_Sinkhorn_Knopp
</p></li>
</ul>
<h3>Title: Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions. (arXiv:2308.13178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13178">http://arxiv.org/abs/2308.13178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13178] Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions](http://arxiv.org/abs/2308.13178) #self-supervised</code></li>
<li>Summary: <p>Text segmentation tasks have a very wide range of application values, such as
image editing, style transfer, watermark removal, etc.However, existing public
datasets are of poor quality of pixel-level labels that have been shown to be
notoriously costly to acquire, both in terms of money and time. At the same
time, when pretraining is performed on synthetic datasets, the data
distribution of the synthetic datasets is far from the data distribution in the
real scene. These all pose a huge challenge to the current pixel-level text
segmentation algorithms.To alleviate the above problems, we propose a
self-supervised scene text segmentation algorithm with layered decoupling of
representations derived from the object-centric manner to segment images into
texts and background. In our method, we propose two novel designs which include
Region Query Module and Representation Consistency Constraints adapting to the
unique properties of text as complements to Auto Encoder, which improves the
network's sensitivity to texts.For this unique design, we treat the
polygon-level masks predicted by the text localization model as extra input
information, and neither utilize any pixel-level mask annotations for training
stage nor pretrain on synthetic datasets.Extensive experiments show the
effectiveness of the method proposed. On several public scene text datasets,
our method outperforms the state-of-the-art unsupervised segmentation
algorithms.
</p></li>
</ul>
<h3>Title: Self-supervised learning for hotspot detection and isolation from thermal images. (arXiv:2308.13204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13204">http://arxiv.org/abs/2308.13204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13204] Self-supervised learning for hotspot detection and isolation from thermal images](http://arxiv.org/abs/2308.13204) #self-supervised</code></li>
<li>Summary: <p>Hotspot detection using thermal imaging has recently become essential in
several industrial applications, such as security applications, health
applications, and equipment monitoring applications. Hotspot detection is of
utmost importance in industrial safety where equipment can develop anomalies.
Hotspots are early indicators of such anomalies. We address the problem of
hotspot detection in thermal images by proposing a self-supervised learning
approach. Self-supervised learning has shown potential as a competitive
alternative to their supervised learning counterparts but their application to
thermography has been limited. This has been due to lack of diverse data
availability, domain specific pre-trained models, standardized benchmarks, etc.
We propose a self-supervised representation learning approach followed by
fine-tuning that improves detection of hotspots by classification. The SimSiam
network based ensemble classifier decides whether an image contains hotspots or
not. Detection of hotspots is followed by precise hotspot isolation. By doing
so, we are able to provide a highly accurate and precise hotspot
identification, applicable to a wide range of applications. We created a novel
large thermal image dataset to address the issue of paucity of easily
accessible thermal images. Our experiments with the dataset created by us and a
publicly available segmentation dataset show the potential of our approach for
hotspot detection and its ability to isolate hotspots with high accuracy. We
achieve a Dice Coefficient of 0.736, the highest when compared with existing
hotspot identification techniques. Our experiments also show self-supervised
learning as a strong contender of supervised learning, providing competitive
metrics for hotspot detection, with the highest accuracy of our approach being
97%.
</p></li>
</ul>
<h3>Title: Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features. (arXiv:2308.13392v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13392">http://arxiv.org/abs/2308.13392</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13392] Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features](http://arxiv.org/abs/2308.13392) #self-supervised</code></li>
<li>Summary: <p>Whilst contrastive learning yields powerful representations by matching
different augmented views of the same instance, it lacks the ability to capture
the similarities between different instances. One popular way to address this
limitation is by learning global features (after the global pooling) to capture
inter-instance relationships based on knowledge distillation, where the global
features of the teacher are used to guide the learning of the global features
of the student. Inspired by cross-modality learning, we extend this existing
framework that only learns from global features by encouraging the global
features and intermediate layer features to learn from each other. This leads
to our novel self-supervised framework: cross-context learning between global
and hypercolumn features (CGH), that enforces the consistency of instance
relations between low- and high-level semantics. Specifically, we stack the
intermediate feature maps to construct a hypercolumn representation so that we
can measure instance relations using two contexts (hypercolumn and global
feature) separately, and then use the relations of one context to guide the
learning of the other. This cross-context learning allows the model to learn
from the differences between the two contexts. The experimental results on
linear classification and downstream tasks show that our method outperforms the
state-of-the-art methods.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13320">http://arxiv.org/abs/2308.13320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13320] Fine-tuning can cripple your foundation model; preserving features may be the solution](http://arxiv.org/abs/2308.13320) #foundation model</code></li>
<li>Summary: <p>Pre-trained foundation models, owing primarily to their enormous capacity and
exposure to vast amount of training data scraped from the internet, enjoy the
advantage of storing knowledge about plenty of real-world concepts. Such models
are typically fine-tuned on downstream datasets to produce remarkable
state-of-the-art performances. While various fine-tuning methods have been
devised and are shown to be highly effective, we observe that a fine-tuned
model's ability to recognize concepts on tasks $\textit{different}$ from the
downstream one is reduced significantly compared to its pre-trained
counterpart. This is clearly undesirable as a huge amount of time and money
went into learning those very concepts in the first place. We call this
undesirable phenomenon "concept forgetting" and via experiments show that most
end-to-end fine-tuning approaches suffer heavily from this side effect. To this
end, we also propose a rather simple fix to this problem by designing a method
called LDIFS (short for $\ell_2$ distance in feature space) that simply
preserves the features of the original foundation model during fine-tuning. We
show that LDIFS significantly reduces concept forgetting without having
noticeable impact on the downstream task performance.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13182">http://arxiv.org/abs/2308.13182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13182] Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon](http://arxiv.org/abs/2308.13182) #generative</code></li>
<li>Summary: <p>With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&amp;E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&amp;E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&amp;E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.
</p></li>
</ul>
<h3>Title: Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map. (arXiv:2308.13245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13245">http://arxiv.org/abs/2308.13245</a></li>
<li>Code URL: <a href="https://github.com/naughtyzz/3d_facial_shape_attribute_translation_ssgmap">https://github.com/naughtyzz/3d_facial_shape_attribute_translation_ssgmap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13245] Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map](http://arxiv.org/abs/2308.13245) #generative</code></li>
<li>Summary: <p>While impressive progress has recently been made in image-oriented facial
attribute translation, shape-oriented 3D facial attribute translation remains
an unsolved issue. This is primarily limited by the lack of 3D generative
models and ineffective usage of 3D facial data. We propose a learning framework
for 3D facial attribute translation to relieve these limitations. Firstly, we
customize a novel geometric map for 3D shape representation and embed it in an
end-to-end generative adversarial network. The geometric map represents 3D
shapes symmetrically on a square image grid, while preserving the neighboring
relationship of 3D vertices in a local least-square sense. This enables
effective learning for the latent representation of data with different
attributes. Secondly, we employ a unified and unpaired learning framework for
multi-domain attribute translation. It not only makes effective usage of data
correlation from multiple domains, but also mitigates the constraint for hardly
accessible paired data. Finally, we propose a hierarchical architecture for the
discriminator to guarantee robust results against both global and local
artifacts. We conduct extensive experiments to demonstrate the advantage of the
proposed framework over the state-of-the-art in generating high-fidelity facial
shapes. Given an input 3D facial shape, the proposed framework is able to
synthesize novel shapes of different attributes, which covers some downstream
applications, such as expression transfer, gender translation, and aging. Code
at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.
</p></li>
</ul>
<h3>Title: ARTIST: ARTificial Intelligence for Simplified Text. (arXiv:2308.13458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13458">http://arxiv.org/abs/2308.13458</a></li>
<li>Code URL: <a href="https://github.com/delftcrowd/artist">https://github.com/delftcrowd/artist</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13458] ARTIST: ARTificial Intelligence for Simplified Text](http://arxiv.org/abs/2308.13458) #generative</code></li>
<li>Summary: <p>Complex text is a major barrier for many citizens when accessing public
information and knowledge. While often done manually, Text Simplification is a
key Natural Language Processing task that aims for reducing the linguistic
complexity of a text while preserving the original meaning. Recent advances in
Generative Artificial Intelligence (AI) have enabled automatic text
simplification both on the lexical and syntactical levels. However, as
applications often focus on English, little is understood about the
effectiveness of Generative AI techniques on low-resource languages such as
Dutch. For this reason, we carry out empirical studies to understand the
benefits and limitations of applying generative technologies for text
simplification and provide the following outcomes: 1) the design and
implementation for a configurable text simplification pipeline that
orchestrates state-of-the-art generative text simplification models, domain and
reader adaptation, and visualisation modules; 2) insights and lessons learned,
showing the strengths of automatic text simplification while exposing the
challenges in handling cultural and commonsense knowledge. These outcomes
represent a first step in the exploration of Dutch text simplification and shed
light on future endeavours both for research and practice.
</p></li>
</ul>
<h3>Title: ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching. (arXiv:2308.13062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13062">http://arxiv.org/abs/2308.13062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13062] ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching](http://arxiv.org/abs/2308.13062) #generative</code></li>
<li>Summary: <p>Security critical software, e.g., OpenSSL, comes with numerous side-channel
leakages left unpatched due to a lack of resources or experts. The situation
will only worsen as the pace of code development accelerates, with developers
relying on Large Language Models (LLMs) to automatically generate code. In this
work, we explore the use of LLMs in generating patches for vulnerable code with
microarchitectural side-channel leakages. For this, we investigate the
generative abilities of powerful LLMs by carefully crafting prompts following a
zero-shot learning approach. All generated code is dynamically analyzed by
leakage detection tools, which are capable of pinpointing information leakage
at the instruction level leaked either from secret dependent accesses or
branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts
are used to generate candidate replacements for vulnerable code, which are then
analyzed for correctness and for leakage resilience. From a cost/performance
perspective, the GPT4-based configuration costs in API calls a mere few cents
per vulnerability fixed. Our results show that LLM-based patching is far more
cost-effective and thus provides a scalable solution. Finally, the framework we
propose will improve in time, especially as vulnerability detection tools and
LLMs mature.
</p></li>
</ul>
<h3>Title: Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13265">http://arxiv.org/abs/2308.13265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13265] Heterogeneous Federated Learning via Personalized Generative Networks](http://arxiv.org/abs/2308.13265) #generative</code></li>
<li>Summary: <p>Federated Learning (FL) allows several clients to construct a common global
machine-learning model without having to share their data. FL, however, faces
the challenge of statistical heterogeneity between the client's data, which
degrades performance and slows down the convergence toward the global model. In
this paper, we provide theoretical proof that minimizing heterogeneity between
clients facilitates the convergence of a global model for every single client.
This becomes particularly important under empirical concept shifts among
clients, rather than merely considering imbalanced classes, which have been
studied until now. Therefore, we propose a method for knowledge transfer
between clients where the server trains client-specific generators. Each
generator generates samples for the corresponding client to remove the conflict
with other clients' models. Experiments conducted on synthetic and real data,
along with a theoretical study, support the effectiveness of our method in
constructing a well-generalizable global model by reducing the conflict between
local models.
</p></li>
</ul>
<h3>Title: Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13278">http://arxiv.org/abs/2308.13278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13278] Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity](http://arxiv.org/abs/2308.13278) #generative</code></li>
<li>Summary: <p>Quality-Diversity is a branch of stochastic optimization that is often
applied to problems from the Reinforcement Learning and control domains in
order to construct repertoires of well-performing policies/skills that exhibit
diversity with respect to a behavior space. Such archives are usually composed
of a finite number of reactive agents which are each associated to a unique
behavior descriptor, and instantiating behavior descriptors outside of that
coarsely discretized space is not straight-forward. While a few recent works
suggest solutions to that issue, the trajectory that is generated is not easily
customizable beyond the specification of a target behavior descriptor. We
propose to jointly solve those problems in environments where semantic
information about static scene elements is available by leveraging a Large
Language Model to augment the repertoire with natural language descriptions of
trajectories, and training a policy conditioned on those descriptions. Thus,
our method allows a user to not only specify an arbitrary target behavior
descriptor, but also provide the model with a high-level textual prompt to
shape the generated trajectory. We also propose an LLM-based approach to
evaluating the performance of such generative agents. Furthermore, we develop a
benchmark based on simulated robot navigation in a 2d maze that we use for
experimental validation.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Burnt area extraction from high-resolution satellite images based on anomaly detection. (arXiv:2308.13367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13367">http://arxiv.org/abs/2308.13367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13367] Burnt area extraction from high-resolution satellite images based on anomaly detection](http://arxiv.org/abs/2308.13367) #anomaly</code></li>
<li>Summary: <p>Wildfire detection using satellite images is a widely studied task in remote
sensing with many applications to fire delineation and mapping. Recently, deep
learning methods have become a scalable solution to automate this task,
especially in the field of unsupervised learning where no training data is
available. This is particularly important in the context of emergency risk
monitoring where fast and effective detection is needed, generally based on
high-resolution satellite data. Among various approaches, Anomaly Detection
(AD) appears to be highly potential thanks to its broad applications in
computer vision, medical imaging, as well as remote sensing. In this work, we
build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE),
a popular reconstruction-based AD method with discrete latent spaces, to
perform unsupervised burnt area extraction. We integrate VQ-VAE into an
end-to-end framework with an intensive post-processing step using dedicated
vegetation, water and brightness indexes. Our experiments conducted on
high-resolution SPOT-6/7 images provide promising results of the proposed
technique, showing its high potential in future research on unsupervised burnt
area extraction.
</p></li>
</ul>
<h3>Title: Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13068">http://arxiv.org/abs/2308.13068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13068] Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology](http://arxiv.org/abs/2308.13068) #anomaly</code></li>
<li>Summary: <p>Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and "fancier" algorithms.
</p></li>
</ul>
<h3>Title: A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13352">http://arxiv.org/abs/2308.13352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13352] A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data](http://arxiv.org/abs/2308.13352) #anomaly</code></li>
<li>Summary: <p>Anomaly detection (AD) tasks have been solved using machine learning
algorithms in various domains and applications. The great majority of these
algorithms use normal data to train a residual-based model, and assign anomaly
scores to unseen samples based on their dissimilarity with the learned normal
regime. The underlying assumption of these approaches is that anomaly-free data
is available for training. This is, however, often not the case in real-world
operational settings, where the training data may be contaminated with a
certain fraction of abnormal samples. Training with contaminated data, in turn,
inevitably leads to a deteriorated AD performance of the residual-based
algorithms.
</p></li>
</ul>
<p>In this paper we introduce a framework for a fully unsupervised refinement of
contaminated training data for AD tasks. The framework is generic and can be
applied to any residual-based machine learning model. We demonstrate the
application of the framework to two public datasets of multivariate time series
machine data from different application fields. We show its clear superiority
over the naive approach of training with contaminated data without refinement.
Moreover, we compare it to the ideal, unrealistic reference in which
anomaly-free data would be available for training. Since the approach exploits
information from the anomalies, and not only from the normal regime, it is
comparable and often outperforms the ideal baseline as well.
</p>

<h2>in-context</h2>
<h3>Title: Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13259">http://arxiv.org/abs/2308.13259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13259] Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering](http://arxiv.org/abs/2308.13259) #in-context</code></li>
<li>Summary: <p>Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown
impressive reasoning ability in various downstream tasks. Even so, suffering
from hallucinations and the inability to access external knowledge, LLMs often
come with incorrect or unfaithful intermediate reasoning steps, especially in
the context of answering knowledge-intensive tasks such as KBQA. To alleviate
this issue, we propose a framework called Knowledge-Driven Chain-of-Thought
(KD-CoT) to verify and modify reasoning traces in CoT via interaction with
external knowledge, and thus overcome the hallucinations and error propagation.
Concretely, we formulate the CoT rationale process of LLMs into a structured
multi-round QA format. In each round, LLMs interact with a QA system that
retrieves external knowledge and produce faithful reasoning traces based on
retrieved precise answers. The structured CoT reasoning of LLMs is facilitated
by our developed KBQA CoT collection, which serves as in-context learning
demonstrations and can also be utilized as feedback augmentation to train a
robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion
datasets demonstrate the effectiveness of proposed KD-CoT in task-solving
reasoning generation, which outperforms the vanilla CoT ICL with an absolute
success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for retrieving knowledge,
achieving significant improvement in Hit performance.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory. (arXiv:2308.13236v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13236">http://arxiv.org/abs/2308.13236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13236] Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory](http://arxiv.org/abs/2308.13236) #memory</code></li>
<li>Summary: <p>Black-box unsupervised domain adaptation (UDA) learns with source predictions
of target data without accessing either source data or source models during
training, and it has clear superiority in data privacy and flexibility in
target network selection. However, the source predictions of target data are
often noisy and training with them is prone to learning collapses. We propose
BiMem, a bi-directional memorization mechanism that learns to remember useful
and representative information to correct noisy pseudo labels on the fly,
leading to robust black-box UDA that can generalize across different visual
recognition tasks. BiMem constructs three types of memory, including sensory
memory, short-term memory, and long-term memory, which interact in a
bi-directional manner for comprehensive and robust memorization of learnt
features. It includes a forward memorization flow that identifies and stores
useful features and a backward calibration flow that rectifies features' pseudo
labels progressively. Extensive experiments show that BiMem achieves superior
domain adaptation performance consistently across various visual recognition
tasks such as image classification, semantic segmentation and object detection.
</p></li>
</ul>
<h3>Title: Kissing to Find a Match: Efficient Low-Rank Permutation Representation. (arXiv:2308.13252v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13252">http://arxiv.org/abs/2308.13252</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13252] Kissing to Find a Match: Efficient Low-Rank Permutation Representation](http://arxiv.org/abs/2308.13252) #memory</code></li>
<li>Summary: <p>Permutation matrices play a key role in matching and assignment problems
across the fields, especially in computer vision and robotics. However, memory
for explicitly representing permutation matrices grows quadratically with the
size of the problem, prohibiting large problem instances. In this work, we
propose to tackle the curse of dimensionality of large permutation matrices by
approximating them using low-rank matrix factorization, followed by a
nonlinearity. To this end, we rely on the Kissing number theory to infer the
minimal rank required for representing a permutation matrix of a given size,
which is significantly smaller than the problem size. This leads to a drastic
reduction in computation and memory costs, e.g., up to $3$ orders of magnitude
less memory for a problem of size $n=20000$, represented using $8.4\times10^5$
elements in two small matrices instead of using a single huge matrix with
$4\times 10^8$ elements. The proposed representation allows for accurate
representations of large permutation matrices, which in turn enables handling
large problems that would have been infeasible otherwise. We demonstrate the
applicability and merits of the proposed approach through a series of
experiments on a range of problems that involve predicting permutation
matrices, from linear and quadratic assignment to shape matching problems.
</p></li>
</ul>
<h3>Title: Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation. (arXiv:2308.13505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13505">http://arxiv.org/abs/2308.13505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13505] Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation](http://arxiv.org/abs/2308.13505) #memory</code></li>
<li>Summary: <p>Current prevailing Video Object Segmentation (VOS) methods usually perform
dense matching between the current and reference frames after extracting their
features. One on hand, the decoupled modeling restricts the targets information
propagation only at high-level feature space. On the other hand, the pixel-wise
matching leads to a lack of holistic understanding of the targets. To overcome
these issues, we propose a unified VOS framework, coined as JointFormer, for
joint modeling the three elements of feature, correspondence, and a compressed
memory. The core design is the Joint Block, utilizing the flexibility of
attention to simultaneously extract feature and propagate the targets
information to the current tokens and the compressed memory token. This scheme
allows to perform extensive information propagation and discriminative feature
learning. To incorporate the long-term temporal targets information, we also
devise a customized online updating mechanism for the compressed memory token,
which can prompt the information flow along the temporal dimension and thus
improve the global modeling capability. Under the design, our method achieves a
new state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) and
YouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existing
works by a large margin.
</p></li>
</ul>
<h3>Title: OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13137">http://arxiv.org/abs/2308.13137</a></li>
<li>Code URL: <a href="https://github.com/opengvlab/omniquant">https://github.com/opengvlab/omniquant</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13137] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](http://arxiv.org/abs/2308.13137) #memory</code></li>
<li>Summary: <p>Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters, which
leads to low performance and fails to deal with extremely low-bit quantization.
To tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(OmniQuant) technique for LLMs, which achieves good performance in diverse
quantization settings while maintaining the computational efficiency of PTQ by
efficiently optimizing various quantization parameters. OmniQuant comprises two
innovative components including Learnable Weight Clipping (LWC) and Learnable
Equivalent Transformation (LET). LWC modulates the extreme values of weights by
optimizing the clipping threshold. Meanwhile, LET tackles activation outliers
by shifting the challenge of quantization from activations to weights through a
learnable equivalent transformation. Operating within a differentiable
framework using block-wise error minimization, OmniQuant can optimize the
quantization process efficiently for both weight-only and weight-activation
quantization. For instance, the LLaMA-2 model family with the size of 7-70B can
be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using
128 samples. Extensive experiments validate OmniQuant's superior performance
across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16,
and W2A16. Additionally, OmniQuant demonstrates effectiveness in
instruction-tuned models and delivers notable improvements in inference speed
and memory reduction on real devices. Codes and models are available at
\url{https://github.com/OpenGVLab/OmniQuant}.
</p></li>
</ul>
<h3>Title: Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13191">http://arxiv.org/abs/2308.13191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13191] Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers](http://arxiv.org/abs/2308.13191) #memory</code></li>
<li>Summary: <p>Although dominant in natural language processing, transformer-based models
remain challenged by the task of long-sequence processing, because the
computational cost of self-attention operations in transformers swells
quadratically with the input sequence length. To alleviate the complexity of
long-sequence processing, we propose a simple framework to enable the
offthe-shelf pre-trained transformers to process much longer sequences, while
the computation and memory costs remain growing linearly with the input
sequence lengths. More specifically, our method divides each long-sequence
input into a batch of chunks, then aligns the interchunk information during the
encoding steps, and finally selects the most representative hidden states from
the encoder for the decoding process. To extract inter-chunk semantic
information, we align the start and end token embeddings among chunks in each
encoding transformer block. To learn an effective hidden selection policy, we
design a dual updating scheme inspired by reinforcement learning, which regards
the decoders of transformers as environments, and the downstream performance
metrics as the rewards to evaluate the hidden selection actions. Our empirical
results on real-world long-text summarization and reading comprehension tasks
demonstrate effective improvements compared to prior longsequence processing
baselines.
</p></li>
</ul>
<h3>Title: A Large-Scale Study of IoT Security Weaknesses and Vulnerabilities in the Wild. (arXiv:2308.13141v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13141">http://arxiv.org/abs/2308.13141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13141] A Large-Scale Study of IoT Security Weaknesses and Vulnerabilities in the Wild](http://arxiv.org/abs/2308.13141) #memory</code></li>
<li>Summary: <p>Internet of Things (IoT) is defined as the connection between places and
physical objects (i.e., things) over the internet/network via smart computing
devices. We observed that IoT software developers share solutions to
programming questions as code examples on three Stack Exchange Q&amp;A sites: Stack
Overflow (SO), Arduino, and Raspberry Pi. Previous research studies found
vulnerabilities/weaknesses in C/C++ code examples shared in Stack Overflow.
However, the studies did not investigate C/C++ code examples related to IoT.
The studies investigated SO code examples only. In this paper, we conduct a
large-scale empirical study of all IoT C/C++ code examples shared in the three
Stack Exchange sites, i.e., SO, Arduino, and Raspberry Pi. From the 11,329
obtained code snippets from the three sites, we identify 29 distinct CWE
(Common Weakness Enumeration) types in 609 snippets. These CWE types can be
categorized into 8 general weakness categories, and we observe that evaluation,
memory, and initialization related weaknesses are the most common to be
introduced by users when posting programming solutions. Furthermore, we find
that 39.58% of the vulnerable code snippets contain instances of CWE types that
can be mapped to real-world occurrences of those CWE types (i.e. CVE
instances). The most number vulnerable IoT code examples was found in Arduino,
followed by SO, and Raspberry Pi. Memory type vulnerabilities are on the rise
in the sites. For example, from the 3595 mapped CVE instances, we find that
28.99% result in Denial of Service (DoS) errors, which is particularly harmful
for network reliant IoT devices such as smart cars. Our study results can guide
various IoT stakeholders to be aware of such vulnerable IoT code examples and
to inform IoT researchers during their development of tools that can help
prevent developers the sharing of such vulnerable code examples in the sites.
[Abridged].
</p></li>
</ul>
<h3>Title: Training normalizing flows with computationally intensive target probability distributions. (arXiv:2308.13294v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13294">http://arxiv.org/abs/2308.13294</a></li>
<li>Code URL: <a href="https://github.com/nmcmc/nmcmc-code">https://github.com/nmcmc/nmcmc-code</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13294] Training normalizing flows with computationally intensive target probability distributions](http://arxiv.org/abs/2308.13294) #memory</code></li>
<li>Summary: <p>Machine learning techniques, in particular the so-called normalizing flows,
are becoming increasingly popular in the context of Monte Carlo simulations as
they can effectively approximate target probability distributions. In the case
of lattice field theories (LFT) the target distribution is given by the
exponential of the action. The common loss function's gradient estimator based
on the "reparametrization trick" requires the calculation of the derivative of
the action with respect to the fields. This can present a significant
computational cost for complicated, non-local actions like e.g. fermionic
action in QCD. In this contribution, we propose an estimator for normalizing
flows based on the REINFORCE algorithm that avoids this issue. We apply it to
two dimensional Schwinger model with Wilson fermions at criticality and show
that it is up to ten times faster in terms of the wall-clock time as well as
requiring up to $30\%$ less memory than the reparameterization trick estimator.
It is also more numerically stable allowing for single precision calculations
and the use of half-float tensor cores. We present an in-depth analysis of the
origins of those improvements. We believe that these benefits will appear also
outside the realm of the LFT, in each case where the target probability
distribution is computationally intensive.
</p></li>
</ul>
<h3>Title: Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13453">http://arxiv.org/abs/2308.13453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13453] Learning to Intervene on Concept Bottlenecks](http://arxiv.org/abs/2308.13453) #memory</code></li>
<li>Summary: <p>While traditional deep learning models often lack interpretability, concept
bottleneck models (CBMs) provide inherent explanations via their concept
representations. Specifically, they allow users to perform interventional
interactions on these concepts by updating the concept values and thus
correcting the predictive output of the model. Traditionally, however, these
interventions are applied to the model only once and discarded afterward. To
rectify this, we present concept bottleneck memory models (CB2M), an extension
to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate
novel situations via a two-fold memory with which it can learn to detect
mistakes and to reapply previous interventions. In this way, a CB2M learns to
automatically improve model performance from a few initially obtained
interventions. If no prior human interventions are available, a CB2M can detect
potential mistakes of the CBM bottleneck and request targeted interventions. In
our experimental evaluations on challenging scenarios like handling
distribution shifts and confounded training data, we illustrate that CB2M are
able to successfully generalize interventions to unseen data and can indeed
identify wrongly inferred concepts. Overall, our results show that CB2M is a
great tool for users to provide interactive feedback on CBMs, e.g., by guiding
a user's interaction and requiring fewer interventions.
</p></li>
</ul>
<h3>Title: Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13466">http://arxiv.org/abs/2308.13466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13466] Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction](http://arxiv.org/abs/2308.13466) #memory</code></li>
<li>Summary: <p>Despite the recent success of Graph Neural Networks (GNNs), it remains
challenging to train GNNs on large-scale graphs due to neighbor explosions. As
a remedy, distributed computing becomes a promising solution by leveraging
abundant computing resources (e.g., GPU). However, the node dependency of graph
data increases the difficulty of achieving high concurrency in distributed GNN
training, which suffers from the massive communication overhead. To address it,
Historical value approximation is deemed a promising class of distributed
training techniques. It utilizes an offline memory to cache historical
information (e.g., node embedding) as an affordable approximation of the exact
value and achieves high concurrency. However, such benefits come at the cost of
involving dated training information, leading to staleness, imprecision, and
convergence issues. To overcome these challenges, this paper proposes SAT
(Staleness-Alleviated Training), a novel and scalable distributed GNN training
framework that reduces the embedding staleness adaptively. The key idea of SAT
is to model the GNN's embedding evolution as a temporal graph and build a model
upon it to predict future embedding, which effectively alleviates the staleness
of the cached historical embedding. We propose an online algorithm to train the
embedding predictor and the distributed GNN alternatively and further provide a
convergence analysis. Empirically, we demonstrate that SAT can effectively
reduce embedding staleness and thus achieve better performance and convergence
speed on multiple large-scale graph datasets.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network. (arXiv:2308.13469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13469">http://arxiv.org/abs/2308.13469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13469] RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network](http://arxiv.org/abs/2308.13469) #few-shot</code></li>
<li>Summary: <p>Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic
segmentation in previously unseen domains with a limited number of annotated
samples. Although existing CD-FSS models focus on cross-domain feature
transformation, relying exclusively on inter-domain knowledge transfer may lead
to the loss of critical intra-domain information. To this end, we propose a
novel residual transformation network (RestNet) that facilitates knowledge
transfer while retaining the intra-domain support-query feature information.
Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module
that maps features to a stable domain-agnostic space using advanced semantics.
Additionally, an Intra-domain Residual Enhancement (IRE) module is designed to
maintain the intra-domain representation of the original discriminant space in
the new space. We also propose a mask prediction strategy based on prototype
fusion to help the model gradually learn how to segment. Our RestNet can
transfer cross-domain knowledge from both inter-domain and intra-domain without
requiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray,
and FSS-1000 show that our RestNet achieves state-of-the-art performance. Our
code will be available soon.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-28]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
