<h2>diffusion</h2>
<h3>Title: DiffAugment: Diffusion based Long-Tailed Visual Relationship Recognition. (arXiv:2401.01387v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01387">http://arxiv.org/abs/2401.01387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01387] DiffAugment: Diffusion based Long-Tailed Visual Relationship Recognition](http://arxiv.org/abs/2401.01387) #diffusion</code></li>
<li>Summary: <p>The task of Visual Relationship Recognition (VRR) aims to identify
relationships between two interacting objects in an image and is particularly
challenging due to the widely-spread and highly imbalanced distribution of
<subject, relation, object> triplets. To overcome the resultant performance
bias in existing VRR approaches, we introduce DiffAugment -- a method which
first augments the tail classes in the linguistic space by making use of
WordNet and then utilizes the generative prowess of Diffusion Models to expand
the visual space for minority classes. We propose a novel hardness-aware
component in diffusion which is based upon the hardness of each <S,R,O> triplet
and demonstrate the effectiveness of hardness-aware diffusion in generating
visual embeddings for the tail classes. We also propose a novel subject and
object based seeding strategy for diffusion sampling which improves the
discriminative capability of the generated visual embeddings. Extensive
experimentation on the GQA-LT dataset shows favorable gains in the
subject/object and relation average per-class accuracy using Diffusion
augmented samples.
</p></li>
</ul>
<h3>Title: ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image and Text. (arXiv:2401.01456v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01456">http://arxiv.org/abs/2401.01456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01456] ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image and Text](http://arxiv.org/abs/2401.01456) #diffusion</code></li>
<li>Summary: <p>Recently, diffusion models have demonstrated their effectiveness in
generating extremely high-quality images and have found wide-ranging
applications, including automatic sketch colorization. However, most existing
models use text to guide the conditional generation, with fewer attempts
exploring the potential advantages of using image tokens as conditional inputs
for networks. As such, this paper exhaustively investigates image-guided
models, specifically targeting reference-based sketch colorization, which aims
to colorize sketch images using reference color images. We investigate three
critical aspects of reference-based diffusion models: the shortcomings compared
to text-based counterparts, the training strategies, and the capability in
zero-shot, sequential text-based manipulation. We introduce two variations of
an image-guided latent diffusion model using different image tokens from the
pre-trained CLIP image encoder, and we propose corresponding manipulation
methods to adjust their results sequentially using weighted text inputs. We
conduct comprehensive evaluations of our models through qualitative and
quantitative experiments, as well as a user study.
</p></li>
</ul>
<h3>Title: S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01520">http://arxiv.org/abs/2401.01520</a></li>
<li>Code URL: <a href="https://github.com/kingkingofall/skip-step-diffusion">https://github.com/kingkingofall/skip-step-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01520] S$^{2}$-DMs:Skip-Step Diffusion Models](http://arxiv.org/abs/2401.01520) #diffusion</code></li>
<li>Summary: <p>Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
</p></li>
</ul>
<h3>Title: SIGNeRF: Scene Integrated Generation for Neural Radiance Fields. (arXiv:2401.01647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01647">http://arxiv.org/abs/2401.01647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01647] SIGNeRF: Scene Integrated Generation for Neural Radiance Fields](http://arxiv.org/abs/2401.01647) #diffusion</code></li>
<li>Summary: <p>Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
</p></li>
</ul>
<h3>Title: DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models. (arXiv:2401.01659v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01659">http://arxiv.org/abs/2401.01659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01659] DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models](http://arxiv.org/abs/2401.01659) #diffusion</code></li>
<li>Summary: <p>Object detection models represented by YOLO series have been widely used and
have achieved great results on the high quality datasets, but not all the
working conditions are ideal. To settle down the problem of locating targets on
low quality datasets, the existing methods either train a new object detection
network, or need a large collection of low-quality datasets to train. However,
we propose a framework in this paper and apply it on the YOLO models called
DiffYOLO. Specifically, we extract feature maps from the denoising diffusion
probabilistic models to enhance the well-trained models, which allows us
fine-tune YOLO on high-quality datasets and test on low-quality datasets. The
results proved this framework can not only prove the performance on noisy
datasets, but also prove the detection results on high-quality test datasets.
We will supplement more experiments later (with various datasets and network
architectures).
</p></li>
</ul>
<h3>Title: Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging. (arXiv:2401.01662v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01662">http://arxiv.org/abs/2401.01662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01662] Simultaneous q-Space Sampling Optimization and Reconstruction for Fast and High-fidelity Diffusion Magnetic Resonance Imaging](http://arxiv.org/abs/2401.01662) #diffusion</code></li>
<li>Summary: <p>Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the
noninvasive investigation of tissue microstructural properties and structural
connectivity in the \textit{in vivo} human brain. However, to effectively
capture the intricate characteristics of water diffusion at various directions
and scales, it is important to employ comprehensive q-space sampling.
Unfortunately, this requirement leads to long scan times, limiting the clinical
applicability of dMRI. To address this challenge, we propose SSOR, a
Simultaneous q-Space sampling Optimization and Reconstruction framework. We
jointly optimize a subset of q-space samples using a continuous representation
of spherical harmonic functions and a reconstruction network. Additionally, we
integrate the unique properties of diffusion magnetic resonance imaging (dMRI)
in both the q-space and image domains by applying $l1$-norm and total-variation
regularization. The experiments conducted on HCP data demonstrate that SSOR has
promising strengths both quantitatively and qualitatively and exhibits
robustness to noise.
</p></li>
</ul>
<h3>Title: AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning. (arXiv:2401.01693v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01693">http://arxiv.org/abs/2401.01693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01693] AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning](http://arxiv.org/abs/2401.01693) #diffusion</code></li>
<li>Summary: <p>Deep learning has shown great potential in accelerating diffusion tensor
imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise
and detail loss in reconstructing the DTI-derived parametric maps especially
when sparsely sampled q-space data are used. This paper proposes a novel
method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to
facilitate fast and accurate DTI with only six measurements. AID-DTI is
equipped with a newly designed Singular Value Decomposition (SVD)-based
regularizer, which can effectively capture fine details while suppressing noise
during network training. Experimental results on Human Connectome Project (HCP)
data consistently demonstrate that the proposed method estimates DTI parameter
maps with fine-grained details and outperforms three state-of-the-art methods
both quantitatively and qualitatively.
</p></li>
</ul>
<h3>Title: aMUSEd: An Open MUSE Reproduction. (arXiv:2401.01808v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01808">http://arxiv.org/abs/2401.01808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01808] aMUSEd: An Open MUSE Reproduction](http://arxiv.org/abs/2401.01808) #diffusion</code></li>
<li>Summary: <p>We present aMUSEd, an open-source, lightweight masked image model (MIM) for
text-to-image generation based on MUSE. With 10 percent of MUSE's parameters,
aMUSEd is focused on fast image generation. We believe MIM is under-explored
compared to latent diffusion, the prevailing approach for text-to-image
generation. Compared to latent diffusion, MIM requires fewer inference steps
and is more interpretable. Additionally, MIM can be fine-tuned to learn
additional styles with only a single image. We hope to encourage further
exploration of MIM by demonstrating its effectiveness on large-scale
text-to-image generation and releasing reproducible training code. We also
release checkpoints for two models which directly produce images at 256x256 and
512x512 resolutions.
</p></li>
</ul>
<h3>Title: Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions. (arXiv:2401.01827v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01827">http://arxiv.org/abs/2401.01827</a></li>
<li>Code URL: <a href="https://github.com/salesforce/lavis">https://github.com/salesforce/lavis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01827] Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions](http://arxiv.org/abs/2401.01827) #diffusion</code></li>
<li>Summary: <p>Most existing video diffusion models (VDMs) are limited to mere text
conditions. Thereby, they are usually lacking in control over visual appearance
and geometry structure of the generated videos. This work presents Moonshot, a
new video generation model that conditions simultaneously on multimodal inputs
of image and text. The model builts upon a core module, called multimodal video
block (MVB), which consists of conventional spatialtemporal layers for
representing video features, and a decoupled cross-attention layer to address
image and text inputs for appearance conditioning. In addition, we carefully
design the model architecture such that it can optionally integrate with
pre-trained image ControlNet modules for geometry visual conditions, without
needing of extra training overhead as opposed to prior methods. Experiments
show that with versatile multimodal conditioning mechanisms, Moonshot
demonstrates significant improvement on visual quality and temporal consistency
compared to existing models. In addition, the model can be easily repurposed
for a variety of generative applications, such as personalized video
generation, image animation and video editing, unveiling its potential to serve
as a fundamental architecture for controllable video generation. Models will be
made public on https://github.com/salesforce/LAVIS.
</p></li>
</ul>
<h3>Title: From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations. (arXiv:2401.01885v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01885">http://arxiv.org/abs/2401.01885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01885] From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations](http://arxiv.org/abs/2401.01885) #diffusion</code></li>
<li>Summary: <p>We present a framework for generating full-bodied photorealistic avatars that
gesture according to the conversational dynamics of a dyadic interaction. Given
speech audio, we output multiple possibilities of gestural motion for an
individual, including face, body, and hands. The key behind our method is in
combining the benefits of sample diversity from vector quantization with the
high-frequency details obtained through diffusion to generate more dynamic,
expressive motion. We visualize the generated motion using highly
photorealistic avatars that can express crucial nuances in gestures (e.g.
sneers and smirks). To facilitate this line of research, we introduce a
first-of-its-kind multi-view conversational dataset that allows for
photorealistic reconstruction. Experiments show our model generates appropriate
and diverse gestures, outperforming both diffusion- and VQ-only methods.
Furthermore, our perceptual evaluation highlights the importance of
photorealism (vs. meshes) in accurately assessing subtle motion details in
conversational gestures. Code and dataset available online.
</p></li>
</ul>
<h3>Title: DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction. (arXiv:2401.01846v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01846">http://arxiv.org/abs/2401.01846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01846] DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction](http://arxiv.org/abs/2401.01846) #diffusion</code></li>
<li>Summary: <p>Forecasting future stock trends remains challenging for academia and industry
due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics
influencing stock prices. In recent years, graph neural networks have achieved
remarkable performance in this problem by formulating multiple stocks as
graph-structured data. However, most of these approaches rely on artificially
defined factors to construct static stock graphs, which fail to capture the
intrinsic interdependencies between stocks that rapidly evolve. In addition,
these methods often ignore the hierarchical features of the stocks and lose
distinctive information within. In this work, we propose a novel graph learning
approach implemented without expert knowledge to address these issues. First,
our approach automatically constructs dynamic stock graphs by entropy-driven
edge generation from a signal processing perspective. Then, we further learn
task-optimal dependencies between stocks via a generalized graph diffusion
process on constructed stock graphs. Last, a decoupled representation learning
scheme is adopted to capture distinctive hierarchical intra-stock features.
Experimental results demonstrate substantial improvements over state-of-the-art
baselines on real-world datasets. Moreover, the ablation study and sensitivity
study further illustrate the effectiveness of the proposed method in modeling
the time-evolving inter-stock and intra-stock dynamics.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Multimodal self-supervised learning for lesion localization. (arXiv:2401.01524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01524">http://arxiv.org/abs/2401.01524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01524] Multimodal self-supervised learning for lesion localization](http://arxiv.org/abs/2401.01524) #self-supervised</code></li>
<li>Summary: <p>Multimodal deep learning utilizing imaging and diagnostic reports has made
impressive progress in the field of medical imaging diagnostics, demonstrating
a particularly strong capability for auxiliary diagnosis in cases where
sufficient annotation information is lacking. Nonetheless, localizing diseases
accurately without detailed positional annotations remains a challenge.
Although existing methods have attempted to utilize local information to
achieve fine-grained semantic alignment, their capability in extracting the
fine-grained semantics of the comprehensive contextual within reports is
limited. To solve this problem, we introduce a new method that takes full
sentences from textual reports as the basic units for local semantic alignment.
Our approach combines chest X-ray images with their corresponding textual
reports, performing contrastive learning at both global and local levels. The
leading results obtained by our method on multiple datasets confirm its
efficacy in the task of lesion localization.
</p></li>
</ul>
<h3>Title: A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01862">http://arxiv.org/abs/2401.01862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01862] A Vision Check-up for Language Models](http://arxiv.org/abs/2401.01862) #self-supervised</code></li>
<li>Summary: <p>What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</p></li>
</ul>
<h3>Title: Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data. (arXiv:2401.01640v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01640">http://arxiv.org/abs/2401.01640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01640] Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data](http://arxiv.org/abs/2401.01640) #self-supervised</code></li>
<li>Summary: <p>Self-supervised learning (SSL) has become the de facto training paradigm of
large models where pre-training is followed by supervised fine-tuning using
domain-specific data and labels. Hypothesizing that SSL models would learn more
generic, hence less biased, representations, this study explores the impact of
pre-training and fine-tuning strategies on fairness (i.e., performing equally
on different demographic breakdowns). Motivated by human-centric applications
on real-world timeseries data, we interpret inductive biases on the model,
layer, and metric levels by systematically comparing SSL models to their
supervised counterparts. Our findings demonstrate that SSL has the capacity to
achieve performance on par with supervised methods while significantly
enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1%
loss in performance through self-supervision. Ultimately, this work underscores
SSL's potential in human-centric computing, particularly high-stakes,
data-scarce application domains like healthcare.
</p></li>
</ul>
<h3>Title: Zero-shot Active Learning Using Self Supervised Learning. (arXiv:2401.01690v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01690">http://arxiv.org/abs/2401.01690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01690] Zero-shot Active Learning Using Self Supervised Learning](http://arxiv.org/abs/2401.01690) #self-supervised</code></li>
<li>Summary: <p>Deep learning algorithms are often said to be data hungry. The performance of
such algorithms generally improve as more and more annotated data is fed into
the model. While collecting unlabelled data is easier (as they can be scraped
easily from the internet), annotating them is a tedious and expensive task.
Given a fixed budget available for data annotation, Active Learning helps
selecting the best subset of data for annotation, such that the deep learning
model when trained over that subset will have maximum generalization
performance under this budget. In this work, we aim to propose a new Active
Learning approach which is model agnostic as well as one doesn't require an
iterative process. We aim to leverage self-supervised learnt features for the
task of Active Learning. The benefit of self-supervised learning, is that one
can get useful feature representation of the input data, without having any
annotation.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Enhancing the medical foundation model with multi-scale and cross-modality feature learning. (arXiv:2401.01583v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01583">http://arxiv.org/abs/2401.01583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01583] Enhancing the medical foundation model with multi-scale and cross-modality feature learning](http://arxiv.org/abs/2401.01583) #foundation model</code></li>
<li>Summary: <p>The development of multi-modal medical foundation models has attracted
significant attention in the field of medicine and healthcare due to their
promising prospects in various clinical applications. One area of focus in this
research direction is the extractions of features at different scales. While
previous studies have explored feature learning at individual scales,
investigation on integrating the diverse scales and modalities of information
is lacking, which may hinder the potential for mutual reinforcement among these
features. This paper aims to bridge this gap by proposing a method that
effectively exploits multi-scale and cross-modality information to enhance the
performance of medical foundation models. The proposed method simultaneously
exploit features at the local, instance, modality and global aspects,
facilitating comprehensive representation learning within the models. We
evaluate the effectiveness of the proposed method on six open-source datasets
across different clinical tasks, demonstrating its ability to enhance the
performance of medical foundation models.
</p></li>
</ul>
<h3>Title: Few-shot Adaptation of Multi-modal Foundation Models: A Survey. (arXiv:2401.01736v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01736">http://arxiv.org/abs/2401.01736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01736] Few-shot Adaptation of Multi-modal Foundation Models: A Survey](http://arxiv.org/abs/2401.01736) #foundation model</code></li>
<li>Summary: <p>Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Physio: An LLM-Based Physiotherapy Advisor. (arXiv:2401.01825v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01825">http://arxiv.org/abs/2401.01825</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01825] Physio: An LLM-Based Physiotherapy Advisor](http://arxiv.org/abs/2401.01825) #generative</code></li>
<li>Summary: <p>The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt.
</p></li>
</ul>
<h3>Title: Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01879">http://arxiv.org/abs/2401.01879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01879] Theoretical guarantees on the best-of-n alignment policy](http://arxiv.org/abs/2401.01879) #generative</code></li>
<li>Summary: <p>A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</p></li>
</ul>
<h3>Title: Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01426">http://arxiv.org/abs/2401.01426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01426] Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference](http://arxiv.org/abs/2401.01426) #generative</code></li>
<li>Summary: <p>Pearl's causal hierarchy establishes a clear separation between
observational, interventional, and counterfactual questions. Researchers
proposed sound and complete algorithms to compute identifiable causal queries
at a given level of the hierarchy using the causal structure and data from the
lower levels of the hierarchy. However, most of these algorithms assume that we
can accurately estimate the probability distribution of the data, which is an
impractical assumption for high-dimensional variables such as images. On the
other hand, modern generative deep learning architectures can be trained to
learn how to accurately sample from such high-dimensional distributions.
Especially with the recent rise of foundation models for images, it is
desirable to leverage pre-trained models to answer causal queries with such
high-dimensional data. To address this, we propose a sequential training
algorithm that, given the causal structure and a pre-trained conditional
generative model, can train a deep causal generative model, which utilizes the
pre-trained model and can provably sample from identifiable interventional and
counterfactual distributions. Our algorithm, called Modular-DCM, uses
adversarial training to learn the network weights, and to the best of our
knowledge, is the first algorithm that can make use of pre-trained models and
provably sample from any identifiable causal query in the presence of latent
confounders with high-dimensional data. We demonstrate the utility of our
algorithm using semi-synthetic and real-world datasets containing images as
variables in the causal structure.
</p></li>
</ul>
<h3>Title: Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01641">http://arxiv.org/abs/2401.01641</a></li>
<li>Code URL: <a href="https://github.com/featurespace/foundation-model-paper">https://github.com/featurespace/foundation-model-paper</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01641] Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences](http://arxiv.org/abs/2401.01641) #generative</code></li>
<li>Summary: <p>Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection. (arXiv:2401.01342v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01342">http://arxiv.org/abs/2401.01342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01342] Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection](http://arxiv.org/abs/2401.01342) #anomaly</code></li>
<li>Summary: <p>The last decades have been characterized by unprecedented technological
advances, many of them powered by modern technologies such as Artificial
Intelligence (AI) and Machine Learning (ML). The world has become more
digitally connected than ever, but we face major challenges. One of the most
significant is cybercrime, which has emerged as a global threat to governments,
businesses, and civil societies. The pervasiveness of digital technologies
combined with a constantly shifting technological foundation has created a
complex and powerful playground for cybercriminals, which triggered a surge in
demand for intelligent threat detection systems based on machine and deep
learning. This paper investigates AI-based cyber threat detection to protect
our modern digital ecosystems. The primary focus is on evaluating ML-based
classifiers and ensembles for anomaly-based malware detection and network
intrusion detection and how to integrate those models in the context of network
security, mobile security, and IoT security. The discussion highlights the
challenges when deploying and integrating AI-enabled cybersecurity solutions
into existing enterprise systems and IT infrastructures, including options to
overcome those challenges. Finally, the paper provides future research
directions to further increase the security and resilience of our modern
digital industries, infrastructures, and ecosystems.
</p></li>
</ul>
<h2>in-context</h2>
<h2>memory</h2>
<h3>Title: Glance and Focus: Memory Prompting for Multi-Event Video Question Answering. (arXiv:2401.01529v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01529">http://arxiv.org/abs/2401.01529</a></li>
<li>Code URL: <a href="https://github.com/byz0e/glance-focus">https://github.com/byz0e/glance-focus</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01529] Glance and Focus: Memory Prompting for Multi-Event Video Question Answering](http://arxiv.org/abs/2401.01529) #memory</code></li>
<li>Summary: <p>Video Question Answering (VideoQA) has emerged as a vital tool to evaluate
agents' ability to understand human daily behaviors. Despite the recent success
of large vision language models in many multi-modal tasks, complex situation
reasoning over videos involving multiple human-object interaction events still
remains challenging. In contrast, humans can easily tackle it by using a series
of episode memories as anchors to quickly locate question-related key moments
for reasoning. To mimic this effective reasoning strategy, we propose the
Glance-Focus model. One simple way is to apply an action detection model to
predict a set of actions as key memories. However, these actions within a
closed set vocabulary are hard to generalize to various video domains. Instead
of that, we train an Encoder-Decoder to generate a set of dynamic event
memories at the glancing stage. Apart from using supervised bipartite matching
to obtain the event memories, we further design an unsupervised memory
generation method to get rid of dependence on event annotations. Next, at the
focusing stage, these event memories act as a bridge to establish the
correlation between the questions with high-level event concepts and low-level
lengthy video content. Given the question, the model first focuses on the
generated key event memory, then focuses on the most relevant moment for
reasoning through our designed multi-level cross-attention mechanism. We
conduct extensive experiments on four Multi-Event VideoQA benchmarks including
STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves
state-of-the-art results, surpassing current large models in various
challenging reasoning tasks. The code and models are available at
https://github.com/ByZ0e/Glance-Focus.
</p></li>
</ul>
<h3>Title: Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01487">http://arxiv.org/abs/2401.01487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01487] Natural Language Processing and Multimodal Stock Price Prediction](http://arxiv.org/abs/2401.01487) #memory</code></li>
<li>Summary: <p>In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</p></li>
</ul>
<h3>Title: ALPC Is In Danger: ALPChecker Detects Spoofing and Blinding. (arXiv:2401.01376v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01376">http://arxiv.org/abs/2401.01376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01376] ALPC Is In Danger: ALPChecker Detects Spoofing and Blinding](http://arxiv.org/abs/2401.01376) #memory</code></li>
<li>Summary: <p>The purpose of this study is to evaluate the possibility of implementing an
attack on ALPC connection in the Windows operating system through the kernel
without closing the connection covertly from programs and the operating system
and to propose a method of protection against this type of attacks.
Asynchronous Local Procedure Call technology (ALPC) is used in various Windows
information protection systems, including antivirus systems (AV) and Endpoint
Detection and Response systems (EDR). To ensure the concealment of malicious
software, attackers need to disrupt the operation of AV, EDR tools, which in
turn can be achieved by destructive impact on the components of the ALPC
technology. Examples of such attacks already exist and are covered in this
paper. To counteract such new threats, it is necessary to advance the
improvement of information security systems and the ALPC security research was
conducted. The most difficult case, Windows kernel driver attack, was
considered. Three attacks on the ALPC connection were carried out, based on
changing the ALPC structures in the kernel memory, which led to creation of
illegitimate connections in the system and the disruption of correct
connections. ALPChecker protection tool has been developed. The tool was
successfully tested on three demonstrated attacks.
</p></li>
</ul>
<h3>Title: Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01458">http://arxiv.org/abs/2401.01458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01458] Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint](http://arxiv.org/abs/2401.01458) #memory</code></li>
<li>Summary: <p>Neural networks (NNs) are increasingly used in always-on safety-critical
applications deployed on hardware accelerators (NN-HAs) employing various
memory technologies. Reliable continuous operation of NN is essential for
safety-critical applications. During online operation, NNs are susceptible to
single and multiple permanent and soft errors due to factors such as radiation,
aging, and thermal effects. Explicit NN-HA testing methods cannot detect
transient faults during inference, are unsuitable for always-on applications,
and require extensive test vector generation and storage. Therefore, in this
paper, we propose the \emph{uncertainty fingerprint} approach representing the
online fault status of NN. Furthermore, we propose a dual head NN topology
specifically designed to produce uncertainty fingerprints and the primary
prediction of the NN in \emph{a single shot}. During the online operation, by
matching the uncertainty fingerprint, we can concurrently self-test NNs with up
to $100\%$ coverage with a low false positive rate while maintaining a similar
performance of the primary task. Compared to existing works, memory overhead is
reduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced
by up to $10000\times$, and false-positive rates are reduced by up to $89\%$.
</p></li>
</ul>
<h3>Title: Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01728">http://arxiv.org/abs/2401.01728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01728] Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices](http://arxiv.org/abs/2401.01728) #memory</code></li>
<li>Summary: <p>Modern deep learning models, growing larger and more complex, have
demonstrated exceptional generalization and accuracy due to training on huge
datasets. This trend is expected to continue. However, the increasing size of
these models poses challenges in training, as traditional centralized methods
are limited by memory constraints at such scales. This paper proposes an
asynchronous decentralized training paradigm for large modern deep learning
models that harnesses the compute power of regular heterogeneous PCs with
limited resources connected across the internet to achieve favourable
performance metrics. Ravnest facilitates decentralized training by efficiently
organizing compute nodes into clusters with similar data transfer rates and
compute capabilities, without necessitating that each node hosts the entire
model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model
Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is
employed to effectively execute global parameter averaging across all clusters.
We have framed our asynchronous SGD loss function as a block structured
optimization problem with delayed updates and derived an optimal convergence
rate of $O\left(\frac{1}{\sqrt{K}}\right)$. We further discuss linear speedup
with respect to the number of participating clusters and the bound on the
staleness parameter.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01482">http://arxiv.org/abs/2401.01482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01482] Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition](http://arxiv.org/abs/2401.01482) #few-shot</code></li>
<li>Summary: <p>Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
</p></li>
</ul>
<h3>Title: LORE++: Logical Location Regression Network for Table Structure Recognition with Pre-training. (arXiv:2401.01522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01522">http://arxiv.org/abs/2401.01522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01522] LORE++: Logical Location Regression Network for Table Structure Recognition with Pre-training](http://arxiv.org/abs/2401.01522) #few-shot</code></li>
<li>Summary: <p>Table structure recognition (TSR) aims at extracting tables in images into
machine-understandable formats. Recent methods solve this problem by predicting
the adjacency relations of detected cell boxes or learning to directly generate
the corresponding markup sequences from the table images. However, existing
approaches either count on additional heuristic rules to recover the table
structures, or face challenges in capturing long-range dependencies within
tables, resulting in increased complexity. In this paper, we propose an
alternative paradigm. We model TSR as a logical location regression problem and
propose a new TSR framework called LORE, standing for LOgical location
REgression network, which for the first time regresses logical location as well
as spatial location of table cells in a unified network. Our proposed LORE is
conceptually simpler, easier to train, and more accurate than other paradigms
of TSR. Moreover, inspired by the persuasive success of pre-trained models on a
number of computer vision and natural language processing tasks, we propose two
pre-training tasks to enrich the spatial and logical representations at the
feature level of LORE, resulting in an upgraded version called LORE++. The
incorporation of pre-training in LORE++ has proven to enjoy significant
advantages, leading to a substantial enhancement in terms of accuracy,
generalization, and few-shot capability compared to its predecessor.
Experiments on standard benchmarks against methods of previous paradigms
demonstrate the superiority of LORE++, which highlights the potential and
promising prospect of the logical location regression paradigm for TSR.
</p></li>
</ul>
<h3>Title: MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning. (arXiv:2401.01591v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01591">http://arxiv.org/abs/2401.01591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01591] MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning](http://arxiv.org/abs/2401.01591) #few-shot</code></li>
<li>Summary: <p>Existing contrastive language-image pre-training aims to learn a joint
representation by matching abundant image-text pairs. However, the number of
image-text pairs in medical datasets is usually orders of magnitude smaller
than that in natural datasets. Besides, medical image-text pairs often involve
numerous complex fine-grained correspondences. This paper aims to enhance the
data efficiency by introducing multiple-to-multiple local relationship modeling
to capture denser supervisions. More specifically, we propose a Medical
Language-Image Pre-training (MLIP) framework, which exploits the limited
image-text medical data more efficiently through patch-sentence matching.
Furthermore, we introduce a masked contrastive learning strategy with semantic
integrity estimation to reduce redundancy in images while preserving the
underlying semantics. Our evaluation results show that MLIP outperforms
previous work in zero/few-shot classification and few-shot segmentation tasks
by a large margin.
</p></li>
</ul>
<h3>Title: Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning. (arXiv:2401.01598v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01598">http://arxiv.org/abs/2401.01598</a></li>
<li>Code URL: <a href="https://github.com/1170300714/lp-dif">https://github.com/1170300714/lp-dif</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01598] Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning](http://arxiv.org/abs/2401.01598) #few-shot</code></li>
<li>Summary: <p>Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new
classes based on very limited training data without forgetting the old ones
encountered. Existing studies solely relied on pure visual networks, while in
this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)
and propose a simple yet effective framework, named Learning Prompt with
Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP
for zero-shot evaluation can substantially outperform the most influential
methods. Then, prompt tuning technique is involved to further improve its
adaptation ability, allowing the model to continually capture specific
knowledge from each session. To prevent the learnable prompt from forgetting
old knowledge in the new session, we propose a pseudo-feature replay approach.
Specifically, we preserve the old knowledge of each class by maintaining a
feature-level Gaussian distribution with a diagonal covariance matrix, which is
estimated by the image features of training images and synthesized features
generated from a VAE. When progressing to a new session, pseudo-features are
sampled from old-class distributions combined with training images of the
current session to optimize the prompt, thus enabling the model to learn new
knowledge while retaining old knowledge. Experiments on three prevalent
benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging
benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the
superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is
publicly available at https://github.<a href="http://export.arxiv.org/abs/com/1170300">com/1170300</a>714/LP-DiF.
</p></li>
</ul>
<h3>Title: Few-shot Image Generation via Information Transfer from the Built Geodesic Surface. (arXiv:2401.01749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01749">http://arxiv.org/abs/2401.01749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01749] Few-shot Image Generation via Information Transfer from the Built Geodesic Surface](http://arxiv.org/abs/2401.01749) #few-shot</code></li>
<li>Summary: <p>Images generated by most of generative models trained with limited data often
exhibit deficiencies in either fidelity, diversity, or both. One effective
solution to address the limitation is few-shot generative model adaption.
However, the type of approaches typically rely on a large-scale pre-trained
model, serving as a source domain, to facilitate information transfer to the
target domain. In this paper, we propose a method called Information Transfer
from the Built Geodesic Surface (ITBGS), which contains two module: Feature
Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization
(I\&amp;R). With the FAGS module, a pseudo-source domain is created by projecting
image features from the training dataset into the Pre-Shape Space, subsequently
generating new features on the Geodesic surface. Thus, no pre-trained models is
needed for the adaption process during the training of generative models with
FAGS. I\&amp;R module are introduced for supervising the interpolated images and
regularizing their relative distances, respectively, to further enhance the
quality of generated images. Through qualitative and quantitative experiments,
we demonstrate that the proposed method consistently achieves optimal or
comparable results across a diverse range of semantically distinct datasets,
even in extremely few-shot scenarios.
</p></li>
</ul>
<h3>Title: Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs. (arXiv:2401.01711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01711">http://arxiv.org/abs/2401.01711</a></li>
<li>Code URL: <a href="https://github.com/sebischair/llm-sp-cqa">https://github.com/sebischair/llm-sp-cqa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01711] Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs](http://arxiv.org/abs/2401.01711) #few-shot</code></li>
<li>Summary: <p>Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.
</p></li>
</ul>
<h3>Title: Does Few-shot Learning Suffer from Backdoor Attacks?. (arXiv:2401.01377v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01377">http://arxiv.org/abs/2401.01377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01377] Does Few-shot Learning Suffer from Backdoor Attacks?](http://arxiv.org/abs/2401.01377) #few-shot</code></li>
<li>Summary: <p>The field of few-shot learning (FSL) has shown promising results in scenarios
where training data is limited, but its vulnerability to backdoor attacks
remains largely unexplored. We first explore this topic by first evaluating the
performance of the existing backdoor attack methods on few-shot learning
scenarios. Unlike in standard supervised learning, existing backdoor attack
methods failed to perform an effective attack in FSL due to two main issues.
Firstly, the model tends to overfit to either benign features or trigger
features, causing a tough trade-off between attack success rate and benign
accuracy. Secondly, due to the small number of training samples, the dirty
label or visible trigger in the support set can be easily detected by victims,
which reduces the stealthiness of attacks. It seemed that FSL could survive
from backdoor attacks. However, in this paper, we propose the Few-shot Learning
Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor
attacks. Specifically, we first generate a trigger to maximize the gap between
poisoned and benign features. It enables the model to learn both benign and
trigger features, which solves the problem of overfitting. To make it more
stealthy, we hide the trigger by optimizing two types of imperceptible
perturbation, namely attractive and repulsive perturbation, instead of
attaching the trigger directly. Once we obtain the perturbations, we can poison
all samples in the benign support set into a hidden poisoned support set and
fine-tune the model on it. Our method demonstrates a high Attack Success Rate
(ASR) in FSL tasks with different few-shot learning paradigms while preserving
clean accuracy and maintaining stealthiness. This study reveals that few-shot
learning still suffers from backdoor attacks, and its security should be given
attention.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2024-01-04]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
