<h2>diffusion</h2>
<h3>Title: DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models. (arXiv:2308.00122v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00122">http://arxiv.org/abs/2308.00122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00122] DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models](http://arxiv.org/abs/2308.00122) #diffusion</code></li>
<li>Summary: <p>We propose DAVIS, a Diffusion model-based Audio-VIusal Separation framework
that solves the audio-visual sound source separation task through a generative
manner. While existing discriminative methods that perform mask regression have
made remarkable progress in this field, they face limitations in capturing the
complex data distribution required for high-quality separation of sounds from
diverse categories. In contrast, DAVIS leverages a generative diffusion model
and a Separation U-Net to synthesize separated magnitudes starting from
Gaussian noises, conditioned on both the audio mixture and the visual footage.
With its generative objective, DAVIS is better suited to achieving the goal of
high-quality sound separation across diverse categories. We compare DAVIS to
existing state-of-the-art discriminative audio-visual separation methods on the
domain-specific MUSIC dataset and the open-domain AVE dataset, and results show
that DAVIS outperforms other methods in separation quality, demonstrating the
advantages of our framework for tackling the audio-visual source separation
task.
</p></li>
</ul>
<h3>Title: InFusion: Inject and Attention Fusion for Multi Concept Zero Shot Text based Video Editing. (arXiv:2308.00135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00135">http://arxiv.org/abs/2308.00135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00135] InFusion: Inject and Attention Fusion for Multi Concept Zero Shot Text based Video Editing](http://arxiv.org/abs/2308.00135) #diffusion</code></li>
<li>Summary: <p>Large text-to-image diffusion models have achieved remarkable success in
generating diverse high-quality images that are closely aligned with text
prompt. But, when these models applied to video the main challenge is to ensure
temporal consistency and coherent editing. In this paper, we proposed InFusion,
a framework for zero-shot text-based video editing leveraging large pre-trained
image diffusion models. Our framework specifically supports editing of multiple
concepts with the pixel level control over diverse concepts mentioned in the
editing prompt. Specifically, we inject the difference of features from U-Net
residual blocks in decoder layers for source and edit prompt, this when
combined with injected attention features make it feasible to query the source
contents and scale the edited concepts along with injection of unedited parts.
The editing is further controlled in fine-grained manner with mask extraction
and attention fusion strategy which cuts the edited part from source and paste
it from the denoising pipeline for target prompt. Our framework is a low cost
alternative for the one-shot tuned models for editing since it does not require
training. We demonstrated the complex concept editing with generalised image
model (Stable Diffusion v1.5) using LoRA. Adaptation is compatible with all the
existing image diffusion techniques. Extensive experimental results demonstrate
the effectiveness over existing methods in rendering high-quality and
temporally consistent videos.
</p></li>
</ul>
<h3>Title: Diffusion Model for Camouflaged Object Detection. (arXiv:2308.00303v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00303">http://arxiv.org/abs/2308.00303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00303] Diffusion Model for Camouflaged Object Detection](http://arxiv.org/abs/2308.00303) #diffusion</code></li>
<li>Summary: <p>Camouflaged object detection is a challenging task that aims to identify
objects that are highly similar to their background. Due to the powerful
noise-to-image denoising capability of denoising diffusion models, in this
paper, we propose a diffusion-based framework for camouflaged object detection,
termed diffCOD, a new framework that considers the camouflaged object
segmentation task as a denoising diffusion process from noisy masks to object
masks. Specifically, the object mask diffuses from the ground-truth masks to a
random distribution, and the designed model learns to reverse this noising
process. To strengthen the denoising learning, the input image prior is encoded
and integrated into the denoising diffusion model to guide the diffusion
process. Furthermore, we design an injection attention module (IAM) to interact
conditional semantic features extracted from the image with the diffusion noise
embedding via the cross-attention mechanism to enhance denoising learning.
Extensive experiments on four widely used COD benchmark datasets demonstrate
that the proposed method achieves favorable performance compared to the
existing 11 state-of-the-art methods, especially in the detailed texture
segmentation of camouflaged objects. Our code will be made publicly available
at: https://github.com/ZNan-Chen/diffCOD.
</p></li>
</ul>
<h3>Title: Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00675">http://arxiv.org/abs/2308.00675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00675] Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models](http://arxiv.org/abs/2308.00675) #diffusion</code></li>
<li>Summary: <p>Today, large language models (LLMs) are taught to use new tools by providing
a few demonstrations of the tool's usage. Unfortunately, demonstrations are
hard to acquire, and can result in undesirable biased usage if the wrong
demonstration is chosen. Even in the rare scenario that demonstrations are
readily available, there is no principled selection protocol to determine how
many and which ones to provide. As tasks grow more complex, the selection
search grows combinatorially and invariably becomes intractable. Our work
provides an alternative to demonstrations: tool documentation. We advocate the
use of tool documentation, descriptions for the individual tool usage, over
demonstrations. We substantiate our claim through three main empirical findings
on 6 tasks across both vision and language modalities. First, on existing
benchmarks, zero-shot prompts with only tool documentation are sufficient for
eliciting proper tool usage, achieving performance on par with few-shot
prompts. Second, on a newly collected realistic tool-use dataset with hundreds
of available tool APIs, we show that tool documentation is significantly more
valuable than demonstrations, with zero-shot documentation significantly
outperforming few-shot without documentation. Third, we highlight the benefits
of tool documentations by tackling image generation and video tracking using
just-released unseen state-of-the-art models as tools. Finally, we highlight
the possibility of using tool documentation to automatically enable new
applications: by using nothing more than the documentation of GroundingDino,
Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the
just-released Grounded-SAM and Track Anything models.
</p></li>
</ul>
<h3>Title: DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification. (arXiv:2308.00146v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00146">http://arxiv.org/abs/2308.00146</a></li>
<li>Code URL: <a href="https://github.com/lmu-dbs/diffusal">https://github.com/lmu-dbs/diffusal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00146] DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification](http://arxiv.org/abs/2308.00146) #diffusion</code></li>
<li>Summary: <p>Node classification is one of the core tasks on attributed graphs, but
successful graph learning solutions require sufficiently labeled data. To keep
annotation costs low, active graph learning focuses on selecting the most
qualitative subset of nodes that maximizes label efficiency. However, deciding
which heuristic is best suited for an unlabeled graph to increase label
efficiency is a persistent challenge. Existing solutions either neglect
aligning the learned model and the sampling method or focus only on limited
selection aspects. They are thus sometimes worse or only equally good as random
sampling. In this work, we introduce a novel active graph learning approach
called DiffusAL, showing significant robustness in diverse settings. Toward
better transferability between different graph structures, we combine three
independent scoring functions to identify the most informative node samples for
labeling in a parameter-free way: i) Model Uncertainty, ii) Diversity
Component, and iii) Node Importance computed via graph diffusion heuristics.
Most of our calculations for acquisition and training can be pre-processed,
making DiffusAL more efficient compared to approaches combining diverse
selection criteria and similarly fast as simpler heuristics. Our experiments on
various benchmark datasets show that, unlike previous methods, our approach
significantly outperforms random selection in 100% of all datasets and labeling
budgets tested.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Visual Geo-localization with Self-supervised Representation Learning. (arXiv:2308.00090v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00090">http://arxiv.org/abs/2308.00090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00090] Visual Geo-localization with Self-supervised Representation Learning](http://arxiv.org/abs/2308.00090) #self-supervised</code></li>
<li>Summary: <p>Visual Geo-localization (VG) has emerged as a significant research area,
aiming to identify geolocation based on visual features. Most VG approaches use
learnable feature extractors for representation learning. Recently,
Self-Supervised Learning (SSL) methods have also demonstrated comparable
performance to supervised methods by using numerous unlabeled images for
representation learning. In this work, we present a novel unified VG-SSL
framework with the goal to enhance performance and training efficiency on a
large VG dataset by SSL methods. Our work incorporates multiple SSL methods
tailored for VG: SimCLR, MoCov2, BYOL, SimSiam, Barlow Twins, and VICReg. We
systematically analyze the performance of different training strategies and
study the optimal parameter settings for the adaptation of SSL methods for the
VG task. The results demonstrate that our method, without the significant
computation and memory usage associated with Hard Negative Mining (HNM), can
match or even surpass the VG performance of the baseline that employs HNM. The
code is available at https://github.com/arplaboratory/VG_SSL.
</p></li>
</ul>
<h3>Title: Relational Contrastive Learning for Scene Text Recognition. (arXiv:2308.00508v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00508">http://arxiv.org/abs/2308.00508</a></li>
<li>Code URL: <a href="https://github.com/thundervvv/rclstr">https://github.com/thundervvv/rclstr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00508] Relational Contrastive Learning for Scene Text Recognition](http://arxiv.org/abs/2308.00508) #self-supervised</code></li>
<li>Summary: <p>Context-aware methods achieved great success in supervised scene text
recognition via incorporating semantic priors from words. We argue that such
prior contextual information can be interpreted as the relations of textual
primitives due to the heterogeneous text and background, which can provide
effective self-supervised labels for representation learning. However, textual
relations are restricted to the finite size of dataset due to lexical
dependencies, which causes the problem of over-fitting and compromises
representation robustness. To this end, we propose to enrich the textual
relations via rearrangement, hierarchy and interaction, and design a unified
framework called RCLSTR: Relational Contrastive Learning for Scene Text
Recognition. Based on causality, we theoretically explain that three modules
suppress the bias caused by the contextual prior and thus guarantee
representation robustness. Experiments on representation quality show that our
method outperforms state-of-the-art self-supervised STR methods. Code is
available at https://github.com/ThunderVVV/RCLSTR.
</p></li>
</ul>
<h3>Title: Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00566">http://arxiv.org/abs/2308.00566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00566] Predicting masked tokens in stochastic locations improves masked image modeling](http://arxiv.org/abs/2308.00566) #self-supervised</code></li>
<li>Summary: <p>Self-supervised learning is a promising paradigm in deep learning that
enables learning from unlabeled data by constructing pretext tasks that require
learning useful representations. In natural language processing, the dominant
pretext task has been masked language modeling (MLM), while in computer vision
there exists an equivalent called Masked Image Modeling (MIM). However, MIM is
challenging because it requires predicting semantic content in accurate
locations. E.g, given an incomplete picture of a dog, we can guess that there
is a tail, but we cannot determine its exact location. In this work, we propose
FlexPredict, a stochastic model that addresses this challenge by incorporating
location uncertainty into the model. Specifically, we condition the model on
stochastic masked token positions to guide the model toward learning features
that are more robust to location uncertainties. Our approach improves
downstream performance on a range of tasks, e.g, compared to MIM baselines,
FlexPredict boosts ImageNet linear probing by 1.6% with ViT-B and by 2.5% for
semi-supervised video segmentation using ViT-L.
</p></li>
</ul>
<h3>Title: AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00688">http://arxiv.org/abs/2308.00688</a></li>
<li>Code URL: <a href="https://github.com/AnyLoc/AnyLoc">https://github.com/AnyLoc/AnyLoc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00688] AnyLoc: Towards Universal Visual Place Recognition](http://arxiv.org/abs/2308.00688) #self-supervised</code></li>
<li>Summary: <p>Visual Place Recognition (VPR) is vital for robot localization. To date, the
most performant VPR approaches are environment- and task-specific: while they
exhibit strong performance in structured environments (predominantly urban
driving), their performance degrades severely in unstructured environments,
rendering most approaches brittle to robust real-world deployment. In this
work, we develop a universal solution to VPR -- a technique that works across a
broad range of structured and unstructured environments (urban, outdoors,
indoors, aerial, underwater, and subterranean environments) without any
re-training or fine-tuning. We demonstrate that general-purpose feature
representations derived from off-the-shelf self-supervised models with no
VPR-specific training are the right substrate upon which to build such a
universal VPR solution. Combining these derived features with unsupervised
feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X
significantly higher performance than existing approaches. We further obtain a
6% improvement in performance by characterizing the semantic properties of
these features, uncovering unique domains which encapsulate datasets from
similar environments. Our detailed experiments and analysis lay a foundation
for building VPR solutions that may be deployed anywhere, anytime, and across
anyview. We encourage the readers to explore our project page and interactive
demos: https://anyloc.github.io/.
</p></li>
</ul>
<h3>Title: EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning. (arXiv:2308.00246v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00246">http://arxiv.org/abs/2308.00246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00246] EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning](http://arxiv.org/abs/2308.00246) #self-supervised</code></li>
<li>Summary: <p>Cognitive load, the amount of mental effort required for task completion,
plays an important role in performance and decision-making outcomes, making its
classification and analysis essential in various sensitive domains. In this
paper, we present a new solution for the classification of cognitive load using
electroencephalogram (EEG). Our model uses a transformer architecture employing
transfer learning between emotions and cognitive load. We pre-train our model
using self-supervised masked autoencoding on emotion-related EEG datasets and
use transfer learning with both frozen weights and fine-tuning to perform
downstream cognitive load classification. To evaluate our method, we carry out
a series of experiments utilizing two publicly available EEG-based emotion
datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive
dataset for downstream cognitive load classification. The results of our
experiments show that our proposed approach achieves strong results and
outperforms conventional single-stage fully supervised learning. Moreover, we
perform detailed ablation and sensitivity studies to evaluate the impact of
different aspects of our proposed solution. This research contributes to the
growing body of literature in affective computing with a focus on cognitive
load, and opens up new avenues for future research in the field of cross-domain
transfer learning using self-supervised pre-training.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding. (arXiv:2308.00353v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00353">http://arxiv.org/abs/2308.00353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00353] Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding](http://arxiv.org/abs/2308.00353) #foundation model</code></li>
<li>Summary: <p>Open-world instance-level scene understanding aims to locate and recognize
unseen object categories that are not present in the annotated dataset. This
task is challenging because the model needs to both localize novel 3D objects
and infer their semantic categories. A key factor for the recent progress in 2D
open-world perception is the availability of large-scale image-text pairs from
the Internet, which cover a wide range of vocabulary concepts. However, this
success is hard to replicate in 3D scenarios due to the scarcity of 3D-text
pairs. To address this challenge, we propose to harness pre-trained
vision-language (VL) foundation models that encode extensive knowledge from
image-text pairs to generate captions for multi-view images of 3D scenes. This
allows us to establish explicit associations between 3D shapes and
semantic-rich captions. Moreover, to enhance the fine-grained visual-semantic
representation learning from captions for object-level categorization, we
design hierarchical point-caption association methods to learn semantic-aware
embeddings that exploit the 3D geometry between 3D points and multi-view
images. In addition, to tackle the localization challenge for novel classes in
the open-world setting, we develop debiased instance localization, which
involves training object grouping modules on unlabeled data using
instance-level pseudo supervision. This significantly improves the
generalization capabilities of instance grouping and thus the ability to
accurately locate novel objects. We conduct extensive experiments on 3D
semantic, instance, and panoptic segmentation tasks, covering indoor and
outdoor scenes across three datasets. Our method outperforms baseline methods
by a significant margin in semantic segmentation (e.g. 34.5%$\sim$65.3%),
instance segmentation (e.g. 21.8%$\sim$54.0%) and panoptic segmentation (e.g.
14.7%$\sim$43.3%). Code will be available.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Controlling Geometric Abstraction and Texture for Artistic Images. (arXiv:2308.00148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00148">http://arxiv.org/abs/2308.00148</a></li>
<li>Code URL: <a href="https://github.com/MartinBuessemeyer/Artistic-Texture-Control">https://github.com/MartinBuessemeyer/Artistic-Texture-Control</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00148] Controlling Geometric Abstraction and Texture for Artistic Images](http://arxiv.org/abs/2308.00148) #generative</code></li>
<li>Summary: <p>We present a novel method for the interactive control of geometric
abstraction and texture in artistic images. Previous example-based stylization
methods often entangle shape, texture, and color, while generative methods for
image synthesis generally either make assumptions about the input image, such
as only allowing faces or do not offer precise editing controls. By contrast,
our holistic approach spatially decomposes the input into shapes and a
parametric representation of high-frequency details comprising the image's
texture, thus enabling independent control of color and texture. Each parameter
in this representation controls painterly attributes of a pipeline of
differentiable stylization filters. The proposed decoupling of shape and
texture enables various options for stylistic editing, including interactive
global and local adjustments of shape, stroke, and painterly attributes such as
surface relief and contours. Additionally, we demonstrate optimization-based
texture style-transfer in the parametric space using reference images and text
prompts, as well as the training of single- and arbitrary style parameter
prediction networks for real-time texture decomposition.
</p></li>
</ul>
<h3>Title: Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities. (arXiv:2308.00307v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00307">http://arxiv.org/abs/2308.00307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00307] Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities](http://arxiv.org/abs/2308.00307) #generative</code></li>
<li>Summary: <p>How can we apply human feedback into generative model? As answer of this
question, in this paper, we show the method applied on denoising problem and
domain adaptation using human feedback. Deep generative models have
demonstrated impressive results in image denoising. However, current image
denoising models often produce inappropriate results when applied to domains
different from the ones they were trained on. If there are <code>Good' and</code>Bad'
result for unseen data, how to raise up quality of `Bad' result. Most methods
use an approach based on generalization of model. However, these methods
require target image for training or adapting unseen domain. In this paper, to
adapting domain, we deal with non-target image for unseen domain, and improve
specific failed image. To address this, we propose a method for fine-tuning
inappropriate results generated in a different domain by utilizing human
feedback. First, we train a generator to denoise images using only the noisy
MNIST digit '0' images. The denoising generator trained on the source domain
leads to unintended results when applied to target domain images. To achieve
domain adaptation, we construct a noise-image denoising generated image data
set and train a reward model predict human feedback. Finally, we fine-tune the
generator on the different domain using the reward model with auxiliary loss
function, aiming to transfer denoising capabilities to target domain. Our
approach demonstrates the potential to efficiently fine-tune a generator
trained on one domain using human feedback from another domain, thereby
enhancing denoising abilities in different domains.
</p></li>
</ul>
<h3>Title: Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00189">http://arxiv.org/abs/2308.00189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00189] Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?](http://arxiv.org/abs/2308.00189) #generative</code></li>
<li>Summary: <p>Coaxing out desired behavior from pretrained models, while avoiding
undesirable ones, has redefined NLP and is reshaping how we interact with
computers. What was once a scientific engineering discipline-in which building
blocks are stacked one on top of the other-is arguably already a complex
systems science, in which emergent behaviors are sought out to support
previously unimagined use cases.
</p></li>
</ul>
<p>Despite the ever increasing number of benchmarks that measure task
performance, we lack explanations of what behaviors language models exhibit
that allow them to complete these tasks in the first place. We argue for a
systematic effort to decompose language model behavior into categories that
explain cross-task performance, to guide mechanistic explanations and help
future-proof analytic research.
</p>

<h3>Title: ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00400">http://arxiv.org/abs/2308.00400</a></li>
<li>Code URL: <a href="https://github.com/zhangbo-nlp/zrigf">https://github.com/zhangbo-nlp/zrigf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00400] ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation](http://arxiv.org/abs/2308.00400) #generative</code></li>
<li>Summary: <p>Image-grounded dialogue systems benefit greatly from integrating visual
information, resulting in high-quality response generation. However, current
models struggle to effectively utilize such information in zero-resource
scenarios, mainly due to the disparity between image and text modalities. To
overcome this challenge, we propose an innovative multimodal framework, called
ZRIGF, which assimilates image-grounded information for dialogue generation in
zero-resource situations. ZRIGF implements a two-stage learning strategy,
comprising contrastive pre-training and generative pre-training. Contrastive
pre-training includes a text-image matching module that maps images and texts
into a unified encoded vector space, along with a text-assisted masked image
modeling module that preserves pre-training visual features and fosters further
multimodal feature alignment. Generative pre-training employs a multimodal
fusion module and an information transfer module to produce insightful
responses based on harmonized multimodal representations. Comprehensive
experiments conducted on both text-based and image-grounded dialogue datasets
demonstrate ZRIGF's efficacy in generating contextually pertinent and
informative responses. Furthermore, we adopt a fully zero-resource scenario in
the image-grounded dialogue dataset to demonstrate our framework's robust
generalization capabilities in novel domains. The code is available at
https://github.com/zhangbo-nlp/ZRIGF.
</p></li>
</ul>
<h3>Title: Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00031">http://arxiv.org/abs/2308.00031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00031] Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges](http://arxiv.org/abs/2308.00031) #generative</code></li>
<li>Summary: <p>Generative Artificial Intelligence (AI) is one of the most exciting
developments in Computer Science of the last decade. At the same time,
Reinforcement Learning (RL) has emerged as a very successful paradigm for a
variety of machine learning tasks. In this survey, we discuss the state of the
art, opportunities and open research questions in applying RL to generative AI.
In particular, we will discuss three types of applications, namely, RL as an
alternative way for generation without specified objectives; as a way for
generating outputs while concurrently maximizing an objective function; and,
finally, as a way of embedding desired characteristics, which cannot be easily
captured by means of an objective function, into the generative process. We
conclude the survey with an in-depth discussion of the opportunities and
challenges in this fascinating emerging area.
</p></li>
</ul>
<h3>Title: Graph Contrastive Learning with Generative Adversarial Network. (arXiv:2308.00535v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00535">http://arxiv.org/abs/2308.00535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00535] Graph Contrastive Learning with Generative Adversarial Network](http://arxiv.org/abs/2308.00535) #generative</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have demonstrated promising results on
exploiting node representations for many downstream tasks through supervised
end-to-end training. To deal with the widespread label scarcity issue in
real-world applications, Graph Contrastive Learning (GCL) is leveraged to train
GNNs with limited or even no labels by maximizing the mutual information
between nodes in its augmented views generated from the original graph.
However, the distribution of graphs remains unconsidered in view generation,
resulting in the ignorance of unseen edges in most existing literature, which
is empirically shown to be able to improve GCL's performance in our
experiments. To this end, we propose to incorporate graph generative
adversarial networks (GANs) to learn the distribution of views for GCL, in
order to i) automatically capture the characteristic of graphs for
augmentations, and ii) jointly train the graph GAN model and the GCL model.
Specifically, we present GACN, a novel Generative Adversarial Contrastive
learning Network for graph representation learning. GACN develops a view
generator and a view discriminator to generate augmented views automatically in
an adversarial style. Then, GACN leverages these views to train a GNN encoder
with two carefully designed self-supervised learning losses, including the
graph contrastive loss and the Bayesian personalized ranking Loss. Furthermore,
we design an optimization framework to train all GACN modules jointly.
Extensive experiments on seven real-world datasets show that GACN is able to
generate high-quality augmented views for GCL and is superior to twelve
state-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly
discovers that the generated views in data augmentation finally conform to the
well-known preferential attachment rule in online networks.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Patch-wise Auto-Encoder for Visual Anomaly Detection. (arXiv:2308.00429v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00429">http://arxiv.org/abs/2308.00429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00429] Patch-wise Auto-Encoder for Visual Anomaly Detection](http://arxiv.org/abs/2308.00429) #anomaly</code></li>
<li>Summary: <p>Anomaly detection without priors of the anomalies is challenging. In the
field of unsupervised anomaly detection, traditional auto-encoder (AE) tends to
fail based on the assumption that by training only on normal images, the model
will not be able to reconstruct abnormal images correctly. On the contrary, we
propose a novel patch-wise auto-encoder (Patch AE) framework, which aims at
enhancing the reconstruction ability of AE to anomalies instead of weakening
it. Each patch of image is reconstructed by corresponding spatially distributed
feature vector of the learned feature representation, i.e., patch-wise
reconstruction, which ensures anomaly-sensitivity of AE. Our method is simple
and efficient. It advances the state-of-the-art performances on Mvtec AD
benchmark, which proves the effectiveness of our model. It shows great
potential in practical industrial application scenarios.
</p></li>
</ul>
<h3>Title: PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps. (arXiv:2308.00538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00538">http://arxiv.org/abs/2308.00538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00538] PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps](http://arxiv.org/abs/2308.00538) #anomaly</code></li>
<li>Summary: <p>We propose PressureTransferNet, a novel method for Human Activity Recognition
(HAR) using ground pressure information. Our approach generates body-specific
dynamic ground pressure profiles for specific activities by leveraging existing
pressure data from different individuals. PressureTransferNet is an
encoder-decoder model taking a source pressure map and a target human attribute
vector as inputs, producing a new pressure map reflecting the target attribute.
To train the model, we use a sensor simulation to create a diverse dataset with
various human attributes and pressure profiles. Evaluation on a real-world
dataset shows its effectiveness in accurately transferring human attributes to
ground pressure profiles across different scenarios. We visually confirm the
fidelity of the synthesized pressure shapes using a physics-based deep learning
model and achieve a binary R-square value of 0.79 on areas with ground contact.
Validation through classification with F1 score (0.911$\pm$0.015) on physical
pressure mat data demonstrates the correctness of the synthesized pressure
maps, making our method valuable for data augmentation, denoising, sensor
simulation, and anomaly detection. Applications span sports science,
rehabilitation, and bio-mechanics, contributing to the development of HAR
systems.
</p></li>
</ul>
<h3>Title: Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model. (arXiv:2308.00074v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00074">http://arxiv.org/abs/2308.00074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00074] Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model](http://arxiv.org/abs/2308.00074) #anomaly</code></li>
<li>Summary: <p>Anomaly detection and its explanation is important in many research areas
such as intrusion detection, fraud detection, unknown attack detection in
network traffic and logs. It is challenging to identify the cause or
explanation of why one instance is an anomaly? and the other is not due to its
unbounded and lack of supervisory nature. The answer to this question is
possible with the emerging technique of explainable artificial intelligence
(XAI). XAI provides tools and techniques to interpret and explain the output
and working of complex models such as Deep Learning (DL). This paper aims to
detect and explain network anomalies with XAI, kernelSHAP method. The same
approach is used to improve the network anomaly detection model in terms of
accuracy, recall, precision and f score. The experiment is conduced with the
latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and
compared. The overall accuracy and F score of OPT_Model (when trained in
unsupervised way) are 0.90 and 0.76, respectively.
</p></li>
</ul>
<h3>Title: A Survey of Time Series Anomaly Detection Methods in the AIOps Domain. (arXiv:2308.00393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00393">http://arxiv.org/abs/2308.00393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00393] A Survey of Time Series Anomaly Detection Methods in the AIOps Domain](http://arxiv.org/abs/2308.00393) #anomaly</code></li>
<li>Summary: <p>Internet-based services have seen remarkable success, generating vast amounts
of monitored key performance indicators (KPIs) as univariate or multivariate
time series. Monitoring and analyzing these time series are crucial for
researchers, service operators, and on-call engineers to detect outliers or
anomalies indicating service failures or significant events. Numerous advanced
anomaly detection methods have emerged to address availability and performance
issues. This review offers a comprehensive overview of time series anomaly
detection in Artificial Intelligence for IT operations (AIOps), which uses AI
capabilities to automate and optimize operational workflows. Additionally, it
explores future directions for real-world and next-generation time-series
anomaly detection based on recent advancements.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00085">http://arxiv.org/abs/2308.00085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00085] Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation](http://arxiv.org/abs/2308.00085) #in-context</code></li>
<li>Summary: <p>Recent approaches to empathetic response generation try to incorporate
commonsense knowledge or reasoning about the causes of emotions to better
understand the user's experiences and feelings. However, these approaches
mainly focus on understanding the causalities of context from the user's
perspective, ignoring the system's perspective. In this paper, we propose a
commonsense-based causality explanation approach for diverse empathetic
response generation that considers both the user's perspective (user's desires
and reactions) and the system's perspective (system's intentions and
reactions). We enhance ChatGPT's ability to reason for the system's perspective
by integrating in-context learning with commonsense knowledge. Then, we
integrate the commonsense-based causality explanation with both ChatGPT and a
T5-based model. Experimental evaluations demonstrate that our method
outperforms other comparable methods on both automatic and human evaluations.
</p></li>
</ul>
<h3>Title: Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.00304">http://arxiv.org/abs/2308.00304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.00304] Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models](http://arxiv.org/abs/2308.00304) #in-context</code></li>
<li>Summary: <p>We consider the problem of eliciting compositional generalization
capabilities in large language models (LLMs) with a novel type of prompting
strategy. Compositional generalization empowers the LLMs to solve problems that
are harder than the ones they have seen (i.e., easy-to-hard generalization),
which is a critical reasoning capability of human-like intelligence. However,
even the current state-of-the-art LLMs still struggle with this form of
reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting,
which instructs LLMs how to compose basic skills to resolve more complex
problems. We find that it is crucial to demonstrate both the skills and the
compositional examples within the same prompting context. With as few as two
examplars, our SKiC prompting initiates strong synergies between skills and
their composition capabilities. Notably, it empowers LLMs to solve unseen
problems that require innovative skill compositions, achieving near-perfect
generalization on a broad range of challenging compositionality tasks.
Intriguingly, SKiC prompting unlocks the latent potential of LLMs, enabling
them to leverage pre-existing internal skills acquired during earlier
pretraining and alignment stages, even when these skills are not explicitly
presented in the prompting context. This results in the capability of LLMs to
solve unseen complex problems by activating and composing these internal
competencies.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-02]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
