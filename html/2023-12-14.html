<h2>diffusion</h2>
<h3>Title: DFGET: Displacement-Field Assisted Graph Energy Transmitter for Gland Instance Segmentation. (arXiv:2312.07584v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07584">http://arxiv.org/abs/2312.07584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07584] DFGET: Displacement-Field Assisted Graph Energy Transmitter for Gland Instance Segmentation](http://arxiv.org/abs/2312.07584) #diffusion</code></li>
<li>Summary: <p>Gland instance segmentation is an essential but challenging task in the
diagnosis and treatment of adenocarcinoma. The existing models usually achieve
gland instance segmentation through multi-task learning and boundary loss
constraint. However, how to deal with the problems of gland adhesion and
inaccurate boundary in segmenting the complex samples remains a challenge. In
this work, we propose a displacement-field assisted graph energy transmitter
(DFGET) framework to solve these problems. Specifically, a novel message
passing manner based on anisotropic diffusion is developed to update the node
features, which can distinguish the isomorphic graphs and improve the
expressivity of graph nodes for complex samples. Using such graph framework,
the gland semantic segmentation map and the displacement field (DF) of the
graph nodes are estimated with two graph network branches. With the constraint
of DF, a graph cluster module based on diffusion theory is presented to improve
the intra-class feature consistency and inter-class feature discrepancy, as
well as to separate the adherent glands from the semantic segmentation maps.
Extensive comparison and ablation experiments on the GlaS dataset demonstrate
the superiority of DFGET and effectiveness of the proposed anisotropic message
passing manner and clustering method. Compared to the best comparative model,
DFGET increases the object-Dice and object-F1 score by 2.5% and 3.4%
respectively, while decreases the object-HD by 32.4%, achieving
state-of-the-art performance.
</p></li>
</ul>
<h3>Title: Characteristic Guidance: Non-linear Correction for DDPM at Large Guidance Scale. (arXiv:2312.07586v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07586">http://arxiv.org/abs/2312.07586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07586] Characteristic Guidance: Non-linear Correction for DDPM at Large Guidance Scale](http://arxiv.org/abs/2312.07586) #diffusion</code></li>
<li>Summary: <p>Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a novel method that provides non-linear correction for
classifier-free guided DDPMs. Such correction forces the guided DDPMs to
respect the Fokker-Planck equation of their underlying diffusion process, in a
way that is first-principle, training-free, derivative-free, and compatible
with existing sampling methods. Experiments show that characteristic guidance
is robust to various applications, offers enhanced control over sample
generation, suppresses color and exposure issues even for latent space
sampling, and can handle physics problems such as the phase transitions.
</p></li>
</ul>
<h3>Title: Uncertainty Visualization via Low-Dimensional Posterior Projections. (arXiv:2312.07804v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07804">http://arxiv.org/abs/2312.07804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07804] Uncertainty Visualization via Low-Dimensional Posterior Projections](http://arxiv.org/abs/2312.07804) #diffusion</code></li>
<li>Summary: <p>In ill-posed inverse problems, it is commonly desirable to obtain insight
into the full spectrum of plausible solutions, rather than extracting only a
single reconstruction. Information about the plausible solutions and their
likelihoods is encoded in the posterior distribution. However, for
high-dimensional data, this distribution is challenging to visualize. In this
work, we introduce a new approach for estimating and visualizing posteriors by
employing energy-based models (EBMs) over low-dimensional subspaces.
Specifically, we train a conditional EBM that receives an input measurement and
a set of directions that span some low-dimensional subspace of solutions, and
outputs the probability density function of the posterior within that space. We
demonstrate the effectiveness of our method across a diverse range of datasets
and image restoration problems, showcasing its strength in uncertainty
quantification and visualization. As we show, our method outperforms a baseline
that projects samples from a diffusion-based posterior sampler, while being
orders of magnitude faster. Furthermore, it is more accurate than a baseline
that assumes a Gaussian posterior.
</p></li>
</ul>
<h3>Title: Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb Prosthetic Users. (arXiv:2312.07854v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07854">http://arxiv.org/abs/2312.07854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07854] Diffusion Models Enable Zero-Shot Pose Estimation for Lower-Limb Prosthetic Users](http://arxiv.org/abs/2312.07854) #diffusion</code></li>
<li>Summary: <p>The application of 2D markerless gait analysis has garnered increasing
interest and application within clinical settings. However, its effectiveness
in the realm of lower-limb amputees has remained less than optimal. In
response, this study introduces an innovative zero-shot method employing image
generation diffusion models to achieve markerless pose estimation for
lower-limb prosthetics, presenting a promising solution to gait analysis for
this specific population. Our approach demonstrates an enhancement in detecting
key points on prosthetic limbs over existing methods, and enables clinicians to
gain invaluable insights into the kinematics of lower-limb amputees across the
gait cycle. The outcomes obtained not only serve as a proof-of-concept for the
feasibility of this zero-shot approach but also underscore its potential in
advancing rehabilitation through gait analysis for this unique population.
</p></li>
</ul>
<h3>Title: SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models. (arXiv:2312.07865v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07865">http://arxiv.org/abs/2312.07865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07865] SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models](http://arxiv.org/abs/2312.07865) #diffusion</code></li>
<li>Summary: <p>Despite the success of diffusion-based customization methods on visual
content creation, increasing concerns have been raised about such techniques
from both privacy and political perspectives. To tackle this issue, several
anti-customization methods have been proposed in very recent months,
predominantly grounded in adversarial attacks. Unfortunately, most of these
methods adopt straightforward designs, such as end-to-end optimization with a
focus on adversarially maximizing the original training loss, thereby
neglecting nuanced internal properties intrinsic to the diffusion model, and
even leading to ineffective optimization in some diffusion time steps. In this
paper, we strive to bridge this gap by undertaking a comprehensive exploration
of these inherent properties, to boost the performance of current
anti-customization approaches. Two aspects of properties are investigated: 1)
We examine the relationship between time step selection and the model's
perception in the frequency domain of images and find that lower time steps can
give much more contributions to adversarial noises. This inspires us to propose
an adaptive greedy search for optimal time steps that seamlessly integrates
with existing anti-customization methods. 2) We scrutinize the roles of
features at different layers during denoising and devise a sophisticated
feature-based optimization framework for anti-customization. Experiments on
facial benchmarks demonstrate that our approach significantly increases
identity disruption, thereby enhancing user privacy and security.
</p></li>
</ul>
<h3>Title: BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. (arXiv:2312.07937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07937">http://arxiv.org/abs/2312.07937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07937] BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics](http://arxiv.org/abs/2312.07937) #diffusion</code></li>
<li>Summary: <p>The recently emerging text-to-motion advances have spired numerous attempts
for convenient and interactive human motion generation. Yet, existing methods
are largely limited to generating body motions only without considering the
rich two-hand motions, let alone handling various conditions like body dynamics
or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal
dataset for two-hand motion generation. Our dataset includes accurate motion
tracking for the human body and hands and provides pair-wised finger-level hand
annotations and body descriptions. We further provide a strong baseline method,
BOTH2Hands, for the novel task: generating vivid two-hand motions from both
implicit body dynamics and explicit text prompts. We first warm up two parallel
body-to-hand and text-to-hand diffusion models and then utilize the
cross-attention transformer for motion blending. Extensive experiments and
cross-validations demonstrate the effectiveness of our approach and dataset for
generating convincing two-hand motions from the hybrid body-and-textual
conditions. Our dataset and code will be disseminated to the community for
future research.
</p></li>
</ul>
<h3>Title: Semantic-aware Data Augmentation for Text-to-image Synthesis. (arXiv:2312.07951v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07951">http://arxiv.org/abs/2312.07951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07951] Semantic-aware Data Augmentation for Text-to-image Synthesis](http://arxiv.org/abs/2312.07951) #diffusion</code></li>
<li>Summary: <p>Data augmentation has been recently leveraged as an effective regularizer in
various vision-language deep neural networks. However, in text-to-image
synthesis (T2Isyn), current augmentation wisdom still suffers from the semantic
mismatch between augmented paired data. Even worse, semantic collapse may occur
when generated images are less semantically constrained. In this paper, we
develop a novel Semantic-aware Data Augmentation (SADA) framework dedicated to
T2Isyn. In particular, we propose to augment texts in the semantic space via an
Implicit Textual Semantic Preserving Augmentation ($ITA$), in conjunction with
a specifically designed Image Semantic Regularization Loss ($L_r$) as Generated
Image Semantic Conservation, to cope well with semantic mismatch and collapse.
As one major contribution, we theoretically show that $ITA$ can certify better
text-image consistency while $L_r$ regularizing the semantics of generated
images would avoid semantic collapse and enhance image quality. Extensive
experiments validate that SADA enhances text-image consistency and improves
image quality significantly in T2Isyn models across various backbones.
Especially, incorporating SADA during the tuning process of Stable Diffusion
models also yields performance improvements.
</p></li>
</ul>
<h3>Title: LMD: Faster Image Reconstruction with Latent Masking Diffusion. (arXiv:2312.07971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07971">http://arxiv.org/abs/2312.07971</a></li>
<li>Code URL: <a href="https://github.com/anonymouspony/lmd">https://github.com/anonymouspony/lmd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07971] LMD: Faster Image Reconstruction with Latent Masking Diffusion](http://arxiv.org/abs/2312.07971) #diffusion</code></li>
<li>Summary: <p>As a class of fruitful approaches, diffusion probabilistic models (DPMs) have
shown excellent advantages in high-resolution image reconstruction. On the
other hand, masked autoencoders (MAEs), as popular self-supervised vision
learners, have demonstrated simpler and more effective image reconstruction and
transfer capabilities on downstream tasks. However, they all require extremely
high training costs, either due to inherent high temporal-dependence (i.e.,
excessively long diffusion steps) or due to artificially low spatial-dependence
(i.e., human-formulated high mask ratio, such as 0.75). To the end, this paper
presents LMD, a faster image reconstruction framework with latent masking
diffusion. First, we propose to project and reconstruct images in latent space
through a pre-trained variational autoencoder, which is theoretically more
efficient than in the pixel-based space. Then, we combine the advantages of
MAEs and DPMs to design a progressive masking diffusion model, which gradually
increases the masking proportion by three different schedulers and reconstructs
the latent features from simple to difficult, without sequentially performing
denoising diffusion as in DPMs or using fixed high masking ratio as in MAEs, so
as to alleviate the high training time-consumption predicament. Our approach
allows for learning high-capacity models and accelerate their training (by 3x
or more) and barely reduces the original accuracy. Inference speed in
downstream tasks also significantly outperforms the previous approaches.
</p></li>
</ul>
<h3>Title: AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing. (arXiv:2312.08019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08019">http://arxiv.org/abs/2312.08019</a></li>
<li>Code URL: <a href="https://github.com/anonymouspony/adap-edit">https://github.com/anonymouspony/adap-edit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08019] AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing](http://arxiv.org/abs/2312.08019) #diffusion</code></li>
<li>Summary: <p>With the great success of text-conditioned diffusion models in creative
text-to-image generation, various text-driven image editing approaches have
attracted the attentions of many researchers. However, previous works mainly
focus on discreteness-sensitive instructions such as adding, removing or
replacing specific objects, background elements or global styles (i.e., hard
editing), while generally ignoring subject-binding but semantically
fine-changing continuity-sensitive instructions such as actions, poses or
adjectives, and so on (i.e., soft editing), which hampers generative AI from
generating user-customized visual contents. To mitigate this predicament, we
propose a spatio-temporal guided adaptive editing algorithm AdapEdit, which
realizes adaptive image editing by introducing a soft-attention strategy to
dynamically vary the guiding degree from the editing conditions to visual
pixels from both temporal and spatial perspectives. Note our approach has a
significant advantage in preserving model priors and does not require model
training, fine-tuning, extra data, or optimization. We present our results over
a wide variety of raw images and editing instructions, demonstrating
competitive performance and showing it significantly outperforms the previous
approaches.
</p></li>
</ul>
<h3>Title: ClusterDDPM: An EM clustering framework with Denoising Diffusion Probabilistic Models. (arXiv:2312.08029v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08029">http://arxiv.org/abs/2312.08029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08029] ClusterDDPM: An EM clustering framework with Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2312.08029) #diffusion</code></li>
<li>Summary: <p>Variational autoencoder (VAE) and generative adversarial networks (GAN) have
found widespread applications in clustering and have achieved significant
success. However, the potential of these approaches may be limited due to VAE's
mediocre generation capability or GAN's well-known instability during
adversarial training. In contrast, denoising diffusion probabilistic models
(DDPMs) represent a new and promising class of generative models that may
unlock fresh dimensions in clustering. In this study, we introduce an
innovative expectation-maximization (EM) framework for clustering using DDPMs.
In the E-step, we aim to derive a mixture of Gaussian priors for the subsequent
M-step. In the M-step, our focus lies in learning clustering-friendly latent
representations for the data by employing the conditional DDPM and matching the
distribution of latent representations to the mixture of Gaussian priors. We
present a rigorous theoretical analysis of the optimization process in the
M-step, proving that the optimizations are equivalent to maximizing the lower
bound of the Q function within the vanilla EM framework under certain
constraints. Comprehensive experiments validate the advantages of the proposed
framework, showcasing superior performance in clustering, unsupervised
conditional generation and latent representation learning.
</p></li>
</ul>
<h3>Title: Compositional Inversion for Stable Diffusion Models. (arXiv:2312.08048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08048">http://arxiv.org/abs/2312.08048</a></li>
<li>Code URL: <a href="https://github.com/zhangxulu1996/compositional-inversion">https://github.com/zhangxulu1996/compositional-inversion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08048] Compositional Inversion for Stable Diffusion Models](http://arxiv.org/abs/2312.08048) #diffusion</code></li>
<li>Summary: <p>Inversion methods, such as Textual Inversion, generate personalized images by
incorporating concepts of interest provided by user images. However, existing
methods often suffer from overfitting issues, where the dominant presence of
inverted concepts leads to the absence of other desired concepts. It stems from
the fact that during inversion, the irrelevant semantics in the user images are
also encoded, forcing the inverted concepts to occupy locations far from the
core distribution in the embedding space. To address this issue, we propose a
method that guides the inversion process towards the core distribution for
compositional embeddings. Additionally, we introduce a spatial regularization
approach to balance the attention on the concepts being composed. Our method is
designed as a post-training approach and can be seamlessly integrated with
other inversion methods. Experimental results demonstrate the effectiveness of
our proposed approach in mitigating the overfitting problem and generating more
diverse and balanced compositions of concepts in the synthesized images. The
source code is available at
https://github.com/zhangxulu1996/Compositional-Inversion.
</p></li>
</ul>
<h3>Title: Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision. (arXiv:2312.08056v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08056">http://arxiv.org/abs/2312.08056</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08056] Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision](http://arxiv.org/abs/2312.08056) #diffusion</code></li>
<li>Summary: <p>Ancient artifacts are an important medium for cultural preservation and
restoration. However, many physical copies of artifacts are either damaged or
lost, leaving a blank space in archaeological and historical studies that calls
for artifact image generation techniques. Despite the significant advancements
in open-domain text-to-image synthesis, existing approaches fail to capture the
important domain knowledge presented in the textual description, resulting in
errors in recreated images such as incorrect shapes and patterns. In this
paper, we propose a novel knowledge-aware artifact image synthesis approach
that brings lost historical objects accurately into their visual forms. We use
a pretrained diffusion model as backbone and introduce three key techniques to
enhance the text-to-image generation framework: 1) we construct prompts with
explicit archaeological knowledge elicited from large language models (LLMs);
2) we incorporate additional textual guidance to correlated historical
expertise in a contrastive manner; 3) we introduce further visual-semantic
constraints on edge and perceptual features that enable our model to learn more
intricate visual details of the artifacts. Compared to existing approaches, our
proposed model produces higher-quality artifact images that align better with
the implicit details and historical knowledge contained within written
documents, thus achieving significant improvements across automatic metrics and
in human evaluation. Our code and data are available at
https://github.com/danielwusg/artifact_diffusion.
</p></li>
</ul>
<h3>Title: Clockwork Diffusion: Efficient Generation With Model-Step Distillation. (arXiv:2312.08128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08128">http://arxiv.org/abs/2312.08128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08128] Clockwork Diffusion: Efficient Generation With Model-Step Distillation](http://arxiv.org/abs/2312.08128) #diffusion</code></li>
<li>Summary: <p>This work aims to improve the efficiency of text-to-image diffusion models.
While diffusion models use computationally expensive UNet-based denoising
operations in every generation step, we identify that not all operations are
equally relevant for the final output quality. In particular, we observe that
UNet layers operating on high-res feature maps are relatively sensitive to
small perturbations. In contrast, low-res feature maps influence the semantic
layout of the final image and can often be perturbed with no noticeable change
in the output. Based on this observation, we propose Clockwork Diffusion, a
method that periodically reuses computation from preceding denoising steps to
approximate low-res feature maps at one or more subsequent steps. For multiple
baselines, and for both text-to-image generation and image editing, we
demonstrate that Clockwork leads to comparable or improved perceptual scores
with drastically reduced computational complexity. As an example, for Stable
Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and
CLIP change.
</p></li>
</ul>
<h3>Title: Concept-centric Personalization with Large-scale Diffusion Priors. (arXiv:2312.08195v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08195">http://arxiv.org/abs/2312.08195</a></li>
<li>Code URL: <a href="https://github.com/priv-creation/concept-centric-personalization">https://github.com/priv-creation/concept-centric-personalization</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08195] Concept-centric Personalization with Large-scale Diffusion Priors](http://arxiv.org/abs/2312.08195) #diffusion</code></li>
<li>Summary: <p>Despite large-scale diffusion models being highly capable of generating
diverse open-world content, they still struggle to match the photorealism and
fidelity of concept-specific generators. In this work, we present the task of
customizing large-scale diffusion priors for specific concepts as
concept-centric personalization. Our goal is to generate high-quality
concept-centric images while maintaining the versatile controllability inherent
to open-world models, enabling applications in diverse tasks such as
concept-centric stylization and image translation. To tackle these challenges,
we identify catastrophic forgetting of guidance prediction from diffusion
priors as the fundamental issue. Consequently, we develop a guidance-decoupled
personalization framework specifically designed to address this task. We
propose Generalized Classifier-free Guidance (GCFG) as the foundational theory
for our framework. This approach extends Classifier-free Guidance (CFG) to
accommodate an arbitrary number of guidances, sourced from a variety of
conditions and models. Employing GCFG enables us to separate conditional
guidance into two distinct components: concept guidance for fidelity and
control guidance for controllability. This division makes it feasible to train
a specialized model for concept guidance, while ensuring both control and
unconditional guidance remain intact. We then present a null-text
Concept-centric Diffusion Model as a concept-specific generator to learn
concept guidance without the need for text annotations. Code will be available
at https://github.com/PRIV-Creation/Concept-centric-Personalization.
</p></li>
</ul>
<h3>Title: PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary Confusion. (arXiv:2312.08323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08323">http://arxiv.org/abs/2312.08323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08323] PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary Confusion](http://arxiv.org/abs/2312.08323) #diffusion</code></li>
<li>Summary: <p>Precise boundary segmentation of volumetric images is a critical task for
image-guided diagnosis and computer-assisted intervention, especially for
boundary confusion in clinical practice. However, U-shape networks cannot
effectively resolve this challenge due to the lack of boundary shape
constraints. Besides, existing methods of refining boundaries overemphasize the
slender structure, which results in the overfitting phenomenon due to networks'
limited abilities to model tiny objects. In this paper, we reconceptualize the
mechanism of boundary generation by encompassing the interaction dynamics with
adjacent regions. Moreover, we propose a unified network termed PnPNet to model
shape characteristics of the confused boundary region. Core ingredients of
PnPNet contain the pushing and pulling branches. Specifically, based on
diffusion theory, we devise the semantic difference module (SDM) from the
pushing branch to squeeze the boundary region. Explicit and implicit
differential information inside SDM significantly boost representation
abilities for inter-class boundaries. Additionally, motivated by the K-means
algorithm, the class clustering module (CCM) from the pulling branch is
introduced to stretch the intersected boundary region. Thus, pushing and
pulling branches will shrink and enlarge the boundary uncertainty respectively.
They furnish two adversarial forces to promote models to output a more precise
delineation of boundaries. We carry out experiments on three challenging public
datasets and one in-house dataset, containing three types of boundary confusion
in model predictions. Experimental results demonstrate the superiority of
PnPNet over other segmentation networks, especially on evaluation metrics of HD
and ASSD. Besides, pushing and pulling branches can serve as plug-and-play
modules to enhance classic U-shape baseline models. Codes are available.
</p></li>
</ul>
<h3>Title: Black-box Membership Inference Attacks against Fine-tuned Diffusion Models. (arXiv:2312.08207v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08207">http://arxiv.org/abs/2312.08207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08207] Black-box Membership Inference Attacks against Fine-tuned Diffusion Models](http://arxiv.org/abs/2312.08207) #diffusion</code></li>
<li>Summary: <p>With the rapid advancement of diffusion-based image-generative models, the
quality of generated images has become increasingly photorealistic. Moreover,
with the release of high-quality pre-trained image-generative models, a growing
number of users are downloading these pre-trained models to fine-tune them with
downstream datasets for various image-generation tasks. However, employing such
powerful pre-trained models in downstream tasks presents significant privacy
leakage risks. In this paper, we propose the first reconstruction-based
membership inference attack framework, tailored for recent diffusion models,
and in the more stringent black-box access setting. Considering four distinct
attack scenarios and three types of attacks, this framework is capable of
targeting any popular conditional generator model, achieving high precision,
evidenced by an impressive AUC of $0.95$.
</p></li>
</ul>
<h3>Title: Noise in the reverse process improves the approximation capabilities of diffusion models. (arXiv:2312.07851v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07851">http://arxiv.org/abs/2312.07851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07851] Noise in the reverse process improves the approximation capabilities of diffusion models](http://arxiv.org/abs/2312.07851) #diffusion</code></li>
<li>Summary: <p>In Score based Generative Modeling (SGMs), the state-of-the-art in generative
modeling, stochastic reverse processes are known to perform better than their
deterministic counterparts. This paper delves into the heart of this
phenomenon, comparing neural ordinary differential equations (ODEs) and neural
stochastic differential equations (SDEs) as reverse processes. We use a control
theoretic perspective by posing the approximation of the reverse process as a
trajectory tracking problem. We analyze the ability of neural SDEs to
approximate trajectories of the Fokker-Planck equation, revealing the
advantages of stochasticity. First, neural SDEs exhibit a powerful regularizing
effect, enabling $L^2$ norm trajectory approximation surpassing the Wasserstein
metric approximation achieved by neural ODEs under similar conditions, even
when the reference vector field or score function is not Lipschitz. Applying
this result, we establish the class of distributions that can be sampled using
score matching in SGMs, relaxing the Lipschitz requirement on the gradient of
the data distribution in existing literature. Second, we show that this
approximation property is preserved when network width is limited to the input
dimension of the network. In this limited width case, the weights act as
control inputs, framing our analysis as a controllability problem for neural
SDEs in probability density space. This sheds light on how noise helps to steer
the system towards the desired solution and illuminates the empirical success
of stochasticity in generative modeling.
</p></li>
</ul>
<h3>Title: Time Series Diffusion Method: A Denoising Diffusion Probabilistic Model for Vibration Signal Generation. (arXiv:2312.07981v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07981">http://arxiv.org/abs/2312.07981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07981] Time Series Diffusion Method: A Denoising Diffusion Probabilistic Model for Vibration Signal Generation](http://arxiv.org/abs/2312.07981) #diffusion</code></li>
<li>Summary: <p>Diffusion models have demonstrated robust data generation capabilities in
various research fields. In this paper, a Time Series Diffusion Method (TSDM)
is proposed for vibration signal generation, leveraging the foundational
principles of diffusion models. The TSDM uses an improved U-net architecture
with attention block to effectively segment and extract features from
one-dimensional time series data. It operates based on forward diffusion and
reverse denoising processes for time-series generation. Experimental validation
is conducted using single-frequency, multi-frequency datasets, and bearing
fault datasets. The results show that TSDM can accurately generate the
single-frequency and multi-frequency features in the time series and retain the
basic frequency features for the diffusion generation results of the bearing
fault series. Finally, TSDM is applied to the small sample fault diagnosis of
three public bearing fault datasets, and the results show that the accuracy of
small sample fault diagnosis of the three datasets is improved by 32.380%,
18.355% and 9.298% at most, respectively
</p></li>
</ul>
<h3>Title: SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space. (arXiv:2312.08200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08200">http://arxiv.org/abs/2312.08200</a></li>
<li>Code URL: <a href="https://github.com/li-yun-chen/spd-ddpm">https://github.com/li-yun-chen/spd-ddpm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08200] SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space](http://arxiv.org/abs/2312.08200) #diffusion</code></li>
<li>Summary: <p>Symmetric positive definite~(SPD) matrices have shown important value and
applications in statistics and machine learning, such as FMRI analysis and
traffic prediction. Previous works on SPD matrices mostly focus on
discriminative models, where predictions are made directly on $E(X|y)$, where
$y$ is a vector and $X$ is an SPD matrix. However, these methods are
challenging to handle for large-scale data, as they need to access and process
the whole data. In this paper, inspired by denoising diffusion probabilistic
model~(DDPM), we propose a novel generative model, termed SPD-DDPM, by
introducing Gaussian distribution in the SPD space to estimate $E(X|y)$.
Moreover, our model is able to estimate $p(X)$ unconditionally and flexibly
without giving $y$. On the one hand, the model conditionally learns $p(X|y)$
and utilizes the mean of samples to obtain $E(X|y)$ as a prediction. On the
other hand, the model unconditionally learns the probability distribution of
the data $p(X)$ and generates samples that conform to this distribution.
Furthermore, we propose a new SPD net which is much deeper than the previous
networks and allows for the inclusion of conditional factors. Experiment
results on toy data and real taxi data demonstrate that our models effectively
fit the data distribution both unconditionally and unconditionally and provide
accurate predictions.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Pre-trained Universal Medical Image Transformer. (arXiv:2312.07630v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07630">http://arxiv.org/abs/2312.07630</a></li>
<li>Code URL: <a href="https://github.com/function2-llx/pumit">https://github.com/function2-llx/pumit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07630] Pre-trained Universal Medical Image Transformer](http://arxiv.org/abs/2312.07630) #self-supervised</code></li>
<li>Summary: <p>Self-supervised learning has emerged as a viable method to leverage the
abundance of unlabeled medical imaging data, addressing the challenge of
labeled data scarcity in medical image analysis. In particular, masked image
modeling (MIM) with visual token reconstruction has shown promising results in
the general computer vision (CV) domain and serves as a candidate for medical
image analysis. However, the presence of heterogeneous 2D and 3D medical images
often limits the volume and diversity of training data that can be effectively
used for a single model structure. In this work, we propose a spatially
adaptive convolution (SAC) module, which adaptively adjusts convolution
parameters based on the voxel spacing of the input images. Employing this SAC
module, we build a universal visual tokenizer and a universal Vision
Transformer (ViT) capable of effectively processing a wide range of medical
images with various imaging modalities and spatial properties. Moreover, in
order to enhance the robustness of the visual tokenizer's reconstruction
objective for MIM, we suggest to generalize the discrete token output of the
visual tokenizer to a probabilistic soft token. We show that the generalized
soft token representation can be effectively integrated with the prior
distribution regularization through a constructive interpretation. As a result,
we pre-train a universal visual tokenizer followed by a universal ViT via
visual token reconstruction on 55 public medical image datasets, comprising
over 9 million 2D slices (including over 48,000 3D images). This represents the
largest, most comprehensive, and diverse dataset for pre-training 3D medical
image models to our knowledge. Experimental results on downstream medical image
classification and segmentation tasks demonstrate the superior performance of
our model and improved label efficiency.
</p></li>
</ul>
<h3>Title: Contextually Affinitive Neighborhood Refinery for Deep Clustering. (arXiv:2312.07806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07806">http://arxiv.org/abs/2312.07806</a></li>
<li>Code URL: <a href="https://github.com/cly234/deepclustering-connr">https://github.com/cly234/deepclustering-connr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07806] Contextually Affinitive Neighborhood Refinery for Deep Clustering](http://arxiv.org/abs/2312.07806) #self-supervised</code></li>
<li>Summary: <p>Previous endeavors in self-supervised learning have enlightened the research
of deep clustering from an instance discrimination perspective. Built upon this
foundation, recent studies further highlight the importance of grouping
semantically similar instances. One effective method to achieve this is by
promoting the semantic structure preserved by neighborhood consistency.
However, the samples in the local neighborhood may be limited due to their
close proximity to each other, which may not provide substantial and diverse
supervision signals. Inspired by the versatile re-ranking methods in the
context of image retrieval, we propose to employ an efficient online re-ranking
process to mine more informative neighbors in a Contextually Affinitive
(ConAff) Neighborhood, and then encourage the cross-view neighborhood
consistency. To further mitigate the intrinsic neighborhood noises near cluster
boundaries, we propose a progressively relaxed boundary filtering strategy to
circumvent the issues brought by noisy neighbors. Our method can be easily
integrated into the generic self-supervised frameworks and outperforms the
state-of-the-art methods on several popular benchmarks.
</p></li>
</ul>
<h3>Title: A Foundational Multimodal Vision Language AI Assistant for Human Pathology. (arXiv:2312.07814v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07814">http://arxiv.org/abs/2312.07814</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07814] A Foundational Multimodal Vision Language AI Assistant for Human Pathology](http://arxiv.org/abs/2312.07814) #self-supervised</code></li>
<li>Summary: <p>The field of computational pathology has witnessed remarkable progress in the
development of both task-specific predictive models and task-agnostic
self-supervised vision encoders. However, despite the explosive growth of
generative artificial intelligence (AI), there has been limited study on
building general purpose, multimodal AI assistants tailored to pathology. Here
we present PathChat, a vision-language generalist AI assistant for human
pathology using an in-house developed foundational vision encoder pretrained on
100 million histology images from over 100,000 patient cases and 1.18 million
pathology image-caption pairs. The vision encoder is then combined with a
pretrained large language model and the whole system is finetuned on over
250,000 diverse disease agnostic visual language instructions. We compare
PathChat against several multimodal vision language AI assistants as well as
GPT4V, which powers the commercially available multimodal general purpose AI
assistant ChatGPT-4. When relevant clinical context is provided with the
histology image, PathChat achieved a diagnostic accuracy of 87% on
multiple-choice questions based on publicly available cases of diverse tissue
origins and disease models. Additionally, using open-ended questions and human
expert evaluation, we found that overall PathChat produced more accurate and
pathologist-preferable responses to diverse queries related to pathology. As an
interactive and general vision language AI assistant that can flexibly handle
both visual and natural language inputs, PathChat can potentially find
impactful applications in pathology education, research, and human-in-the-loop
clinical decision making.
</p></li>
</ul>
<h3>Title: Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking. (arXiv:2312.07955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07955">http://arxiv.org/abs/2312.07955</a></li>
<li>Code URL: <a href="https://github.com/livxue/poisoncam">https://github.com/livxue/poisoncam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07955] Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking](http://arxiv.org/abs/2312.07955) #self-supervised</code></li>
<li>Summary: <p>Researchers have recently found that Self-Supervised Learning (SSL) is
vulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via
a few poisoned examples in the training dataset and maliciously manipulate the
behavior of downstream models. To defend against SSL backdoor attacks, a
feasible route is to detect and remove the poisonous samples in the training
set. However, the existing SSL backdoor defense method fails to detect the
poisonous samples precisely. In this paper, we propose to erase the SSL
backdoor by cluster activation masking and propose a novel PoisonCAM method.
After obtaining the threat model trained on the poisoned dataset, our method
can precisely detect poisonous samples based on the assumption that masking the
backdoor trigger can effectively change the activation of a downstream
clustering model. In experiments, our PoisonCAM achieves 96% accuracy for
backdoor trigger detection compared to 3% of the state-of-the-art method on
poisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves
the performance of the trained SSL model under backdoor attacks compared to the
state-of-the-art method. Our code will be available at
https://github.com/LivXue/PoisonCAM.
</p></li>
</ul>
<h3>Title: Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix. (arXiv:2312.08009v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08009">http://arxiv.org/abs/2312.08009</a></li>
<li>Code URL: <a href="https://github.com/kwwcv/ssmp">https://github.com/kwwcv/ssmp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08009] Semi-Supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix](http://arxiv.org/abs/2312.08009) #self-supervised</code></li>
<li>Summary: <p>Class-agnostic motion prediction methods aim to comprehend motion within
open-world scenarios, holding significance for autonomous driving systems.
However, training a high-performance model in a fully-supervised manner always
requires substantial amounts of manually annotated data, which can be both
expensive and time-consuming to obtain. To address this challenge, our study
explores the potential of semi-supervised learning (SSL) for class-agnostic
motion prediction. Our SSL framework adopts a consistency-based self-training
paradigm, enabling the model to learn from unlabeled data by generating pseudo
labels through test-time inference. To improve the quality of pseudo labels, we
propose a novel motion selection and re-generation module. This module
effectively selects reliable pseudo labels and re-generates unreliable ones.
Furthermore, we propose two data augmentation strategies: temporal sampling and
BEVMix. These strategies facilitate consistency regularization in SSL.
Experiments conducted on nuScenes demonstrate that our SSL method can surpass
the self-supervised approach by a large margin by utilizing only a tiny
fraction of labeled data. Furthermore, our method exhibits comparable
performance to weakly and some fully supervised methods. These results
highlight the ability of our method to strike a favorable balance between
annotation costs and performance. Code will be available at
https://github.com/kwwcv/SSMP.
</p></li>
</ul>
<h3>Title: Novel View Synthesis with View-Dependent Effects from a Single Image. (arXiv:2312.08071v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08071">http://arxiv.org/abs/2312.08071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08071] Novel View Synthesis with View-Dependent Effects from a Single Image](http://arxiv.org/abs/2312.08071) #self-supervised</code></li>
<li>Summary: <p>In this paper, we firstly consider view-dependent effects into single
image-based novel view synthesis (NVS) problems. For this, we propose to
exploit the camera motion priors in NVS to model view-dependent appearance or
effects (VDE) as the negative disparity in the scene. By recognizing
specularities "follow" the camera motion, we infuse VDEs into the input images
by aggregating input pixel colors along the negative depth region of the
epipolar lines. Also, we propose a `relaxed volumetric rendering' approximation
that allows computing the densities in a single pass, improving efficiency for
NVS from single images. Our method can learn single-image NVS from image
sequences only, which is a completely self-supervised learning method, for the
first time requiring neither depth nor camera pose annotations. We present
extensive experiment results and show that our proposed method can learn NVS
with VDEs, outperforming the SOTA single-view NVS methods on the RealEstate10k
and MannequinChallenge datasets.
</p></li>
</ul>
<h3>Title: PAD: Self-Supervised Pre-Training with Patchwise-Scale Adapter for Infrared Images. (arXiv:2312.08192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08192">http://arxiv.org/abs/2312.08192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08192] PAD: Self-Supervised Pre-Training with Patchwise-Scale Adapter for Infrared Images](http://arxiv.org/abs/2312.08192) #self-supervised</code></li>
<li>Summary: <p>Self-supervised learning (SSL) for RGB images has achieved significant
success, yet there is still limited research on SSL for infrared images,
primarily due to three prominent challenges: 1) the lack of a suitable
large-scale infrared pre-training dataset, 2) the distinctiveness of non-iconic
infrared images rendering common pre-training tasks like masked image modeling
(MIM) less effective, and 3) the scarcity of fine-grained textures making it
particularly challenging to learn general image features. To address these
issues, we construct a Multi-Scene Infrared Pre-training (MSIP) dataset
comprising 178,756 images, and introduce object-sensitive random RoI cropping,
an image preprocessing method, to tackle the challenge posed by non-iconic
images. To alleviate the impact of weak textures on feature learning, we
propose a pre-training paradigm called Pre-training with ADapter (PAD), which
uses adapters to learn domain-specific features while freezing parameters
pre-trained on ImageNet to retain the general feature extraction capability.
This new paradigm is applicable to any transformer-based SSL method.
Furthermore, to achieve more flexible coordination between pre-trained and
newly-learned features in different layers and patches, a patchwise-scale
adapter with dynamically learnable scale factors is introduced. Extensive
experiments on three downstream tasks show that PAD, with only 1.23M
pre-trainable parameters, outperforms other baseline paradigms including
continual full pre-training on MSIP. Our code and dataset are available at
https://github.com/casiatao/PAD.
</p></li>
</ul>
<h3>Title: Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches. (arXiv:2312.08230v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08230">http://arxiv.org/abs/2312.08230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08230] Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches](http://arxiv.org/abs/2312.08230) #self-supervised</code></li>
<li>Summary: <p>Symmetry detection, especially partial and extrinsic symmetry, is essential
for various downstream tasks, like 3D geometry completion, segmentation,
compression and structure-aware shape encoding or generation. In order to
detect partial extrinsic symmetries, we propose to learn rotation, reflection,
translation and scale invariant local shape features for geodesic point cloud
patches via contrastive learning, which are robust across multiple classes and
generalize over different datasets. We show that our approach is able to
extract multiple valid solutions for this ambiguous problem. Furthermore, we
introduce a novel benchmark test for partial extrinsic symmetry detection to
evaluate our method. Lastly, we incorporate the detected symmetries together
with a region growing algorithm to demonstrate a downstream task with the goal
of computing symmetry-aware partitions of 3D shapes. To our knowledge, we are
the first to propose a self-supervised data-driven method for partial extrinsic
symmetry detection.
</p></li>
</ul>
<h3>Title: Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems. (arXiv:2312.08198v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08198">http://arxiv.org/abs/2312.08198</a></li>
<li>Code URL: <a href="https://github.com/clarin-pl/model-based-data-acquisition">https://github.com/clarin-pl/model-based-data-acquisition</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08198] Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems](http://arxiv.org/abs/2312.08198) #self-supervised</code></li>
<li>Summary: <p>Data annotated by humans is a source of knowledge by describing the
peculiarities of the problem and therefore fueling the decision process of the
trained model. Unfortunately, the annotation process for subjective natural
language processing (NLP) problems like offensiveness or emotion detection is
often very expensive and time-consuming. One of the inevitable risks is to
spend some of the funds and annotator effort on annotations that do not provide
any additional knowledge about the specific task. To minimize these costs, we
propose a new model-based approach that allows the selection of tasks annotated
individually for each text in a multi-task scenario. The experiments carried
out on three datasets, dozens of NLP tasks, and thousands of annotations show
that our method allows up to 40% reduction in the number of annotations with
negligible loss of knowledge. The results also emphasize the need to collect a
diverse amount of data required to efficiently train a model, depending on the
subjectivity of the annotation task. We also focused on measuring the relation
between subjective tasks by evaluating the model in single-task and multi-task
scenarios. Moreover, for some datasets, training only on the labels predicted
by our model improved the efficiency of task selection as a self-supervised
learning regularization technique.
</p></li>
</ul>
<h3>Title: Optimizing Likelihood-free Inference using Self-supervised Neural Symmetry Embeddings. (arXiv:2312.07615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07615">http://arxiv.org/abs/2312.07615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07615] Optimizing Likelihood-free Inference using Self-supervised Neural Symmetry Embeddings](http://arxiv.org/abs/2312.07615) #self-supervised</code></li>
<li>Summary: <p>Likelihood-free inference is quickly emerging as a powerful tool to perform
fast/effective parameter estimation. We demonstrate a technique of optimizing
likelihood-free inference to make it even faster by marginalizing symmetries in
a physical problem. In this approach, physical symmetries, for example,
time-translation are learned using joint-embedding via self-supervised learning
with symmetry data augmentations. Subsequently, parameter inference is
performed using a normalizing flow where the embedding network is used to
summarize the data before conditioning the parameters. We present this approach
on two simple physical problems and we show faster convergence in a smaller
number of parameters compared to a normalizing flow that does not use a
pre-trained symmetry-informed representation.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects. (arXiv:2312.08344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08344">http://arxiv.org/abs/2312.08344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08344] FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects](http://arxiv.org/abs/2312.08344) #foundation model</code></li>
<li>Summary: <p>We present FoundationPose, a unified foundation model for 6D object pose
estimation and tracking, supporting both model-based and model-free setups. Our
approach can be instantly applied at test-time to a novel object without
fine-tuning, as long as its CAD model is given, or a small number of reference
images are captured. We bridge the gap between these two setups with a neural
implicit representation that allows for effective novel view synthesis, keeping
the downstream pose estimation modules invariant under the same unified
framework. Strong generalizability is achieved via large-scale synthetic
training, aided by a large language model (LLM), a novel transformer-based
architecture, and contrastive learning formulation. Extensive evaluation on
multiple public datasets involving challenging scenarios and objects indicate
our unified approach outperforms existing methods specialized for each task by
a large margin. In addition, it even achieves comparable results to
instance-level methods despite the reduced assumptions. Project page:
https://nvlabs.github.io/FoundationPose/
</p></li>
</ul>
<h3>Title: Beyond Top-Class Agreement: Using Divergences to Forecast Performance under Distribution Shift. (arXiv:2312.08033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08033">http://arxiv.org/abs/2312.08033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08033] Beyond Top-Class Agreement: Using Divergences to Forecast Performance under Distribution Shift](http://arxiv.org/abs/2312.08033) #foundation model</code></li>
<li>Summary: <p>Knowing if a model will generalize to data 'in the wild' is crucial for safe
deployment. To this end, we study model disagreement notions that consider the
full predictive distribution - specifically disagreement based on Hellinger
distance, Jensen-Shannon and Kullback-Leibler divergence. We find that
divergence-based scores provide better test error estimates and detection rates
on out-of-distribution data compared to their top-1 counterparts. Experiments
involve standard vision and foundation models.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Understanding (Un)Intended Memorization in Text-to-Image Generative Models. (arXiv:2312.07550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07550">http://arxiv.org/abs/2312.07550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07550] Understanding (Un)Intended Memorization in Text-to-Image Generative Models](http://arxiv.org/abs/2312.07550) #generative</code></li>
<li>Summary: <p>Multimodal machine learning, especially text-to-image models like Stable
Diffusion and DALL-E 3, has gained significance for transforming text into
detailed images.
</p></li>
</ul>
<p>Despite their growing use and remarkable generative capabilities, there is a
pressing need for a detailed examination of these models' behavior,
particularly with respect to memorization. Historically, memorization in
machine learning has been context-dependent, with diverse definitions emerging
from classification tasks to complex models like Large Language Models (LLMs)
and Diffusion models. Yet, a definitive concept of memorization that aligns
with the intricacies of text-to-image synthesis remains elusive. This
understanding is vital as memorization poses privacy risks yet is essential for
meeting user expectations, especially when generating representations of
underrepresented entities. In this paper, we introduce a specialized definition
of memorization tailored to text-to-image models, categorizing it into three
distinct types according to user expectations. We closely examine the subtle
distinctions between intended and unintended memorization, emphasizing the
importance of balancing user privacy with the generative quality of the model
outputs. Using the Stable Diffusion model, we offer examples to validate our
memorization definitions and clarify their application.
</p>

<h3>Title: Stable Rivers: A Case Study in the Application of Text-to-Image Generative Models for Earth Sciences. (arXiv:2312.07833v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07833">http://arxiv.org/abs/2312.07833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07833] Stable Rivers: A Case Study in the Application of Text-to-Image Generative Models for Earth Sciences](http://arxiv.org/abs/2312.07833) #generative</code></li>
<li>Summary: <p>Text-to-image (TTI) generative models can be used to generate photorealistic
images from a given text-string input. These models offer great potential to
mitigate challenges to the uptake of machine learning in the earth sciences.
However, the rapid increase in their use has raised questions about fairness
and biases, with most research to-date focusing on social and cultural areas
rather than domain-specific considerations. We conducted a case study for the
earth sciences, focusing on the field of fluvial geomorphology, where we
evaluated subject-area specific biases in the training data and downstream
model performance of Stable Diffusion (v1.5). In addition to perpetuating
Western biases, we found that the training data over-represented scenic
locations, such as famous rivers and waterfalls, and showed serious under- and
over-representation of many morphological and environmental terms. Despite
biased training data, we found that with careful prompting, the Stable
Diffusion model was able to generate photorealistic synthetic river images
reproducing many important environmental and morphological characteristics.
Furthermore, conditional control techniques, such as the use of condition maps
with ControlNet were effective for providing additional constraints on output
images. Despite great potential for the use of TTI models in the earth sciences
field, we advocate for caution in sensitive applications, and advocate for
domain-specific reviews of training data and image generation biases to
mitigate perpetuation of existing biases.
</p></li>
</ul>
<h3>Title: 3DGEN: A GAN-based approach for generating novel 3D models from image data. (arXiv:2312.08094v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08094">http://arxiv.org/abs/2312.08094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08094] 3DGEN: A GAN-based approach for generating novel 3D models from image data](http://arxiv.org/abs/2312.08094) #generative</code></li>
<li>Summary: <p>The recent advances in text and image synthesis show a great promise for the
future of generative models in creative fields. However, a less explored area
is the one of 3D model generation, with a lot of potential applications to game
design, video production, and physical product design. In our paper, we present
3DGEN, a model that leverages the recent work on both Neural Radiance Fields
for object reconstruction and GAN-based image generation. We show that the
proposed architecture can generate plausible meshes for objects of the same
category as the training images and compare the resulting meshes with the
state-of-the-art baselines, leading to visible uplifts in generation quality.
</p></li>
</ul>
<h3>Title: A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing. (arXiv:2312.08256v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08256">http://arxiv.org/abs/2312.08256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08256] A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing](http://arxiv.org/abs/2312.08256) #generative</code></li>
<li>Summary: <p>Recent advances in the field of generative models and in particular
generative adversarial networks (GANs) have lead to substantial progress for
controlled image editing, especially compared with the pre-deep learning era.
Despite their powerful ability to apply realistic modifications to an image,
these methods often lack properties like disentanglement (the capacity to edit
attributes independently). In this paper, we propose an auto-encoder which
re-organizes the latent space of StyleGAN, so that each attribute which we wish
to edit corresponds to an axis of the new latent space, and furthermore that
the latent axes are decorrelated, encouraging disentanglement. We work in a
compressed version of the latent space, using Principal Component Analysis,
meaning that the parameter complexity of our autoencoder is reduced, leading to
short training times ($\sim$ 45 mins). Qualitative and quantitative results
demonstrate the editing capabilities of our approach, with greater
disentanglement than competing methods, while maintaining fidelity to the
original image with respect to identity. Our autoencoder architecture simple
and straightforward, facilitating implementation.
</p></li>
</ul>
<h3>Title: PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. (arXiv:2312.07559v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07559">http://arxiv.org/abs/2312.07559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07559] PaperQA: Retrieval-Augmented Generative Agent for Scientific Research](http://arxiv.org/abs/2312.07559) #generative</code></li>
<li>Summary: <p>Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA's matches expert human researchers on LitQA.
</p></li>
</ul>
<h3>Title: Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models. (arXiv:2312.07592v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07592">http://arxiv.org/abs/2312.07592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07592] Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models](http://arxiv.org/abs/2312.07592) #generative</code></li>
<li>Summary: <p>In the current era, a multitude of language models has emerged to cater to
user inquiries. Notably, the GPT-3.5 Turbo language model has gained
substantial attention as the underlying technology for ChatGPT. Leveraging
extensive parameters, this model adeptly responds to a wide range of questions.
However, due to its reliance on internal knowledge, the accuracy of responses
may not be absolute. This article scrutinizes ChatGPT as a Question Answering
System (QAS), comparing its performance to other existing QASs. The primary
focus is on evaluating ChatGPT's proficiency in extracting responses from
provided paragraphs, a core QAS capability. Additionally, performance
comparisons are made in scenarios without a surrounding passage. Multiple
experiments, exploring response hallucination and considering question
complexity, were conducted on ChatGPT. Evaluation employed well-known Question
Answering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across
English and Persian languages. Metrics such as F-score, exact match, and
accuracy were employed in the assessment. The study reveals that, while ChatGPT
demonstrates competence as a generative model, it is less effective in question
answering compared to task-specific models. Providing context improves its
performance, and prompt engineering enhances precision, particularly for
questions lacking explicit answers in provided paragraphs. ChatGPT excels at
simpler factual questions compared to "how" and "why" question types. The
evaluation highlights occurrences of hallucinations, where ChatGPT provides
responses to questions without available answers in the provided context.
</p></li>
</ul>
<h3>Title: A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07913">http://arxiv.org/abs/2312.07913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07913] A Survey of Text Watermarking in the Era of Large Language Models](http://arxiv.org/abs/2312.07913) #generative</code></li>
<li>Summary: <p>In recent years, significant advancements have been made in the text
generation capabilities of Large Language Models (LLMs), demonstrating
exceptional performance in downstream tasks such as abstract summarization,
dialogue generation, and data-to-text conversion. However, their generative
abilities also pose risks such as the rapid spread of fake news, infringement
of datasets/LLM copyrights, and challenges to academic integrity. Text
watermarking technology emerges as a potential solution. By embedding invisible
yet detectable patterns in generated texts, it helps in tracking and verifying
text origins, thus preventing misuse and piracy.
</p></li>
</ul>
<p>This survey aims to comprehensively summarize current text watermarking
technologies, covering three main aspects: (1) an overview and comparison of
different text watermarking techniques; (2) evaluation methods for text
watermarking algorithms, including their success rate, impact on text quality,
robustness, and unforgeability; (3) potential applications of text watermarking
technologys. This survey aims to help researchers thoroughly understanding the
text watermarking technologies, thereby fostering further development.
</p>

<h3>Title: Combining propensity score methods with variational autoencoders for generating synthetic data in presence of latent sub-groups. (arXiv:2312.07781v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07781">http://arxiv.org/abs/2312.07781</a></li>
<li>Code URL: <a href="https://github.com/kianaf/latentsubgroups">https://github.com/kianaf/latentsubgroups</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07781] Combining propensity score methods with variational autoencoders for generating synthetic data in presence of latent sub-groups](http://arxiv.org/abs/2312.07781) #generative</code></li>
<li>Summary: <p>In settings requiring synthetic data generation based on a clinical cohort,
e.g., due to data protection regulations, heterogeneity across individuals
might be a nuisance that we need to control or faithfully preserve. The sources
of such heterogeneity might be known, e.g., as indicated by sub-groups labels,
or might be unknown and thus reflected only in properties of distributions,
such as bimodality or skewness. We investigate how such heterogeneity can be
preserved and controlled when obtaining synthetic data from variational
autoencoders (VAEs), i.e., a generative deep learning technique that utilizes a
low-dimensional latent representation. To faithfully reproduce unknown
heterogeneity reflected in marginal distributions, we propose to combine VAEs
with pre-transformations. For dealing with known heterogeneity due to
sub-groups, we complement VAEs with models for group membership, specifically
from propensity score regression. The evaluation is performed with a realistic
simulation design that features sub-groups and challenging marginal
distributions. The proposed approach faithfully recovers the latter, compared
to synthetic data approaches that focus purely on marginal distributions.
Propensity scores add complementary information, e.g., when visualized in the
latent space, and enable sampling of synthetic data with or without sub-group
specific characteristics. We also illustrate the proposed approach with real
data from an international stroke trial that exhibits considerable distribution
differences between study sites, in addition to bimodality. These results
indicate that describing heterogeneity by statistical approaches, such as
propensity score regression, might be more generally useful for complementing
generative deep learning for obtaining synthetic data that faithfully reflects
structure from clinical cohorts.
</p></li>
</ul>
<h3>Title: Synthetic Data: Can We Trust Statistical Estimators?. (arXiv:2312.07837v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07837">http://arxiv.org/abs/2312.07837</a></li>
<li>Code URL: <a href="https://github.com/syndara-lab/inferential-utility-workshop">https://github.com/syndara-lab/inferential-utility-workshop</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07837] Synthetic Data: Can We Trust Statistical Estimators?](http://arxiv.org/abs/2312.07837) #generative</code></li>
<li>Summary: <p>The increasing interest in data sharing makes synthetic data appealing.
However, the analysis of synthetic data raises a unique set of methodological
challenges. In this work, we highlight the importance of inferential utility
and provide empirical evidence against naive inference from synthetic data
(that handles these as if they were really observed). We argue that the rate of
false-positive findings (type 1 error) will be unacceptably high, even when the
estimates are unbiased. One of the reasons is the underestimation of the true
standard error, which may even progressively increase with larger sample sizes
due to slower convergence. This is especially problematic for deep generative
models. Before publishing synthetic data, it is essential to develop
statistical inference tools for such data.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Efficient Representation of the Activation Space in Deep Neural Networks. (arXiv:2312.08143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08143">http://arxiv.org/abs/2312.08143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08143] Efficient Representation of the Activation Space in Deep Neural Networks](http://arxiv.org/abs/2312.08143) #anomaly</code></li>
<li>Summary: <p>The representations of the activation space of deep neural networks (DNNs)
are widely utilized for tasks like natural language processing, anomaly
detection and speech recognition. Due to the diverse nature of these tasks and
the large size of DNNs, an efficient and task-independent representation of
activations becomes crucial. Empirical p-values have been used to quantify the
relative strength of an observed node activation compared to activations
created by already-known inputs. Nonetheless, keeping raw data for these
calculations increases memory resource consumption and raises privacy concerns.
To this end, we propose a model-agnostic framework for creating representations
of activations in DNNs using node-specific histograms to compute p-values of
observed activations without retaining already-known inputs. Our proposed
approach demonstrates promising potential when validated with multiple network
architectures across various downstream tasks and compared with the kernel
density estimates and brute-force empirical baselines. In addition, the
framework reduces memory usage by 30% with up to 4 times faster p-value
computing time while maintaining state of-the-art detection power in downstream
tasks such as the detection of adversarial attacks and synthesized content.
Moreover, as we do not persist raw data at inference time, we could potentially
reduce susceptibility to attacks and privacy issues.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization. (arXiv:2312.07763v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07763">http://arxiv.org/abs/2312.07763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07763] Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization](http://arxiv.org/abs/2312.07763) #in-context</code></li>
<li>Summary: <p>The meaning of complex phrases in natural language is composed of their
individual components. The task of compositional generalization evaluates a
model's ability to understand new combinations of components. Previous studies
trained smaller, task-specific models, which exhibited poor generalization.
While large language models (LLMs) exhibit impressive generalization abilities
on many tasks through in-context learning (ICL), their potential for
compositional generalization remains unexplored. In this paper, we first
empirically investigate prevailing ICL methods in compositional generalization.
We find that they struggle with complex compositional questions due to
cumulative errors in long reasoning steps and intricate logic required for
tool-making. Consequently, we propose a human-guided tool manipulation
framework (HTM) that generates tools for sub-questions and integrates multiple
tools. Our method enhances the effectiveness of tool creation and usage with
minimal human effort. Experiments show that our method achieves
state-of-the-art performance on two compositional generalization benchmarks and
outperforms existing methods on the most challenging test split by 70%.
</p></li>
</ul>
<h3>Title: Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models. (arXiv:2312.08303v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08303">http://arxiv.org/abs/2312.08303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08303] Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](http://arxiv.org/abs/2312.08303) #in-context</code></li>
<li>Summary: <p>Toxic content detection is crucial for online services to remove
inappropriate content that violates community standards. To automate the
detection process, prior works have proposed varieties of machine learning (ML)
approaches to train Language Models (LMs) for toxic content detection. However,
both their accuracy and transferability across datasets are limited. Recently,
Large Language Models (LLMs) have shown promise in toxic content detection due
to their superior zero-shot and few-shot in-context learning ability as well as
broad transferability on ML tasks. However, efficiently designing prompts for
LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder
their deployments in production. To address these challenges, in this work, we
propose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling
LLMs for toxic content detection. Specifically, we design a novel prompting
method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection
performance and extract high-quality rationales. DToT can automatically select
more fine-grained context to re-prompt LLMs when their responses lack
confidence. Additionally, we use the rationales extracted via DToT to fine-tune
student LMs. Our experimental results on various datasets demonstrate that DToT
can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs
fine-tuned with rationales extracted via DToT outperform baselines on all
datasets with up to 16.9\% accuracy improvement, while being more than 60x
smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned
with rationales exhibit better cross-dataset transferability.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply. (arXiv:2312.07636v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07636">http://arxiv.org/abs/2312.07636</a></li>
<li>Code URL: <a href="https://github.com/tab-ct/contsup">https://github.com/tab-ct/contsup</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07636] Go beyond End-to-End Training: Boosting Greedy Local Learning with Context Supply](http://arxiv.org/abs/2312.07636) #memory</code></li>
<li>Summary: <p>Traditional end-to-end (E2E) training of deep networks necessitates storing
intermediate activations for back-propagation, resulting in a large memory
footprint on GPUs and restricted model parallelization. As an alternative,
greedy local learning partitions the network into gradient-isolated modules and
trains supervisely based on local preliminary losses, thereby providing
asynchronous and parallel training methods that substantially reduce memory
cost. However, empirical experiments reveal that as the number of segmentations
of the gradient-isolated module increases, the performance of the local
learning scheme degrades substantially, severely limiting its expansibility. To
avoid this issue, we theoretically analyze the greedy local learning from the
standpoint of information theory and propose a ContSup scheme, which
incorporates context supply between isolated modules to compensate for
information loss. Experiments on benchmark datasets (i.e. CIFAR, SVHN, STL-10)
achieve SOTA results and indicate that our proposed method can significantly
improve the performance of greedy local learning with minimal memory and
computational overhead, allowing for the boost of the number of isolated
modules. Our codes are available at https://github.com/Tab-ct/ContSup.
</p></li>
</ul>
<h3>Title: DTL: Disentangled Transfer Learning for Visual Recognition. (arXiv:2312.07856v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07856">http://arxiv.org/abs/2312.07856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07856] DTL: Disentangled Transfer Learning for Visual Recognition](http://arxiv.org/abs/2312.07856) #memory</code></li>
<li>Summary: <p>When pre-trained models become rapidly larger, the cost of fine-tuning on
downstream tasks steadily increases, too. To economically fine-tune these
models, parameter-efficient transfer learning (PETL) is proposed, which only
tunes a tiny subset of trainable parameters to efficiently learn quality
representations. However, current PETL methods are facing the dilemma that
during training the GPU memory footprint is not effectively reduced as
trainable parameters. PETL will likely fail, too, if the full fine-tuning
encounters the out-of-GPU-memory issue. This phenomenon happens because
trainable parameters from these methods are generally entangled with the
backbone, such that a lot of intermediate states have to be stored in GPU
memory for gradient propagation. To alleviate this problem, we introduce
Disentangled Transfer Learning (DTL), which disentangles the trainable
parameters from the backbone using a lightweight Compact Side Network (CSN). By
progressively extracting task-specific information with a few low-rank linear
mappings and appropriately adding the information back to the backbone, CSN
effectively realizes knowledge transfer in various downstream tasks. We
conducted extensive experiments to validate the effectiveness of our method.
The proposed method not only reduces a large amount of GPU memory usage and
trainable parameters, but also outperforms existing PETL methods by a
significant margin in accuracy, achieving new state-of-the-art on several
standard benchmarks.
</p></li>
</ul>
<h3>Title: Enhance Sketch Recognition's Explainability via Semantic Component-Level Parsing. (arXiv:2312.07875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07875">http://arxiv.org/abs/2312.07875</a></li>
<li>Code URL: <a href="https://github.com/guangmingzhu/sketchesc">https://github.com/guangmingzhu/sketchesc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07875] Enhance Sketch Recognition's Explainability via Semantic Component-Level Parsing](http://arxiv.org/abs/2312.07875) #memory</code></li>
<li>Summary: <p>Free-hand sketches are appealing for humans as a universal tool to depict the
visual world. Humans can recognize varied sketches of a category easily by
identifying the concurrence and layout of the intrinsic semantic components of
the category, since humans draw free-hand sketches based a common consensus
that which types of semantic components constitute each sketch category. For
example, an airplane should at least have a fuselage and wings. Based on this
analysis, a semantic component-level memory module is constructed and embedded
in the proposed structured sketch recognition network in this paper. The memory
keys representing semantic components of each sketch category can be
self-learned and enhance the recognition network's explainability. Our proposed
networks can deal with different situations of sketch recognition, i.e., with
or without semantic components labels of strokes. Experiments on the SPG and
SketchIME datasets demonstrate the memory module's flexibility and the
recognition network's explainability. The code and data are available at
https://github.com/GuangmingZhu/SketchESC.
</p></li>
</ul>
<h3>Title: Memory-Efficient Reversible Spiking Neural Networks. (arXiv:2312.07922v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07922">http://arxiv.org/abs/2312.07922</a></li>
<li>Code URL: <a href="https://github.com/mi804/revsnn">https://github.com/mi804/revsnn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07922] Memory-Efficient Reversible Spiking Neural Networks](http://arxiv.org/abs/2312.07922) #memory</code></li>
<li>Summary: <p>Spiking neural networks (SNNs) are potential competitors to artificial neural
networks (ANNs) due to their high energy-efficiency on neuromorphic hardware.
However, SNNs are unfolded over simulation time steps during the training
process. Thus, SNNs require much more memory than ANNs, which impedes the
training of deeper SNN models. In this paper, we propose the reversible spiking
neural network to reduce the memory cost of intermediate activations and
membrane potentials during training. Firstly, we extend the reversible
architecture along temporal dimension and propose the reversible spiking block,
which can reconstruct the computational graph and recompute all intermediate
variables in forward pass with a reverse process. On this basis, we adopt the
state-of-the-art SNN models to the reversible variants, namely reversible
spiking ResNet (RevSResNet) and reversible spiking transformer (RevSFormer).
Through experiments on static and neuromorphic datasets, we demonstrate that
the memory cost per image of our reversible SNNs does not increase with the
network depth. On CIFAR10 and CIFAR100 datasets, our RevSResNet37 and
RevSFormer-4-384 achieve comparable accuracies and consume 3.79x and 3.00x
lower GPU memory per image than their counterparts with roughly identical model
complexity and parameters. We believe that this work can unleash the memory
constraints in SNN training and pave the way for training extremely large and
deep SNNs. The code is available at https://github.com/mi804/RevSNN.git.
</p></li>
</ul>
<h3>Title: ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields. (arXiv:2312.08136v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08136">http://arxiv.org/abs/2312.08136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08136] ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields](http://arxiv.org/abs/2312.08136) #memory</code></li>
<li>Summary: <p>Recent advances in neural rendering have shown that, albeit slow, implicit
compact models can learn a scene's geometries and view-dependent appearances
from multiple views. To maintain such a small memory footprint but achieve
faster inference times, recent works have adopted `sampler' networks that
adaptively sample a small subset of points along each ray in the implicit
neural radiance fields. Although these methods achieve up to a 10$\times$
reduction in rendering time, they still suffer from considerable quality
degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,
which provides an optimal trade-off between memory footprint (similar to NeRF),
speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is
equipped with a novel projection-aware sampling (PAS) network together with a
new training strategy for ray exploration and exploitation, allowing for
efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art
metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding
0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our
exploration and exploitation training strategy allows ProNeRF to learn the full
scenes' color and density distributions while also learning efficient ray
sampling focused on the highest-density regions. We provide extensive
experimental results that support the effectiveness of our method on the widely
adopted forward-facing and 360 datasets, LLFF and Blender, respectively.
</p></li>
</ul>
<h3>Title: PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection. (arXiv:2312.08371v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08371">http://arxiv.org/abs/2312.08371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08371] PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection](http://arxiv.org/abs/2312.08371) #memory</code></li>
<li>Summary: <p>Recent temporal LiDAR-based 3D object detectors achieve promising performance
based on the two-stage proposal-based approach. They generate 3D box candidates
from the first-stage dense detector, followed by different temporal aggregation
methods. However, these approaches require per-frame objects or whole point
clouds, posing challenges related to memory bank utilization. Moreover, point
clouds and trajectory features are combined solely based on concatenation,
which may neglect effective interactions between them. In this paper, we
propose a point-trajectory transformer with long short-term memory for
efficient temporal 3D object detection. To this end, we only utilize point
clouds of current-frame objects and their historical trajectories as input to
minimize the memory bank storage requirement. Furthermore, we introduce modules
to encode trajectory features, focusing on long short-term and future-aware
perspectives, and then effectively aggregate them with point cloud features. We
conduct extensive experiments on the large-scale Waymo dataset to demonstrate
that our approach performs well against state-of-the-art methods. Code and
models will be made publicly available at https://github.com/kuanchihhuang/PTT.
</p></li>
</ul>
<h3>Title: FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems. (arXiv:2312.07743v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07743">http://arxiv.org/abs/2312.07743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07743] FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems](http://arxiv.org/abs/2312.07743) #memory</code></li>
<li>Summary: <p>Word2Vec remains one of the highly-impactful innovations in the field of
Natural Language Processing (NLP) that represents latent grammatical and
syntactical information in human text with dense vectors in a low dimension.
Word2Vec has high computational cost due to the algorithm's inherent
sequentiality, intensive memory accesses, and the large vocabularies it
represents. While prior studies have investigated technologies to explore
parallelism and improve memory system performance, they struggle to effectively
gain throughput on powerful GPUs.
</p></li>
</ul>
<p>We identify memory data access and latency as the primary bottleneck in prior
works on GPUs, which prevents highly optimized kernels from attaining the
architecture's peak performance. We present a novel algorithm, FULL-W2V, which
maximally exploits the opportunities for data reuse in the W2V algorithm and
leverages GPU architecture and resources to reduce access to low memory levels
and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU
global memory significantly, e.g., by more than 89\%, compared to prior
state-of-the-art GPU implementations, resulting in significant performance
improvement that scales across successive hardware generations. Our prototype
implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to
Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards
with the same embedding quality. In-depth analysis indicates that the reduction
of memory accesses through register and shared memory caching and
high-throughput shared memory reduction leads to a significantly improved
arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP
and other domains.
</p>

<h3>Title: Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?. (arXiv:2312.07797v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07797">http://arxiv.org/abs/2312.07797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07797] Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?](http://arxiv.org/abs/2312.07797) #memory</code></li>
<li>Summary: <p>Undoubtedly that the Bidirectional Encoder representations from Transformers
is the most powerful technique in making Natural Language Processing tasks such
as Named Entity Recognition, Question &amp; Answers or Sentiment Analysis, however,
the use of traditional techniques remains a major potential for the improvement
of recent models, in particular word tokenization techniques and embeddings,
but also the improvement of neural network architectures which are now the core
of each architecture. recent. In this paper, we conduct a comparative study
between Fine-Tuning the Bidirectional Encoder Representations from Transformers
and a method of concatenating two embeddings to boost the performance of a
stacked Bidirectional Long Short-Term Memory-Bidirectional Gated Recurrent
Units model; these two approaches are applied in the context of sentiment
analysis of shopping places in Morocco. A search for the best learning rate was
made at the level of the two approaches, and a comparison of the best
optimizers was made for each sentence embedding combination with regard to the
second approach.
</p></li>
</ul>
<h3>Title: SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention. (arXiv:2312.07987v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07987">http://arxiv.org/abs/2312.07987</a></li>
<li>Code URL: <a href="https://github.com/robertcsordas/moe_attention">https://github.com/robertcsordas/moe_attention</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07987] SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention](http://arxiv.org/abs/2312.07987) #memory</code></li>
<li>Summary: <p>The costly self-attention layers in modern Transformers require memory and
compute quadratic in sequence length. Existing approximation methods usually
underperform and fail to obtain significant speedups in practice. Here we
present SwitchHead - a novel method that reduces both compute and memory
requirements and achieves wall-clock speedup, while matching the language
modeling performance of baseline Transformers with the same parameter budget.
SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output
projections and requires 4 to 8 times fewer attention matrices than standard
Transformers. Our novel attention can also be combined with MoE MLP layers,
resulting in an efficient fully-MoE "SwitchHead" Transformer model. Our code is
public.
</p></li>
</ul>
<h3>Title: Okapi: A Lightweight Architecture for Secure Speculation Exploiting Locality of Memory Accesses. (arXiv:2312.08156v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08156">http://arxiv.org/abs/2312.08156</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08156] Okapi: A Lightweight Architecture for Secure Speculation Exploiting Locality of Memory Accesses](http://arxiv.org/abs/2312.08156) #memory</code></li>
<li>Summary: <p>This paper introduces Okapi, an innovative hardware/software cross-layer
architecture designed to mitigate Transient Execution Side Channel (TES)
attacks, including Spectre variants, in modern computing systems. A key
contribution of Okapi is a set of security features building upon each other to
offer various trade-offs between performance and security. At its core, Okapi
allows for speculative data accesses if the targeted memory region has already
been accessed non-speculatively before in the same trust domain. It delays
first-time accesses until the speculation is resolved.
</p></li>
</ul>
<p>Okapi stands out for its flexibility in security implementation. For
environments with less stringent security needs, Okapi's features can be
deactivated to eliminate performance overhead. When activated, the hardware
modifications alone provide robust protection against transient execution
attacks at a thread-level granularity, including all universal read gadgets
like Spectre-PHT and Spectre-BTB. This incurs an average performance overhead
of only 3.6 % for the SPEC CPU2017 benchmark suite.
</p>
<p>On top, Okapi introduces the OkapiReset instruction for additional
software-level security support. This instruction, which can be manually
inserted by developers or automatically via a compiler extension, allows for
fully secure speculation and for trust domain sizes smaller than a thread.
While the manual insertion of OkapiReset incurs an additional 0.6 % performance
overhead, the automated compiler extension approach results in a 23.1 %
overhead for making a cryptographic library fully secure. With an approximate
0.4 % hardware overhead, Okapi provides a highly scalable and adaptable
solution for secure speculation in state-of-the-art processor design.
</p>

<h3>Title: IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable $k$-Means. (arXiv:2312.07759v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07759">http://arxiv.org/abs/2312.07759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07759] IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable $k$-Means](http://arxiv.org/abs/2312.07759) #memory</code></li>
<li>Summary: <p>Compressing large neural networks with minimal performance loss is crucial to
enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight
quantization method that uses an attention-based clustering algorithm called
differentiable $k$-means (DKM). Despite achieving state-of-the-art results,
DKM's performance is constrained by its heavy memory dependency. We propose an
implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major
memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$
be the number of weight-vectors, and $b$ be the number of bits per cluster
address. IDKM reduces the overall memory complexity of a single $k$-means layer
from $\mathcal{O}(t \cdot m \cdot 2^b)$ to $\mathcal{O}( m \cdot 2^b)$. We also
introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for
which the time complexity of the gradient calculation is independent of $t$ as
well. We provide a proof of concept of our methods by showing that, under the
same settings, IDKM achieves comparable performance to DKM with less compute
time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural
network, Resnet18, on hardware where DKM cannot train at all.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08010">http://arxiv.org/abs/2312.08010</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08010] EZ-CLIP: Efficient Zeroshot Video Action Recognition](http://arxiv.org/abs/2312.08010) #few-shot</code></li>
<li>Summary: <p>Recent advancements in large-scale pre-training of visual-language models on
paired image-text data have demonstrated impressive generalization capabilities
for zero-shot tasks. Building on this success, efforts have been made to adapt
these image-based visual-language models, such as CLIP, for videos extending
their zero-shot capabilities to the video domain. While these adaptations have
shown promising results, they come at a significant computational cost and
struggle with effectively modeling the crucial temporal aspects inherent to the
video domain. In this study, we present EZ-CLIP, a simple and efficient
adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal
visual prompting for seamless temporal adaptation, requiring no fundamental
alterations to the core CLIP architecture while preserving its remarkable
generalization abilities. Moreover, we introduce a novel learning objective
that guides the temporal visual prompts to focus on capturing motion, thereby
enhancing its learning capabilities from video data. We conducted extensive
experiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP
for zero-shot learning and base-to-novel video action recognition, and also
demonstrating its potential for few-shot generalization.Impressively, with a
mere 5.2 million learnable parameters (as opposed to the 71.1 million in the
prior best model), EZ-CLIP can be efficiently trained on a single GPU,
outperforming existing approaches in several evaluations.
</p></li>
</ul>
<h3>Title: LAMM: Label Alignment for Multi-Modal Prompt Learning. (arXiv:2312.08212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08212">http://arxiv.org/abs/2312.08212</a></li>
<li>Code URL: <a href="https://github.com/gaojingsheng/lamm">https://github.com/gaojingsheng/lamm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08212] LAMM: Label Alignment for Multi-Modal Prompt Learning](http://arxiv.org/abs/2312.08212) #few-shot</code></li>
<li>Summary: <p>With the success of pre-trained visual-language (VL) models such as CLIP in
visual representation tasks, transferring pre-trained models to downstream
tasks has become a crucial paradigm. Recently, the prompt tuning paradigm,
which draws inspiration from natural language processing (NLP), has made
significant progress in VL field. However, preceding methods mainly focus on
constructing prompt templates for text and visual inputs, neglecting the gap in
class label representations between the VL models and downstream tasks. To
address this challenge, we introduce an innovative label alignment method named
\textbf{LAMM}, which can dynamically adjust the category embeddings of
downstream datasets through end-to-end training. Moreover, to achieve a more
appropriate label distribution, we propose a hierarchical loss, encompassing
the alignment of the parameter space, feature space, and logits space. We
conduct experiments on 11 downstream vision datasets and demonstrate that our
method significantly improves the performance of existing multi-modal prompt
learning models in few-shot scenarios, exhibiting an average accuracy
improvement of 2.31(\%) compared to the state-of-the-art methods on 16 shots.
Moreover, our methodology exhibits the preeminence in continual learning
compared to other prompt tuning methods. Importantly, our method is synergistic
with existing prompt tuning methods and can boost the performance on top of
them. Our code and dataset will be publicly available at
https://github.com/gaojingsheng/LAMM.
</p></li>
</ul>
<h3>Title: Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue. (arXiv:2312.07868v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07868">http://arxiv.org/abs/2312.07868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07868] Graph vs](http://arxiv.org/abs/2312.07868) #few-shot</code></li>
<li>Summary: <p>Knowledge-grounded dialogue is a task of generating an informative response
based on both the dialogue history and external knowledge source. In general,
there are two forms of knowledge: manually annotated knowledge graphs and
knowledge text from website. From various evaluation viewpoints, each type of
knowledge has advantages and downsides. To further distinguish the principles
and determinants from the intricate factors, we conduct a thorough experiment
and study on the task to answer three essential questions. The questions
involve the choice of appropriate knowledge form, the degree of mutual effects
between knowledge and the model selection, and the few-shot performance of
knowledge. Supported by statistical shreds of evidence, we offer conclusive
solutions and sensible suggestions for directions and standards of future
research.
</p></li>
</ul>
<h3>Title: Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification. (arXiv:2312.07961v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07961">http://arxiv.org/abs/2312.07961</a></li>
<li>Code URL: <a href="https://github.com/ckgconstruction/bdcp">https://github.com/ckgconstruction/bdcp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07961] Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification](http://arxiv.org/abs/2312.07961) #few-shot</code></li>
<li>Summary: <p>Few-shot named entity recognition (NER) aims to recognize novel named
entities in low-resource domains utilizing existing knowledge. However, the
present few-shot NER models assume that the labeled data are all clean without
noise or outliers, and there are few works focusing on the robustness of the
cross-domain transfer learning ability to textual adversarial attacks in
Few-shot NER. In this work, we comprehensively explore and assess the
robustness of few-shot NER models under textual adversarial attack scenario,
and found the vulnerability of existing few-shot NER models. Furthermore, we
propose a robust two-stage few-shot NER method with Boundary Discrimination and
Correlation Purification (BDCP). Specifically, in the span detection stage, the
entity boundary discriminative module is introduced to provide a highly
distinguishing boundary representation space to detect entity spans. In the
entity typing stage, the correlations between entities and contexts are
purified by minimizing the interference information and facilitating
correlation generalization to alleviate the perturbations caused by textual
adversarial attacks. In addition, we construct adversarial examples for
few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive
evaluations on those two groups of few-shot NER datasets containing adversarial
examples demonstrate the robustness and superiority of the proposed method.
</p></li>
</ul>
<h3>Title: Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning. (arXiv:2312.08027v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08027">http://arxiv.org/abs/2312.08027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08027] Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning](http://arxiv.org/abs/2312.08027) #few-shot</code></li>
<li>Summary: <p>Large language models (LLMs) can be used as accessible and intelligent
chatbots by constructing natural language queries and directly inputting the
prompt into the large language model. However, different prompt' constructions
often lead to uncertainty in the answers and thus make it hard to utilize the
specific knowledge of LLMs (like ChatGPT). To alleviate this, we use an
interpretable structure to explain the prompt learning principle in LLMs, which
certificates that the effectiveness of language models is determined by
position changes of the task's related tokens. Therefore, we propose MTPrompt,
a multi-dimensional task prompt learning method consisting based on
task-related object, summary, and task description information. By
automatically building and searching for appropriate prompts, our proposed
MTPrompt achieves the best results on few-shot samples setting and five
different datasets. In addition, we demonstrate the effectiveness and stability
of our method in different experimental settings and ablation experiments. In
interaction with large language models, embedding more task-related information
into prompts will make it easier to stimulate knowledge embedded in large
language models.
</p></li>
</ul>
<h3>Title: Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4. (arXiv:2312.08317v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.08317">http://arxiv.org/abs/2312.08317</a></li>
<li>Code URL: <a href="https://github.com/yan-scnu/prompted_dynamic_detection">https://github.com/yan-scnu/prompted_dynamic_detection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.08317] Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4](http://arxiv.org/abs/2312.08317) #few-shot</code></li>
<li>Summary: <p>Dynamic analysis methods effectively identify shelled, wrapped, or obfuscated
malware, thereby preventing them from invading computers. As a significant
representation of dynamic malware behavior, the API (Application Programming
Interface) sequence, comprised of consecutive API calls, has progressively
become the dominant feature of dynamic analysis methods. Though there have been
numerous deep learning models for malware detection based on API sequences, the
quality of API call representations produced by those models is limited. These
models cannot generate representations for unknown API calls, which weakens
both the detection performance and the generalization. Further, the concept
drift phenomenon of API calls is prominent. To tackle these issues, we
introduce a prompt engineering-assisted malware dynamic analysis using GPT-4.
In this method, GPT-4 is employed to create explanatory text for each API call
within the API sequence. Afterward, the pre-trained language model BERT is used
to obtain the representation of the text, from which we derive the
representation of the API sequence. Theoretically, this proposed method is
capable of generating representations for all API calls, excluding the
necessity for dataset training during the generation process. Utilizing the
representation, a CNN-based detection model is designed to extract the feature.
We adopt five benchmark datasets to validate the performance of the proposed
model. The experimental results reveal that the proposed detection algorithm
performs better than the state-of-the-art method (TextCNN). Specifically, in
cross-database experiments and few-shot learning experiments, the proposed
model achieves excellent detection performance and almost a 100% recall rate
for malware, verifying its superior generalization performance. The code is
available at: github.com/yan-scnu/Prompted_Dynamic_Detection.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-12-14]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
