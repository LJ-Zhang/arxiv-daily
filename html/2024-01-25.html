<h2>diffusion</h2>
<h3>Title: Towards Multi-domain Face Landmark Detection with Synthetic Data from Diffusion model. (arXiv:2401.13191v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13191">http://arxiv.org/abs/2401.13191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13191] Towards Multi-domain Face Landmark Detection with Synthetic Data from Diffusion model](http://arxiv.org/abs/2401.13191) #diffusion</code></li>
<li>Summary: <p>Recently, deep learning-based facial landmark detection for in-the-wild faces
has achieved significant improvement. However, there are still challenges in
face landmark detection in other domains (e.g. cartoon, caricature, etc). This
is due to the scarcity of extensively annotated training data. To tackle this
concern, we design a two-stage training approach that effectively leverages
limited datasets and the pre-trained diffusion model to obtain aligned pairs of
landmarks and face in multiple domains. In the first stage, we train a
landmark-conditioned face generation model on a large dataset of real faces. In
the second stage, we fine-tune the above model on a small dataset of
image-landmark pairs with text prompts for controlling the domain. Our new
designs enable our method to generate high-quality synthetic paired datasets
from multiple domains while preserving the alignment between landmarks and
facial features. Finally, we fine-tuned a pre-trained face landmark detection
model on the synthetic dataset to achieve multi-domain face landmark detection.
Our qualitative and quantitative results demonstrate that our method
outperforms existing methods on multi-domain face landmark detection.
</p></li>
</ul>
<h3>Title: Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval. (arXiv:2401.13329v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13329">http://arxiv.org/abs/2401.13329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13329] Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval](http://arxiv.org/abs/2401.13329) #diffusion</code></li>
<li>Summary: <p>Video Moment Retrieval (VMR) requires precise modelling of fine-grained
moment-text associations to capture intricate visual-language relationships.
Due to the lack of a diverse and generalisable VMR dataset to facilitate
learning scalable moment-text associations, existing methods resort to joint
training on both source and target domain videos for cross-domain applications.
Meanwhile, recent developments in vision-language multimodal models pre-trained
on large-scale image-text and/or video-text pairs are only based on coarse
associations (weakly labelled). They are inadequate to provide fine-grained
moment-text correlations required for cross-domain VMR. In this work, we solve
the problem of unseen cross-domain VMR, where certain visual and textual
concepts do not overlap across domains, by only utilising target domain
sentences (text prompts) without accessing their videos. To that end, we
explore generative video diffusion for fine-grained editing of source videos
controlled by the target sentences, enabling us to simulate target domain
videos. We address two problems in video editing for optimising unseen domain
VMR: (1) generation of high-quality simulation videos of different moments with
subtle distinctions, (2) selection of simulation videos that complement
existing source training videos without introducing harmful noise or
unnecessary repetitions. On the first problem, we formulate a two-stage video
diffusion generation controlled simultaneously by (1) the original video
structure of a source video, (2) subject specifics, and (3) a target sentence
prompt. This ensures fine-grained variations between video moments. On the
second problem, we introduce a hybrid selection mechanism that combines two
quantitative metrics for noise filtering and one qualitative metric for
leveraging VMR prediction on simulation video selection.
</p></li>
</ul>
<h3>Title: UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion. (arXiv:2401.13388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13388">http://arxiv.org/abs/2401.13388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13388] UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion](http://arxiv.org/abs/2401.13388) #diffusion</code></li>
<li>Summary: <p>Existing text-to-image diffusion models primarily generate images from text
prompts. However, the inherent conciseness of textual descriptions poses
challenges in faithfully synthesizing images with intricate details, such as
specific entities or scenes. This paper presents \textbf{UNIMO-G}, a simple
multimodal conditional diffusion framework that operates on multimodal prompts
with interleaved textual and visual inputs, which demonstrates a unified
ability for both text-driven and subject-driven image generation. UNIMO-G
comprises two core components: a Multimodal Large Language Model (MLLM) for
encoding multimodal prompts, and a conditional denoising diffusion network for
generating images based on the encoded multimodal input. We leverage a
two-stage training strategy to effectively train the framework: firstly
pre-training on large-scale text-image pairs to develop conditional image
generation capabilities, and then instruction tuning with multimodal prompts to
achieve unified image generation proficiency. A well-designed data processing
pipeline involving language grounding and image segmentation is employed to
construct multi-modal prompts. UNIMO-G excels in both text-to-image generation
and zero-shot subject-driven synthesis, and is notably effective in generating
high-fidelity images from complex multimodal prompts involving multiple image
entities.
</p></li>
</ul>
<h3>Title: Contractive Diffusion Probabilistic Models. (arXiv:2401.13115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13115">http://arxiv.org/abs/2401.13115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13115] Contractive Diffusion Probabilistic Models](http://arxiv.org/abs/2401.13115) #diffusion</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have emerged as a promising technology
in generative modeling. The success of DPMs relies on two ingredients: time
reversal of Markov diffusion processes and score matching. Most existing work
implicitly assumes that score matching is close to perfect, while this
assumption is questionable. In view of possibly unguaranteed score matching, we
propose a new criterion -- the contraction of backward sampling in the design
of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including
contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance
preserving (sub-VP) stochastic differential equations (SDEs). The key insight
is that the contraction in the backward process narrows score matching errors,
as well as discretization error. Thus, the proposed CDPMs are robust to both
sources of error. Our proposal is supported by theoretical results, and is
corroborated by experiments. Notably, contractive sub-VP shows the best
performance among all known SDE-based DPMs on the CIFAR-10 dataset.
</p></li>
</ul>
<h3>Title: RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing. (arXiv:2401.13282v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13282">http://arxiv.org/abs/2401.13282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13282] RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing](http://arxiv.org/abs/2401.13282) #diffusion</code></li>
<li>Summary: <p>Forecasting complex system dynamics, particularly for long-term predictions,
is persistently hindered by error accumulation and computational burdens. This
study presents RefreshNet, a multiscale framework developed to overcome these
challenges, delivering an unprecedented balance between computational
efficiency and predictive accuracy. RefreshNet incorporates convolutional
autoencoders to identify a reduced order latent space capturing essential
features of the dynamics, and strategically employs multiple recurrent neural
network (RNN) blocks operating at varying temporal resolutions within the
latent space, thus allowing the capture of latent dynamics at multiple temporal
scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks
to reset inputs of finer blocks, effectively controlling and alleviating error
accumulation. This design demonstrates superiority over existing techniques
regarding computational efficiency and predictive accuracy, especially in
long-term forecasting. The framework is validated using three benchmark
applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and
Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms
state-of-the-art methods in long-term forecasting accuracy and speed, marking a
significant advancement in modeling complex systems and opening new avenues in
understanding and predicting their behavior.
</p></li>
</ul>
<h3>Title: Classification of Radiologically Isolated Syndrome and Clinically Isolated Syndrome with Machine-Learning Techniques. (arXiv:2401.13301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13301">http://arxiv.org/abs/2401.13301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13301] Classification of Radiologically Isolated Syndrome and Clinically Isolated Syndrome with Machine-Learning Techniques](http://arxiv.org/abs/2401.13301) #diffusion</code></li>
<li>Summary: <p>Background and purpose: The unanticipated detection by magnetic resonance
imaging (MRI) in the brain of asymptomatic subjects of white matter lesions
suggestive of multiple sclerosis (MS) has been named radiologically isolated
syndrome (RIS). As the difference between early MS [i.e. clinically isolated
syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to
improve detection of the subclinical form without interfering with MRI as there
are radiological diagnostic criteria for that. Our objective was to use
machine-learning classification methods to identify morphometric measures that
help to discriminate patients with RIS from those with CIS.
</p></li>
</ul>
<p>Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers
(cortical thickness, cortical and subcortical grey matter volume, and white
matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS
for single-subject level classification.
</p>
<p>Results: The best proposed models to predict the diagnosis of CIS and RIS
were based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers
using only three features: the left rostral middle frontal gyrus volume and the
fractional anisotropy values in the right amygdala and right lingual gyrus. The
Naive Bayes obtained the highest accuracy [overall classification, 0.765; area
under the receiver operating characteristic (AUROC), 0.782].
</p>
<p>Conclusions: A machine-learning approach applied to multimodal MRI data may
differentiate between the earliest clinical expressions of MS (CIS and RIS)
with an accuracy of 78%.
</p>
<p>Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically
isolated syndrome; diffusion tensor imaging; machine-learning; magnetic
resonance imaging; multiple sclerosis; radiologically isolated syndrome.
</p>

<h2>self-supervised</h2>
<h3>Title: Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13270">http://arxiv.org/abs/2401.13270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13270] Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics](http://arxiv.org/abs/2401.13270) #self-supervised</code></li>
<li>Summary: <p>Automatic image colorization is inherently an ill-posed problem with
uncertainty, which requires an accurate semantic understanding of scenes to
estimate reasonable colors for grayscale images. Although recent
interaction-based methods have achieved impressive performance, it is still a
very difficult task to infer realistic and accurate colors for automatic
colorization. To reduce the difficulty of semantic understanding of grayscale
scenes, this paper tries to utilize corresponding audio, which naturally
contains extra semantic information about the same scene. Specifically, a novel
audio-infused automatic image colorization (AIAIC) network is proposed, which
consists of three stages. First, we take color image semantics as a bridge and
pretrain a colorization network guided by color image semantics. Second, the
natural co-occurrence of audio and video is utilized to learn the color
semantic correlations between audio and visual scenes. Third, the implicit
audio semantic representation is fed into the pretrained network to finally
realize the audio-guided colorization. The whole process is trained in a
self-supervised manner without human annotation. In addition, an audiovisual
colorization dataset is established for training and testing. Experiments
demonstrate that audio guidance can effectively improve the performance of
automatic colorization, especially for some scenes that are difficult to
understand only from visual modality.
</p></li>
</ul>
<h3>Title: Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation. (arXiv:2401.13581v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13581">http://arxiv.org/abs/2401.13581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13581] Towards Efficient and Effective Deep Clustering with Dynamic Grouping and Prototype Aggregation](http://arxiv.org/abs/2401.13581) #self-supervised</code></li>
<li>Summary: <p>Previous contrastive deep clustering methods mostly focus on instance-level
information while overlooking the member relationship within groups/clusters,
which may significantly undermine their representation learning and clustering
capability. Recently, some group-contrastive methods have been developed,
which, however, typically rely on the samples of the entire dataset to obtain
pseudo labels and lack the ability to efficiently update the group assignments
in a batch-wise manner. To tackle these critical issues, we present a novel
end-to-end deep clustering framework with dynamic grouping and prototype
aggregation, termed as DigPro. Specifically, the proposed dynamic grouping
extends contrastive learning from instance-level to group-level, which is
effective and efficient for timely updating groups. Meanwhile, we perform
contrastive learning on prototypes in a spherical feature space, termed as
prototype aggregation, which aims to maximize the inter-cluster distance.
Notably, with an expectation-maximization framework, DigPro simultaneously
takes advantage of compact intra-cluster connections, well-separated clusters,
and efficient group updating during the self-supervised training. Extensive
experiments on six image benchmarks demonstrate the superior performance of our
approach over the state-of-the-art. Code is available at
https://github.com/Regan-Zhang/DigPro.
</p></li>
</ul>
<h3>Title: Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13227">http://arxiv.org/abs/2401.13227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13227] Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models](http://arxiv.org/abs/2401.13227) #self-supervised</code></li>
<li>Summary: <p>Exploring the application of large-scale language models to graph learning is
a novel endeavor. However, the vast amount of information inherent in large
graphs poses significant challenges to this process. This paper focuses on the
link prediction task and introduces LPNL (Link Prediction via Natural
Language), a framework based on a large language model designed for scalable
link prediction on large-scale heterogeneous graphs.We design novel prompts for
link prediction that articulate graph details in natural language. We propose a
two-stage sampling pipeline to extract crucial information from large-scale
heterogeneous graphs, and a divide-and-conquer strategy to control the input
token count within predefined limits, addressing the challenge of overwhelming
information. We fine-tune a T5 model based on our self-supervised learning
designed for for link prediction. Extensive experiments on a large public
heterogeneous graphs demonstrate that LPNL outperforms various advanced
baselines, highlighting its remarkable performance in link prediction tasks on
large-scale graphs.
</p></li>
</ul>
<h3>Title: DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning. (arXiv:2401.13621v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13621">http://arxiv.org/abs/2401.13621</a></li>
<li>Code URL: <a href="https://github.com/xinghaow99/denosent">https://github.com/xinghaow99/denosent</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13621] DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning](http://arxiv.org/abs/2401.13621) #self-supervised</code></li>
<li>Summary: <p>Contrastive-learning-based methods have dominated sentence representation
learning. These methods regularize the representation space by pulling similar
sentence representations closer and pushing away the dissimilar ones and have
been proven effective in various NLP tasks, e.g., semantic textual similarity
(STS) tasks. However, it is challenging for these methods to learn fine-grained
semantics as they only learn from the inter-sentence perspective, i.e., their
supervision signal comes from the relationship between data samples. In this
work, we propose a novel denoising objective that inherits from another
perspective, i.e., the intra-sentence perspective. By introducing both discrete
and continuous noise, we generate noisy sentences and then train our model to
restore them to their original form. Our empirical evaluations demonstrate that
this approach delivers competitive results on both semantic textual similarity
(STS) and a wide range of transfer tasks, standing up well in comparison to
contrastive-learning-based methods. Notably, the proposed intra-sentence
denoising objective complements existing inter-sentence contrastive
methodologies and can be integrated with them to further enhance performance.
Our code is available at https://github.com/xinghaow99/DenoSent.
</p></li>
</ul>
<h3>Title: Adaptive Crowdsourcing Via Self-Supervised Learning. (arXiv:2401.13239v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13239">http://arxiv.org/abs/2401.13239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13239] Adaptive Crowdsourcing Via Self-Supervised Learning](http://arxiv.org/abs/2401.13239) #self-supervised</code></li>
<li>Summary: <p>Common crowdsourcing systems average estimates of a latent quantity of
interest provided by many crowdworkers to produce a group estimate. We develop
a new approach -- just-predict-others -- that leverages self-supervised
learning and a novel aggregation scheme. This approach adapts weights assigned
to crowdworkers based on estimates they provided for previous quantities. When
skills vary across crowdworkers or their estimates correlate, the weighted sum
offers a more accurate group estimate than the average. Existing algorithms
such as expectation maximization can, at least in principle, produce similarly
accurate group estimates. However, their computational requirements become
onerous when complex models, such as neural networks, are required to express
relationships among crowdworkers. Just-predict-others accommodates such
complexity as well as many other practical challenges. We analyze the efficacy
of just-predict-others through theoretical and computational studies. Among
other things, we establish asymptotic optimality as the number of engagements
per crowdworker grows.
</p></li>
</ul>
<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: CCA: Collaborative Competitive Agents for Image Editing. (arXiv:2401.13011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13011">http://arxiv.org/abs/2401.13011</a></li>
<li>Code URL: <a href="https://github.com/tiankaihang/cca">https://github.com/tiankaihang/cca</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13011] CCA: Collaborative Competitive Agents for Image Editing](http://arxiv.org/abs/2401.13011) #generative</code></li>
<li>Summary: <p>This paper presents a novel generative model, Collaborative Competitive
Agents (CCA), which leverages the capabilities of multiple Large Language
Models (LLMs) based agents to execute complex tasks. Drawing inspiration from
Generative Adversarial Networks (GANs), the CCA system employs two equal-status
generator agents and a discriminator agent. The generators independently
process user instructions and generate results, while the discriminator
evaluates the outputs, and provides feedback for the generator agents to
further reflect and improve the generation results. Unlike the previous
generative model, our system can obtain the intermediate steps of generation.
This allows each generator agent to learn from other successful executions due
to its transparency, enabling a collaborative competition that enhances the
quality and robustness of the system's results. The primary focus of this study
is image editing, demonstrating the CCA's ability to handle intricate
instructions robustly. The paper's main contributions include the introduction
of a multi-agent-based generative model with controllable intermediate steps
and iterative optimization, a detailed examination of agent relationships, and
comprehensive experiments on image editing. Code is available at
\href{https://github.com/TiankaiHang/CCA}{https://github.com/TiankaiHang/CCA}.
</p></li>
</ul>
<h3>Title: Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size. (arXiv:2401.13205v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13205">http://arxiv.org/abs/2401.13205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13205] Boosting the Transferability of Adversarial Examples via Local Mixup and Adaptive Step Size](http://arxiv.org/abs/2401.13205) #generative</code></li>
<li>Summary: <p>Adversarial examples are one critical security threat to various visual
applications, where injected human-imperceptible perturbations can confuse the
output.Generating transferable adversarial examples in the black-box setting is
crucial but challenging in practice. Existing input-diversity-based methods
adopt different image transformations, but may be inefficient due to
insufficient input diversity and an identical perturbation step size. Motivated
by the fact that different image regions have distinctive weights in
classification, this paper proposes a black-box adversarial generative
framework by jointly designing enhanced input diversity and adaptive step
sizes. We design local mixup to randomly mix a group of transformed adversarial
images, strengthening the input diversity. For precise adversarial generation,
we project the perturbation into the $tanh$ space to relax the boundary
constraint. Moreover, the step sizes of different regions can be dynamically
adjusted by integrating a second-order momentum.Extensive experiments on
ImageNet validate that our framework can achieve superior transferability
compared to state-of-the-art baselines.
</p></li>
</ul>
<h3>Title: Dual-modal Dynamic Traceback Learning for Medical Report Generation. (arXiv:2401.13267v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13267">http://arxiv.org/abs/2401.13267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13267] Dual-modal Dynamic Traceback Learning for Medical Report Generation](http://arxiv.org/abs/2401.13267) #generative</code></li>
<li>Summary: <p>With increasing reliance on medical imaging in clinical practices, automated
report generation from medical images is in great demand. Existing report
generation methods typically adopt an encoder-decoder deep learning framework
to build a uni-directional image-to-report mapping. However, such a framework
ignores the bi-directional mutual associations between images and reports, thus
incurring difficulties in associating the intrinsic medical meanings between
them. Recent generative representation learning methods have demonstrated the
benefits of dual-modal learning from both image and text modalities. However,
these methods exhibit two major drawbacks for medical report generation: 1)
they tend to capture morphological information and have difficulties in
capturing subtle pathological semantic information, and 2) they predict masked
text rely on both unmasked images and text, inevitably degrading performance
when inference is based solely on images. In this study, we propose a new
report generation framework with dual-modal dynamic traceback learning (DTrace)
to overcome the two identified drawbacks and enable dual-modal learning for
medical report generation. To achieve this, our DTrace introduces a traceback
mechanism to control the semantic validity of generated content via
self-assessment. Further, our DTrace introduces a dynamic learning strategy to
adapt to various proportions of image and text input, enabling report
generation without reliance on textual input during inference. Extensive
experiments on two well-benchmarked datasets (IU-Xray and MIMIC-CXR) show that
our DTrace outperforms state-of-the-art medical report generation methods.
</p></li>
</ul>
<h3>Title: Generative Human Motion Stylization in Latent Space. (arXiv:2401.13505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13505">http://arxiv.org/abs/2401.13505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13505] Generative Human Motion Stylization in Latent Space](http://arxiv.org/abs/2401.13505) #generative</code></li>
<li>Summary: <p>Human motion stylization aims to revise the style of an input motion while
keeping its content unaltered. Unlike existing works that operate directly in
pose space, we leverage the latent space of pretrained autoencoders as a more
expressive and robust representation for motion extraction and infusion.
Building upon this, we present a novel generative model that produces diverse
stylization results of a single motion (latent) code. During training, a motion
code is decomposed into two coding components: a deterministic content code,
and a probabilistic style code adhering to a prior distribution; then a
generator massages the random combination of content and style codes to
reconstruct the corresponding motion codes. Our approach is versatile, allowing
the learning of probabilistic style space from either style labeled or
unlabeled motions, providing notable flexibility in stylization as well. In
inference, users can opt to stylize a motion using style cues from a reference
motion or a label. Even in the absence of explicit style input, our model
facilitates novel re-stylization by sampling from the unconditional style prior
distribution. Experimental results show that our proposed stylization models,
despite their lightweight design, outperform the state-of-the-arts in style
reeanactment, content preservation, and generalization across various
applications and settings. Project Page: https://yxmu.foo/GenMoStyle
</p></li>
</ul>
<h3>Title: Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13555">http://arxiv.org/abs/2401.13555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13555] Benchmarking the Fairness of Image Upsampling Methods](http://arxiv.org/abs/2401.13555) #generative</code></li>
<li>Summary: <p>Recent years have witnessed a rapid development of deep generative models for
creating synthetic media, such as images and videos. While the practical
applications of these models in everyday tasks are enticing, it is crucial to
assess the inherent risks regarding their fairness. In this work, we introduce
a comprehensive framework for benchmarking the performance and fairness of
conditional generative models. We develop a set of
metrics$\unicode{x2013}$inspired by their supervised fairness
counterparts$\unicode{x2013}$to evaluate the models on their fairness and
diversity. Focusing on the specific application of image upsampling, we create
a benchmark covering a wide variety of modern upsampling methods. As part of
the benchmark, we introduce UnfairFace, a subset of FairFace that replicates
the racial distribution of common large-scale face datasets. Our empirical
study highlights the importance of using an unbiased training set and reveals
variations in how the algorithms respond to dataset imbalances. Alarmingly, we
find that none of the considered methods produces statistically fair and
diverse results.
</p></li>
</ul>
<h3>Title: Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild. (arXiv:2401.13627v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13627">http://arxiv.org/abs/2401.13627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13627] Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild](http://arxiv.org/abs/2401.13627) #generative</code></li>
<li>Summary: <p>We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image
restoration method that harnesses generative prior and the power of model
scaling up. Leveraging multi-modal techniques and advanced generative prior,
SUPIR marks a significant advance in intelligent and realistic image
restoration. As a pivotal catalyst within SUPIR, model scaling dramatically
enhances its capabilities and demonstrates new potential for image restoration.
We collect a dataset comprising 20 million high-resolution, high-quality images
for model training, each enriched with descriptive text annotations. SUPIR
provides the capability to restore images guided by textual prompts, broadening
its application scope and potential. Moreover, we introduce negative-quality
prompts to further improve perceptual quality. We also develop a
restoration-guided sampling method to suppress the fidelity issue encountered
in generative-based restoration. Experiments demonstrate SUPIR's exceptional
restoration effects and its novel capacity to manipulate restoration through
textual prompts.
</p></li>
</ul>
<h3>Title: Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection. (arXiv:2401.13327v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13327">http://arxiv.org/abs/2401.13327</a></li>
<li>Code URL: <a href="https://github.com/luckyos-code/privacy-preserving-smartwatch-health-data-generation-using-dp-gans">https://github.com/luckyos-code/privacy-preserving-smartwatch-health-data-generation-using-dp-gans</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13327] Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](http://arxiv.org/abs/2401.13327) #generative</code></li>
<li>Summary: <p>Smartwatch health sensor data is increasingly utilized in smart health
applications and patient monitoring, including stress detection. However, such
medical data often comprises sensitive personal information and is
resource-intensive to acquire for research purposes. In response to this
challenge, we introduce the privacy-aware synthetization of multi-sensor
smartwatch health readings related to moments of stress. Our method involves
the generation of synthetic sequence data through Generative Adversarial
Networks (GANs), coupled with the implementation of Differential Privacy (DP)
safeguards for protecting patient information during model training. To ensure
the integrity of our synthetic data, we employ a range of quality assessments
and monitor the plausibility between synthetic and original data. To test the
usefulness, we create private machine learning models on a commonly used,
albeit small, stress detection dataset, exploring strategies for enhancing the
existing data foundation with our synthetic data. Through our GAN-based
augmentation methods, we observe improvements in model performance, both in
non-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We
underline the potential of differentially private synthetic data in optimizing
utility-privacy trade-offs, especially with limited availability of real
training samples.
</p></li>
</ul>
<h3>Title: Compositional Generative Inverse Design. (arXiv:2401.13171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13171">http://arxiv.org/abs/2401.13171</a></li>
<li>Code URL: <a href="https://github.com/ai4science-westlakeu/cindm">https://github.com/ai4science-westlakeu/cindm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13171] Compositional Generative Inverse Design](http://arxiv.org/abs/2401.13171) #generative</code></li>
<li>Summary: <p>Inverse design, where we seek to design input variables in order to optimize
an underlying objective function, is an important problem that arises across
fields such as mechanical engineering to aerospace engineering. Inverse design
is typically formulated as an optimization problem, with recent works
leveraging optimization across learned dynamics models. However, as models are
optimized they tend to fall into adversarial modes, preventing effective
sampling. We illustrate that by instead optimizing over the learned energy
function captured by the diffusion model, we can avoid such adversarial
examples and significantly improve design performance. We further illustrate
how such a design system is compositional, enabling us to combine multiple
different diffusion models representing subcomponents of our desired system to
design systems with every specified component. In an N-body interaction task
and a challenging 2D multi-airfoil design task, we demonstrate that by
composing the learned diffusion model at test time, our method allows us to
design initial states and boundary shapes that are more complex than those in
the training data. Our method outperforms state-of-the-art neural inverse
design method by an average of 41.5% in prediction MAE and 14.3% in design
objective for the N-body dataset and discovers formation flying to minimize
drag in the multi-airfoil design task. Project website and code can be found at
https://github.com/AI4Science-WestlakeU/cindm.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection. (arXiv:2401.13551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13551">http://arxiv.org/abs/2401.13551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13551] Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection](http://arxiv.org/abs/2401.13551) #anomaly</code></li>
<li>Summary: <p>Without human annotations, a typical Unsupervised Video Anomaly Detection
(UVAD) method needs to train two models that generate pseudo labels for each
other. In previous work, the two models are closely entangled with each other,
and it is not known how to upgrade their method without modifying their
training framework significantly. Second, previous work usually adopts fixed
thresholding to obtain pseudo labels, however the user-specified threshold is
not reliable which inevitably introduces errors into the training process. To
alleviate these two problems, we propose a novel interleaved framework that
alternately trains a One-Class Classification (OCC) model and a
Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can
be easily replaced with other OCC or WS models, which facilitates our method to
upgrade with the most recent developments in both fields. For handling the
fixed thresholding problem, we break through the conventional cognitive
boundary and propose a weighted OCC model that can be trained on both normal
and abnormal data. We also propose an adaptive mechanism for automatically
finding the optimal threshold for the WS model in a loose to strict manner.
Experiments demonstrate that the proposed UVAD method outperforms previous
approaches.
</p></li>
</ul>
<h3>Title: Multitask Active Learning for Graph Anomaly Detection. (arXiv:2401.13210v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13210">http://arxiv.org/abs/2401.13210</a></li>
<li>Code URL: <a href="https://github.com/ahachang/mitigate">https://github.com/ahachang/mitigate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13210] Multitask Active Learning for Graph Anomaly Detection](http://arxiv.org/abs/2401.13210) #anomaly</code></li>
<li>Summary: <p>In the web era, graph machine learning has been widely used on ubiquitous
graph-structured data. As a pivotal component for bolstering web security and
enhancing the robustness of graph-based applications, the significance of graph
anomaly detection is continually increasing. While Graph Neural Networks (GNNs)
have demonstrated efficacy in supervised and semi-supervised graph anomaly
detection, their performance is contingent upon the availability of sufficient
ground truth labels. The labor-intensive nature of identifying anomalies from
complex graph structures poses a significant challenge in real-world
applications. Despite that, the indirect supervision signals from other tasks
(e.g., node classification) are relatively abundant. In this paper, we propose
a novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE.
Firstly, by coupling node classification tasks, MITIGATE obtains the capability
to detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE
quantifies the informativeness of nodes by the confidence difference across
tasks, allowing samples with conflicting predictions to provide informative yet
not excessively challenging information for subsequent training. Finally, to
enhance the likelihood of selecting representative nodes that are distant from
known patterns, MITIGATE adopts a masked aggregation mechanism for distance
measurement, considering both inherent features of nodes and current labeled
status. Empirical studies on four datasets demonstrate that MITIGATE
significantly outperforms the state-of-the-art methods for anomaly detection.
Our code is publicly available at: https://github.com/AhaChang/MITIGATE.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: MaLA-500: Massive Language Adaptation of Large Language Models. (arXiv:2401.13303v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13303">http://arxiv.org/abs/2401.13303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13303] MaLA-500: Massive Language Adaptation of Large Language Models](http://arxiv.org/abs/2401.13303) #in-context</code></li>
<li>Summary: <p>Large language models have advanced the state of the art in natural language
processing. However, their predominant design for English or a limited set of
languages creates a substantial gap in their effectiveness for low-resource
languages. To bridge this gap, we introduce MaLA-500, a novel large language
model designed to cover an extensive range of 534 languages. To train MaLA-500,
we employ vocabulary extension and continued pretraining on LLaMA 2 with
Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves
state-of-the-art in-context learning results. We release MaLA-500 at
https://huggingface.co/MaLA-LM
</p></li>
</ul>
<h3>Title: SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13527">http://arxiv.org/abs/2401.13527</a></li>
<li>Code URL: <a href="https://github.com/0nutation/speechgpt">https://github.com/0nutation/speechgpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13527] SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation](http://arxiv.org/abs/2401.13527) #in-context</code></li>
<li>Summary: <p>Benefiting from effective speech modeling, current Speech Large Language
Models (SLLMs) have demonstrated exceptional capabilities in in-context speech
generation and efficient generalization to unseen speakers. However, the
prevailing information modeling process is encumbered by certain redundancies,
leading to inefficiencies in speech generation. We propose Chain-of-Information
Generation (CoIG), a method for decoupling semantic and perceptual information
in large-scale speech generation. Building on this, we develop SpeechGPT-Gen,
an 8-billion-parameter SLLM efficient in semantic and perceptual information
modeling. It comprises an autoregressive model based on LLM for semantic
information modeling and a non-autoregressive model employing flow matching for
perceptual information modeling. Additionally, we introduce the novel approach
of infusing semantic information into the prior distribution to enhance the
efficiency of flow matching. Extensive experimental results demonstrate that
SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice
conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable
proficiency in capturing and modeling speech's semantic and perceptual
dimensions. Code and models are available at
https://github.com/0nutation/SpeechGPT.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: PlaceFormer: Transformer-based Visual Place Recognition using Multi-Scale Patch Selection and Fusion. (arXiv:2401.13082v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13082">http://arxiv.org/abs/2401.13082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13082] PlaceFormer: Transformer-based Visual Place Recognition using Multi-Scale Patch Selection and Fusion](http://arxiv.org/abs/2401.13082) #memory</code></li>
<li>Summary: <p>Visual place recognition is a challenging task in the field of computer
vision, and autonomous robotics and vehicles, which aims to identify a location
or a place from visual inputs. Contemporary methods in visual place recognition
employ convolutional neural networks and utilize every region within the image
for the place recognition task. However, the presence of dynamic and
distracting elements in the image may impact the effectiveness of the place
recognition process. Therefore, it is meaningful to focus on task-relevant
regions of the image for improved recognition. In this paper, we present
PlaceFormer, a novel transformer-based approach for visual place recognition.
PlaceFormer employs patch tokens from the transformer to create global image
descriptors, which are then used for image retrieval. To re-rank the retrieved
images, PlaceFormer merges the patch tokens from the transformer to form
multi-scale patches. Utilizing the transformer's self-attention mechanism, it
selects patches that correspond to task-relevant areas in an image. These
selected patches undergo geometric verification, generating similarity scores
across different patch sizes. Subsequently, spatial scores from each patch size
are fused to produce a final similarity score. This score is then used to
re-rank the images initially retrieved using global image descriptors.
Extensive experiments on benchmark datasets demonstrate that PlaceFormer
outperforms several state-of-the-art methods in terms of accuracy and
computational efficiency, requiring less time and memory.
</p></li>
</ul>
<h3>Title: Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery. (arXiv:2401.13325v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13325">http://arxiv.org/abs/2401.13325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13325] Memory Consistency Guided Divide-and-Conquer Learning for Generalized Category Discovery](http://arxiv.org/abs/2401.13325) #memory</code></li>
<li>Summary: <p>Generalized category discovery (GCD) aims at addressing a more realistic and
challenging setting of semi-supervised learning, where only part of the
category labels are assigned to certain training samples. Previous methods
generally employ naive contrastive learning or unsupervised clustering scheme
for all the samples. Nevertheless, they usually ignore the inherent critical
information within the historical predictions of the model being trained.
Specifically, we empirically reveal that a significant number of salient
unlabeled samples yield consistent historical predictions corresponding to
their ground truth category. From this observation, we propose a Memory
Consistency guided Divide-and-conquer Learning framework (MCDL). In this
framework, we introduce two memory banks to record historical prediction of
unlabeled data, which are exploited to measure the credibility of each sample
in terms of its prediction consistency. With the guidance of credibility, we
can design a divide-and-conquer learning strategy to fully utilize the
discriminative information of unlabeled data while alleviating the negative
influence of noisy labels. Extensive experimental results on multiple
benchmarks demonstrate the generality and superiority of our method, where our
method outperforms state-of-the-art models by a large margin on both seen and
unseen classes of the generic image recognition and challenging semantic shift
settings (i.e.,with +8.4% gain on CUB and +8.1% on Standford Cars).
</p></li>
</ul>
<h3>Title: SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation. (arXiv:2401.13560v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13560">http://arxiv.org/abs/2401.13560</a></li>
<li>Code URL: <a href="https://github.com/ge-xing/segmamba">https://github.com/ge-xing/segmamba</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13560] SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation](http://arxiv.org/abs/2401.13560) #memory</code></li>
<li>Summary: <p>The Transformer architecture has shown a remarkable ability in modeling
global relationships. However, it poses a significant computational challenge
when processing high-dimensional medical images. This hinders its development
and widespread adoption in this task. Mamba, as a State Space Model (SSM),
recently emerged as a notable manner for long-range dependencies in sequential
modeling, excelling in natural language processing filed with its remarkable
memory efficiency and computational speed. Inspired by its success, we
introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation
\textbf{Mamba} model, designed to effectively capture long-range dependencies
within whole volume features at every scale. Our SegMamba, in contrast to
Transformer-based methods, excels in whole volume feature modeling from a state
space model standpoint, maintaining superior processing speed, even with volume
features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments
on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our
SegMamba. The code for SegMamba is available at:
https://github.com/ge-xing/SegMamba
</p></li>
</ul>
<h3>Title: Identifying Risk Patterns in Brazilian Police Reports Preceding Femicides: A Long Short Term Memory (LSTM) Based Analysis. (arXiv:2401.12980v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.12980">http://arxiv.org/abs/2401.12980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.12980] Identifying Risk Patterns in Brazilian Police Reports Preceding Femicides: A Long Short Term Memory (LSTM) Based Analysis](http://arxiv.org/abs/2401.12980) #memory</code></li>
<li>Summary: <p>Femicide refers to the killing of a female victim, often perpetrated by an
intimate partner or family member, and is also associated with gender-based
violence. Studies have shown that there is a pattern of escalating violence
leading up to these killings, highlighting the potential for prevention if the
level of danger to the victim can be assessed. Machine learning offers a
promising approach to address this challenge by predicting risk levels based on
textual descriptions of the violence. In this study, we employed the Long Short
Term Memory (LSTM) technique to identify patterns of behavior in Brazilian
police reports preceding femicides. Our first objective was to classify the
content of these reports as indicating either a lower or higher risk of the
victim being murdered, achieving an accuracy of 66%. In the second approach, we
developed a model to predict the next action a victim might experience within a
sequence of patterned events. Both approaches contribute to the understanding
and assessment of the risks associated with domestic violence, providing
authorities with valuable insights to protect women and prevent situations from
escalating.
</p></li>
</ul>
<h3>Title: Estimating the severity of dental and oral problems via sentiment classification over clinical reports. (arXiv:2401.12993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.12993">http://arxiv.org/abs/2401.12993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.12993] Estimating the severity of dental and oral problems via sentiment classification over clinical reports](http://arxiv.org/abs/2401.12993) #memory</code></li>
<li>Summary: <p>Analyzing authors' sentiments in texts as a technique for identifying text
polarity can be practical and useful in various fields, including medicine and
dentistry. Currently, due to factors such as patients' limited knowledge about
their condition, difficulties in accessing specialist doctors, or fear of
illness, particularly in pandemic conditions, there might be a delay between
receiving a radiology report and consulting a doctor. In some cases, this delay
can pose significant risks to the patient, making timely decision-making
crucial. Having an automatic system that can inform patients about the
deterioration of their condition by analyzing the text of radiology reports
could greatly impact timely decision-making. In this study, a dataset
comprising 1,134 cone-beam computed tomography (CBCT) photo reports was
collected from the Shiraz University of Medical Sciences. Each case was
examined, and an expert labeled a severity level for the patient's condition on
each document. After preprocessing all the text data, a deep learning model
based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
network architecture, known as CNN-LSTM, was developed to detect the severity
level of the patient's problem based on sentiment analysis in the radiologist's
report. The model's performance was evaluated on two datasets, each with two
and four classes, in both imbalanced and balanced scenarios. Finally, to
demonstrate the effectiveness of our model, we compared its performance with
that of other classification models. The results, along with one-way ANOVA and
Tukey's test, indicated that our proposed model (CNN-LSTM) performed the best
according to precision, recall, and f-measure criteria. This suggests that it
can be a reliable model for estimating the severity of oral and dental
diseases, thereby assisting patients.
</p></li>
</ul>
<h3>Title: Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13200">http://arxiv.org/abs/2401.13200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13200] Topology-aware Embedding Memory for Learning on Expanding Graphs](http://arxiv.org/abs/2401.13200) #memory</code></li>
<li>Summary: <p>Memory replay based techniques have shown great success for continual
learning with incrementally accumulated Euclidean data. Directly applying them
to continually expanding graphs, however, leads to the potential memory
explosion problem due to the need to buffer representative nodes and their
associated topological neighborhood structures. To this end, we systematically
analyze the key challenges in the memory explosion problem, and present a
general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs)
with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed
framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$
to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree,
$L$: the radius of the GNN receptive field}, but also fully utilizes the
topological information for memory replay. Specifically, PDGNNs decouple
trainable parameters from the computation ego-subgraph via
\textit{Topology-aware Embeddings} (TEs), which compress ego-subgraphs into
compact vectors (i.e., TEs) to reduce the memory consumption. Based on this
framework, we discover a unique \textit{pseudo-training effect} in continual
learning on expanding graphs and this effect motivates us to develop a novel
\textit{coverage maximization sampling} strategy that can enhance the
performance with a tight memory budget. Thorough empirical studies demonstrate
that, by tackling the memory explosion problem and incorporating topological
information into memory replay, PDGNNs with TEM significantly outperform
state-of-the-art techniques, especially in the challenging class-incremental
setting.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning. (arXiv:2401.13499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13499">http://arxiv.org/abs/2401.13499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13499] LDCA: Local Descriptors with Contextual Augmentation for Few-Shot Learning](http://arxiv.org/abs/2401.13499) #few-shot</code></li>
<li>Summary: <p>Few-shot image classification has emerged as a key challenge in the field of
computer vision, highlighting the capability to rapidly adapt to new tasks with
minimal labeled data. Existing methods predominantly rely on image-level
features or local descriptors, often overlooking the holistic context
surrounding these descriptors. In this work, we introduce a novel approach
termed "Local Descriptor with Contextual Augmentation (LDCA)". Specifically,
this method bridges the gap between local and global understanding uniquely by
leveraging an adaptive global contextual enhancement module. This module
incorporates a visual transformer, endowing local descriptors with contextual
awareness capabilities, ranging from broad global perspectives to intricate
surrounding nuances. By doing so, LDCA transcends traditional descriptor-based
approaches, ensuring each local feature is interpreted within its larger visual
narrative. Extensive experiments underscore the efficacy of our method, showing
a maximal absolute improvement of 20\% over the next-best on fine-grained
classification datasets, thus demonstrating significant advancements in
few-shot classification tasks.
</p></li>
</ul>
<h3>Title: Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode. (arXiv:2401.13613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13613">http://arxiv.org/abs/2401.13613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13613] Enhancing Image Retrieval : A Comprehensive Study on Photo Search using the CLIP Mode](http://arxiv.org/abs/2401.13613) #few-shot</code></li>
<li>Summary: <p>Photo search, the task of retrieving images based on textual queries, has
witnessed significant advancements with the introduction of CLIP (Contrastive
Language-Image Pretraining) model. CLIP leverages a vision-language pre
training approach, wherein it learns a shared representation space for images
and text, enabling cross-modal understanding. This model demonstrates the
capability to understand the semantic relationships between diverse image and
text pairs, allowing for efficient and accurate retrieval of images based on
natural language queries. By training on a large-scale dataset containing
images and their associated textual descriptions, CLIP achieves remarkable
generalization, providing a powerful tool for tasks such as zero-shot learning
and few-shot classification. This abstract summarizes the foundational
principles of CLIP and highlights its potential impact on advancing the field
of photo search, fostering a seamless integration of natural language
understanding and computer vision for improved information retrieval in
multimedia applications
</p></li>
</ul>
<h3>Title: Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection. (arXiv:2401.12988v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.12988">http://arxiv.org/abs/2401.12988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.12988] Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection](http://arxiv.org/abs/2401.12988) #few-shot</code></li>
<li>Summary: <p>This study harnesses state-of-the-art AI technology for chronic disease
management, specifically in detecting various mental disorders through
user-generated textual content. Existing studies typically rely on fully
supervised machine learning, which presents challenges such as the
labor-intensive manual process of annotating extensive training data for each
disease and the need to design specialized deep learning architectures for each
problem. To address such challenges, we propose a novel framework that
leverages advanced AI techniques, including large language models and
multi-prompt engineering. Specifically, we address two key technical challenges
in data-driven chronic disease management: (1) developing personalized prompts
to represent each user's uniqueness and (2) incorporating medical knowledge
into prompts to provide context for chronic disease detection, instruct
learning objectives, and operationalize prediction goals. We evaluate our
method using four mental disorders, which are prevalent chronic diseases
worldwide, as research cases. On the depression detection task, our method (F1
= 0.975~0.978) significantly outperforms traditional supervised learning
paradigms, including feature engineering (F1 = 0.760) and architecture
engineering (F1 = 0.756). Meanwhile, our approach demonstrates success in
few-shot learning, i.e., requiring only a minimal number of training examples
to detect chronic diseases based on user-generated textual content (i.e., only
2, 10, or 100 subjects). Moreover, our method can be generalized to other
mental disorder detection tasks, including anorexia, pathological gambling, and
self-harm (F1 = 0.919~0.978).
</p></li>
</ul>
<h3>Title: From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning. (arXiv:2401.13229v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.13229">http://arxiv.org/abs/2401.13229</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.13229] From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning](http://arxiv.org/abs/2401.13229) #few-shot</code></li>
<li>Summary: <p>A major challenge in Natural Language Processing is obtaining annotated data
for supervised learning. An option is the use of crowdsourcing platforms for
data annotation. However, crowdsourcing introduces issues related to the
annotator's experience, consistency, and biases. An alternative is to use
zero-shot methods, which in turn have limitations compared to their few-shot or
fully supervised counterparts. Recent advancements driven by large language
models show potential, but struggle to adapt to specialized domains with
severely limited data. The most common approaches therefore involve the human
itself randomly annotating a set of datapoints to build initial datasets. But
randomly sampling data to be annotated is often inefficient as it ignores the
characteristics of the data and the specific needs of the model. The situation
worsens when working with imbalanced datasets, as random sampling tends to
heavily bias towards the majority classes, leading to excessive annotated data.
To address these issues, this paper contributes an automatic and informed data
selection architecture to build a small dataset for few-shot learning. Our
proposal minimizes the quantity and maximizes diversity of data selected for
human annotation, while improving model performance.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2024-01-25]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
