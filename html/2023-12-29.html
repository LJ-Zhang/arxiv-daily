<h2>diffusion</h2>
<h3>Title: Iterative Prompt Relabeling for diffusion model with RLDF. (arXiv:2312.16204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16204">http://arxiv.org/abs/2312.16204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16204] Iterative Prompt Relabeling for diffusion model with RLDF](http://arxiv.org/abs/2312.16204) #diffusion</code></li>
<li>Summary: <p>Diffusion models have shown impressive performance in many domains, including
image generation, time series prediction, and reinforcement learning. The
algorithm demonstrates superior performance over the traditional GAN and
transformer based methods. However, the model's capability to follow natural
language instructions (e.g., spatial relationships between objects, generating
complex scenes) is still unsatisfactory. This has been an important research
area to enhance such capability. Prior works adopt reinforcement learning to
adjust the behavior of the diffusion models. However, RL methods not only
require careful reward design and complex hyperparameter tuning, but also fails
to incorporate rich natural language feedback. In this work, we propose
iterative prompt relabeling (IP-RLDF), a novel algorithm that aligns images to
text through iterative image sampling and prompt relabeling. IP-RLDF first
samples a batch of images conditioned on the text, then relabels the text
prompts of unmatched text-image pairs with classifier feedback. We conduct
thorough experiments on three different models, including SDv2, GLIGEN, and
SDXL, testing their capability to generate images following instructions. With
IP-RLDF, we improved up to 15.22% (absolute improvement) on the challenging
spatial relation VISOR benchmark, demonstrating superior performance compared
to previous RL methods.
</p></li>
</ul>
<h3>Title: Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks. (arXiv:2312.16218v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16218">http://arxiv.org/abs/2312.16218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16218] Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks](http://arxiv.org/abs/2312.16218) #diffusion</code></li>
<li>Summary: <p>Solving image-to-3D from a single view is an ill-posed problem, and current
neural reconstruction methods addressing it through diffusion models still rely
on scene-specific optimization, constraining their generalization capability.
To overcome the limitations of existing approaches regarding generalization and
consistency, we introduce a novel neural rendering technique. Our approach
employs the signed distance function as the surface representation and
incorporates generalizable priors through geometry-encoding volumes and
HyperNetworks. Specifically, our method builds neural encoding volumes from
generated multi-view inputs. We adjust the weights of the SDF network
conditioned on an input image at test-time to allow model adaptation to novel
scenes in a feed-forward manner via HyperNetworks. To mitigate artifacts
derived from the synthesized views, we propose the use of a volume transformer
module to improve the aggregation of image features instead of processing each
viewpoint separately. Through our proposed method, dubbed as Hyper-VolTran, we
avoid the bottleneck of scene-specific optimization and maintain consistency
across the images generated from multiple viewpoints. Our experiments show the
advantages of our proposed approach with consistent results and rapid
generation.
</p></li>
</ul>
<h3>Title: Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face Synthesis. (arXiv:2312.16274v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16274">http://arxiv.org/abs/2312.16274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16274] Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face Synthesis](http://arxiv.org/abs/2312.16274) #diffusion</code></li>
<li>Summary: <p>Recent progress in multi-modal conditioned face synthesis has enabled the
creation of visually striking and accurately aligned facial images. Yet,
current methods still face issues with scalability, limited flexibility, and a
one-size-fits-all approach to control strength, not accounting for the
differing levels of conditional entropy, a measure of unpredictability in data
given some condition, across modalities. To address these challenges, we
introduce a novel uni-modal training approach with modal surrogates, coupled
with an entropy-aware modal-adaptive modulation, to support flexible, scalable,
and scalable multi-modal conditioned face synthesis network. Our uni-modal
training with modal surrogate that only leverage uni-modal data, use modal
surrogate to decorate condition with modal-specific characteristic and serve as
linker for inter-modal collaboration , fully learns each modality control in
face synthesis process as well as inter-modal collaboration. The entropy-aware
modal-adaptive modulation finely adjust diffusion noise according to
modal-specific characteristics and given conditions, enabling well-informed
step along denoising trajectory and ultimately leading to synthesis results of
high fidelity and quality. Our framework improves multi-modal face synthesis
under various conditions, surpassing current methods in image quality and
fidelity, as demonstrated by our thorough experimental results.
</p></li>
</ul>
<h3>Title: State-of-the-Art in Nudity Classification: A Comparative Analysis. (arXiv:2312.16338v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16338">http://arxiv.org/abs/2312.16338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16338] State-of-the-Art in Nudity Classification: A Comparative Analysis](http://arxiv.org/abs/2312.16338) #diffusion</code></li>
<li>Summary: <p>This paper presents a comparative analysis of existing nudity classification
techniques for classifying images based on the presence of nudity, with a focus
on their application in content moderation. The evaluation focuses on CNN-based
models, vision transformer, and popular open-source safety checkers from Stable
Diffusion and Large-scale Artificial Intelligence Open Network (LAION). The
study identifies the limitations of current evaluation datasets and highlights
the need for more diverse and challenging datasets. The paper discusses the
potential implications of these findings for developing more accurate and
effective image classification systems on online platforms. Overall, the study
emphasizes the importance of continually improving image classification models
to ensure the safety and well-being of platform users. The project page,
including the demonstrations and results is publicly available at
https://github.com/fcakyon/content-moderation-deep-learning.
</p></li>
</ul>
<h3>Title: Natural Adversarial Patch Generation Method Based on Latent Diffusion Model. (arXiv:2312.16401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16401">http://arxiv.org/abs/2312.16401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16401] Natural Adversarial Patch Generation Method Based on Latent Diffusion Model](http://arxiv.org/abs/2312.16401) #diffusion</code></li>
<li>Summary: <p>Recently, some research show that deep neural networks are vulnerable to the
adversarial attacks, the well-trainned samples or patches could be used to
trick the neural network detector or human visual perception. However, these
adversarial patches, with their conspicuous and unusual patterns, lack
camouflage and can easily raise suspicion in the real world. To solve this
problem, this paper proposed a novel adversarial patch method called the Latent
Diffusion Patch (LDP), in which, a pretrained encoder is first designed to
compress the natural images into a feature space with key characteristics. Then
trains the diffusion model using the above feature space. Finally, explore the
latent space of the pretrained diffusion model using the image denoising
technology. It polishes the patches and images through the powerful natural
abilities of diffusion models, making them more acceptable to the human visual
system. Experimental results, both digital and physical worlds, show that LDPs
achieve a visual subjectivity score of 87.3%, while still maintaining effective
attack capabilities.
</p></li>
</ul>
<h3>Title: SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16476">http://arxiv.org/abs/2312.16476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16476] SVGDreamer: Text Guided SVG Generation with Diffusion Model](http://arxiv.org/abs/2312.16476) #diffusion</code></li>
<li>Summary: <p>Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity.
</p></li>
</ul>
<h3>Title: PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion. (arXiv:2312.16486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16486">http://arxiv.org/abs/2312.16486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16486] PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion](http://arxiv.org/abs/2312.16486) #diffusion</code></li>
<li>Summary: <p>Current large-scale diffusion models represent a giant leap forward in
conditional image synthesis, capable of interpreting diverse cues like text,
human poses, and edges. However, their reliance on substantial computational
resources and extensive data collection remains a bottleneck. On the other
hand, the integration of existing diffusion models, each specialized for
different controls and operating in unique latent spaces, poses a challenge due
to incompatible image resolutions and latent space embedding structures,
hindering their joint use. Addressing these constraints, we present
"PanGu-Draw", a novel latent diffusion model designed for resource-efficient
text-to-image synthesis that adeptly accommodates multiple control signals. We
first propose a resource-efficient Time-Decoupling Training Strategy, which
splits the monolithic text-to-image model into structure and texture
generators. Each generator is trained using a regimen that maximizes data
utilization and computational efficiency, cutting data preparation by 48% and
reducing training resources by 51%. Secondly, we introduce "Coop-Diffusion", an
algorithm that enables the cooperative use of various pre-trained diffusion
models with different latent spaces and predefined resolutions within a unified
denoising process. This allows for multi-control image synthesis at arbitrary
resolutions without the necessity for additional data or retraining. Empirical
validations of Pangu-Draw show its exceptional prowess in text-to-image and
multi-control image generation, suggesting a promising direction for future
model training efficiencies and generation versatility. The largest 5B T2I
PanGu-Draw model is released on the Ascend platform. Project page:
https://pangu-draw.github.io
</p></li>
</ul>
<h3>Title: Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection. (arXiv:2312.16649v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16649">http://arxiv.org/abs/2312.16649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16649] Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection](http://arxiv.org/abs/2312.16649) #diffusion</code></li>
<li>Summary: <p>In this paper, we study the problem of generalizable synthetic image
detection, aiming to detect forgery images from diverse generative methods,
e.g., GANs and diffusion models. Cutting-edge solutions start to explore the
benefits of pre-trained models, and mainly follow the fixed paradigm of solely
training an attached classifier, e.g., combining frozen CLIP-ViT with a
learnable linear layer in UniFD. However, our analysis shows that such a fixed
paradigm is prone to yield detectors with insufficient learning regarding
forgery representations. We attribute the key challenge to the lack of forgery
adaptation, and present a novel forgery-aware adaptive transformer approach,
namely FatFormer. Based on the pre-trained vision-language spaces of CLIP,
FatFormer introduces two core designs for the adaption to build generalized
forgery representations. First, motivated by the fact that both image and
frequency analysis are essential for synthetic image detection, we develop a
forgery-aware adapter to adapt image features to discern and integrate local
forgery traces within image and frequency domains. Second, we find that
considering the contrastive objectives between adapted image features and text
prompt embeddings, a previously overlooked aspect, results in a nontrivial
generalization improvement. Accordingly, we introduce language-guided alignment
to supervise the forgery adaptation with image and text prompts in FatFormer.
Experiments show that, by coupling these two designs, our approach tuned on
4-class ProGAN data attains a remarkable detection performance, achieving an
average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen
diffusion models with 95% accuracy.
</p></li>
</ul>
<h3>Title: I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models. (arXiv:2312.16693v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16693">http://arxiv.org/abs/2312.16693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16693] I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models](http://arxiv.org/abs/2312.16693) #diffusion</code></li>
<li>Summary: <p>In the rapidly evolving domain of digital content generation, the focus has
shifted from text-to-image (T2I) models to more advanced video diffusion
models, notably text-to-video (T2V) and image-to-video (I2V). This paper
addresses the intricate challenge posed by I2V: converting static images into
dynamic, lifelike video sequences while preserving the original image fidelity.
Traditional methods typically involve integrating entire images into diffusion
processes or using pretrained encoders for cross attention. However, these
approaches often necessitate altering the fundamental weights of T2I models,
thereby restricting their reusability. We introduce a novel solution, namely
I2V-Adapter, designed to overcome such limitations. Our approach preserves the
structural integrity of T2I models and their inherent motion modules. The
I2V-Adapter operates by processing noised video frames in parallel with the
input image, utilizing a lightweight adapter module. This module acts as a
bridge, efficiently linking the input to the model's self-attention mechanism,
thus maintaining spatial details without requiring structural changes to the
T2I model. Moreover, I2V-Adapter requires only a fraction of the parameters of
conventional models and ensures compatibility with existing community-driven
T2I models and controlling tools. Our experimental results demonstrate
I2V-Adapter's capability to produce high-quality video outputs. This
performance, coupled with its versatility and reduced need for trainable
parameters, represents a substantial advancement in the field of AI-driven
video generation, particularly for creative applications.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: TEMP3D: Temporally Continuous 3D Human Pose Estimation Under Occlusions. (arXiv:2312.16221v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16221">http://arxiv.org/abs/2312.16221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16221] TEMP3D: Temporally Continuous 3D Human Pose Estimation Under Occlusions](http://arxiv.org/abs/2312.16221) #self-supervised</code></li>
<li>Summary: <p>Existing 3D human pose estimation methods perform remarkably well in both
monocular and multi-view settings. However, their efficacy diminishes
significantly in the presence of heavy occlusions, which limits their practical
utility. For video sequences, temporal continuity can help infer accurate
poses, especially in heavily occluded frames. In this paper, we aim to leverage
this potential of temporal continuity through human motion priors, coupled with
large-scale pre-training on 3D poses and self-supervised learning, to enhance
3D pose estimation in a given video sequence. This leads to a temporally
continuous 3D pose estimate on unlabelled in-the-wild videos, which may contain
occlusions, while exclusively relying on pre-trained 3D pose models. We propose
an unsupervised method named TEMP3D that aligns a motion prior model on a given
in-the-wild video using existing SOTA single image-based 3D pose estimation
methods to give temporally continuous output under occlusions. To evaluate our
method, we test it on the Occluded Human3.6M dataset, our custom-built dataset
which contains significantly large (up to 100%) human body occlusions
incorporated into the Human3.6M dataset. We achieve SOTA results on Occluded
Human3.6M and the OcMotion dataset while maintaining competitive performance on
non-occluded data. URL: https://sites.google.com/ucr.edu/temp3d
</p></li>
</ul>
<h3>Title: Soft Contrastive Learning for Time Series. (arXiv:2312.16424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16424">http://arxiv.org/abs/2312.16424</a></li>
<li>Code URL: <a href="https://github.com/seunghan96/softclt">https://github.com/seunghan96/softclt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16424] Soft Contrastive Learning for Time Series](http://arxiv.org/abs/2312.16424) #self-supervised</code></li>
<li>Summary: <p>Contrastive learning has shown to be effective to learn representations from
time series in a self-supervised way. However, contrasting similar time series
instances or values from adjacent timestamps within a time series leads to
ignore their inherent correlations, which results in deteriorating the quality
of learned representations. To address this issue, we propose SoftCLT, a simple
yet effective soft contrastive learning strategy for time series. This is
achieved by introducing instance-wise and temporal contrastive loss with soft
assignments ranging from zero to one. Specifically, we define soft assignments
for 1) instance-wise contrastive loss by the distance between time series on
the data space, and 2) temporal contrastive loss by the difference of
timestamps. SoftCLT is a plug-and-play method for time series contrastive
learning that improves the quality of learned representations without bells and
whistles. In experiments, we demonstrate that SoftCLT consistently improves the
performance in various downstream tasks including classification,
semi-supervised learning, transfer learning, and anomaly detection, showing
state-of-the-art performance. Code is available at this repository:
https://github.com/seunghan96/softclt.
</p></li>
</ul>
<h3>Title: Learning to Embed Time Series Patches Independently. (arXiv:2312.16427v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16427">http://arxiv.org/abs/2312.16427</a></li>
<li>Code URL: <a href="https://github.com/seunghan96/pits">https://github.com/seunghan96/pits</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16427] Learning to Embed Time Series Patches Independently](http://arxiv.org/abs/2312.16427) #self-supervised</code></li>
<li>Summary: <p>Masked time series modeling has recently gained much attention as a
self-supervised representation learning strategy for time series. Inspired by
masked image modeling in computer vision, recent works first patchify and
partially mask out time series, and then train Transformers to capture the
dependencies between patches by predicting masked patches from unmasked
patches. However, we argue that capturing such patch dependencies might not be
an optimal strategy for time series representation learning; rather, learning
to embed patches independently results in better time series representations.
Specifically, we propose to use 1) the simple patch reconstruction task, which
autoencode each patch without looking at other patches, and 2) the simple
patch-wise MLP that embeds each patch independently. In addition, we introduce
complementary contrastive learning to hierarchically capture adjacent time
series information efficiently. Our proposed method improves time series
forecasting and classification performance compared to state-of-the-art
Transformer-based models, while it is more efficient in terms of the number of
parameters and training/inference time. Code is available at this repository:
https://github.com/seunghan96/pits.
</p></li>
</ul>
<h3>Title: Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community Structures. (arXiv:2312.16788v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16788">http://arxiv.org/abs/2312.16788</a></li>
<li>Code URL: <a href="https://github.com/nslab-cuk/community-aware-graph-transformer">https://github.com/nslab-cuk/community-aware-graph-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16788] Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community Structures](http://arxiv.org/abs/2312.16788) #self-supervised</code></li>
<li>Summary: <p>This study utilizes community structures to address node degree biases in
message-passing (MP) via learnable graph augmentations and novel graph
transformers. Recent augmentation-based methods showed that MP neural networks
often perform poorly on low-degree nodes, leading to degree biases due to a
lack of messages reaching low-degree nodes. Despite their success, most methods
use heuristic or uniform random augmentations, which are non-differentiable and
may not always generate valuable edges for learning representations. In this
paper, we propose Community-aware Graph Transformers, namely CGT, to learn
degree-unbiased representations based on learnable augmentations and graph
transformers by extracting within community structures. We first design a
learnable graph augmentation to generate more within-community edges connecting
low-degree nodes through edge perturbation. Second, we propose an improved
self-attention to learn underlying proximity and the roles of nodes within the
community. Third, we propose a self-supervised learning task that could learn
the representations to preserve the global graph structure and regularize the
graph augmentations. Extensive experiments on various benchmark datasets showed
CGT outperforms state-of-the-art baselines and significantly improves the node
degree biases. The source code is available at
https://github.com/NSLab-CUK/Community-aware-Graph-Transformer.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection. (arXiv:2312.16202v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16202">http://arxiv.org/abs/2312.16202</a></li>
<li>Code URL: <a href="https://github.com/KyanChen/TTP">https://github.com/KyanChen/TTP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16202] Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection](http://arxiv.org/abs/2312.16202) #foundation model</code></li>
<li>Summary: <p>Change detection, a prominent research area in remote sensing, is pivotal in
observing and analyzing surface transformations. Despite significant
advancements achieved through deep learning-based methods, executing
high-precision change detection in spatio-temporally complex remote sensing
scenarios still presents a substantial challenge. The recent emergence of
foundation models, with their powerful universality and generalization
capabilities, offers potential solutions. However, bridging the gap of data and
tasks remains a significant obstacle. In this paper, we introduce Time
Travelling Pixels (TTP), a novel approach that integrates the latent knowledge
of the SAM foundation model into change detection. This method effectively
addresses the domain shift in general knowledge transfer and the challenge of
expressing homogeneous and heterogeneous characteristics of multi-temporal
images. The state-of-the-art results obtained on the LEVIR-CD underscore the
efficacy of the TTP. The Code is available at \url{https://kychen.me/TTP}.
</p></li>
</ul>
<h3>Title: DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision. (arXiv:2312.16256v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16256">http://arxiv.org/abs/2312.16256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16256] DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision](http://arxiv.org/abs/2312.16256) #foundation model</code></li>
<li>Summary: <p>We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.
</p></li>
</ul>
<h3>Title: Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings. (arXiv:2312.16410v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16410">http://arxiv.org/abs/2312.16410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16410] Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings](http://arxiv.org/abs/2312.16410) #foundation model</code></li>
<li>Summary: <p>The field of Remote Sensing (RS) widely employs Change Detection (CD) on
very-high-resolution (VHR) images. A majority of extant deep-learning-based
methods hinge on annotated samples to complete the CD process. Recently, the
emergence of Vision Foundation Model (VFM) enables zero-shot predictions in
particular vision tasks. In this work, we propose an unsupervised CD method
named Segment Change Model (SCM), built upon the Segment Anything Model (SAM)
and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates
features extracted at different scales and integrates them in a top-down manner
to enhance discriminative change edges. We further design an innovative
Piecewise Semantic Attention (PSA) scheme, which can offer semantic
representation without training, thereby minimize pseudo change phenomenon.
Through conducting experiments on two public datasets, the proposed SCM
increases the mIoU from 46.09% to 53.67% on the LEVIR-CD dataset, and from
47.56% to 52.14% on the WHU-CD dataset. Our codes are available at
https://github.com/StephenApX/UCD-SCM.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: AI Mirage: The Impostor Bias and the Deepfake Detection Challenge in the Era of Artificial Illusions. (arXiv:2312.16220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16220">http://arxiv.org/abs/2312.16220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16220] AI Mirage: The Impostor Bias and the Deepfake Detection Challenge in the Era of Artificial Illusions](http://arxiv.org/abs/2312.16220) #generative</code></li>
<li>Summary: <p>This paper provides a comprehensive analysis of cognitive biases in forensics
and digital forensics, examining their implications for decision-making
processes in these fields. It explores the various types of cognitive biases
that may arise during forensic investigations and digital forensic analyses,
such as confirmation bias, expectation bias, overconfidence in errors,
contextual bias, and attributional biases. It also evaluates existing methods
and techniques used to mitigate cognitive biases in these contexts, assessing
the effectiveness of interventions aimed at reducing biases and improving
decision-making outcomes. Additionally, this paper introduces a new cognitive
bias, called "impostor bias", that may affect the use of generative Artificial
Intelligence (AI) tools in forensics and digital forensics. The impostor bias
is the tendency to doubt the authenticity or validity of the output generated
by AI tools, such as deepfakes, in the form of audio, images, and videos. This
bias may lead to erroneous judgments or false accusations, undermining the
reliability and credibility of forensic evidence. The paper discusses the
potential causes and consequences of the impostor bias, and suggests some
strategies to prevent or counteract it. By addressing these topics, this paper
seeks to offer valuable insights into understanding cognitive biases in
forensic practices and provide recommendations for future research and
practical applications to enhance the objectivity and validity of forensic
investigations.
</p></li>
</ul>
<h3>Title: MetaScript: Few-Shot Handwritten Chinese Content Generation via Generative Adversarial Networks. (arXiv:2312.16251v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16251">http://arxiv.org/abs/2312.16251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16251] MetaScript: Few-Shot Handwritten Chinese Content Generation via Generative Adversarial Networks](http://arxiv.org/abs/2312.16251) #generative</code></li>
<li>Summary: <p>In this work, we propose MetaScript, a novel Chinese content generation
system designed to address the diminishing presence of personal handwriting
styles in the digital representation of Chinese characters. Our approach
harnesses the power of few-shot learning to generate Chinese characters that
not only retain the individual's unique handwriting style but also maintain the
efficiency of digital typing. Trained on a diverse dataset of handwritten
styles, MetaScript is adept at producing high-quality stylistic imitations from
minimal style references and standard fonts. Our work demonstrates a practical
solution to the challenges of digital typography in preserving the personal
touch in written communication, particularly in the context of Chinese script.
Notably, our system has demonstrated superior performance in various
evaluations, including recognition accuracy, inception score, and Frechet
inception distance. At the same time, the training conditions of our model are
easy to meet and facilitate generalization to real applications.
</p></li>
</ul>
<h3>Title: Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16414">http://arxiv.org/abs/2312.16414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16414] Bellman Optimal Step-size Straightening of Flow-Matching Models](http://arxiv.org/abs/2312.16414) #generative</code></li>
<li>Summary: <p>Flow matching is a powerful framework for generating high-quality samples in
various applications, especially image synthesis. However, the intensive
computational demands of these models, especially during the fine-tuning
process and sampling processes, pose significant challenges for low-resource
scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS)
technique for distilling flow-matching generative models: it aims specifically
for a few step efficient image sampling while adhering to a computational
budget constraint. First, this technique involves a dynamic programming
algorithm that optimizes the step sizes of the pretrained network. Then, it
refines the velocity network to match the optimal step sizes, aiming to
straighten the generation paths. Extensive experimental evaluations across
image generation tasks demonstrate the efficacy of BOSS in terms of both
resource utilization and image quality. Our results reveal that BOSS achieves
substantial gains in efficiency while maintaining competitive sample quality,
effectively bridging the gap between low-resource constraints and the demanding
requirements of flow-matching generative models. Our paper also fortifies the
responsible development of artificial intelligence, offering a more sustainable
generative model that reduces computational costs and environmental footprints.
Our code can be found at https://anonymous.4open.science/r/DRL-8E88.
</p></li>
</ul>
<h3>Title: HMP: Hand Motion Priors for Pose and Shape Estimation from Video. (arXiv:2312.16737v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16737">http://arxiv.org/abs/2312.16737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16737] HMP: Hand Motion Priors for Pose and Shape Estimation from Video](http://arxiv.org/abs/2312.16737) #generative</code></li>
<li>Summary: <p>Understanding how humans interact with the world necessitates accurate 3D
hand pose estimation, a task complicated by the hand's high degree of
articulation, frequent occlusions, self-occlusions, and rapid motions. While
most existing methods rely on single-image inputs, videos have useful cues to
address aforementioned issues. However, existing video-based 3D hand datasets
are insufficient for training feedforward models to generalize to in-the-wild
scenarios. On the other hand, we have access to large human motion capture
datasets which also include hand motions, e.g. AMASS. Therefore, we develop a
generative motion prior specific for hands, trained on the AMASS dataset which
features diverse and high-quality hand motions. This motion prior is then
employed for video-based 3D hand motion estimation following a latent
optimization approach. Our integration of a robust motion prior significantly
enhances performance, especially in occluded scenarios. It produces stable,
temporally consistent results that surpass conventional single-frame methods.
We demonstrate our method's efficacy via qualitative and quantitative
evaluations on the HO3D and DexYCB datasets, with special emphasis on an
occlusion-focused subset of HO3D. Code is available at
https://hmp.is.tue.mpg.de
</p></li>
</ul>
<h3>Title: Active Third-Person Imitation Learning. (arXiv:2312.16365v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16365">http://arxiv.org/abs/2312.16365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16365] Active Third-Person Imitation Learning](http://arxiv.org/abs/2312.16365) #generative</code></li>
<li>Summary: <p>We consider the problem of third-person imitation learning with the
additional challenge that the learner must select the perspective from which
they observe the expert. In our setting, each perspective provides only limited
information about the expert's behavior, and the learning agent must carefully
select and combine information from different perspectives to achieve
competitive performance. This setting is inspired by real-world imitation
learning applications, e.g., in robotics, a robot might observe a human
demonstrator via camera and receive information from different perspectives
depending on the camera's position. We formalize the aforementioned active
third-person imitation learning problem, theoretically analyze its
characteristics, and propose a generative adversarial network-based active
learning approach. Empirically, we demstrate that our proposed approach can
effectively learn from expert demonstrations and explore the importance of
different architectural choices for the learner's performance.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: ReSynthDetect: A Fundus Anomaly Detection Network with Reconstruction and Synthetic Features. (arXiv:2312.16470v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16470">http://arxiv.org/abs/2312.16470</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16470] ReSynthDetect: A Fundus Anomaly Detection Network with Reconstruction and Synthetic Features](http://arxiv.org/abs/2312.16470) #anomaly</code></li>
<li>Summary: <p>Detecting anomalies in fundus images through unsupervised methods is a
challenging task due to the similarity between normal and abnormal tissues, as
well as their indistinct boundaries. The current methods have limitations in
accurately detecting subtle anomalies while avoiding false positives. To
address these challenges, we propose the ReSynthDetect network which utilizes a
reconstruction network for modeling normal images, and an anomaly generator
that produces synthetic anomalies consistent with the appearance of fundus
images. By combining the features of consistent anomaly generation and image
reconstruction, our method is suited for detecting fundus abnormalities. The
proposed approach has been extensively tested on benchmark datasets such as
EyeQ and IDRiD, demonstrating state-of-the-art performance in both image-level
and pixel-level anomaly detection. Our experiments indicate a substantial 9%
improvement in AUROC on EyeQ and a significant 17.1% improvement in AUPR on
IDRiD.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: How Robust are LLMs to In-Context Majority Label Bias?. (arXiv:2312.16549v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16549">http://arxiv.org/abs/2312.16549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16549] How Robust are LLMs to In-Context Majority Label Bias?](http://arxiv.org/abs/2312.16549) #in-context</code></li>
<li>Summary: <p>In the In-Context Learning (ICL) setup, various forms of label biases can
manifest. One such manifestation is majority label bias, which arises when the
distribution of labeled examples in the in-context samples is skewed towards
one or more specific classes making Large Language Models (LLMs) more prone to
predict those labels. Such discrepancies can arise from various factors,
including logistical constraints, inherent biases in data collection methods,
limited access to diverse data sources, etc. which are unavoidable in a
real-world industry setup. In this work, we study the robustness of in-context
learning in LLMs to shifts that occur due to majority label bias within the
purview of text classification tasks. Prior works have shown that in-context
learning with LLMs is susceptible to such biases. In our study, we go one level
deeper and show that the robustness boundary varies widely for different models
and tasks, with certain LLMs being highly robust (~90%) to majority label bias.
Additionally, our findings also highlight the impact of model size and the
richness of instructional prompts contributing towards model robustness. We
restrict our study to only publicly available open-source models to ensure
transparency and reproducibility.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web. (arXiv:2312.16457v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16457">http://arxiv.org/abs/2312.16457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16457] City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web](http://arxiv.org/abs/2312.16457) #memory</code></li>
<li>Summary: <p>NeRF has significantly advanced 3D scene reconstruction, capturing intricate
details across various environments. Existing methods have successfully
leveraged radiance field baking to facilitate real-time rendering of small
scenes. However, when applied to large-scale scenes, these techniques encounter
significant challenges, struggling to provide a seamless real-time experience
due to limited resources in computation, memory, and bandwidth. In this paper,
we propose City-on-Web, which represents the whole scene by partitioning it
into manageable blocks, each with its own Level-of-Detail, ensuring high
fidelity, efficient memory management and fast rendering. Meanwhile, we
carefully design the training and inference process such that the final
rendering result on web is consistent with training. Thanks to our novel
representation and carefully designed training/inference process, we are the
first to achieve real-time rendering of large-scale scenes in
resource-constrained environments. Extensive experimental results demonstrate
that our method facilitates real-time rendering of large-scale scenes on a web
platform, achieving 32FPS at 1080P resolution with an RTX 3060 GPU, while
simultaneously achieving a quality that closely rivals that of state-of-the-art
methods. Project page: https://ustc3dv.github.io/City-on-Web/
</p></li>
</ul>
<h3>Title: Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding. (arXiv:2312.16477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16477">http://arxiv.org/abs/2312.16477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16477] Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding](http://arxiv.org/abs/2312.16477) #memory</code></li>
<li>Summary: <p>In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance.
</p></li>
</ul>
<h3>Title: Disentangled Continual Learning: Separating Memory Edits from Model Updates. (arXiv:2312.16731v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16731">http://arxiv.org/abs/2312.16731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16731] Disentangled Continual Learning: Separating Memory Edits from Model Updates](http://arxiv.org/abs/2312.16731) #memory</code></li>
<li>Summary: <p>The ability of machine learning systems to learn continually is hindered by
catastrophic forgetting, the tendency of neural networks to overwrite existing
knowledge when learning a new task. Existing continual learning methods
alleviate this problem through regularisation, parameter isolation, or
rehearsal, and are typically evaluated on benchmarks consisting of a handful of
tasks. We propose a novel conceptual approach to continual classification that
aims to disentangle class-specific information that needs to be memorised from
the class-agnostic knowledge that encapsulates generalization. We store the
former in a buffer that can be easily pruned or updated when new categories
arrive, while the latter is represented with a neural network that generalizes
across tasks. We show that the class-agnostic network does not suffer from
catastrophic forgetting and by leveraging it to perform classification, we
improve accuracy on past tasks over time. In addition, our approach supports
open-set classification and one-shot generalization. To test our conceptual
framework, we introduce Infinite dSprites, a tool for creating continual
classification and disentanglement benchmarks of arbitrary length with full
control over generative factors. We show that over a sufficiently long time
horizon all major types of continual learning methods break down, while our
approach enables continual learning over hundreds of tasks with explicit
control over memorization and forgetting.
</p></li>
</ul>
<h3>Title: LeanVec: Search your vectors faster by making them fit. (arXiv:2312.16335v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16335">http://arxiv.org/abs/2312.16335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16335] LeanVec: Search your vectors faster by making them fit](http://arxiv.org/abs/2312.16335) #memory</code></li>
<li>Summary: <p>Modern deep learning models have the ability to generate high-dimensional
vectors whose similarity reflects semantic resemblance. Thus, similarity
search, i.e., the operation of retrieving those vectors in a large collection
that are similar to a given query, has become a critical component of a wide
range of applications that demand highly accurate and timely answers. In this
setting, the high vector dimensionality puts similarity search systems under
compute and memory pressure, leading to subpar performance. Additionally,
cross-modal retrieval tasks have become increasingly common, e.g., where a user
inputs a text query to find the most relevant images for that query. However,
these queries often have different distributions than the database embeddings,
making it challenging to achieve high accuracy. In this work, we present
LeanVec, a framework that combines linear dimensionality reduction with vector
quantization to accelerate similarity search on high-dimensional vectors while
maintaining accuracy. We present LeanVec variants for in-distribution (ID) and
out-of-distribution (OOD) queries. LeanVec-ID yields accuracies on par with
those from recently introduced deep learning alternatives whose computational
overhead precludes their usage in practice. LeanVec-OOD uses a novel technique
for dimensionality reduction that considers the query and database
distributions to simultaneously boost the accuracy and the performance of the
framework even further (even presenting competitive results when the query and
database distributions match). All in all, our extensive and varied
experimental results show that LeanVec produces state-of-the-art results, with
up to 3.7x improvement in search throughput and up to 4.9x faster index build
time over the state of the art.
</p></li>
</ul>
<h3>Title: Photovoltaic power forecasting using quantum machine learning. (arXiv:2312.16379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16379">http://arxiv.org/abs/2312.16379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16379] Photovoltaic power forecasting using quantum machine learning](http://arxiv.org/abs/2312.16379) #memory</code></li>
<li>Summary: <p>Predicting solar panel power output is crucial for advancing the energy
transition but is complicated by the variable and non-linear nature of solar
energy. This is influenced by numerous meteorological factors, geographical
positioning, and photovoltaic cell properties, posing significant challenges to
forecasting accuracy and grid stability. Our study introduces a suite of
solutions centered around hybrid quantum neural networks designed to tackle
these complexities. The first proposed model, the Hybrid Quantum Long
Short-Term Memory, surpasses all tested models by over 40% lower mean absolute
and mean squared errors. The second proposed model, Hybrid Quantum
Sequence-to-Sequence neural network, once trained, predicts photovoltaic power
with 16% lower mean absolute error for arbitrary time intervals without the
need for prior meteorological data, highlighting its versatility. Moreover, our
hybrid models perform better even when trained on limited datasets, underlining
their potential utility in data-scarce scenarios. These findings represent a
stride towards resolving time series prediction challenges in energy power
forecasting through hybrid quantum models, showcasing the transformative
potential of quantum machine learning in catalyzing the renewable energy
transition.
</p></li>
</ul>
<h3>Title: Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16430">http://arxiv.org/abs/2312.16430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16430] Preference as Reward, Maximum Preference Optimization with Importance Sampling](http://arxiv.org/abs/2312.16430) #memory</code></li>
<li>Summary: <p>Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO's pairwise loss still can't s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
</p></li>
</ul>
<h3>Title: FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs. (arXiv:2312.16542v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16542">http://arxiv.org/abs/2312.16542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16542] FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs](http://arxiv.org/abs/2312.16542) #memory</code></li>
<li>Summary: <p>Graph Neural Network (GNN) ushered in a new era of machine learning with
interconnected datasets. While traditional neural networks can only be trained
on independent samples, GNN allows for the inclusion of inter-sample
interactions in the training process. This gain, however, incurs additional
memory cost, rendering most GNNs unscalable for real-world applications
involving vast and complicated networks with tens of millions of nodes (e.g.,
social circles, web graphs, and brain graphs). This means that storing the
graph in the main memory can be difficult, let alone training the GNN model
with significantly less GPU memory. While much of the recent literature has
focused on either mini-batching GNN methods or quantization, graph reduction
methods remain largely scarce. Furthermore, present graph reduction approaches
have several drawbacks. First, most graph reduction focuses only on the
inference stage (e.g., condensation and distillation) and requires full graph
GNN training, which does not reduce training memory footprint. Second, many
methods focus solely on the graph's structural aspect, ignoring the initial
population feature-label distribution, resulting in a skewed post-reduction
label distribution. Here, we propose a Feature-Label COnstrained graph Net
collapse, FALCON, to address these limitations. Our three core contributions
lie in (i) designing FALCON, a topology-aware graph reduction technique that
preserves feature-label distribution; (ii) implementation of FALCON with other
memory reduction methods (i.e., mini-batched GNN and quantization) for further
memory reduction; (iii) extensive benchmarking and ablation studies against
SOTA methods to evaluate FALCON memory reduction. Our extensive results show
that FALCON can significantly collapse various public datasets while achieving
equal prediction quality across GNN models. Code:
https://github.com/basiralab/FALCON
</p></li>
</ul>
<h3>Title: Learning Scalable Structural Representations for Link Prediction with Bloom Signatures. (arXiv:2312.16784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16784">http://arxiv.org/abs/2312.16784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16784] Learning Scalable Structural Representations for Link Prediction with Bloom Signatures](http://arxiv.org/abs/2312.16784) #memory</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have shown great potential in learning on
graphs, but they are known to perform sub-optimally on link prediction tasks.
Existing GNNs are primarily designed to learn node-wise representations and
usually fail to capture pairwise relations between target nodes, which proves
to be crucial for link prediction. Recent works resort to learning more
expressive edge-wise representations by enhancing vanilla GNNs with structural
features such as labeling tricks and link prediction heuristics, but they
suffer from high computational overhead and limited scalability. To tackle this
issue, we propose to learn structural link representations by augmenting the
message-passing framework of GNNs with Bloom signatures. Bloom signatures are
hashing-based compact encodings of node neighborhoods, which can be efficiently
merged to recover various types of edge-wise structural features. We further
show that any type of neighborhood overlap-based heuristic can be estimated by
a neural network that takes Bloom signatures as input. GNNs with Bloom
signatures are provably more expressive than vanilla GNNs and also more
scalable than existing edge-wise models. Experimental results on five standard
link prediction benchmarks show that our proposed model achieves comparable or
better performance than existing edge-wise GNN models while being 3-200
$\times$ faster and more memory-efficient for online inference.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: GRSDet: Learning to Generate Local Reverse Samples for Few-shot Object Detection. (arXiv:2312.16571v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16571">http://arxiv.org/abs/2312.16571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16571] GRSDet: Learning to Generate Local Reverse Samples for Few-shot Object Detection](http://arxiv.org/abs/2312.16571) #few-shot</code></li>
<li>Summary: <p>Few-shot object detection (FSOD) aims to achieve object detection only using
a few novel class training data. Most of the existing methods usually adopt a
transfer-learning strategy to construct the novel class distribution by
transferring the base class knowledge. However, this direct way easily results
in confusion between the novel class and other similar categories in the
decision space. To address the problem, we propose generating local reverse
samples (LRSamples) in Prototype Reference Frames to adaptively adjust the
center position and boundary range of the novel class distribution to learn
more discriminative novel class samples for FSOD. Firstly, we propose a Center
Calibration Variance Augmentation (CCVA) module, which contains the selection
rule of LRSamples, the generator of LRSamples, and augmentation on the
calibrated distribution centers. Specifically, we design an intra-class feature
converter (IFC) as the generator of CCVA to learn the selecting rule. By
transferring the knowledge of IFC from the base training to fine-tuning, the
IFC generates plentiful novel samples to calibrate the novel class
distribution. Moreover, we propose a Feature Density Boundary Optimization
(FDBO) module to adaptively adjust the importance of samples depending on their
distance from the decision boundary. It can emphasize the importance of the
high-density area of the similar class (closer decision boundary area) and
reduce the weight of the low-density area of the similar class (farther
decision boundary area), thus optimizing a clearer decision boundary for each
category. We conduct extensive experiments to demonstrate the effectiveness of
our proposed method. Our method achieves consistent improvement on the Pascal
VOC and MS COCO datasets based on DeFRCN and MFDC baselines.
</p></li>
</ul>
<h3>Title: Task Contamination: Language Models May Not Be Few-Shot Anymore. (arXiv:2312.16337v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.16337">http://arxiv.org/abs/2312.16337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.16337] Task Contamination: Language Models May Not Be Few-Shot Anymore](http://arxiv.org/abs/2312.16337) #few-shot</code></li>
<li>Summary: <p>Large language models (LLMs) offer impressive performance in various
zero-shot and few-shot tasks. However, their success in zero-shot and few-shot
settings may be affected by task contamination, a potential limitation that has
not been thoroughly examined. This paper investigates how zero-shot and
few-shot performance of LLMs has changed chronologically over time. Utilizing
GPT-3 series models and several other recent open-sourced LLMs, and controlling
for dataset difficulty, we find that on datasets released before the LLM
training data creation date, LLMs perform surprisingly better than on datasets
released after. This strongly indicates that, for many LLMs, there exists task
contamination on zero-shot and few-shot evaluation for datasets released prior
to the LLMs' training data creation date. Additionally, we utilize training
data inspection, task example extraction, and a membership inference attack,
which reveal further evidence of task contamination. Importantly, we find that
for classification tasks with no possibility of task contamination, LLMs rarely
demonstrate statistically significant improvements over simple majority
baselines, in both zero and few-shot settings.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-12-29]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
