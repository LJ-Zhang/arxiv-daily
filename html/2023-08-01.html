<h2>diffusion</h2>
<h3>Title: RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects. (arXiv:2307.15988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15988">http://arxiv.org/abs/2307.15988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15988] RGB-D-Fusion: Image Conditioned Depth Diffusion of Humanoid Subjects](http://arxiv.org/abs/2307.15988) #diffusion</code></li>
<li>Summary: <p>We present RGB-D-Fusion, a multi-modal conditional denoising diffusion
probabilistic model to generate high resolution depth maps from low-resolution
monocular RGB images of humanoid subjects. RGB-D-Fusion first generates a
low-resolution depth map using an image conditioned denoising diffusion
probabilistic model and then upsamples the depth map using a second denoising
diffusion probabilistic model conditioned on a low-resolution RGB-D image. We
further introduce a novel augmentation technique, depth noise augmentation, to
increase the robustness of our super-resolution model.
</p></li>
</ul>
<h3>Title: Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models. (arXiv:2307.15990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15990">http://arxiv.org/abs/2307.15990</a></li>
<li>Code URL: <a href="https://github.com/yuxin-zhang-jasmine/drus-v1">https://github.com/yuxin-zhang-jasmine/drus-v1</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15990] Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models](http://arxiv.org/abs/2307.15990) #diffusion</code></li>
<li>Summary: <p>Ultrasound image reconstruction can be approximately cast as a linear inverse
problem that has traditionally been solved with penalized optimization using
the $l_1$ or $l_2$ norm, or wavelet-based terms. However, such regularization
functions often struggle to balance the sparsity and the smoothness. A
promising alternative is using learned priors to make the prior knowledge
closer to reality. In this paper, we rely on learned priors under the framework
of Denoising Diffusion Restoration Models (DDRM), initially conceived for
restoration tasks with natural images. We propose and test two adaptions of
DDRM to ultrasound inverse problem models, DRUS and WDRUS. Our experiments on
synthetic and PICMUS data show that from a single plane wave our method can
achieve image quality comparable to or better than DAS and state-of-the-art
methods. The code is available at:
https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1.
</p></li>
</ul>
<h3>Title: HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation. (arXiv:2307.16183v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16183">http://arxiv.org/abs/2307.16183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16183] HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation](http://arxiv.org/abs/2307.16183) #diffusion</code></li>
<li>Summary: <p>In this paper, we study Text-to-3D content generation leveraging 2D diffusion
priors to enhance the quality and detail of the generated 3D models. Recent
progress (Magic3D) in text-to-3D has shown that employing high-resolution
(e.g., 512 x 512) renderings can lead to the production of high-quality 3D
models using latent diffusion priors. To enable rendering at even higher
resolutions, which has the potential to further augment the quality and detail
of the models, we propose a novel approach that combines multiple noise
estimation processes with a pretrained 2D diffusion prior. Distinct from the
Bar-Tal et al.s' study which binds multiple denoised results to generate images
from texts, our approach integrates the computation of scoring distillation
losses such as SDS loss and VSD loss which are essential techniques for the 3D
content generation with 2D diffusion priors. We experimentally evaluated the
proposed approach. The results show that the proposed approach can generate
high-quality details compared to the baselines.
</p></li>
</ul>
<h3>Title: An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid. (arXiv:2307.16149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16149">http://arxiv.org/abs/2307.16149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16149] An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid](http://arxiv.org/abs/2307.16149) #diffusion</code></li>
<li>Summary: <p>Energy theft detection (ETD) and energy consumption forecasting (ECF) are two
interconnected challenges in smart grid systems. Addressing these issues
collectively is crucial for ensuring system security. This paper addresses the
interconnected challenges of ETD and ECF in smart grid systems. The proposed
solution combines long short-term memory (LSTM) and a denoising diffusion
probabilistic model (DDPM) to generate input reconstruction and forecasting. By
leveraging the reconstruction and forecasting errors, the system identifies
instances of energy theft, with the methods based on reconstruction error and
forecasting error complementing each other in detecting different types of
attacks. Through extensive experiments on real-world and synthetic datasets,
the proposed scheme outperforms baseline methods in ETD and ECF problems. The
ensemble method significantly enhances ETD performance, accurately detecting
energy theft attacks that baseline methods fail to detect. The research offers
a comprehensive and effective solution for addressing ETD and ECF challenges,
demonstrating promising results and improved security in smart grid systems.
</p></li>
</ul>
<h3>Title: ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks. (arXiv:2307.16092v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16092">http://arxiv.org/abs/2307.16092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16092] ADR-GNN: Advection-Diffusion-Reaction Graph Neural Networks](http://arxiv.org/abs/2307.16092) #diffusion</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have shown remarkable success in learning
representations for graph-structured data. However, GNNs still face challenges
in modeling complex phenomena that involve advection. In this paper, we propose
a novel GNN architecture based on Advection-Diffusion-Reaction systems, called
ADR-GNN. Advection models the directed transportation of information, diffusion
captures the local smoothing of information, and reaction represents the
non-linear transformation of information in channels. We provide an analysis of
the qualitative behavior of ADR-GNN, that shows the benefit of combining
advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate
ADR-GNN on real-world node classification and spatio-temporal datasets, and
show that it improves or offers competitive performance compared to
state-of-the-art networks.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: HandMIM: Pose-Aware Self-Supervised Learning for 3D Hand Mesh Estimation. (arXiv:2307.16061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16061">http://arxiv.org/abs/2307.16061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16061] HandMIM: Pose-Aware Self-Supervised Learning for 3D Hand Mesh Estimation](http://arxiv.org/abs/2307.16061) #self-supervised</code></li>
<li>Summary: <p>With an enormous number of hand images generated over time, unleashing pose
knowledge from unlabeled images for supervised hand mesh estimation is an
emerging yet challenging topic. To alleviate this issue, semi-supervised and
self-supervised approaches have been proposed, but they are limited by the
reliance on detection models or conventional ResNet backbones. In this paper,
inspired by the rapid progress of Masked Image Modeling (MIM) in visual
classification tasks, we propose a novel self-supervised pre-training strategy
for regressing 3D hand mesh parameters. Our approach involves a unified and
multi-granularity strategy that includes a pseudo keypoint alignment module in
the teacher-student framework for learning pose-aware semantic class tokens.
For patch tokens with detailed locality, we adopt a self-distillation manner
between teacher and student network based on MIM pre-training. To better fit
low-level regression tasks, we incorporate pixel reconstruction tasks for
multi-level representation learning. Additionally, we design a strong pose
estimation baseline using a simple vanilla vision Transformer (ViT) as the
backbone and attach a PyMAF head after tokens for regression. Extensive
experiments demonstrate that our proposed approach, named HandMIM, achieves
strong performance on various hand mesh estimation tasks. Notably, HandMIM
outperforms specially optimized architectures, achieving 6.29mm and 8.00mm
PAVPE (Vertex-Point-Error) on challenging FreiHAND and HO3Dv2 test sets,
respectively, establishing new state-of-the-art records on 3D hand mesh
estimation.
</p></li>
</ul>
<h3>Title: Self-Supervised Learning of Gait-Based Biomarkers. (arXiv:2307.16321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16321">http://arxiv.org/abs/2307.16321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16321] Self-Supervised Learning of Gait-Based Biomarkers](http://arxiv.org/abs/2307.16321) #self-supervised</code></li>
<li>Summary: <p>Markerless motion capture (MMC) is revolutionizing gait analysis in clinical
settings by making it more accessible, raising the question of how to extract
the most clinically meaningful information from gait data. In multiple fields
ranging from image processing to natural language processing, self-supervised
learning (SSL) from large amounts of unannotated data produces very effective
representations for downstream tasks. However, there has only been limited use
of SSL to learn effective representations of gait and movement, and it has not
been applied to gait analysis with MMC. One SSL objective that has not been
applied to gait is contrastive learning, which finds representations that place
similar samples closer together in the learned space. If the learned similarity
metric captures clinically meaningful differences, this could produce a useful
representation for many downstream clinical tasks. Contrastive learning can
also be combined with causal masking to predict future timesteps, which is an
appealing SSL objective given the dynamical nature of gait. We applied these
techniques to gait analyses performed with MMC in a rehabilitation hospital
from a diverse clinical population. We find that contrastive learning on
unannotated gait data learns a representation that captures clinically
meaningful information. We probe this learned representation using the
framework of biomarkers and show it holds promise as both a diagnostic and
response biomarker, by showing it can accurately classify diagnosis from gait
and is responsive to inpatient therapy, respectively. We ultimately hope these
learned representations will enable predictive and prognostic gait-based
biomarkers that can facilitate precision rehabilitation through greater use of
MMC to quantify movement in rehabilitation.
</p></li>
</ul>
<h3>Title: Mispronunciation detection using self-supervised speech representations. (arXiv:2307.16324v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16324">http://arxiv.org/abs/2307.16324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16324] Mispronunciation detection using self-supervised speech representations](http://arxiv.org/abs/2307.16324) #self-supervised</code></li>
<li>Summary: <p>In recent years, self-supervised learning (SSL) models have produced
promising results in a variety of speech-processing tasks, especially in
contexts of data scarcity. In this paper, we study the use of SSL models for
the task of mispronunciation detection for second language learners. We compare
two downstream approaches: 1) training the model for phone recognition (PR)
using native English data, and 2) training a model directly for the target task
using non-native English data. We compare the performance of these two
approaches for various SSL representations as well as a representation
extracted from a traditional DNN-based speech recognition model. We evaluate
the models on L2Arctic and EpaDB, two datasets of non-native speech annotated
with pronunciation labels at the phone level. Overall, we find that using a
downstream model trained for the target task gives the best performance and
that most upstream models perform similarly for the task.
</p></li>
</ul>
<h3>Title: MUSE: Multi-View Contrastive Learning for Heterophilic Graphs. (arXiv:2307.16026v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16026">http://arxiv.org/abs/2307.16026</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16026] MUSE: Multi-View Contrastive Learning for Heterophilic Graphs](http://arxiv.org/abs/2307.16026) #self-supervised</code></li>
<li>Summary: <p>In recent years, self-supervised learning has emerged as a promising approach
in addressing the issues of label dependency and poor generalization
performance in traditional GNNs. However, existing self-supervised methods have
limited effectiveness on heterophilic graphs, due to the homophily assumption
that results in similar node representations for connected nodes. In this work,
we propose a multi-view contrastive learning model for heterophilic graphs,
namely, MUSE. Specifically, we construct two views to capture the information
of the ego node and its neighborhood by GNNs enhanced with contrastive
learning, respectively. Then we integrate the information from these two views
to fuse the node representations. Fusion contrast is utilized to enhance the
effectiveness of fused node representations. Further, considering that the
influence of neighboring contextual information on information fusion may vary
across different ego nodes, we employ an information fusion controller to model
the diversity of node-neighborhood similarity at both the local and global
levels. Finally, an alternating training scheme is adopted to ensure that
unsupervised node representation learning and information fusion controller can
mutually reinforce each other. We conduct extensive experiments to evaluate the
performance of MUSE on 9 benchmark datasets. Our results show the effectiveness
of MUSE on both node classification and clustering tasks.
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: Open-Set Domain Adaptation with Visual-Language Foundation Models. (arXiv:2307.16204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16204">http://arxiv.org/abs/2307.16204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16204] Open-Set Domain Adaptation with Visual-Language Foundation Models](http://arxiv.org/abs/2307.16204) #foundation model</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) has proven to be very effective in
transferring knowledge obtained from a source domain with labeled data to a
target domain with unlabeled data. Owing to the lack of labeled data in the
target domain and the possible presence of unknown classes, open-set domain
adaptation (ODA) has emerged as a potential solution to identify these classes
during the training phase. Although existing ODA approaches aim to solve the
distribution shifts between the source and target domains, most methods
fine-tuned ImageNet pre-trained models on the source domain with the adaptation
on the target domain. Recent visual-language foundation models (VLFM), such as
Contrastive Language-Image Pre-Training (CLIP), are robust to many distribution
shifts and, therefore, should substantially improve the performance of ODA. In
this work, we explore generic ways to adopt CLIP, a popular VLFM, for ODA. We
investigate the performance of zero-shot prediction using CLIP, and then
propose an entropy optimization strategy to assist the ODA models with the
outputs of CLIP. The proposed approach achieves state-of-the-art results on
various benchmarks, demonstrating its effectiveness in addressing the ODA
problem.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network. (arXiv:2307.15860v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15860">http://arxiv.org/abs/2307.15860</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15860] What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network](http://arxiv.org/abs/2307.15860) #generative</code></li>
<li>Summary: <p>In recent decades, Generative Adversarial Network (GAN) and its variants have
achieved unprecedented success in image synthesis. However, well-trained GANs
are under the threat of illegal steal or leakage. The prior studies on remote
ownership verification assume a black-box setting where the defender can query
the suspicious model with specific inputs, which we identify is not enough for
generation tasks. To this end, in this paper, we propose a novel IP protection
scheme for GANs where ownership verification can be done by checking outputs
only, without choosing the inputs (i.e., box-free setting). Specifically, we
make use of the unexploited potential of the discriminator to learn a
hypersphere that captures the unique distribution learned by the paired
generator. Extensive evaluations on two popular GAN tasks and more than 10 GAN
architectures demonstrate our proposed scheme to effectively verify the
ownership. Our proposed scheme shown to be immune to popular input-based
removal attacks and robust against other existing attacks. The source code and
models are available at
https://github.com/AbstractTeen/gan_ownership_verification
</p></li>
</ul>
<h3>Title: Fingerprints of Generative Models in the Frequency Domain. (arXiv:2307.15977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15977">http://arxiv.org/abs/2307.15977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15977] Fingerprints of Generative Models in the Frequency Domain](http://arxiv.org/abs/2307.15977) #generative</code></li>
<li>Summary: <p>It is verified in existing works that CNN-based generative models leave
unique fingerprints on generated images. There is a lack of analysis about how
they are formed in generative models. Interpreting network components in the
frequency domain, we derive sources for frequency distribution and grid-like
pattern discrepancies exhibited on the spectrum. These insights are leveraged
to develop low-cost synthetic models, which generate images emulating the
frequency patterns observed in real generative models. The resulting
fingerprint extractor pre-trained on synthetic data shows superior
transferability in verifying, identifying, and analyzing the relationship of
real CNN-based generative models such as GAN, VAE, Flow, and diffusion.
</p></li>
</ul>
<h3>Title: SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16125">http://arxiv.org/abs/2307.16125</a></li>
<li>Code URL: <a href="https://github.com/ailab-cvc/seed-bench">https://github.com/ailab-cvc/seed-bench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16125] SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension](http://arxiv.org/abs/2307.16125) #generative</code></li>
<li>Summary: <p>Based on powerful Large Language Models (LLMs), recent generative Multimodal
Large Language Models (MLLMs) have gained prominence as a pivotal research
area, exhibiting remarkable capability for both comprehension and generation.
In this work, we address the evaluation of generative comprehension in MLLMs as
a preliminary step towards a comprehensive assessment of generative models, by
introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple
choice questions with accurate human annotations (x 6 larger than existing
benchmarks), which spans 12 evaluation dimensions including the comprehension
of both the image and video modality. We develop an advanced pipeline for
generating multiple-choice questions that target specific evaluation
dimensions, integrating both automatic filtering and manual verification
processes. Multiple-choice questions with groundtruth options derived from
human annotation enables an objective and efficient assessment of model
performance, eliminating the need for human or GPT intervention during
evaluation. We further evaluate the performance of 18 models across all 12
dimensions, covering both the spatial and temporal understanding. By revealing
the limitations of existing MLLMs through evaluation results, we aim for
SEED-Bench to provide insights for motivating future research. We will launch
and consistently maintain a leaderboard to provide a platform for the community
to assess and investigate model capability.
</p></li>
</ul>
<h3>Title: StylePrompter: All Styles Need Is Attention. (arXiv:2307.16151v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16151">http://arxiv.org/abs/2307.16151</a></li>
<li>Code URL: <a href="https://github.com/i2-multimedia-lab/styleprompter">https://github.com/i2-multimedia-lab/styleprompter</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16151] StylePrompter: All Styles Need Is Attention](http://arxiv.org/abs/2307.16151) #generative</code></li>
<li>Summary: <p>GAN inversion aims at inverting given images into corresponding latent codes
for Generative Adversarial Networks (GANs), especially StyleGAN where exists a
disentangled latent space that allows attribute-based image manipulation at
latent level. As most inversion methods build upon Convolutional Neural
Networks (CNNs), we transfer a hierarchical vision Transformer backbone
innovatively to predict $\mathcal{W^+}$ latent codes at token level. We further
apply a Style-driven Multi-scale Adaptive Refinement Transformer (SMART) in
$\mathcal{F}$ space to refine the intermediate style features of the generator.
By treating style features as queries to retrieve lost identity information
from the encoder's feature maps, SMART can not only produce high-quality
inverted images but also surprisingly adapt to editing tasks. We then prove
that StylePrompter lies in a more disentangled $\mathcal{W^+}$ and show the
controllability of SMART. Finally, quantitative and qualitative experiments
demonstrate that StylePrompter can achieve desirable performance in balancing
reconstruction quality and editability, and is "smart" enough to fit into most
edits, outperforming other $\mathcal{F}$-involved inversion methods.
</p></li>
</ul>
<h3>Title: Stylized Projected GAN: A Novel Architecture for Fast and Realistic Image Generation. (arXiv:2307.16275v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16275">http://arxiv.org/abs/2307.16275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16275] Stylized Projected GAN: A Novel Architecture for Fast and Realistic Image Generation](http://arxiv.org/abs/2307.16275) #generative</code></li>
<li>Summary: <p>Generative Adversarial Networks are used for generating the data using a
generator and a discriminator, GANs usually produce high-quality images, but
training GANs in an adversarial setting is a difficult task. GANs require high
computation power and hyper-parameter regularization for converging. Projected
GANs tackle the training difficulty of GANs by using transfer learning to
project the generated and real samples into a pre-trained feature space.
Projected GANs improve the training time and convergence but produce artifacts
in the generated images which reduce the quality of the generated samples, we
propose an optimized architecture called Stylized Projected GANs which
integrates the mapping network of the Style GANs with Skip Layer Excitation of
Fast GAN. The integrated modules are incorporated within the generator
architecture of the Fast GAN to mitigate the problem of artifacts in the
generated images.
</p></li>
</ul>
<h3>Title: Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI. (arXiv:2307.15715v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15715">http://arxiv.org/abs/2307.15715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15715] Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI](http://arxiv.org/abs/2307.15715) #generative</code></li>
<li>Summary: <p>Primary care professionals struggle to keep up to date with the latest
scientific literature critical in guiding evidence-based practice related to
their daily work. To help solve the above-mentioned problem, we employed
generative artificial intelligence techniques based on large-scale language
models to summarize abstracts of scientific papers. Our objective is to
investigate the potential of generative artificial intelligence in diminishing
the cognitive load experienced by practitioners, thus exploring its ability to
alleviate mental effort and burden. The study participants were provided with
two use cases related to preventive care and behavior change, simulating a
search for new scientific literature. The study included 113 university
students from Slovenia and the United States randomized into three distinct
study groups. The first group was assigned to the full abstracts. The second
group was assigned to the short abstracts generated by AI. The third group had
the option to select a full abstract in addition to the AI-generated short
summary. Each use case study included ten retrieved abstracts. Our research
demonstrates that the use of generative AI for literature review is efficient
and effective. The time needed to answer questions related to the content of
abstracts was significantly lower in groups two and three compared to the first
group using full abstracts. The results, however, also show significantly lower
accuracy in extracted knowledge in cases where full abstract was not available.
Such a disruptive technology could significantly reduce the time required for
healthcare professionals to keep up with the most recent scientific literature;
nevertheless, further developments are needed to help them comprehend the
knowledge accurately.
</p></li>
</ul>
<h3>Title: Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16039">http://arxiv.org/abs/2307.16039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16039] Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2307.16039) #generative</code></li>
<li>Summary: <p>A key technology for the development of large language models (LLMs) involves
instruction tuning that helps align the models' responses with human
expectations to realize impressive learning abilities. Two major approaches for
instruction tuning characterize supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), which are currently applied to produce the
best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for
research and development efforts, various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Alpaca, Vicuna, to name a few.
However, existing open-source LLMs have only been instruction-tuned for English
and a few popular languages, thus hindering their impacts and accessibility to
many other languages in the world. Among a few very recent work to explore
instruction tuning for LLMs in multiple languages, SFT has been used as the
only approach to instruction-tune LLMs for multiple languages. This has left a
significant gap for fine-tuned LLMs based on RLHF in diverse languages and
raised important questions on how RLHF can boost the performance of
multilingual instruction tuning. To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based on RLHF for multiple languages.
Okapi introduces instruction and response-ranked data in 26 diverse languages
to facilitate the experiments and development of future multilingual LLM
research. We also present benchmark datasets to enable the evaluation of
generative LLMs in multiple languages. Our experiments demonstrate the
advantages of RLHF for multilingual instruction over SFT for different base
models and datasets. Our framework and resources are released at
\url{https://github.com/nlp-uoregon/Okapi}.
</p></li>
</ul>
<h3>Title: A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16200">http://arxiv.org/abs/2307.16200</a></li>
<li>Code URL: <a href="https://github.com/flyingcat-fa/ktgf">https://github.com/flyingcat-fa/ktgf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16200] A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction](http://arxiv.org/abs/2307.16200) #generative</code></li>
<li>Summary: <p>This paper focuses on term-status pair extraction from medical dialogues
(MD-TSPE), which is essential in diagnosis dialogue systems and the automatic
scribe of electronic medical records (EMRs). In the past few years, works on
MD-TSPE have attracted increasing research attention, especially after the
remarkable progress made by generative methods. However, these generative
methods output a whole sequence consisting of term-status pairs in one stage
and ignore integrating prior knowledge, which demands a deeper understanding to
model the relationship between terms and infer the status of each term. This
paper presents a knowledge-enhanced two-stage generative framework (KTGF) to
address the above challenges. Using task-specific prompts, we employ a single
model to complete the MD-TSPE through two phases in a unified generative form:
we generate all terms the first and then generate the status of each generated
term. In this way, the relationship between terms can be learned more
effectively from the sequence containing only terms in the first phase, and our
designed knowledge-enhanced prompt in the second phase can leverage the
category and status candidates of the generated term for status generation.
Furthermore, our proposed special status ``not mentioned" makes more terms
available and enriches the training data in the second phase, which is critical
in the low-resource setting. The experiments on the Chunyu and CMDD datasets
show that the proposed method achieves superior results compared to the
state-of-the-art models in the full training and low-resource settings.
</p></li>
</ul>
<h3>Title: SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems. (arXiv:2307.15786v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15786">http://arxiv.org/abs/2307.15786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15786] SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems](http://arxiv.org/abs/2307.15786) #generative</code></li>
<li>Summary: <p>A CF explainer identifies the minimum modifications in the input that would
alter the model's output to its complement. In other words, a CF explainer
computes the minimum modifications required to cross the model's decision
boundary. Current deep generative CF models often work with user-selected
features rather than focusing on the discriminative features of the black-box
model. Consequently, such CF examples may not necessarily lie near the decision
boundary, thereby contradicting the definition of CFs. To address this issue,
we propose in this paper a novel approach that leverages saliency maps to
generate more informative CF explanations. Source codes are available at:
https://github.com/Amir-Samadi//Saliency_Aware_CF.
</p></li>
</ul>
<h3>Title: Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16164">http://arxiv.org/abs/2307.16164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16164] Adaptive learning of density ratios in RKHS](http://arxiv.org/abs/2307.16164) #generative</code></li>
<li>Summary: <p>Estimating the ratio of two probability densities from finitely many
observations of the densities is a central problem in machine learning and
statistics with applications in two-sample testing, divergence estimation,
generative modeling, covariate shift adaptation, conditional density
estimation, and novelty detection. In this work, we analyze a large class of
density ratio estimation methods that minimize a regularized Bregman divergence
between the true density ratio and a model in a reproducing kernel Hilbert
space (RKHS). We derive new finite-sample error bounds, and we propose a
Lepskii type parameter choice principle that minimizes the bounds without
knowledge of the regularity of the density ratio. In the special case of
quadratic loss, our method adaptively achieves a minimax optimal error rate. A
numerical illustration is provided.
</p></li>
</ul>
<h2>anomaly</h2>
<h3>Title: Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15807">http://arxiv.org/abs/2307.15807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15807] Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping](http://arxiv.org/abs/2307.15807) #anomaly</code></li>
<li>Summary: <p>Anomaly detection is critical in the smart industry for preventing equipment
failure, reducing downtime, and improving safety. Internet of Things (IoT) has
enabled the collection of large volumes of data from industrial machinery,
providing a rich source of information for Anomaly Detection. However, the
volume and complexity of data generated by the Internet of Things ecosystems
make it difficult for humans to detect anomalies manually. Machine learning
(ML) algorithms can automate anomaly detection in industrial machinery by
analyzing generated data. Besides, each technique has specific strengths and
weaknesses based on the data nature and its corresponding systems. However, the
current systematic mapping studies on Anomaly Detection primarily focus on
addressing network and cybersecurity-related problems, with limited attention
given to the industrial sector. Additionally, these studies do not cover the
challenges involved in using ML for Anomaly Detection in industrial machinery
within the context of the IoT ecosystems. This paper presents a systematic
mapping study on Anomaly Detection for industrial machinery using IoT devices
and ML algorithms to address this gap. The study comprehensively evaluates 84
relevant studies spanning from 2016 to 2023, providing an extensive review of
Anomaly Detection research. Our findings identify the most commonly used
algorithms, preprocessing techniques, and sensor types. Additionally, this
review identifies application areas and points to future challenges and
research opportunities.
</p></li>
</ul>
<h2>in-context</h2>
<h3>Title: Distractor generation for multiple-choice questions with predictive prompting and large language models. (arXiv:2307.16338v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.16338">http://arxiv.org/abs/2307.16338</a></li>
<li>Code URL: <a href="https://github.com/semerekiros/distractgpt">https://github.com/semerekiros/distractgpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.16338] Distractor generation for multiple-choice questions with predictive prompting and large language models](http://arxiv.org/abs/2307.16338) #in-context</code></li>
<li>Summary: <p>Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable
performance across various tasks and have garnered significant attention from
both researchers and practitioners. However, in an educational context, we
still observe a performance gap in generating distractors -- i.e., plausible
yet incorrect answers -- with LLMs for multiple-choice questions (MCQs). In
this study, we propose a strategy for guiding LLMs such as ChatGPT, in
generating relevant distractors by prompting them with question items
automatically retrieved from a question bank as well-chosen in-context
examples. We evaluate our LLM-based solutions using a quantitative assessment
on an existing test set, as well as through quality annotations by human
experts, i.e., teachers. We found that on average 53% of the generated
distractors presented to the teachers were rated as high-quality, i.e.,
suitable for immediate use as is, outperforming the state-of-the-art model. We
also show the gains of our approach 1 in generating high-quality distractors by
comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with
static examples.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-08-01]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
