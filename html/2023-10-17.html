<h2>diffusion</h2>
<h3>Title: PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation. (arXiv:2310.09458v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09458">http://arxiv.org/abs/2310.09458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09458] PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation](http://arxiv.org/abs/2310.09458) #diffusion</code></li>
<li>Summary: <p>Recent advances in zero-shot text-to-3D human generation, which employ the
human model prior (eg, SMPL) or Score Distillation Sampling (SDS) with
pre-trained text-to-image diffusion models, have been groundbreaking. However,
SDS may provide inaccurate gradient directions under the weak diffusion
guidance, as it tends to produce over-smoothed results and generate body
textures that are inconsistent with the detailed mesh geometry. Therefore,
directly leverage existing strategies for high-fidelity text-to-3D human
texturing is challenging. In this work, we propose a model called PaintHuman to
addresses the challenges from two aspects. We first propose a novel score
function, Denoised Score Distillation (DSD), which directly modifies the SDS by
introducing negative gradient components to iteratively correct the gradient
direction and generate high-quality textures. In addition, we use the depth map
as a geometric guidance to ensure the texture is semantically aligned to human
mesh surfaces. To guarantee the quality of rendered results, we employ
geometry-aware networks to predict surface materials and render realistic human
textures. Extensive experiments, benchmarked against state-of-the-art methods,
validate the efficacy of our approach.
</p></li>
</ul>
<h3>Title: Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner. (arXiv:2310.09469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09469">http://arxiv.org/abs/2310.09469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09469] Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner](http://arxiv.org/abs/2310.09469) #diffusion</code></li>
<li>Summary: <p>A diffusion model, which is formulated to produce an image using thousands of
denoising steps, usually suffers from a slow inference speed. Existing
acceleration algorithms simplify the sampling by skipping most steps yet
exhibit considerable performance degradation. By viewing the generation of
diffusion models as a discretized integrating process, we argue that the
quality drop is partly caused by applying an inaccurate integral direction to a
timestep interval. To rectify this issue, we propose a timestep aligner that
helps find a more accurate integral direction for a particular interval at the
minimum cost. Specifically, at each denoising step, we replace the original
parameterization by conditioning the network on a new timestep, which is
obtained by aligning the sampling distribution to the real distribution.
Extensive experiments show that our plug-in design can be trained efficiently
and boost the inference performance of various state-of-the-art acceleration
methods, especially when there are few denoising steps. For example, when using
10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of
DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate
set of timesteps. Code will be made publicly available.
</p></li>
</ul>
<h3>Title: Unified High-binding Watermark for Unconditional Image Generation Models. (arXiv:2310.09479v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09479">http://arxiv.org/abs/2310.09479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09479] Unified High-binding Watermark for Unconditional Image Generation Models](http://arxiv.org/abs/2310.09479) #diffusion</code></li>
<li>Summary: <p>Deep learning techniques have implemented many unconditional image generation
(UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images
(also known as AI-Generated Content, AIGC for short) produced by these models
bring urgent needs for intellectual property protection such as data
traceability and copyright certification. An attacker can steal the output
images of the target model and use them as part of the training data to train a
private surrogate UIG model. The implementation mechanisms of UIG models are
diverse and complex, and there is no unified and effective protection and
verification method at present. To address these issues, we propose a two-stage
unified watermark verification mechanism with high-binding effects for such
models. In the first stage, we use an encoder to invisibly write the watermark
image into the output images of the original AIGC tool, and reversely extract
the watermark image through the corresponding decoder. In the second stage, we
design the decoder fine-tuning process, and the fine-tuned decoder can make
correct judgments on whether the suspicious model steals the original AIGC tool
data. Experiments demonstrate our method can complete the verification work
with almost zero false positive rate under the condition of only using the
model output images. Moreover, the proposed method can achieve data steal
verification across different types of UIG models, which further increases the
practicality of the method.
</p></li>
</ul>
<h3>Title: Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09484">http://arxiv.org/abs/2310.09484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09484] Exploring the Design Space of Diffusion Autoencoders for Face Morphing](http://arxiv.org/abs/2310.09484) #diffusion</code></li>
<li>Summary: <p>Face morphs created by Diffusion Autoencoders are a recent innovation and the
design space of such an approach has not been well explored. We explore three
axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM
solver, and 3) partial sampling through small amounts of added noise.
</p></li>
</ul>
<h3>Title: Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09336">http://arxiv.org/abs/2310.09336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09336] Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task](http://arxiv.org/abs/2310.09336) #diffusion</code></li>
<li>Summary: <p>Modern generative models exhibit unprecedented capabilities to generate
extremely realistic data. However, given the inherent compositionality of the
real world, reliable use of these models in practical applications requires
that they exhibit the capability to compose a novel set of concepts to generate
outputs not seen in the training data set. Prior work demonstrates that recent
diffusion models do exhibit intriguing compositional generalization abilities,
but also fail unpredictably. Motivated by this, we perform a controlled study
for understanding compositional generalization in conditional diffusion models
in a synthetic setting, varying different attributes of the training data and
measuring the model's ability to generate samples out-of-distribution. Our
results show: (i) the order in which the ability to generate samples from a
concept and compose them emerges is governed by the structure of the underlying
data-generating process; (ii) performance on compositional tasks exhibits a
sudden ``emergence'' due to multiplicative reliance on the performance of
constituent tasks, partially explaining emergent phenomena seen in generative
models; and (iii) composing concepts with lower frequency in the training data
to generate out-of-distribution samples requires considerably more optimization
steps compared to generating in-distribution samples. Overall, our study lays a
foundation for understanding capabilities and compositionality in generative
models from a data-centric perspective.
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance. (arXiv:2310.09507v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09507">http://arxiv.org/abs/2310.09507</a></li>
<li>Code URL: <a href="https://github.com/jlianglab/ark">https://github.com/jlianglab/ark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09507] Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance](http://arxiv.org/abs/2310.09507) #self-supervised</code></li>
<li>Summary: <p>Deep learning nowadays offers expert-level and sometimes even
super-expert-level performance, but achieving such performance demands massive
annotated data for training (e.g., Google's proprietary CXR Foundation Model
(CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays
(CXRs)). Numerous datasets are publicly available in medical imaging but
individually small and heterogeneous in expert labels. We envision a powerful
and robust foundation model that can be trained by aggregating numerous small
public datasets. To realize this vision, we have developed Ark, a framework
that accrues and reuses knowledge from heterogeneous expert annotations in
various datasets. As a proof of concept, we have trained two Ark models on
335,484 and 704,363 CXRs, respectively, by merging several datasets including
ChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range
of imaging tasks covering both classification and segmentation via fine-tuning,
linear-probing, and gender-bias analysis, and demonstrated our Ark's superior
and robust performance over the SOTA fully/self-supervised baselines and
Google's proprietary CXR-FM. This enhanced performance is attributed to our
simple yet powerful observation that aggregating numerous public datasets
diversifies patient populations and accrues knowledge from diverse experts,
yielding unprecedented performance yet saving annotation cost. With all codes
and pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark
exerts an important impact on open science, as accruing and reusing knowledge
from expert annotations in public datasets can potentially surpass the
performance of proprietary models trained on unusually large data, inspiring
many more researchers worldwide to share codes and datasets to build open
foundation models, accelerate open science, and democratize deep learning for
medical imaging.
</p></li>
</ul>
<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Efficient Apple Maturity and Damage Assessment: A Lightweight Detection Model with GAN and Attention Mechanism. (arXiv:2310.09347v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09347">http://arxiv.org/abs/2310.09347</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09347] Efficient Apple Maturity and Damage Assessment: A Lightweight Detection Model with GAN and Attention Mechanism](http://arxiv.org/abs/2310.09347) #generative</code></li>
<li>Summary: <p>This study proposes a method based on lightweight convolutional neural
networks (CNN) and generative adversarial networks (GAN) for apple ripeness and
damage level detection tasks. Initially, a lightweight CNN model is designed by
optimizing the model's depth and width, as well as employing advanced model
compression techniques, successfully reducing the model's parameter and
computational requirements, thus enhancing real-time performance in practical
applications. Simultaneously, attention mechanisms are introduced, dynamically
adjusting the importance of different feature layers to improve the performance
in object detection tasks. To address the issues of sample imbalance and
insufficient sample size, GANs are used to generate realistic apple images,
expanding the training dataset and enhancing the model's recognition capability
when faced with apples of varying ripeness and damage levels. Furthermore, by
applying the object detection network for damage location annotation on damaged
apples, the accuracy of damage level detection is improved, providing a more
precise basis for decision-making. Experimental results show that in apple
ripeness grading detection, the proposed model achieves 95.6\%, 93.8\%, 95.0\%,
and 56.5 in precision, recall, accuracy, and FPS, respectively. In apple damage
level detection, the proposed model reaches 95.3\%, 93.7\%, and 94.5\% in
precision, recall, and mAP, respectively. In both tasks, the proposed method
outperforms other mainstream models, demonstrating the excellent performance
and high practical value of the proposed method in apple ripeness and damage
level detection tasks.
</p></li>
</ul>
<h3>Title: A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09430">http://arxiv.org/abs/2310.09430</a></li>
<li>Code URL: <a href="https://github.com/strong-ai-lab/logical-and-abstract-reasoning">https://github.com/strong-ai-lab/logical-and-abstract-reasoning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09430] A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks](http://arxiv.org/abs/2310.09430) #generative</code></li>
<li>Summary: <p>Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named "ReClor-plus",
"LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
"none of the other options are correct", and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model's
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
</p></li>
</ul>
<h3>Title: One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09499">http://arxiv.org/abs/2310.09499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09499] One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models](http://arxiv.org/abs/2310.09499) #generative</code></li>
<li>Summary: <p>Various Large Language Models(LLMs) from the Generative Pretrained
Transformer~(GPT) family have achieved outstanding performances in a wide range
of text generation tasks. However, the enormous model sizes have hindered their
practical use in real-world applications due to high inference latency.
Therefore, improving the efficiencies of LLMs through quantization, pruning,
and other means has been a key issue in LLM studies. In this work, we propose a
method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs
to at least 50\% sparsity without the need of any retraining. It allocates
sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced
error while maintaining the overall sparsity level. The advantages of the
proposed method exhibit even more when the sparsity is extremely high.
Furthermore, our method is compatible with quantization, enabling further
compression of LLMs.
</p></li>
</ul>
<h3>Title: Uncertainty Quantification using Generative Approach. (arXiv:2310.09338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09338">http://arxiv.org/abs/2310.09338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09338] Uncertainty Quantification using Generative Approach](http://arxiv.org/abs/2310.09338) #generative</code></li>
<li>Summary: <p>We present the Incremental Generative Monte Carlo (IGMC) method, designed to
measure uncertainty in deep neural networks using deep generative approaches.
IGMC iteratively trains generative models, adding their output to the dataset,
to compute the posterior distribution of the expectation of a random variable.
We provide a theoretical guarantee of the convergence rate of IGMC relative to
the sample size and sampling depth. Due to its compatibility with deep
generative approaches, IGMC is adaptable to both neural network classification
and regression tasks. We empirically study the behavior of IGMC on the MNIST
digit classification task.
</p></li>
</ul>
<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation. (arXiv:2310.09424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09424">http://arxiv.org/abs/2310.09424</a></li>
<li>Code URL: <a href="https://github.com/NVIDIA/NeMo">https://github.com/NVIDIA/NeMo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09424] SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation](http://arxiv.org/abs/2310.09424) #in-context</code></li>
<li>Summary: <p>We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
</p></li>
</ul>
<h2>memory</h2>
<h3>Title: Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09297">http://arxiv.org/abs/2310.09297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09297] Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms](http://arxiv.org/abs/2310.09297) #memory</code></li>
<li>Summary: <p>How humans and machines make sense of current inputs for relation reasoning
and question-answering while putting the perceived information into context of
our past memories, has been a challenging conundrum in cognitive science and
artificial intelligence. Inspired by human brain's memory system and cognitive
architectures, we propose a PMI framework that consists of perception, memory
and inference components. Notably, the memory module comprises working and
long-term memory, with the latter endowed with a higher-order structure to
retain more accumulated knowledge and experiences. Through a differentiable
competitive write access, current perceptions update working memory, which is
later merged with long-term memory via outer product associations, averting
memory overflow and minimizing information conflicts. In the inference module,
relevant information is retrieved from two separate memory origins and
associatively integrated to attain a more comprehensive and precise
interpretation of current perceptions. We exploratively apply our PMI to
improve prevailing Transformers and CNN models on question-answering tasks like
bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image
classification tasks, and in each case, our PMI enhancements consistently
outshine their original counterparts significantly. Visualization analyses
reveal that memory consolidation, along with the interaction and integration of
information from diverse memory sources, substantially contributes to the model
effectiveness on inference tasks.
</p></li>
</ul>
<h3>Title: Computational analyses of linguistic features with schizophrenic and autistic traits along with formal thought disorders. (arXiv:2310.09494v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09494">http://arxiv.org/abs/2310.09494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09494] Computational analyses of linguistic features with schizophrenic and autistic traits along with formal thought disorders](http://arxiv.org/abs/2310.09494) #memory</code></li>
<li>Summary: <p>[See full abstract in the pdf] Formal Thought Disorder (FTD), which is a
group of symptoms in cognition that affects language and thought, can be
observed through language. FTD is seen across such developmental or psychiatric
disorders as Autism Spectrum Disorder (ASD) or Schizophrenia, and its related
Schizotypal Personality Disorder (SPD). This paper collected a Japanese
audio-report dataset with score labels related to ASD and SPD through a
crowd-sourcing service from the general population. We measured language
characteristics with the 2nd edition of the Social Responsiveness Scale (SRS2)
and the Schizotypal Personality Questionnaire (SPQ), including an odd speech
subscale from SPQ to quantify the FTD symptoms. We investigated the following
four research questions through machine-learning-based score predictions: (RQ1)
How are schizotypal and autistic measures correlated? (RQ2) What is the most
suitable task to elicit FTD symptoms? (RQ3) Does the length of speech affect
the elicitation of FTD symptoms? (RQ4) Which features are critical for
capturing FTD symptoms? We confirmed that an FTD-related subscale, odd speech,
was significantly correlated with both the total SPQ and SRS scores, although
they themselves were not correlated significantly. Our regression analysis
indicated that longer speech about a negative memory elicited more FTD
symptoms. The ablation study confirmed the importance of function words and
both the abstract and temporal features for FTD-related odd speech estimation.
In contrast, content words were effective only in the SRS predictions, and
content words were effective only in the SPQ predictions, a result that implies
the differences between SPD-like and ASD-like symptoms. Data and programs used
in this paper can be found here:
https://sites.google.com/view/sagatake/resource.
</p></li>
</ul>
<h3>Title: Attentive Multi-Layer Perceptron for Non-autoregressive Generation. (arXiv:2310.09512v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09512">http://arxiv.org/abs/2310.09512</a></li>
<li>Code URL: <a href="https://github.com/shark-nlp/attentivemlp">https://github.com/shark-nlp/attentivemlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09512] Attentive Multi-Layer Perceptron for Non-autoregressive Generation](http://arxiv.org/abs/2310.09512) #memory</code></li>
<li>Summary: <p>Autoregressive~(AR) generation almost dominates sequence generation for its
efficacy. Recently, non-autoregressive~(NAR) generation gains increasing
popularity for its efficiency and growing efficacy. However, its efficiency is
still bottlenecked by quadratic complexity in sequence lengths, which is
prohibitive for scaling to long sequence generation and few works have been
done to mitigate this problem. In this paper, we propose a novel MLP variant,
\textbf{A}ttentive \textbf{M}ulti-\textbf{L}ayer \textbf{P}erceptron~(AMLP), to
produce a generation model with linear time and space complexity. Different
from classic MLP with static and learnable projection matrices, AMLP leverages
adaptive projections computed from inputs in an attentive mode. The
sample-aware adaptive projections enable communications among tokens in a
sequence, and model the measurement between the query and key space.
Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient
NAR-AMLP architecture with linear time and space complexity. Empirical results
show that such marriage architecture surpasses competitive efficient NAR
models, by a significant margin on text-to-speech synthesis and machine
translation. We also test AMLP's self- and cross-attention ability separately
with extensive ablation experiments, and find them comparable or even superior
to the other efficient models. The efficiency analysis further shows that AMLP
extremely reduces the memory cost against vanilla non-autoregressive models for
long sequences.
</p></li>
</ul>
<h3>Title: Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations. (arXiv:2310.09434v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09434">http://arxiv.org/abs/2310.09434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09434] Learning nonlinear integral operators via Recurrent Neural Networks and its application in solving Integro-Differential Equations](http://arxiv.org/abs/2310.09434) #memory</code></li>
<li>Summary: <p>In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent
Neural Networks) to learn and represent nonlinear integral operators that
appear in nonlinear integro-differential equations (IDEs). The LSTM-RNN
representation of the nonlinear integral operator allows us to turn a system of
nonlinear integro-differential equations into a system of ordinary differential
equations for which many efficient solvers are available. Furthermore, because
the use of LSTM-RNN representation of the nonlinear integral operator in an IDE
eliminates the need to perform a numerical integration in each numerical time
evolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can
be reduced to $O(n_T)$ from $O(n_T^2)$ if a $n_T$-step trajectory is to be
computed. We illustrate the efficiency and robustness of this LSTM-RNN-based
numerical IDE solver with a model problem. Additionally, we highlight the
generalizability of the learned integral operator by applying it to IDEs driven
by different external forces. As a practical application, we show how this
methodology can effectively solve the Dyson's equation for quantum many-body
systems.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: Plug-and-Play Feature Generation for Few-Shot Medical Image Classification. (arXiv:2310.09471v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09471">http://arxiv.org/abs/2310.09471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09471] Plug-and-Play Feature Generation for Few-Shot Medical Image Classification](http://arxiv.org/abs/2310.09471) #few-shot</code></li>
<li>Summary: <p>Few-shot learning (FSL) presents immense potential in enhancing model
generalization and practicality for medical image classification with limited
training data; however, it still faces the challenge of severe overfitting in
classifier training due to distribution bias caused by the scarce training
samples. To address the issue, we propose MedMFG, a flexible and lightweight
plug-and-play method designed to generate sufficient class-distinctive features
from limited samples. Specifically, MedMFG first re-represents the limited
prototypes to assign higher weights for more important information features.
Then, the prototypes are variationally generated into abundant effective
features. Finally, the generated features and prototypes are together to train
a more generalized classifier. Experiments demonstrate that MedMFG outperforms
the previous state-of-the-art methods on cross-domain benchmarks involving the
transition from natural images to medical images, as well as medical images
with different lesions. Notably, our method achieves over 10% performance
improvement compared to several baselines. Fusion experiments further validate
the adaptability of MedMFG, as it seamlessly integrates into various backbones
and baselines, consistently yielding improvements of over 2.9% across all
results.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-10-17]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
