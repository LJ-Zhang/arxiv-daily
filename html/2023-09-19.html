<h2>diffusion</h2>
<h3>Title: Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?. (arXiv:2309.08760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08760">http://arxiv.org/abs/2309.08760</a></li>
<li>Code URL: <a href="https://github.com/aibhishek/Biased-Attention">https://github.com/aibhishek/Biased-Attention</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08760] Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?](http://arxiv.org/abs/2309.08760) #diffusion</code></li>
<li>Summary: <p>Deep neural networks used in computer vision have been shown to exhibit many
social biases such as gender bias. Vision Transformers (ViTs) have become
increasingly popular in computer vision applications, outperforming
Convolutional Neural Networks (CNNs) in many tasks such as image
classification. However, given that research on mitigating bias in computer
vision has primarily focused on CNNs, it is important to evaluate the effect of
a different network architecture on the potential for bias amplification. In
this paper we therefore introduce a novel metric to measure bias in
architectures, Accuracy Difference. We examine bias amplification when models
belonging to these two architectures are used as a part of large multimodal
models, evaluating the different image encoders of Contrastive Language Image
Pretraining which is an important model used in many generative models such as
DALL-E and Stable Diffusion. Our experiments demonstrate that architecture can
play a role in amplifying social biases due to the different techniques
employed by the models for feature extraction and embedding as well as their
different learning properties. This research found that ViTs amplified gender
bias to a greater extent than CNNs
</p></li>
</ul>
<h2>self-supervised</h2>
<h3>Title: Personalized Food Image Classification: Benchmark Datasets and New Baseline. (arXiv:2309.08744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08744">http://arxiv.org/abs/2309.08744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08744] Personalized Food Image Classification: Benchmark Datasets and New Baseline](http://arxiv.org/abs/2309.08744) #self-supervised</code></li>
<li>Summary: <p>Food image classification is a fundamental step of image-based dietary
assessment, enabling automated nutrient analysis from food images. Many current
methods employ deep neural networks to train on generic food image datasets
that do not reflect the dynamism of real-life food consumption patterns, in
which food images appear sequentially over time, reflecting the progression of
what an individual consumes. Personalized food classification aims to address
this problem by training a deep neural network using food images that reflect
the consumption pattern of each individual. However, this problem is
under-explored and there is a lack of benchmark datasets with individualized
food consumption patterns due to the difficulty in data collection. In this
work, we first introduce two benchmark personalized datasets including the
Food101-Personal, which is created based on surveys of daily dietary patterns
from participants in the real world, and the VFNPersonal, which is developed
based on a dietary study. In addition, we propose a new framework for
personalized food image classification by leveraging self-supervised learning
and temporal image feature information. Our method is evaluated on both
benchmark datasets and shows improved performance compared to existing works.
The dataset has been made available at:
https://skynet.ecn.purdue.edu/~pan161/dataset_personal.html
</p></li>
</ul>
<h2>foundation model</h2>
<h3>Title: MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation. (arXiv:2309.08842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08842">http://arxiv.org/abs/2309.08842</a></li>
<li>Code URL: <a href="https://github.com/cchen-cc/ma-sam">https://github.com/cchen-cc/ma-sam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08842] MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation](http://arxiv.org/abs/2309.08842) #foundation model</code></li>
<li>Summary: <p>The Segment Anything Model (SAM), a foundation model for general image
segmentation, has demonstrated impressive zero-shot performance across numerous
natural image segmentation tasks. However, SAM's performance significantly
declines when applied to medical images, primarily due to the substantial
disparity between natural and medical image domains. To effectively adapt SAM
to medical images, it is important to incorporate critical third-dimensional
information, i.e., volumetric or temporal knowledge, during fine-tuning.
Simultaneously, we aim to harness SAM's pre-trained weights within its original
2D backbone to the fullest extent. In this paper, we introduce a
modality-agnostic SAM adaptation framework, named as MA-SAM, that is applicable
to various volumetric and video medical data. Our method roots in the
parameter-efficient fine-tuning strategy to update only a small portion of
weight increments while preserving the majority of SAM's pre-trained weights.
By injecting a series of 3D adapters into the transformer blocks of the image
encoder, our method enables the pre-trained 2D backbone to extract
third-dimensional information from input data. The effectiveness of our method
has been comprehensively evaluated on four medical image segmentation tasks, by
using 10 public datasets across CT, MRI, and surgical video data. Remarkably,
without using any prompt, our method consistently outperforms various
state-of-the-art 3D approaches, surpassing nnU-Net by 0.9%, 2.6%, and 9.9% in
Dice for CT multi-organ segmentation, MRI prostate segmentation, and surgical
scene segmentation respectively. Our model also demonstrates strong
generalization, and excels in challenging tumor segmentation when prompts are
used. Our code is available at: https://github.com/cchen-cc/MA-SAM.
</p></li>
</ul>
<h3>Title: Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08632">http://arxiv.org/abs/2309.08632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08632] Pretraining on the Test Set Is All You Need](http://arxiv.org/abs/2309.08632) #foundation model</code></li>
<li>Summary: <p>Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.
</p></li>
</ul>
<h2>generative</h2>
<h3>Title: Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer. (arXiv:2309.08851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08851">http://arxiv.org/abs/2309.08851</a></li>
<li>Code URL: <a href="https://github.com/abhibha1807/robustifying_visual_perception">https://github.com/abhibha1807/robustifying_visual_perception</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08851] Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer](http://arxiv.org/abs/2309.08851) #generative</code></li>
<li>Summary: <p>The deployment of autonomous agents in real-world scenarios is challenged by
"unknown unknowns", i.e. novel unexpected environments not encountered during
training, such as degraded signs. While existing research focuses on anomaly
detection and class imbalance, it often fails to address truly novel scenarios.
Our approach enhances visual perception by leveraging the Variational
Prototyping Encoder (VPE) to adeptly identify and handle novel inputs, then
incrementally augmenting data using neural style transfer to enrich
underrepresented data. By comparing models trained solely on original datasets
with those trained on a combination of original and augmented datasets, we
observed a notable improvement in the performance of the latter. This
underscores the critical role of data augmentation in enhancing model
robustness. Our findings suggest the potential benefits of incorporating
generative models for domain-specific augmentation strategies.
</p></li>
</ul>
<h3>Title: ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08636">http://arxiv.org/abs/2309.08636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08636] ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert](http://arxiv.org/abs/2309.08636) #generative</code></li>
<li>Summary: <p>Historically, proficient writing was deemed essential for human advancement,
with creative expression viewed as one of the hallmarks of human achievement.
However, recent advances in generative AI have marked an inflection point in
this narrative, including for scientific writing. This article provides a
comprehensive analysis of the capabilities and limitations of six AI chatbots
in scholarly writing in the humanities and archaeology. The methodology was
based on tagging AI generated content for quantitative accuracy and qualitative
precision by human experts. Quantitative accuracy assessed the factual
correctness, while qualitative precision gauged the scientific contribution.
While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in
recombining existing knowledge, they failed in generating original scientific
content. As a side note, our results also suggest that with ChatGPT-4 the size
of the LLMs has plateaued. Furthermore, the paper underscores the intricate and
recursive nature of human research. This process of transforming raw data into
refined knowledge is computationally irreducible, which highlights the
challenges AI chatbots face in emulating human originality in scientific
writing. In conclusion, while large language models have revolutionised content
generation, their ability to produce original scientific contributions in the
humanities remains limited. We expect that this will change in the near future
with the evolution of current LLM-based AI chatbots towards LLM-powered
software.
</p></li>
</ul>
<h3>Title: Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08902">http://arxiv.org/abs/2309.08902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08902] Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models](http://arxiv.org/abs/2309.08902) #generative</code></li>
<li>Summary: <p>LLMs are increasingly powerful and widely used to assist users in a variety
of tasks. This use risks the introduction of LLM biases to consequential
decisions such as job hiring, human performance evaluation, and criminal
sentencing. Bias in NLP systems along the lines of gender and ethnicity has
been widely studied, especially for specific stereotypes (e.g., Asians are good
at math). In this paper, we investigate bias along less studied, but still
consequential, dimensions, such as age and beauty, measuring subtler correlated
decisions that LLMs (specially autoregressive language models) make between
social groups and unrelated positive and negative attributes. We ask whether
LLMs hold wide-reaching biases of positive or negative sentiment for specific
social groups similar to the ``what is beautiful is good'' bias found in people
in experimental psychology. We introduce a template-generated dataset of
sentence completion tasks that asks the model to select the most appropriate
attribute to complete an evaluative statement about a person described as a
member of a specific social group. We also reverse the completion task to
select the social group based on an attribute. Finally, we report the
correlations that we find for multiple cutting-edge LLMs. This dataset can be
used as a benchmark to evaluate progress in more generalized biases and the
templating technique can be used to expand the benchmark with minimal
additional human annotation.
</p></li>
</ul>
<h2>anomaly</h2>
<h2>in-context</h2>
<h2>memory</h2>
<h3>Title: Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution. (arXiv:2309.08919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08919">http://arxiv.org/abs/2309.08919</a></li>
<li>Code URL: <a href="https://github.com/wenyu1009/rtsrn">https://github.com/wenyu1009/rtsrn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08919] Pixel Adapter: A Graph-Based Post-Processing Approach for Scene Text Image Super-Resolution](http://arxiv.org/abs/2309.08919) #memory</code></li>
<li>Summary: <p>Current Scene text image super-resolution approaches primarily focus on
extracting robust features, acquiring text information, and complex training
strategies to generate super-resolution images. However, the upsampling module,
which is crucial in the process of converting low-resolution images to
high-resolution ones, has received little attention in existing works. To
address this issue, we propose the Pixel Adapter Module (PAM) based on graph
attention to address pixel distortion caused by upsampling. The PAM effectively
captures local structural information by allowing each pixel to interact with
its neighbors and update features. Unlike previous graph attention mechanisms,
our approach achieves 2-3 orders of magnitude improvement in efficiency and
memory utilization by eliminating the dependency on sparse adjacency matrices
and introducing a sliding window approach for efficient parallel computation.
Additionally, we introduce the MLP-based Sequential Residual Block (MSRB) for
robust feature extraction from text images, and a Local Contour Awareness loss
($\mathcal{L}_{lca}$) to enhance the model's perception of details.
Comprehensive experiments on TextZoom demonstrate that our proposed method
generates high-quality super-resolution images, surpassing existing methods in
recognition accuracy. For single-stage and multi-stage strategies, we achieved
improvements of 0.7\% and 2.6\%, respectively, increasing the performance from
52.6\% and 53.7\% to 53.3\% and 56.3\%. The code is available at
https://github.com/wenyu1009/RTSRN.
</p></li>
</ul>
<h3>Title: Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08708">http://arxiv.org/abs/2309.08708</a></li>
<li>Code URL: <a href="https://github.com/mlsw/dynamic-embedding-pruning">https://github.com/mlsw/dynamic-embedding-pruning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08708] Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning](http://arxiv.org/abs/2309.08708) #memory</code></li>
<li>Summary: <p>The extensive memory footprint of pre-trained language models (PLMs) can
hinder deployment in memory-constrained settings, such as cloud environments or
on-device. PLMs use embedding matrices to represent extensive vocabularies,
forming a large proportion of the model parameters. While previous work towards
parameter-efficient PLM development has considered pruning parameters within
the transformer layers, pruning the embedding matrix as part of fine-tuning or
inference has yet to be explored. We first demonstrate that a significant
proportion of the vocabulary remains unused in these scenarios. We then propose
a simple yet effective approach that leverages this finding to minimize the
memory footprint of the embedding matrix. We show that this approach provides
substantial reductions in memory usage across a wide range of models and tasks.
Notably, our approach maintains equivalent downstream task performance while
allowing a more efficient use of compute resources.
</p></li>
</ul>
<h3>Title: MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08868">http://arxiv.org/abs/2309.08868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08868] MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding](http://arxiv.org/abs/2309.08868) #memory</code></li>
<li>Summary: <p>International Classification of Diseases (ICD) coding is the task of
assigning ICD diagnosis codes to clinical notes. This can be challenging given
the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000
tokens). However, unlike the single-pass reading process in previous works,
humans tend to read the text and label definitions again to get more confident
answers. Moreover, although pretrained language models have been used to
address these problems, they suffer from huge memory usage. To address the
above problems, we propose a simple but effective model called the Multi-Hop
Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is
deployed to get more precise and informative representations. Extensive
experiments on three benchmark MIMIC datasets indicate that our method achieves
significantly better or competitive performance on all seven metrics, with much
fewer parameters to optimize.
</p></li>
</ul>
<h3>Title: Experimental Assessment of a Forward-Collision Warning System Fusing Deep Learning and Decentralized Radio Sensing. (arXiv:2309.08737v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08737">http://arxiv.org/abs/2309.08737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08737] Experimental Assessment of a Forward-Collision Warning System Fusing Deep Learning and Decentralized Radio Sensing](http://arxiv.org/abs/2309.08737) #memory</code></li>
<li>Summary: <p>This paper presents the idea of an automatic forward-collision warning system
based on a decentralized radio sensing (RS) approach. In this framework, a
vehicle in receiving mode employs a continuous waveform (CW) transmitted by a
second vehicle as a probe signal to detect oncoming vehicles and warn the
driver of a potential forward collision. Such a CW can easily be incorporated
as a pilot signal within the data frame of current multicarrier vehicular
communication systems. Detection of oncoming vehicles is performed by a deep
learning (DL) module that analyzes the features of the Doppler signature
imprinted on the CW probe signal by a rapidly approaching vehicle. This
decentralized CW RS approach was assessed experimentally using data collected
by a series of field trials conducted in a two-lanes high-speed highway.
Detection performance was evaluated for two different DL models: a long
short-term memory network and a convolutional neural network. The obtained
results demonstrate the feasibility of the envisioned forward-collision warning
system based on the fusion of DL and decentralized CW RS.
</p></li>
</ul>
<h2>few-shot</h2>
<h3>Title: An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis. (arXiv:2309.08777v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08777">http://arxiv.org/abs/2309.08777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08777] An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis](http://arxiv.org/abs/2309.08777) #few-shot</code></li>
<li>Summary: <p>Sentiment analysis is a crucial task in natural language processing that
involves identifying and extracting subjective sentiment from text.
Self-training has recently emerged as an economical and efficient technique for
developing sentiment analysis models by leveraging a small amount of labeled
data and a larger amount of unlabeled data. However, the performance of a
self-training procedure heavily relies on the choice of the instance selection
strategy, which has not been studied thoroughly. This paper presents an
empirical study on various instance selection strategies for self-training on
two public sentiment datasets, and investigates the influence of the strategy
and hyper-parameters on the performance of self-training in various few-shot
settings.
</p></li>
</ul>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script>new ClipboardJS("#copy",{text:function(trigger){var res="[[2023-09-19]]\n\n";var input=document.querySelectorAll("input");for(var i=0;i<input.length;i++){if(input[i].type=="checkbox"&&input[i].checked){res+="- "+input[i].nextSibling.nodeValue+"\n"}}res+="\n";return res}}).on("success",function(e){e.clearSelection()});</script>
<button id="copy">Copy All</button>
