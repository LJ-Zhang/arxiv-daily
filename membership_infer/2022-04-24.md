### Title: An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models
* Paper ID: 2204.11351v1
* Paper URL: [http://arxiv.org/abs/2204.11351v1](http://arxiv.org/abs/2204.11351v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Nowadays, the interpretation of why a machine learning (ML) model makes
certain inferences is as crucial as the accuracy of such inferences. Some ML
models like the decision tree possess inherent interpretability that can be
directly comprehended by humans. Others like artificial neural networks (ANN),
however, rely on external methods to uncover the deduction mechanism. SHapley
Additive exPlanations (SHAP) is one of such external methods, which requires a
background dataset when interpreting ANNs. Generally, a background dataset
consists of instances randomly sampled from the training dataset. However, the
sampling size and its effect on SHAP remain to be unexplored. In our empirical
study on the MIMIC-III dataset, we show that the two core explanations - SHAP
values and variable rankings fluctuate when using different background datasets
acquired from random sampling, indicating that users cannot unquestioningly
trust the one-shot interpretation from SHAP. Luckily, such fluctuation
decreases with the increase of the background dataset size. Also, we notice an
U-shape in the stability assessment of SHAP variable rankings, demonstrating
that SHAP is more reliable in ranking the most and least important variables
compared to moderately important ones. Overall, our results suggest that users
should take into account how background data affects SHAP results, with
improved SHAP stability as the background sample size increases.

### Title: Identification and Statistical Decision Theory
* Paper ID: 2204.11318v1
* Paper URL: [http://arxiv.org/abs/2204.11318v1](http://arxiv.org/abs/2204.11318v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Econometricians have usefully separated study of estimation into
identification and statistical components. Identification analysis aims to
place an informative upper bound on what may be learned about population
parameters of interest with specified sample data. Statistical decision theory
has studied decision making with sample data without reference to
identification. This paper asks if and how identification analysis is useful to
statistical decision theory. I show that the answer is positive and simple when
the relevant parameter (true state of nature) is point identified. However, a
subtlety arises when the true state is partially identified, and a decision
must be made under ambiguity. Then the performance of some criteria,
particularly minimax regret, is enhanced by permitting randomized choice of an
action, which essentially requires availability of sample data. I show that an
informative upper bound on the performance of decision making holds when the
knowledge assumed in identification analysis is combined with sample data
enabling randomized choice. I emphasize that using sample data to randomize
choice is conceptually distinct from its traditional econometric use to infer
population parameters.

### Title: Occupation time of a renewal process coupled to a discrete Markov chain
* Paper ID: 2204.11228v1
* Paper URL: [http://arxiv.org/abs/2204.11228v1](http://arxiv.org/abs/2204.11228v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: A semi-Markov process is one that changes states in accordance with a Markov
chain but takes a random amount of time between changes. We consider the
generalisation to semi-Markov processes of the classical Lamperti law for the
occupation time of a two-state Markov process. We provide an explicit
expression in Laplace space for the distribution of an arbitrary linear
combination of the occupation times in the various states of the process. We
discuss several consequences of this result. In particular, we infer the
limiting distribution of this quantity rescaled by time in the long-time
scaling regime, as well as the finite-time corrections to its moments.

### Title: Bounding the Effects of Continuous Treatments for Hidden Confounders
* Paper ID: 2204.11206v1
* Paper URL: [http://arxiv.org/abs/2204.11206v1](http://arxiv.org/abs/2204.11206v1)
* Updated Date: 2022-04-24
* Code URL: [https://github.com/marmarelis/treatmentcurves.jl](https://github.com/marmarelis/treatmentcurves.jl)
* Summary: Causal inference involves the disentanglement of effects due to a treatment
variable from those of confounders, observed as covariates or not. Since one
outcome is ever observed at a time, the problem turns into one of predicting
counterfactuals on every individual in the dataset. Observational studies
complicate this endeavor by permitting dependencies between the treatment and
other variables in the sample. If the covariates influence the propensity of
treatment, then one suffers from covariate shift. Should the outcome and the
treatment be affected by another variable even after accounting for the
covariates, there is also hidden confounding. That is immeasurable by
definition. Rather, one must study the worst possible consequences of bounded
levels of hidden confounding on downstream decision-making. We explore this
problem in the case of continuous treatments. We develop a framework to compute
ignorance intervals on the partially identified dose-response curves, which
enable us to quantify the susceptibility of our inference to hidden
confounders. Our method is supported by simulations as well as empirical tests
based on two observational studies.

### Title: RedMulE: A Compact FP16 Matrix-Multiplication Accelerator for Adaptive Deep Learning on RISC-V-Based Ultra-Low-Power SoCs
* Paper ID: 2204.11192v1
* Paper URL: [http://arxiv.org/abs/2204.11192v1](http://arxiv.org/abs/2204.11192v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: The fast proliferation of extreme-edge applications using Deep Learning (DL)
based algorithms required dedicated hardware to satisfy extreme-edge
applications' latency, throughput, and precision requirements. While inference
is achievable in practical cases, online finetuning and adaptation of general
DL models are still highly challenging. One of the key stumbling stones is the
need for parallel floating-point operations, which are considered unaffordable
on sub-100 mW extreme-edge SoCs. We tackle this problem with RedMulE
(Reduced-precision matrix Multiplication Engine), a parametric low-power
hardware accelerator for FP16 matrix multiplications - the main kernel of DL
training and inference - conceived for tight integration within a cluster of
tiny RISC-V cores based on the PULP (Parallel Ultra-Low-Power) architecture. In
22 nm technology, a 32-FMA RedMulE instance occupies just 0.07 mm^2 (14% of an
8-core RISC-V cluster) and achieves up to 666 MHz maximum operating frequency,
for a throughput of 31.6 MAC/cycle (98.8% utilization). We reach a
cluster-level power consumption of 43.5 mW and a full-cluster energy efficiency
of 688 16-bit GFLOPS/W. Overall, RedMulE features up to 4.65x higher energy
efficiency and 22x speedup over SW execution on 8 RISC-V cores.

### Title: Realistic Evaluation of Transductive Few-Shot Learning
* Paper ID: 2204.11181v1
* Paper URL: [http://arxiv.org/abs/2204.11181v1](http://arxiv.org/abs/2204.11181v1)
* Updated Date: 2022-04-24
* Code URL: [https://github.com/oveilleux/realistic_transductive_few_shot](https://github.com/oveilleux/realistic_transductive_few_shot)
* Summary: Transductive inference is widely used in few-shot learning, as it leverages
the statistics of the unlabeled query set of a few-shot task, typically
yielding substantially better performances than its inductive counterpart. The
current few-shot benchmarks use perfectly class-balanced tasks at inference. We
argue that such an artificial regularity is unrealistic, as it assumes that the
marginal label probability of the testing samples is known and fixed to the
uniform distribution. In fact, in realistic scenarios, the unlabeled query sets
come with arbitrary and unknown label marginals. We introduce and study the
effect of arbitrary class distributions within the query sets of few-shot tasks
at inference, removing the class-balance artefact. Specifically, we model the
marginal probabilities of the classes as Dirichlet-distributed random
variables, which yields a principled and realistic sampling within the simplex.
This leverages the current few-shot benchmarks, building testing tasks with
arbitrary class distributions. We evaluate experimentally state-of-the-art
transductive methods over 3 widely used data sets, and observe, surprisingly,
substantial performance drops, even below inductive methods in some cases.
Furthermore, we propose a generalization of the mutual-information loss, based
on $\alpha$-divergences, which can handle effectively class-distribution
variations. Empirically, we show that our transductive $\alpha$-divergence
optimization outperforms state-of-the-art methods across several data sets,
models and few-shot settings. Our code is publicly available at
https://github.com/oveilleux/Realistic_Transductive_Few_Shot.

### Title: RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning
* Paper ID: 2204.11167v1
* Paper URL: [http://arxiv.org/abs/2204.11167v1](http://arxiv.org/abs/2204.11167v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Reasoning about visual relationships is central to how humans interpret the
visual world. This task remains challenging for current deep learning
algorithms since it requires addressing three key technical problems jointly:
1) identifying object entities and their properties, 2) inferring semantic
relations between pairs of entities, and 3) generalizing to novel
object-relation combinations, i.e., systematic generalization. In this work, we
use vision transformers (ViTs) as our base model for visual reasoning and make
better use of concepts defined as object entities and their relations to
improve the reasoning ability of ViTs. Specifically, we introduce a novel
concept-feature dictionary to allow flexible image feature retrieval at
training time with concept keys. This dictionary enables two new concept-guided
auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a
local task for facilitating semantic object-centric correspondence learning. To
examine the systematic generalization of visual reasoning models, we introduce
systematic splits for the standard HICO and GQA benchmarks. We show the
resulting model, Concept-guided Vision Transformer (or RelViT for short)
significantly outperforms prior approaches on HICO and GQA by 16% and 13% in
the original split, and by 43% and 18% in the systematic split. Our ablation
analyses also reveal our model's compatibility with multiple ViT variants and
robustness to hyper-parameters.

### Title: Subgroup Fairness in Graph-based Spam Detection
* Paper ID: 2204.11164v1
* Paper URL: [http://arxiv.org/abs/2204.11164v1](http://arxiv.org/abs/2204.11164v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Fake reviews are prevalent on review websites such as Amazon and Yelp. GNN is
the state-of-the-art method that can detect suspicious reviewers by exploiting
the topologies of the graph connecting reviewers, reviews, and target products.
However, the discrepancy in the detection accuracy over different groups of
reviewers causes discriminative treatment of different reviewers of the
websites, leading to less engagement and trustworthiness of such websites. The
complex dependencies over the review graph introduce difficulties in teasing
out subgroups of reviewers that are hidden within larger groups and are treated
unfairly. There is no previous study that defines and discovers the subtle
subgroups to improve equitable treatment of reviewers. This paper addresses the
challenges of defining, discovering, and utilizing subgroup memberships for
fair spam detection. We first define a subgroup membership that can lead to
discrepant accuracy in the subgroups. Since the subgroup membership is usually
not observable while also important to guide the GNN detector to balance the
treatment, we design a model that jointly infers the hidden subgroup
memberships and exploits the membership for calibrating the target GNN's
detection accuracy across subgroups. Comprehensive results on two large Yelp
review datasets demonstrate that the proposed model can be trained to treat the
subgroups more fairly.

### Title: Discovery of extended structure around open cluster COIN-Gaia 13 based on Gaia EDR3
* Paper ID: 2204.11160v1
* Paper URL: [http://arxiv.org/abs/2204.11160v1](http://arxiv.org/abs/2204.11160v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: COIN-Gaia 13 is a newly discovered open cluster revealed by Gaia DR2 data. It
is a nearby open cluster with a distance of about 513 pc. Combined with the
five-dimensional astrometric data of Gaia EDR3 with higher accuracy, we use the
membership assignment algorithm (pyUPMASK) to determine the membership of
COIN-Gaia 13 in a large extended spatial region. The cluster has found 478
candidate members. After obtaining reliable cluster members, we further study
its basic properties and spatial distribution. Our results show that there is
an obvious extended structure of the cluster in the X-Y plane. This elongated
structure is distributed along the spiral arm, and the whole length is about
270 pc. The cluster age is 250 Myr, the total mass is about 439 M$_\odot$, and
the tidal radius of the cluster is about 11 pc. Since more than half of the
member stars (352 stars) are located outside twice the tidal radius, it is
suspected that this cluster is undergoing the dynamic dissolution process.
Furthermore, the spatial distribution and kinematic analysis indicate that the
extended structure in COIN-Gaia 13 is more likely to be caused by the
differential rotation of the Galaxy.

