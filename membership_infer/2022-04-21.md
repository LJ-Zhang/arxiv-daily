### Title: TorchSparse: Efficient Point Cloud Inference Engine
* Paper ID: 2204.10319v1
* Paper URL: [http://arxiv.org/abs/2204.10319v1](http://arxiv.org/abs/2204.10319v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/mit-han-lab/torchsparse](https://github.com/mit-han-lab/torchsparse)
* Summary: Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.

### Title: Feature anomaly detection system (FADS) for intelligent manufacturing
* Paper ID: 2204.10318v1
* Paper URL: [http://arxiv.org/abs/2204.10318v1](http://arxiv.org/abs/2204.10318v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.

### Title: SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space
* Paper ID: 2204.10245v1
* Paper URL: [http://arxiv.org/abs/2204.10245v1](http://arxiv.org/abs/2204.10245v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.

### Title: BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
* Paper ID: 2204.10209v1
* Paper URL: [http://arxiv.org/abs/2204.10209v1](http://arxiv.org/abs/2204.10209v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.

### Title: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
* Paper ID: 2204.10149v1
* Paper URL: [http://arxiv.org/abs/2204.10149v1](http://arxiv.org/abs/2204.10149v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.

### Title: Toward Fast, Flexible, and Robust Low-Light Image Enhancement
* Paper ID: 2204.10137v1
* Paper URL: [http://arxiv.org/abs/2204.10137v1](http://arxiv.org/abs/2204.10137v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/vis-opt-group/sci](https://github.com/vis-opt-group/sci)
* Summary: Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.

### Title: OSSO: Obtaining Skeletal Shape from Outside
* Paper ID: 2204.10129v1
* Paper URL: [http://arxiv.org/abs/2204.10129v1](http://arxiv.org/abs/2204.10129v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/MarilynKeller/OSSO](https://github.com/MarilynKeller/OSSO)
* Summary: We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.

### Title: Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
* Paper ID: 2204.10090v1
* Paper URL: [http://arxiv.org/abs/2204.10090v1](http://arxiv.org/abs/2204.10090v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.

### Title: Time Window Frechet and Metric-Based Edit Distance for Passively Collected Trajectories
* Paper ID: 2204.10053v1
* Paper URL: [http://arxiv.org/abs/2204.10053v1](http://arxiv.org/abs/2204.10053v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The advances of modern localization techniques and the wide spread of mobile
devices have provided us great opportunities to collect and mine human mobility
trajectories. In this work, we focus on passively collected trajectories, which
are sequences of time-stamped locations that mobile entities visit. To analyse
such trajectories, a crucial part is a measure of similarity between two
trajectories. We propose the time-window Frechet distance, which enforces the
maximum temporal separation between points of two trajectories that can be
paired in the calculation of the Frechet distance, and the metric-based edit
distance which incorporates the underlying metric in the computation of the
insertion and deletion costs. Using these measures, we can cluster trajectories
to infer group motion patterns. We look at the $k$-gather problem which
requires each cluster to have at least $k$ trajectories. We prove that k-gather
remains NP-hard under edit distance, metric-based edit distance and Jaccard
distance. Finally, we improve over previous results on discrete Frechet
distance and show that there is no strongly sub-quadratic time with
approximation factor less than $1.61$ in two dimensional setting unless SETH
fails.

### Title: Understanding the Domain Gap in LiDAR Object Detection Networks
* Paper ID: 2204.10024v1
* Paper URL: [http://arxiv.org/abs/2204.10024v1](http://arxiv.org/abs/2204.10024v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.

### Title: Hardy spaces and quasiconformal maps in the Heisenberg group
* Paper ID: 2204.10016v1
* Paper URL: [http://arxiv.org/abs/2204.10016v1](http://arxiv.org/abs/2204.10016v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We define Hardy spaces $H^p$, $0<p<\infty$, for quasiconformal mappings on
the Kor\'{a}nyi unit ball $B$ in the first Heisenberg group $\mathbb{H}^1$. Our
definition is stated in terms of the Heisenberg polar coordinates introduced by
Kor\'{a}nyi and Reimann, and Balogh and Tyson. First, we prove the existence of
$p_0(K)>0$ such that every $K$-quasiconformal map $f:B \to f(B) \subset
\mathbb{H}^1$ belongs to $H^p$ for all $0<p<p_0(K)$. Second, we give two
equivalent conditions for the $H^p$ membership of a quasiconformal map $f$, one
in terms of the radial limits of $f$, and one using a nontangential maximal
function of $f$. As an application, we characterize Carleson measures on $B$
via integral inequalities for quasiconformal mappings on $B$ and their radial
limits. Our paper thus extends results by Astala and Koskela, Jerison and
Weitsman, Nolder, and Zinsmeister, from $\mathbb{R}^n$ to $\mathbb{H}^1$. A
crucial difference between the proofs in $\mathbb{R}^n$ and $\mathbb{H}^1$ is
caused by the nonisotropic nature of the Kor\'{a}nyi unit sphere with its two
characteristic points.

### Title: Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
* Paper ID: 2204.09992v1
* Paper URL: [http://arxiv.org/abs/2204.09992v1](http://arxiv.org/abs/2204.09992v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.

### Title: CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
* Paper ID: 2204.09914v1
* Paper URL: [http://arxiv.org/abs/2204.09914v1](http://arxiv.org/abs/2204.09914v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.

### Title: Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
* Paper ID: 2204.09903v1
* Paper URL: [http://arxiv.org/abs/2204.09903v1](http://arxiv.org/abs/2204.09903v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/chunbolang/DCP](https://github.com/chunbolang/DCP)
* Summary: Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.

### Title: Functional Horseshoe Smoothing for Functional Trend Estimation
* Paper ID: 2204.09898v1
* Paper URL: [http://arxiv.org/abs/2204.09898v1](http://arxiv.org/abs/2204.09898v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Due to developments in instruments and computers, functional observations are
increasingly popular. However, effective methodologies for flexibly estimating
the underlying trends with valid uncertainty quantification for a sequence of
functional data (e.g. functional time series) are still scarce. In this work,
we develop a locally adaptive smoothing method, called functional horseshoe
smoothing, by introducing a shrinkage prior to the general order of differences
of functional variables. This allows us to capture abrupt changes by taking
advantage of the shrinkage capability and also to assess uncertainty by
Bayesian inference. The fully Bayesian framework also allows the selection of
the number of basis functions via the posterior predictive loss. Also, by
taking advantage of the nature of functional data, this method is able to
handle heterogeneously observed data without data augmentation. We show the
theoretical properties of the proposed prior distribution and the posterior
mean, and finally demonstrate them through simulation studies and applications
to a real-world dataset.

### Title: Non-autoregressive Model for Full-line Code Completion
* Paper ID: 2204.09877v1
* Paper URL: [http://arxiv.org/abs/2204.09877v1](http://arxiv.org/abs/2204.09877v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Code completion tools are frequently used by software developers to
accelerate software development by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full line of code) has been
proved more efficient than predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRegressive (AR) decoders to
generate tokens in a left-to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previously generated tokens, which
leads to high latency in inference. To improve the efficiency and accuracy of
full-line code completion, in this paper, we propose a Non-AutoRegressive (NAR)
model for code completion boosted by a syntax-aware sampling strategy. Our
experimental results on two widely used datasets suggest that our model
outperforms both AR and NAR baselines on full-line code completion, and it is
faster than the AR model with up to 9 times speed-up.

### Title: Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy
* Paper ID: 2204.09873v1
* Paper URL: [http://arxiv.org/abs/2204.09873v1](http://arxiv.org/abs/2204.09873v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Respiratory motion during radiotherapy causes uncertainty in the tumor's
location, which is typically addressed by an increased radiation area and a
decreased dose. As a result, the treatments' efficacy is reduced. The recently
proposed hybrid MR-linac scanner holds the promise to efficiently deal with
such respiratory motion through real-time adaptive MR-guided radiotherapy
(MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the
radiotherapy plan should be adapted in real-time according to the estimated
motion-fields. All of this should be performed with a total latency of
maximally 200 ms, including data acquisition and reconstruction. A measure of
confidence in such estimated motion-fields is highly desirable, for instance to
ensure the patient's safety in case of unexpected and undesirable motion. In
this work, we propose a framework based on Gaussian Processes to infer 3D
motion-fields and uncertainty maps in real-time from only three readouts of
MR-data. We demonstrated an inference frame rate up to 69 Hz including data
acquisition and reconstruction, thereby exploiting the limited amount of
required MR-data. Additionally, we designed a rejection criterion based on the
motion-field uncertainty maps to demonstrate the framework's potential for
quality assurance. The framework was validated in silico and in vivo on healthy
volunteer data (n=5) acquired using an MR-linac, thereby taking into account
different breathing patterns and controlled bulk motion. Results indicate
end-point-errors with a 75th percentile below 1mm in silico, and a correct
detection of erroneous motion estimates with the rejection criterion.
Altogether, the results show the potential of the framework for application in
real-time MR-guided radiotherapy with an MR-linac.

### Title: The $θ$-augmented model for Bayesian semiparametric inference on functional parameters
* Paper ID: 2204.09862v1
* Paper URL: [http://arxiv.org/abs/2204.09862v1](http://arxiv.org/abs/2204.09862v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.

### Title: Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
* Paper ID: 2204.09860v1
* Paper URL: [http://arxiv.org/abs/2204.09860v1](http://arxiv.org/abs/2204.09860v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/xiaoyuan1996/galr](https://github.com/xiaoyuan1996/galr)
* Summary: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

### Title: A Masked Image Reconstruction Network for Document-level Relation Extraction
* Paper ID: 2204.09851v1
* Paper URL: [http://arxiv.org/abs/2204.09851v1](http://arxiv.org/abs/2204.09851v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.

### Title: FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation
* Paper ID: 2204.09850v1
* Paper URL: [http://arxiv.org/abs/2204.09850v1](http://arxiv.org/abs/2204.09850v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way.

### Title: 6GAN: IPv6 Multi-Pattern Target Generation via Generative Adversarial Nets with Reinforcement Learning
* Paper ID: 2204.09839v1
* Paper URL: [http://arxiv.org/abs/2204.09839v1](http://arxiv.org/abs/2204.09839v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cuitianyu961030/6gan](https://github.com/cuitianyu961030/6gan)
* Summary: Global IPv6 scanning has always been a challenge for researchers because of
the limited network speed and computational power. Target generation algorithms
are recently proposed to overcome the problem for Internet assessments by
predicting a candidate set to scan. However, IPv6 custom address configuration
emerges diverse addressing patterns discouraging algorithmic inference.
Widespread IPv6 alias could also mislead the algorithm to discover aliased
regions rather than valid host targets. In this paper, we introduce 6GAN, a
novel architecture built with Generative Adversarial Net (GAN) and
reinforcement learning for multi-pattern target generation. 6GAN forces
multiple generators to train with a multi-class discriminator and an alias
detector to generate non-aliased active targets with different addressing
pattern types. The rewards from the discriminator and the alias detector help
supervise the address sequence decision-making process. After adversarial
training, 6GAN's generators could keep a strong imitating ability for each
pattern and 6GAN's discriminator obtains outstanding pattern discrimination
ability with a 0.966 accuracy. Experiments indicate that our work outperformed
the state-of-the-art target generation algorithms by reaching a higher-quality
candidate set.

### Title: Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
* Paper ID: 2204.09817v1
* Paper URL: [http://arxiv.org/abs/2204.09817v1](http://arxiv.org/abs/2204.09817v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.

### Title: TorchSparse: Efficient Point Cloud Inference Engine
* Paper ID: 2204.10319v1
* Paper URL: [http://arxiv.org/abs/2204.10319v1](http://arxiv.org/abs/2204.10319v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/mit-han-lab/torchsparse](https://github.com/mit-han-lab/torchsparse)
* Summary: Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.

### Title: Feature anomaly detection system (FADS) for intelligent manufacturing
* Paper ID: 2204.10318v1
* Paper URL: [http://arxiv.org/abs/2204.10318v1](http://arxiv.org/abs/2204.10318v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.

### Title: SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space
* Paper ID: 2204.10245v1
* Paper URL: [http://arxiv.org/abs/2204.10245v1](http://arxiv.org/abs/2204.10245v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.

### Title: BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
* Paper ID: 2204.10209v1
* Paper URL: [http://arxiv.org/abs/2204.10209v1](http://arxiv.org/abs/2204.10209v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.

### Title: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
* Paper ID: 2204.10149v1
* Paper URL: [http://arxiv.org/abs/2204.10149v1](http://arxiv.org/abs/2204.10149v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.

### Title: Toward Fast, Flexible, and Robust Low-Light Image Enhancement
* Paper ID: 2204.10137v1
* Paper URL: [http://arxiv.org/abs/2204.10137v1](http://arxiv.org/abs/2204.10137v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/vis-opt-group/sci](https://github.com/vis-opt-group/sci)
* Summary: Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.

### Title: OSSO: Obtaining Skeletal Shape from Outside
* Paper ID: 2204.10129v1
* Paper URL: [http://arxiv.org/abs/2204.10129v1](http://arxiv.org/abs/2204.10129v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/MarilynKeller/OSSO](https://github.com/MarilynKeller/OSSO)
* Summary: We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.

### Title: Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
* Paper ID: 2204.10090v1
* Paper URL: [http://arxiv.org/abs/2204.10090v1](http://arxiv.org/abs/2204.10090v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.

### Title: Time Window Frechet and Metric-Based Edit Distance for Passively Collected Trajectories
* Paper ID: 2204.10053v1
* Paper URL: [http://arxiv.org/abs/2204.10053v1](http://arxiv.org/abs/2204.10053v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The advances of modern localization techniques and the wide spread of mobile
devices have provided us great opportunities to collect and mine human mobility
trajectories. In this work, we focus on passively collected trajectories, which
are sequences of time-stamped locations that mobile entities visit. To analyse
such trajectories, a crucial part is a measure of similarity between two
trajectories. We propose the time-window Frechet distance, which enforces the
maximum temporal separation between points of two trajectories that can be
paired in the calculation of the Frechet distance, and the metric-based edit
distance which incorporates the underlying metric in the computation of the
insertion and deletion costs. Using these measures, we can cluster trajectories
to infer group motion patterns. We look at the $k$-gather problem which
requires each cluster to have at least $k$ trajectories. We prove that k-gather
remains NP-hard under edit distance, metric-based edit distance and Jaccard
distance. Finally, we improve over previous results on discrete Frechet
distance and show that there is no strongly sub-quadratic time with
approximation factor less than $1.61$ in two dimensional setting unless SETH
fails.

### Title: Understanding the Domain Gap in LiDAR Object Detection Networks
* Paper ID: 2204.10024v1
* Paper URL: [http://arxiv.org/abs/2204.10024v1](http://arxiv.org/abs/2204.10024v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.

### Title: Hardy spaces and quasiconformal maps in the Heisenberg group
* Paper ID: 2204.10016v1
* Paper URL: [http://arxiv.org/abs/2204.10016v1](http://arxiv.org/abs/2204.10016v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We define Hardy spaces $H^p$, $0<p<\infty$, for quasiconformal mappings on
the Kor\'{a}nyi unit ball $B$ in the first Heisenberg group $\mathbb{H}^1$. Our
definition is stated in terms of the Heisenberg polar coordinates introduced by
Kor\'{a}nyi and Reimann, and Balogh and Tyson. First, we prove the existence of
$p_0(K)>0$ such that every $K$-quasiconformal map $f:B \to f(B) \subset
\mathbb{H}^1$ belongs to $H^p$ for all $0<p<p_0(K)$. Second, we give two
equivalent conditions for the $H^p$ membership of a quasiconformal map $f$, one
in terms of the radial limits of $f$, and one using a nontangential maximal
function of $f$. As an application, we characterize Carleson measures on $B$
via integral inequalities for quasiconformal mappings on $B$ and their radial
limits. Our paper thus extends results by Astala and Koskela, Jerison and
Weitsman, Nolder, and Zinsmeister, from $\mathbb{R}^n$ to $\mathbb{H}^1$. A
crucial difference between the proofs in $\mathbb{R}^n$ and $\mathbb{H}^1$ is
caused by the nonisotropic nature of the Kor\'{a}nyi unit sphere with its two
characteristic points.

### Title: Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
* Paper ID: 2204.09992v1
* Paper URL: [http://arxiv.org/abs/2204.09992v1](http://arxiv.org/abs/2204.09992v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.

### Title: CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
* Paper ID: 2204.09914v1
* Paper URL: [http://arxiv.org/abs/2204.09914v1](http://arxiv.org/abs/2204.09914v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.

### Title: Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
* Paper ID: 2204.09903v1
* Paper URL: [http://arxiv.org/abs/2204.09903v1](http://arxiv.org/abs/2204.09903v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/chunbolang/DCP](https://github.com/chunbolang/DCP)
* Summary: Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.

### Title: Functional Horseshoe Smoothing for Functional Trend Estimation
* Paper ID: 2204.09898v1
* Paper URL: [http://arxiv.org/abs/2204.09898v1](http://arxiv.org/abs/2204.09898v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Due to developments in instruments and computers, functional observations are
increasingly popular. However, effective methodologies for flexibly estimating
the underlying trends with valid uncertainty quantification for a sequence of
functional data (e.g. functional time series) are still scarce. In this work,
we develop a locally adaptive smoothing method, called functional horseshoe
smoothing, by introducing a shrinkage prior to the general order of differences
of functional variables. This allows us to capture abrupt changes by taking
advantage of the shrinkage capability and also to assess uncertainty by
Bayesian inference. The fully Bayesian framework also allows the selection of
the number of basis functions via the posterior predictive loss. Also, by
taking advantage of the nature of functional data, this method is able to
handle heterogeneously observed data without data augmentation. We show the
theoretical properties of the proposed prior distribution and the posterior
mean, and finally demonstrate them through simulation studies and applications
to a real-world dataset.

### Title: Non-autoregressive Model for Full-line Code Completion
* Paper ID: 2204.09877v1
* Paper URL: [http://arxiv.org/abs/2204.09877v1](http://arxiv.org/abs/2204.09877v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Code completion tools are frequently used by software developers to
accelerate software development by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full line of code) has been
proved more efficient than predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRegressive (AR) decoders to
generate tokens in a left-to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previously generated tokens, which
leads to high latency in inference. To improve the efficiency and accuracy of
full-line code completion, in this paper, we propose a Non-AutoRegressive (NAR)
model for code completion boosted by a syntax-aware sampling strategy. Our
experimental results on two widely used datasets suggest that our model
outperforms both AR and NAR baselines on full-line code completion, and it is
faster than the AR model with up to 9 times speed-up.

### Title: Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy
* Paper ID: 2204.09873v1
* Paper URL: [http://arxiv.org/abs/2204.09873v1](http://arxiv.org/abs/2204.09873v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Respiratory motion during radiotherapy causes uncertainty in the tumor's
location, which is typically addressed by an increased radiation area and a
decreased dose. As a result, the treatments' efficacy is reduced. The recently
proposed hybrid MR-linac scanner holds the promise to efficiently deal with
such respiratory motion through real-time adaptive MR-guided radiotherapy
(MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the
radiotherapy plan should be adapted in real-time according to the estimated
motion-fields. All of this should be performed with a total latency of
maximally 200 ms, including data acquisition and reconstruction. A measure of
confidence in such estimated motion-fields is highly desirable, for instance to
ensure the patient's safety in case of unexpected and undesirable motion. In
this work, we propose a framework based on Gaussian Processes to infer 3D
motion-fields and uncertainty maps in real-time from only three readouts of
MR-data. We demonstrated an inference frame rate up to 69 Hz including data
acquisition and reconstruction, thereby exploiting the limited amount of
required MR-data. Additionally, we designed a rejection criterion based on the
motion-field uncertainty maps to demonstrate the framework's potential for
quality assurance. The framework was validated in silico and in vivo on healthy
volunteer data (n=5) acquired using an MR-linac, thereby taking into account
different breathing patterns and controlled bulk motion. Results indicate
end-point-errors with a 75th percentile below 1mm in silico, and a correct
detection of erroneous motion estimates with the rejection criterion.
Altogether, the results show the potential of the framework for application in
real-time MR-guided radiotherapy with an MR-linac.

### Title: The $θ$-augmented model for Bayesian semiparametric inference on functional parameters
* Paper ID: 2204.09862v1
* Paper URL: [http://arxiv.org/abs/2204.09862v1](http://arxiv.org/abs/2204.09862v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.

### Title: Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
* Paper ID: 2204.09860v1
* Paper URL: [http://arxiv.org/abs/2204.09860v1](http://arxiv.org/abs/2204.09860v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/xiaoyuan1996/galr](https://github.com/xiaoyuan1996/galr)
* Summary: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

### Title: A Masked Image Reconstruction Network for Document-level Relation Extraction
* Paper ID: 2204.09851v1
* Paper URL: [http://arxiv.org/abs/2204.09851v1](http://arxiv.org/abs/2204.09851v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.

### Title: FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation
* Paper ID: 2204.09850v1
* Paper URL: [http://arxiv.org/abs/2204.09850v1](http://arxiv.org/abs/2204.09850v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way.

### Title: 6GAN: IPv6 Multi-Pattern Target Generation via Generative Adversarial Nets with Reinforcement Learning
* Paper ID: 2204.09839v1
* Paper URL: [http://arxiv.org/abs/2204.09839v1](http://arxiv.org/abs/2204.09839v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cuitianyu961030/6gan](https://github.com/cuitianyu961030/6gan)
* Summary: Global IPv6 scanning has always been a challenge for researchers because of
the limited network speed and computational power. Target generation algorithms
are recently proposed to overcome the problem for Internet assessments by
predicting a candidate set to scan. However, IPv6 custom address configuration
emerges diverse addressing patterns discouraging algorithmic inference.
Widespread IPv6 alias could also mislead the algorithm to discover aliased
regions rather than valid host targets. In this paper, we introduce 6GAN, a
novel architecture built with Generative Adversarial Net (GAN) and
reinforcement learning for multi-pattern target generation. 6GAN forces
multiple generators to train with a multi-class discriminator and an alias
detector to generate non-aliased active targets with different addressing
pattern types. The rewards from the discriminator and the alias detector help
supervise the address sequence decision-making process. After adversarial
training, 6GAN's generators could keep a strong imitating ability for each
pattern and 6GAN's discriminator obtains outstanding pattern discrimination
ability with a 0.966 accuracy. Experiments indicate that our work outperformed
the state-of-the-art target generation algorithms by reaching a higher-quality
candidate set.

### Title: Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
* Paper ID: 2204.09817v1
* Paper URL: [http://arxiv.org/abs/2204.09817v1](http://arxiv.org/abs/2204.09817v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.

