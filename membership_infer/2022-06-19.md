### Title: Resource-Efficient Separation Transformer
* Paper ID: 2206.09507v1
* Paper URL: [http://arxiv.org/abs/2206.09507v1](http://arxiv.org/abs/2206.09507v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Transformers have recently achieved state-of-the-art performance in speech
separation. These models, however, are computationally-demanding and require a
lot of learnable parameters. This paper explores Transformer-based speech
separation with a reduced computational cost. Our main contribution is the
development of the Resource-Efficient Separation Transformer (RE-SepFormer), a
self-attention-based architecture that reduces the computational burden in two
ways. First, it uses non-overlapping blocks in the latent space. Second, it
operates on compact latent summaries calculated from each chunk. The
RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and
WHAM! datasets in both causal and non-causal settings. Remarkably, it scales
significantly better than the previous Transformer and RNN-based architectures
in terms of memory and inference-time, making it more suitable for processing
long mixtures.

### Title: Log-GPIS-MOP: A Unified Representation for Mapping, Odometry and Planning
* Paper ID: 2206.09506v1
* Paper URL: [http://arxiv.org/abs/2206.09506v1](http://arxiv.org/abs/2206.09506v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Whereas dedicated scene representations are required for each different tasks
in conventional robotic systems, this paper demonstrates that a unified
representation can be used directly for multiple key tasks. We propose the
Log-Gaussian Process Implicit Surface for Mapping, Odometry and Planning
(Log-GPIS-MOP): a probabilistic framework for surface reconstruction,
localisation and navigation based on a unified representation. Our framework
applies a logarithmic transformation to a Gaussian Process Implicit Surface
(GPIS) formulation to recover a global representation that accurately captures
the Euclidean distance field with gradients and, at the same time, the implicit
surface. By directly estimate the distance field and its gradient through
Log-GPIS inference, the proposed incremental odometry technique computes the
optimal alignment of an incoming frame, and fuses it globally to produce a map.
Concurrently, an optimisation-based planner computes a safe collision-free path
using the same Log-GPIS surface representation. We validate the proposed
framework on simulated and real datasets in 2D and 3D and benchmark against the
state-of-the-art approaches. Our experiments show that Log-GPIS-MOP produces
competitive results in sequential odometry, surface mapping and obstacle
avoidance.

### Title: A Parallel Implementation of Computing Mean Average Precision
* Paper ID: 2206.09504v1
* Paper URL: [http://arxiv.org/abs/2206.09504v1](http://arxiv.org/abs/2206.09504v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Mean Average Precision (mAP) has been widely used for evaluating the quality
of object detectors, but an efficient implementation is still absent. Current
implementations can only count true positives (TP's) and false positives (FP's)
for one class at a time by looping through every detection of that class
sequentially. Not only are these approaches inefficient, but they are also
inconvenient for reporting validation mAP during training. We propose a
parallelized alternative that can process mini-batches of detected bounding
boxes (DTBB's) and ground truth bounding boxes (GTBB's) as inference goes such
that mAP can be instantly calculated after inference is finished. Loops and
control statements in sequential implementations are replaced with extensive
uses of broadcasting, masking, and indexing. All operators involved are
supported by popular machine learning frameworks such as PyTorch and
TensorFlow. As a result, our implementation is much faster and can easily fit
into typical training routines. A PyTorch version of our implementation is
available at https://github.com/bwangca/fast-map.

### Title: On the Limitations of Stochastic Pre-processing Defenses
* Paper ID: 2206.09491v1
* Paper URL: [http://arxiv.org/abs/2206.09491v1](http://arxiv.org/abs/2206.09491v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Defending against adversarial examples remains an open problem. A common
belief is that randomness at inference increases the cost of finding
adversarial inputs. An example of such a defense is to apply a random
transformation to inputs prior to feeding them to the model. In this paper, we
empirically and theoretically investigate such stochastic pre-processing
defenses and demonstrate that they are flawed. First, we show that most
stochastic defenses are weaker than previously thought; they lack sufficient
randomness to withstand even standard attacks like projected gradient descent.
This casts doubt on a long-held assumption that stochastic defenses invalidate
attacks designed to evade deterministic defenses and force attackers to
integrate the Expectation over Transformation (EOT) concept. Second, we show
that stochastic defenses confront a trade-off between adversarial robustness
and model invariance; they become less effective as the defended model acquires
more invariance to their randomization. Future work will need to decouple these
two effects. Our code is available in the supplementary material.

### Title: SNN2ANN: A Fast and Memory-Efficient Training Framework for Spiking Neural Networks
* Paper ID: 2206.09449v1
* Paper URL: [http://arxiv.org/abs/2206.09449v1](http://arxiv.org/abs/2206.09449v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Spiking neural networks are efficient computation models for low-power
environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions
are successful techniques for SNN training. Nevertheless, the spike-base BP
training is slow and requires large memory costs. Though ANN2NN provides a
low-cost way to train SNNs, it requires many inference steps to mimic the
well-trained ANN for good performance. In this paper, we propose a SNN-to-ANN
(SNN2ANN) framework to train the SNN in a fast and memory-efficient way. The
SNN2ANN consists of 2 components: a) a weight sharing architecture between ANN
and SNN and b) spiking mapping units. Firstly, the architecture trains the
weight-sharing parameters on the ANN branch, resulting in fast training and low
memory costs for SNN. Secondly, the spiking mapping units ensure that the
activation values of the ANN are the spiking features. As a result, the
classification error of the SNN can be optimized by training the ANN branch.
Besides, we design an adaptive threshold adjustment (ATA) algorithm to address
the noisy spike problem. Experiment results show that our SNN2ANN-based models
perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet).
Moreover, the SNN2ANN can achieve comparable accuracy under 0.625x time steps,
0.377x training time, 0.27x GPU memory costs, and 0.33x spike activities of the
Spike-based BP model.

### Title: Meta-Analysis of the Accuracy of Syndromic Surveillance
* Paper ID: 2206.09430v1
* Paper URL: [http://arxiv.org/abs/2206.09430v1](http://arxiv.org/abs/2206.09430v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: We present the first meta-analysis of co-evolutionary learning networks for
digital disease surveillance research over last 10 years. In doing so, we show
the co-evolution and dynamical changes that occurred in academic research
related to digital disease surveillance for improving accuracy, approach and
results. Using dynamic network analysis, we are able to show the incorporation
of social media-based analytics and algorithms which have been proposed and
later improved by other researchers as co-evolutionary learning networks. This
essentially demonstrates how we improve our research and increase accuracy
through feedback loop for correcting the behaviour of an open system and
perhaps infer learning patterns, reliability and validity using 10 years
scientific research in digital disease surveillance.

### Title: LordNet: Learning to Solve Parametric Partial Differential Equations without Simulated Data
* Paper ID: 2206.09418v1
* Paper URL: [http://arxiv.org/abs/2206.09418v1](http://arxiv.org/abs/2206.09418v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Neural operators, as a powerful approximation to the non-linear operators
between infinite-dimensional function spaces, have proved to be promising in
accelerating the solution of partial differential equations (PDE). However, it
requires a large amount of simulated data which can be costly to collect,
resulting in a chicken-egg dilemma and limiting its usage in solving PDEs. To
jump out of the dilemma, we propose a general data-free paradigm where the
neural network directly learns physics from the mean squared residual (MSR)
loss constructed by the discretized PDE. We investigate the physical
information in the MSR loss and identify the challenge that the neural network
must have the capacity to model the long range entanglements in the spatial
domain of the PDE, whose patterns vary in different PDEs. Therefore, we propose
the low-rank decomposition network (LordNet) which is tunable and also
efficient to model various entanglements. Specifically, LordNet learns a
low-rank approximation to the global entanglements with simple fully connected
layers, which extracts the dominant pattern with reduced computational cost.
The experiments on solving Poisson's equation and Navier-Stokes equation
demonstrate that the physical constraints by the MSR loss can lead to better
accuracy and generalization ability of the neural network. In addition, LordNet
outperforms other modern neural network architectures in both PDEs with the
fewest parameters and the fastest inference speed. For Navier-Stokes equation,
the learned operator is over 50 times faster than the finite difference
solution with the same computational resources.

### Title: Oscillation Modes and Gravitational Waves from Strangeon Stars
* Paper ID: 2206.09407v1
* Paper URL: [http://arxiv.org/abs/2206.09407v1](http://arxiv.org/abs/2206.09407v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: The strong interaction at low energy scales determines the equation of state
(EOS) of supranuclear matters in neutron stars (NSs). It is conjectured that
the bulk dense matter may be composed of strangeons, which are quark clusters
with nearly equal numbers of $u$, $d$, and $s$ quarks. To characterize the
strong-repulsive interaction at short distance and the nonrelativistic nature
of strangeons, a phenomenological Lennard-Jones model with two parameters is
used to describe the EOS of strangeon stars (SSs). For the first time, we
investigate the oscillation modes of non-rotating SSs and obtain their
frequencies for various parameterizations of the EOS. We find that the
properties of radial oscillations of SSs are different from those of NSs,
especially for stars with relatively low central energy densities. Moreover, we
calculate the $f$-mode frequency of nonradial oscillations of SSs within the
relativistic Cowling approximation. The frequencies of the $f$-mode of SSs are
found to be in the range from $6.7\,$kHz to $ 8.7\,\rm{kHz}$. Finally, we study
the universal relations between the $f$-mode frequency and global properties of
SSs, such as the compactness and the tidal deformability. The results we
obtained are relevant to pulsar timing and gravitational waves, and will help
to probe NSs' EOSs and infer nonperturbative behaviours in quantum
chromodynamics.

### Title: Productive Reproducible Workflows for DNNs: A Case Study for Industrial Defect Detection
* Paper ID: 2206.09359v1
* Paper URL: [http://arxiv.org/abs/2206.09359v1](http://arxiv.org/abs/2206.09359v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: As Deep Neural Networks (DNNs) have become an increasingly ubiquitous
workload, the range of libraries and tooling available to aid in their
development and deployment has grown significantly. Scalable, production
quality tools are freely available under permissive licenses, and are
accessible enough to enable even small teams to be very productive. However
within the research community, awareness and usage of said tools is not
necessarily widespread, and researchers may be missing out on potential
productivity gains from exploiting the latest tools and workflows. This paper
presents a case study where we discuss our recent experience producing an
end-to-end artificial intelligence application for industrial defect detection.
We detail the high level deep learning libraries, containerized workflows,
continuous integration/deployment pipelines, and open source code templates we
leveraged to produce a competitive result, matching the performance of other
ranked solutions to our three target datasets. We highlight the value that
exploiting such systems can bring, even for research, and detail our solution
and present our best results in terms of accuracy and inference time on a
server class GPU, as well as inference times on a server class CPU, and a
Raspberry Pi 4.

### Title: What is Where by Looking: Weakly-Supervised Open-World Phrase-Grounding without Text Inputs
* Paper ID: 2206.09358v1
* Paper URL: [http://arxiv.org/abs/2206.09358v1](http://arxiv.org/abs/2206.09358v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Given an input image, and nothing else, our method returns the bounding boxes
of objects in the image and phrases that describe the objects. This is achieved
within an open world paradigm, in which the objects in the input image may not
have been encountered during the training of the localization mechanism.
Moreover, training takes place in a weakly supervised setting, where no
bounding boxes are provided. To achieve this, our method combines two
pre-trained networks: the CLIP image-to-text matching score and the BLIP image
captioning tool. Training takes place on COCO images and their captions and is
based on CLIP. Then, during inference, BLIP is used to generate a hypothesis
regarding various regions of the current image. Our work generalizes weakly
supervised segmentation and phrase grounding and is shown empirically to
outperform the state of the art in both domains. It also shows very convincing
results in the novel task of weakly-supervised open-world purely visual
phrase-grounding presented in our work. For example, on the datasets used for
benchmarking phrase-grounding, our method results in a very modest degradation
in comparison to methods that employ human captions as an additional input. Our
code is available at https://github.com/talshaharabany/what-is-where-by-looking
and a live demo can be found at
https://talshaharabany/what-is-where-by-looking.

### Title: LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks
* Paper ID: 2206.09333v1
* Paper URL: [http://arxiv.org/abs/2206.09333v1](http://arxiv.org/abs/2206.09333v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: High-throughput Genomics is ushering a new era in personalized health care,
and targeted drug design and delivery. Mining these large datasets, and
obtaining calibrated predictions is of immediate relevance and utility. In our
work, we develop methods for Gene Expression Inference based on Deep neural
networks. However, unlike typical Deep learning methods, our inferential
technique, while achieving state-of-the-art performance in terms of accuracy,
can also provide explanations, and report uncertainty estimates. We adopt the
Quantile Regression framework to predict full conditional quantiles for a given
set of house keeping gene expressions. Conditional quantiles, in addition to
being useful in providing rich interpretations of the predictions, are also
robust to measurement noise. However, check loss, used in quantile regression
to drive the estimation process is not differentiable. We propose log-cosh as a
smooth-alternative to the check loss. We apply our methods on GEO microarray
dataset. We also extend the method to binary classification setting.
Furthermore, we investigate other consequences of the smoothness of the loss in
faster convergence.

