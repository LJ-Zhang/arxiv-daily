### Title: Persistent homology in cosmic shear II: A tomographic analysis of DES-Y1
* Paper ID: 2204.11831v1
* Paper URL: [http://arxiv.org/abs/2204.11831v1](http://arxiv.org/abs/2204.11831v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: We demonstrate how to use persistent homology for cosmological parameter
inference in a tomographic cosmic shear survey. We obtain the first
cosmological parameter constraints from persistent homology by applying our
method to the first-year data of the Dark Energy Survey.
  To obtain these constraints, we analyse the topological structure of the
matter distribution by extracting persistence diagrams from signal-to-noise
maps of aperture masses. This presents a natural extension to the widely used
peak count statistics. Extracting the persistence diagrams from the
cosmo-SLICS, a suite of $N$-body simulations with variable cosmological
parameters, we interpolate the signal using Gaussian Processes and marginalise
over the most relevant systematic effects, including intrinsic alignments and
baryonic effects.
  We find for the structure growth parameter $S_8=0.747^{+0.025}_{-0.031}$,
which is in full agreement with other late-time probes. We also constrain the
intrinsic alignment parameter to $A=1.54\pm 0.52$, ruling out the case of no
intrinsic alignments at a $3\sigma$-level.

### Title: Zero-Shot Logit Adjustment
* Paper ID: 2204.11822v1
* Paper URL: [http://arxiv.org/abs/2204.11822v1](http://arxiv.org/abs/2204.11822v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/cdb342/ijcai-2022-zla](https://github.com/cdb342/ijcai-2022-zla)
* Summary: Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses
challenges in recognizing the novel classes in the test phase. The development
of generative models enables current GZSL techniques to probe further into the
semantic-visual link, culminating in a two-stage form that includes a generator
and a classifier. However, existing generation-based methods focus on enhancing
the generator's effect while neglecting the improvement of the classifier. In
this paper, we first conduct an analysis of two properties of the generated
pseudo unseen sample: bias and homogeneity. Then, we perform variational
Bayesian inference to back-derive the evaluation metrics, which reflects the
balance of the seen and unseen classes. As a consequence of our derivation, the
aforementioned two properties are incorporated into the classifier training as
seen-unseen priors via logit adjustment. The Zero-Shot Logit Adjustment further
puts semantic-based classifiers into effect in generation-based GZSL. Our
experiments demonstrate that the proposed technique achieves the state of the
art when combined with the basic generator, and it can improve various
generative zero-shot learning frameworks. Our codes are available on
\url{https://github.com/cdb342/IJCAI-2022-ZLA}.

### Title: Parallel Synthesis for Autoregressive Speech Generation
* Paper ID: 2204.11806v1
* Paper URL: [http://arxiv.org/abs/2204.11806v1](http://arxiv.org/abs/2204.11806v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Autoregressive models have achieved outstanding performance in neural speech
synthesis tasks. Though they can generate highly natural human speech, the
iterative generation inevitably makes the synthesis time proportional to the
utterance's length, leading to low efficiency. Many works were dedicated to
generating the whole speech time sequence in parallel and then proposed
GAN-based, flow-based, and score-based models. This paper proposed a new
thought for the autoregressive generation. Instead of iteratively predicting
samples in a time sequence, the proposed model performs frequency-wise
autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to
synthesize speech. In FAR, a speech utterance is first split into different
frequency subbands. The proposed model generates a subband conditioned on the
previously generated one. A full band speech can then be reconstructed by using
these generated subbands and a synthesis filter bank. Similarly, in BAR, an
8-bit quantized signal is generated iteratively from the first bit. By
redesigning the autoregressive method to compute in domains other than the time
domain, the number of iterations in the proposed model is no longer
proportional to the utterance's length but the number of subbands/bits. The
inference efficiency is hence significantly increased. Besides, a post-filter
is employed to sample audio signals from output posteriors, and its training
objective is designed based on the characteristics of the proposed
autoregressive methods. The experimental results show that the proposed model
is able to synthesize speech faster than real-time without GPU acceleration.
Compared with the baseline autoregressive and non-autoregressive models, the
proposed model achieves better MOS and shows its good generalization ability
while synthesizing 44 kHz speech or utterances from unseen speakers.

### Title: SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech
* Paper ID: 2204.11792v1
* Paper URL: [http://arxiv.org/abs/2204.11792v1](http://arxiv.org/abs/2204.11792v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made
fast and high-quality speech synthesis possible. However, current NAR-TTS
models usually use phoneme sequence as input and thus cannot understand the
tree-structured syntactic information of the input sequence, which hurts the
prosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and
light-weight NAR-TTS model, which integrates tree-structured syntactic
information into the prosody modeling modules in PortaSpeech
\cite{ren2021portaspeech}. Specifically, 1) We build a syntactic graph based on
the dependency tree of the input sentence, then process the text encoding with
a syntactic graph encoder to extract the syntactic information. 2) We
incorporate the extracted syntactic encoding with PortaSpeech to improve the
prosody prediction. 3) We introduce a multi-length discriminator to replace the
flow-based post-net in PortaSpeech, which simplifies the training pipeline and
improves the inference speed, while keeping the naturalness of the generated
audio. Experiments on three datasets not only show that the tree-structured
syntactic information grants SyntaSpeech the ability to synthesize better audio
with expressive prosody, but also demonstrate the generalization ability of
SyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech.
Ablation studies demonstrate the necessity of each component in SyntaSpeech.
Source code and audio samples are available at https://syntaspeech.github.io

### Title: Blind Equalization and Channel Estimation in Coherent Optical Communications Using Variational Autoencoders
* Paper ID: 2204.11776v1
* Paper URL: [http://arxiv.org/abs/2204.11776v1](http://arxiv.org/abs/2204.11776v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: We investigate the potential of adaptive blind equalizers based on
variational inference for carrier recovery in optical communications. These
equalizers are based on a low-complexity approximation of maximum likelihood
channel estimation. We generalize the concept of variational autoencoder (VAE)
equalizers to higher order modulation formats encompassing probabilistic
constellation shaping (PCS), ubiquitous in optical communications, oversampling
at the receiver, and dual-polarization transmission. Besides black-box
equalizers based on convolutional neural networks, we propose a model-based
equalizer based on a linear butterfly filter and train the filter coefficients
using the variational inference paradigm. As a byproduct, the VAE also provides
a reliable channel estimation. We analyze the VAE in terms of performance and
flexibility over a classical additive white Gaussian noise (AWGN) channel with
inter-symbol interference (ISI) and over a dispersive linear optical
dual-polarization channel. We show that it can extend the application range of
blind adaptive equalizers by outperforming the state-of-the-art
constant-modulus algorithm (CMA) for PCS for both fixed but also time-varying
channels. The evaluation is accompanied with a hyperparameter analysis.

### Title: LightDefectNet: A Highly Compact Deep Anti-Aliased Attention Condenser Neural Network Architecture for Light Guide Plate Surface Defect Detection
* Paper ID: 2204.11765v1
* Paper URL: [http://arxiv.org/abs/2204.11765v1](http://arxiv.org/abs/2204.11765v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Light guide plates are essential optical components widely used in a diverse
range of applications ranging from medical lighting fixtures to back-lit TV
displays. An essential step in the manufacturing of light guide plates is the
quality inspection of defects such as scratches, bright/dark spots, and
impurities. This is mainly done in industry through manual visual inspection
for plate pattern irregularities, which is time-consuming and prone to human
error and thus act as a significant barrier to high-throughput production.
Advances in deep learning-driven computer vision has led to the exploration of
automated visual quality inspection of light guide plates to improve inspection
consistency, accuracy, and efficiency. However, given the cost constraints in
visual inspection scenarios, the widespread adoption of deep learning-driven
computer vision methods for inspecting light guide plates has been greatly
limited due to high computational requirements. In this study, we explore the
utilization of machine-driven design exploration with computational and
"best-practices" constraints as well as L$_1$ paired classification discrepancy
loss to create LightDefectNet, a highly compact deep anti-aliased attention
condenser neural network architecture tailored specifically for light guide
plate surface defect detection in resource-constrained scenarios. Experiments
show that LightDetectNet achieves a detection accuracy of $\sim$98.2% on the
LGPSDD benchmark while having just 770K parameters ($\sim$33$\times$ and
$\sim$6.9$\times$ lower than ResNet-50 and EfficientNet-B0, respectively) and
$\sim$93M FLOPs ($\sim$88$\times$ and $\sim$8.4$\times$ lower than ResNet-50
and EfficientNet-B0, respectively) and $\sim$8.8$\times$ faster inference speed
than EfficientNet-B0 on an embedded ARM processor.

### Title: Compression-Complexity with Ordinal Patterns for Robust Causal Inference in Irregularly-Sampled Time Series
* Paper ID: 2204.11731v1
* Paper URL: [http://arxiv.org/abs/2204.11731v1](http://arxiv.org/abs/2204.11731v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Distinguishing cause from effect is a scientific challenge resisting
solutions from mathematics, statistics, information theory and computer
science. Compression-Complexity Causality (CCC) is a recently proposed
interventional measure of causality, inspired by Wiener-Granger's idea. It
estimates causality based on change in dynamical compression-complexity (or
compressibility) of the effect variable, given the cause variable. CCC works
with minimal assumptions on given data and is robust to irregular-sampling,
missing-data and finite-length effects. However, it only works for
one-dimensional time series. We propose an ordinal pattern symbolization scheme
to encode multidimensional patterns into one-dimensional symbolic sequences,
and thus introduce the Permutation CCC (PCCC), which retains all advantages of
the original CCC and can be applied to data from multidimensional systems with
potentially hidden variables. PCCC is tested on numerical simulations and
applied to paleoclimate data characterized by irregular and uncertain sampling
and limited numbers of samples.

### Title: Five key exoplanet questions answered via the analysis of 25 hot Jupiter atmospheres in eclipse
* Paper ID: 2204.11729v1
* Paper URL: [http://arxiv.org/abs/2204.11729v1](http://arxiv.org/abs/2204.11729v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Population studies of exoplanets are key to unlocking their statistical
properties. So far the inferred properties have been mostly limited to
planetary, orbital and stellar parameters extracted from, e.g., Kepler, radial
velocity, and GAIA data. More recently an increasing number of exoplanet
atmospheres have been observed in detail from space and the ground. Generally,
however, these atmospheric studies have focused on individual planets, with the
exception of a couple of works which have detected the presence of water vapor
and clouds in populations of gaseous planets via transmission spectroscopy.
Here, using a suite of retrieval tools, we analyse spectroscopic and
photometric data of 25 hot Jupiters, obtained with the Hubble and Spitzer Space
Telescopes via the eclipse technique. By applying the tools uniformly across
the entire set of 25 planets, we extract robust trends in the thermal structure
and chemical properties of hot Jupiters not obtained in past studies. With the
recent launch of JWST and the upcoming missions Twinkle, and Ariel, population
based studies of exoplanet atmospheres, such as the one presented here, will be
a key approach to understanding planet characteristics, formation, and
evolution in our galaxy.

### Title: The Galactic 3D large-scale dust distribution via Gaussian process regression on spherical coordinates
* Paper ID: 2204.11715v1
* Paper URL: [http://arxiv.org/abs/2204.11715v1](http://arxiv.org/abs/2204.11715v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Knowing the Galactic 3D dust distribution is relevant for understanding many
processes in the interstellar medium and for correcting many astronomical
observations for dust absorption and emission. Here, we aim for a 3D
reconstruction of the Galactic dust distribution with an increase in the number
of meaningful resolution elements by orders of magnitude with respect to
previous reconstructions, while taking advantage of the dust's spatial
correlations to inform the dust map. We use iterative grid refinement to define
a log-normal process in spherical coordinates. This log-normal process assumes
a fixed correlation structure, which was inferred in an earlier reconstruction
of Galactic dust. Our map is informed through 111 Million data points,
combining data of PANSTARRS, 2MASS, Gaia DR2 and ALLWISE. The log-normal
process is discretized to 122 Billion degrees of freedom, a factor of 400 more
than our previous map. We derive the most probable posterior map and an
uncertainty estimate using natural gradient descent and the Fisher-Laplace
approximation. The dust reconstruction covers a quarter of the volume of our
Galaxy, with a maximum coordinate distance of $16\,\text{kpc}$, and meaningful
information can be found up to at distances of $4\,$kpc, still improving upon
our earlier map by a factor of 5 in maximal distance, of $900$ in volume, and
of about eighteen in angular grid resolution. Unfortunately, the maximum
posterior approach chosen to make the reconstruction computational affordable
introduces artifacts and reduces the accuracy of our uncertainty estimate.
Despite of the apparent limitations of the presented 3D dust map, a good part
of the reconstructed structures are confirmed by independent maser
observations. Thus, the map is a step towards reliable 3D Galactic cartography
and already can serve for a number of tasks, if used with care.

### Title: NIHAO XXVII: Crossing the green valley
* Paper ID: 2204.11579v1
* Paper URL: [http://arxiv.org/abs/2204.11579v1](http://arxiv.org/abs/2204.11579v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: The transition of high-mass galaxies from being blue and star forming to
being red and dead is a crucial step in galaxy evolution, yet not fully
understood. In this work, we use the NIHAO suite of galaxy simulations to
investigate the relation between the transition time through the green valley
and other galaxy properties. The typical green valley crossing time of our
galaxies is approximately 400 Myr, somewhat shorter than observational
estimates. The crossing of the green valley is triggered by the onset of AGN
feedback and the subsequent shut down of star formation. Interestingly the time
spent in the green valley is not related to any other galaxy properties, such
as stellar age or metallicity, or the time at which the star formation
quenching takes place. The crossing time is set by two main contributions: the
ageing of the current stellar population and the residual star formation in the
green valley. These effects are of comparable magnitude, while major and minor
mergers have a negligible contribution. Most interestingly, we find the time
that a galaxy spends to travel through the green valley is twice the
$e$-folding time of the star formation quenching. This result is stable against
galaxy properties and the exact numerical implementation of AGN feedback in the
simulation. Assuming a typical crossing time of about one Gyr inferred from
observations, our results imply that any mechanism or process aiming to quench
star formation, must do it on a typical timescale of 500 Myr.

### Title: Deep Reinforcement Learning for Orienteering Problems Based on Decomposition
* Paper ID: 2204.11575v1
* Paper URL: [http://arxiv.org/abs/2204.11575v1](http://arxiv.org/abs/2204.11575v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: This paper presents a new method for solving an orienteering problem (OP) by
breaking it down into two parts: a knapsack problem (KP) and a traveling
salesman problem (TSP). A KP solver is responsible for picking nodes, while a
TSP solver is responsible for designing the proper path and assisting the KP
solver in judging constraint violations. To address constraints, we propose a
dual-population coevolutionary algorithm (DPCA) as the KP solver, which
simultaneously maintains both feasible and infeasible populations. A dynamic
pointer network (DYPN) is introduced as the TSP solver, which takes city
locations as inputs and immediately outputs a permutation of nodes. The model,
which is trained by reinforcement learning, can capture both the structural and
dynamic patterns of the given problem. The model can generalize to other
instances with different scales and distributions. Experimental results show
that the proposed algorithm can outperform conventional approaches in terms of
training, inference, and generalization ability.

### Title: Goal-driven Self-Attentive Recurrent Networks for Trajectory Prediction
* Paper ID: 2204.11561v1
* Paper URL: [http://arxiv.org/abs/2204.11561v1](http://arxiv.org/abs/2204.11561v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/luigifilippochiara/Goal-SAR](https://github.com/luigifilippochiara/Goal-SAR)
* Summary: Human trajectory forecasting is a key component of autonomous vehicles,
social-aware robots and advanced video-surveillance applications. This
challenging task typically requires knowledge about past motion, the
environment and likely destination areas. In this context, multi-modality is a
fundamental aspect and its effective modeling can be beneficial to any
architecture. Inferring accurate trajectories is nevertheless challenging, due
to the inherently uncertain nature of the future. To overcome these
difficulties, recent models use different inputs and propose to model human
intentions using complex fusion mechanisms. In this respect, we propose a
lightweight attention-based recurrent backbone that acts solely on past
observed positions. Although this backbone already provides promising results,
we demonstrate that its prediction accuracy can be improved considerably when
combined with a scene-aware goal-estimation module. To this end, we employ a
common goal module, based on a U-Net architecture, which additionally extracts
semantic information to predict scene-compliant destinations. We conduct
extensive experiments on publicly-available datasets (i.e. SDD, inD, ETH/UCY)
and show that our approach performs on par with state-of-the-art techniques
while reducing model complexity.

### Title: Robust inference for non-destructive one-shot device testing under step-stress model with exponential lifetimes
* Paper ID: 2204.11560v1
* Paper URL: [http://arxiv.org/abs/2204.11560v1](http://arxiv.org/abs/2204.11560v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: One-shot devices analysis involves an extreme case of interval censoring,
wherein one can only know whether the failure time is either before or after
the test time. Some kind of one-shot devices do not get destroyed when tested,
and so can continue within the experiment, providing extra information for
inference, if they did not fail before an inspection time. In addition, their
reliability can be rapidly estimated via accelerated life tests (ALTs) by
running the tests at varying and higher stress levels than working conditions.
In particular, step-stress tests allow the experimenter to increase the stress
levels at pre-fixed times gradually during the life-testing experiment. The
cumulative exposure model is commonly assumed for step-stress models, relating
the lifetime distribution of units at one stress level to the lifetime
distributions at preceding stress levels. In this paper,vwe develop robust
estimators and Z-type test statistics based on the density power divergence
(DPD) for testing linear null hypothesis for non-destructive one-shot devices
under the step-stress ALTs with exponential lifetime distribution. We study
asymptotic and robustness properties of the estimators and test statistics,
yielding point estimation and confidence intervals for different lifetime
characteristic such as reliability, distribution quantiles and mean lifetime of
the devices. A simulation study is carried out to assess the performance of the
methods of inference developed here and some real-life data sets are analyzed
finally for illustrative purpose.

### Title: ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference
* Paper ID: 2204.11458v1
* Paper URL: [http://arxiv.org/abs/2204.11458v1](http://arxiv.org/abs/2204.11458v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: State-of-the-art neural models typically encode document-query pairs using
cross-attention for re-ranking. To this end, models generally utilize an
encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach.
These paradigms, however, are not without flaws, i.e., running the model on all
query-document pairs at inference-time incurs a significant computational cost.
This paper proposes a new training and inference paradigm for re-ranking. We
propose to finetune a pretrained encoder-decoder model using in the form of
document to query generation. Subsequently, we show that this encoder-decoder
architecture can be decomposed into a decoder-only language model during
inference. This results in significant inference time speedups since the
decoder-only architecture only needs to learn to interpret static encoder
embeddings during inference. Our experiments show that this new paradigm
achieves results that are comparable to the more expensive cross-attention
ranking approaches while being up to 6.8X faster. We believe this work paves
the way for more efficient neural rankers that leverage large pretrained
models.

### Title: Fine-tuning Pruned Networks with Linear Over-parameterization
* Paper ID: 2204.11444v1
* Paper URL: [http://arxiv.org/abs/2204.11444v1](http://arxiv.org/abs/2204.11444v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Structured pruning compresses neural networks by reducing channels (filters)
for fast inference and low footprint at run-time. To restore accuracy after
pruning, fine-tuning is usually applied to pruned networks. However, too few
remaining parameters in pruned networks inevitably bring a great challenge to
fine-tuning to restore accuracy. To address this challenge, we propose a novel
method that first linearly over-parameterizes the compact layers in pruned
networks to enlarge the number of fine-tuning parameters and then
re-parameterizes them to the original layers after fine-tuning. Specifically,
we equivalently expand the convolution/linear layer with several consecutive
convolution/linear layers that do not alter the current output feature maps.
Furthermore, we utilize similarity-preserving knowledge distillation that
encourages the over-parameterized block to learn the immediate data-to-data
similarities of the corresponding dense layer to maintain its feature learning
ability. The proposed method is comprehensively evaluated on CIFAR-10 and
ImageNet which significantly outperforms the vanilla fine-tuning strategy,
especially for large pruning ratio.

### Title: ATLASGAL-selected massive clumps in the inner Galaxy. X. Observations of atomic carbon at 492 GHz
* Paper ID: 2204.11414v1
* Paper URL: [http://arxiv.org/abs/2204.11414v1](http://arxiv.org/abs/2204.11414v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: (Abridged) In this paper, we investigate the physical conditions of
[CI]-traced gas in high-mass star-forming regions by analyzing APEX [CI] 492
GHz single-pointing observations of the ATLASGAL Top100 sources along with
other multi-wavelength data. Our 98 sources are clearly detected in [CI] 492
GHz emission, and the observed integrated intensities and line widths tend to
increase toward evolved stages of star formation. In addition to these "main"
components that are associated with the Top100 sample, 41 emission and two
absorption features are identified by their velocities toward 28 and two lines
of sight respectively as "secondary" components. The secondary components have
systematically smaller integrated intensities and line widths than the main
components. We found that [CI] 492 GHz and 13CO(2-1) are well correlated with
the 13CO(2-1)-to-[CI] 492 GHz integrated intensity ratio varying from 0.2 to
5.3. In addition, we derived the H2-to-[CI] conversion factor, X(CI), by
dividing 870 micron-based H2 column densities by the observed [CI] 492 GHz
integrated intensities and found that X(CI) ranges from 2.3e20 to 1.3e22 with a
median of 1.7e21. In contrast to the strong correlation with 13CO(2-1), [CI]
492 GHz has a scattered relation with the 870 micron-traced molecular gas.
Finally, we performed LTE and non-LTE analyses of the [CI] 492 GHz and 809 GHz
data for a subset of the Top100 sample and inferred that [CI] emission likely
originates from warm (kinetic temperature > 60 K), optically thin (opacity <
0.5), and highly pressurized (thermal pressure ~ e5 to e8 K/cm3) regions.

### Title: Energy-Efficient Classification at the Wireless Edge with Reliability Guarantees
* Paper ID: 2204.10399v2
* Paper URL: [http://arxiv.org/abs/2204.10399v2](http://arxiv.org/abs/2204.10399v2)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Learning at the edge is a challenging task from several perspectives, since
data must be collected by end devices (e.g. sensors), possibly pre-processed
(e.g. data compression), and finally processed remotely to output the result of
training and/or inference phases. This involves heterogeneous resources, such
as radio, computing and learning related parameters. In this context, we
propose an algorithm that dynamically selects data encoding scheme, local
computing resources, uplink radio parameters, and remote computing resources,
to perform a classification task with the minimum average end devices' energy
consumption, under E2E delay and inference reliability constraints. Our method
does not assume any prior knowledge of the statistics of time varying context
parameters, while it only requires the solution of low complexity per-slot
deterministic optimization problems, based on instantaneous observations of
these parameters and that of properly defined state variables. Numerical
results on convolutional neural network based image classification illustrate
the effectiveness of our method in striking the best trade-off between energy,
delay and inference reliability.

### Title: BeiDou-G2: Past and Present
* Paper ID: 2204.09258v2
* Paper URL: [http://arxiv.org/abs/2204.09258v2](http://arxiv.org/abs/2204.09258v2)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: In January 2022, the defunct satellite BeiDou-G2 was pulled out of
geostationary orbit by Shijian-21 to a graveyard orbit. For safe docking and
operation, it was necessary to determine the rotation state in advance. In this
paper, we show the evolution of the rotation of the BeiDou-G2 satellite based
on the photometry observation data for the past 10 years. The rotational speed
of BeiDou-G2 was found to be annual oscillation, mainly due to the solar
radiation. Based on the evolution of BeiDou-G2's rotation speed and its orbit,
we confirmed that in the last 10 years, the satellite had six abnormal events.
These abnormal events were mainly due to the increase in the rotation speed
caused by suspected fuel leakages. Additionally, the abnormal events included
one collision in 2012, which was inferred to be the trigger of the fuel
leakages in the following years. No rotational speed abnormalities occurred
again after 2017, probably due to the complete release of the residual fuel.
The parameters and the propagating models after one incidence of solar panel
damage in 2014 and one fragment in 2016, with the standard errors for
propagating over 1 year of the rotational axis less than 3{\deg} and rotational
speed being 0.11{\deg}/s, were believed to be able to satisfy the accuracy
requirements of the rotation state well at the moment of docking.

