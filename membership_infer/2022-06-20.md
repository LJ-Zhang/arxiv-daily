### Title: SynWMD: Syntax-aware Word Mover's Distance for Sentence Similarity Evaluation
* Paper ID: 2206.10029v1
* Paper URL: [http://arxiv.org/abs/2206.10029v1](http://arxiv.org/abs/2206.10029v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Word Mover's Distance (WMD) computes the distance between words and models
text similarity with the moving cost between words in two text sequences. Yet,
it does not offer good performance in sentence similarity evaluation since it
does not incorporate word importance and fails to take inherent contextual and
structural information in a sentence into account. An improved WMD method using
the syntactic parse tree, called Syntax-aware Word Mover's Distance (SynWMD),
is proposed to address these two shortcomings in this work. First, a weighted
graph is built upon the word co-occurrence statistics extracted from the
syntactic parse trees of sentences. The importance of each word is inferred
from graph connectivities. Second, the local syntactic parsing structure of
words is considered in computing the distance between words. To demonstrate the
effectiveness of the proposed SynWMD, we conduct experiments on 6 textual
semantic similarity (STS) datasets and 4 sentence classification datasets.
Experimental results show that SynWMD achieves state-of-the-art performance on
STS tasks. It also outperforms other WMD-based methods on sentence
classification tasks.

### Title: High-accuracy Rb$_{2}^+$ interaction potentials based on coupled cluster calculations
* Paper ID: 2206.10016v1
* Paper URL: [http://arxiv.org/abs/2206.10016v1](http://arxiv.org/abs/2206.10016v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: This work discusses a protocol for constructing highly accurate potential
energy curves (PECs) for the lowest two states of Rb$_{2}^+$, i.e.
$X\,{}^2{\Sigma}{_g^+}$ and $(1) {}^2\Sigma{_u^+}$, using an additivity scheme
based on coupled-cluster theory. The approach exploits the findings of our
previous work [J. Schnabel, L. Cheng and A. K\"ohn, J. Chem. Phys. 155, 124101
(2021)] to avoid the unphysical repulsive long-range barrier occurring for
symmetric molecular ions when perturbative estimates of higher-order cluster
operators are employed. Furthermore, care was taken to reproduce the physically
correct exchange splitting of the $X {}^2{\Sigma}{_g^+}$ and $(1)
{}^2{\Sigma}{_u^+}$ PECs. The accuracy of our computational approach is
benchmarked for ionization energies of Rb and for spectroscopic constants as
well as vibrational levels of the $a {}^3{\Sigma}{_u^+}$ triplet state of
Rb\textsubscript{2}. We study high-level correlation contributions, high-level
relativistic effects and inner-shell correlation contributions and find very
good agreement with experimental reference values for the atomic ionization
potential and the binding energy of Rb$_{2}$ in the $a\,{}^3{\Sigma}{_u^+}$
triplet state. Our final best estimate for the binding energy of the Rb$_{2}^+$
$X {}^2{\Sigma}{_g^+}$ state including zero-point vibrational contributions is
$D_0 = 6179\,\mathrm{cm}^{-1}$ with an estimated error bound of
$\mathcal{O}(\pm 30\,\mathrm{cm}^{-1})$. This value is smaller than the
experimentally inferred lower bond of $D_0\ge 6307.5\,\mathrm{cm}^{-1}$ [Bellos
et al., Phys. Rev. A 87, 012508 (2013)] and will require further investigation.
For the $(1) {}^2{\Sigma}{_u^+}$ state a shallow potential with $D_0 =
78.4\,\mathrm{cm}^{-1}$ and an error bound of $\pm 9\,\mathrm{cm}^{-1}$ is
computed.

### Title: Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments
* Paper ID: 2206.10013v1
* Paper URL: [http://arxiv.org/abs/2206.10013v1](http://arxiv.org/abs/2206.10013v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We develop a new, principled algorithm for estimating the contribution of
training data points to the behavior of a deep learning model, such as a
specific prediction it makes. Our algorithm estimates the AME, a quantity that
measures the expected (average) marginal effect of adding a data point to a
subset of the training data, sampled from a given distribution. When subsets
are sampled from the uniform distribution, the AME reduces to the well-known
Shapley value. Our approach is inspired by causal inference and randomized
experiments: we sample different subsets of the training data to train multiple
submodels, and evaluate each submodel's behavior. We then use a LASSO
regression to jointly estimate the AME of each data point, based on the subset
compositions. Under sparsity assumptions ($k \ll N$ datapoints have large AME),
our estimator requires only $O(k\log N)$ randomized submodel trainings,
improving upon the best prior Shapley value estimators.

### Title: Modelling Populations of Interaction Networks via Distance Metrics
* Paper ID: 2206.09995v1
* Paper URL: [http://arxiv.org/abs/2206.09995v1](http://arxiv.org/abs/2206.09995v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Network data arises through observation of relational information between a
collection of entities. Recent work in the literature has independently
considered when (i) one observes a sample of networks, connectome data in
neuroscience being a ubiquitous example, and (ii) the units of observation
within a network are edges or paths, such as emails between people or a series
of page visits to a website by a user, often referred to as interaction network
data. The intersection of these two cases, however, is yet to be considered. In
this paper, we propose a new Bayesian modelling framework to analyse such data.
Given a practitioner-specified distance metric between observations, we define
families of models through location and scale parameters, akin to a Gaussian
distribution, with subsequent inference of model parameters providing reasoned
statistical summaries for this non-standard data structure. To facilitate
inference, we propose specialised Markov chain Monte Carlo (MCMC) schemes
capable of sampling from doubly-intractable posterior distributions over
discrete and multi-dimensional parameter spaces. Through simulation studies we
confirm the efficacy of our methodology and inference scheme, whilst its
application we illustrate via an example analysis of a location-based social
network (LSBN) data set.

### Title: A detailed analysis of the Gl 486 planetary system
* Paper ID: 2206.09990v1
* Paper URL: [http://arxiv.org/abs/2206.09990v1](http://arxiv.org/abs/2206.09990v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The Gl 486 system consists of a very nearby, relatively bright, weakly active
M3.5 V star at just 8 pc with a warm transiting rocky planet of about 1.3
R_Terra and 3.0 M_Terra that is ideal for both transmission and emission
spectroscopy and for testing interior models of telluric planets. To prepare
for future studies, we collected light curves of seven new transits observed
with the CHEOPS space mission and new radial velocities obtained with
MAROON-X/Gemini North and CARMENES/Calar Alto telescopes, together with
previously published spectroscopic and photometric data from the two
spectrographs and TESS. We also performed interferometric observations with the
CHARA Array and new photometric monitoring with a suite of smaller telescopes.
From interferometry, we measure a limb-darkened disc angular size of the star
Gl 486. Together with a corrected Gaia EDR3 parallax, we obtain a stellar
radius. We also measure a stellar rotation period at P_rot ~ 49.9 d, an upper
limit to its XUV (5-920 AA) flux with new Hubble/STIS data, and, for the first
time, a variety of element abundances (Fe, Mg, Si, V, Sr, Zr, Rb) and C/O
ratio. Besides, we impose restrictive constraints on the presence of additional
components, either stellar or substellar, in the system. With the input stellar
parameters and the radial-velocity and transit data, we determine the radius
and mass of the planet Gl 486 b at R_p = 1.343+/0.063 R_Terra and M_p =
3.00+/-0.13 M_Terra. From the planet parameters and the stellar element
abundances, we infer the most probable models of planet internal structure and
composition, which are consistent with a relatively small metallic core with
respect to the Earth, a deep silicate mantle, and a thin volatile upper layer.
With all these ingredients, we outline prospects for Gl 486 b atmospheric
studies, especially with forthcoming James Webb Space Telescope observations
(abridged).

### Title: Uncertainty and bias of cosmology and astrophysical population model from statistical dark sirens
* Paper ID: 2206.09984v1
* Paper URL: [http://arxiv.org/abs/2206.09984v1](http://arxiv.org/abs/2206.09984v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Gravitational-wave (GW) radiation from a coalescing compact binary is a
standard siren as the luminosity distance of each event can be directly
measured from the amplitude of the signal. One possibility to constrain
cosmology using the GW siren is to perform statistical inference on a
population of binary black hole (BBH) events. In essence, this statistical
method can be viewed as follows. We can modify the shape of the distribution of
observed BBH events by changing cosmological parameters until it eventually
matches the distribution constructed from an astrophysical population model,
thereby allowing us to determine the cosmological parameters. In this work, we
derive the Cram\'er-Rao bound for both cosmological parameters and those
governing the astrophysical population model from this statistical dark siren
method by examining the Fisher information contained in the event distribution.
Our study provides analytical insights and enables fast yet accurate
estimations of the statistical accuracy of dark siren cosmology. Furthermore,
we consider the bias in cosmology due to unmodeled substructures in the merger
rate and the mass distribution. We find a $1\%$ deviation in the astrophysical
model can lead to a more than $1\%$ error in the Hubble constant. This could
limit the accuracy of dark siren cosmology when there are more than $10^4$ BBH
events detected.

### Title: Polarized X-rays Constrain The Disk-Jet Geometry in a Black Hole X-ray Binary
* Paper ID: 2206.09972v1
* Paper URL: [http://arxiv.org/abs/2206.09972v1](http://arxiv.org/abs/2206.09972v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In a black hole X-ray binary (XRB) system, gas accreted from a normal star
onto a black hole glows brightly in X-rays. We report on an observation of the
XRB Cygnus X-1 (Cyg X-1) by the Imaging X-ray Polarimetry Explorer IXPE)
yielding the first highly significant detection of X-ray polarization from an
accreting black hole. The electric vector polarization angle aligns with the
outflowing radio jet, supporting the hypothesis that the jet is launched from
the inner X-ray emitting region. The higher than expected 2-8 keV polarization
degree of 4.0+-0.2% implies that the accretion disk is viewed more edge-on than
inferred from the orbital parameters. The spectropolarimetric data reveal that
the hot X-ray emitting plasma is extended in the plane of the accretion disk
rather than along the jet axis.

### Title: Lightcurves and Rotations of Trans-Neptunian Objects in the 2:1 Mean Motion Resonance with Neptune
* Paper ID: 2206.09949v1
* Paper URL: [http://arxiv.org/abs/2206.09949v1](http://arxiv.org/abs/2206.09949v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We report the rotational lightcurves of 21 trans-Neptunian objects (TNOs) in
Neptune's 2:1 mean motion resonance obtained with the 6.5 m Magellan-Baade
telescope and the 4.3 m Lowell Discovery Telescope. The main survey's goal is
to find objects displaying a large lightcurve amplitude which is indicative of
contact binaries or highly elongated objects. In our sample, two 2:1 resonant
TNOs showed a significant short-term lightcurve amplitude: 2002 VD$_{130}$ and
(531074) 2012 DX$_{98}$. The full lightcurve of 2012 DX$_{98}$ infers a
periodicity of 20.80$\pm$0.06h and amplitude of 0.56$\pm$0.03mag whereas 2002
VD$_{130}$ rotates in 9.85$\pm$0.07h with a 0.31$\pm$0.04mag lightcurve
amplitude. Based on lightcurve morphology, we classify (531074) 2012 DX$_{98}$
as a likely contact binary, but 2002 VD$_{130}$ as a likely single elongated
object. Based on our sample and the lightcurves reported in the literature, we
estimate the lower percentage of nearly equal-sized contact binaries at only
7-14$\%$ in the 2:1 resonance, which is comparable to the low fraction reported
for the dynamically Cold Classical trans-Neptunian objects. This low contact
binary fraction in the 2:1 Neptune resonance is consistent with the lower
estimate of the recent numerical modeling. We report the Sloan g', r', i'
surface colors of 2002 VD$_{130}$ which is an ultra-red TNO whereas 2012
DX$_{98}$ is a very red object based on published surface colors.

### Title: Integrating Machine Learning with Mechanistic Models for Predicting the Yield Strength of High Entropy Alloys
* Paper ID: 2206.09944v1
* Paper URL: [http://arxiv.org/abs/2206.09944v1](http://arxiv.org/abs/2206.09944v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Accelerating the design of materials with targeted properties is one of the
key materials informatics tasks. The most common approach takes a data-driven
motivation, where the underlying knowledge is incorporated in the form of
domain-inspired input features. Machine learning (ML) models are then built to
establish the input-output relationships. An alternative approach involves
leveraging mechanistic models, where the domain knowledge is incorporated in a
predefined functional form. These mechanistic models are meticulously
formulated through observations to validate specific hypotheses, and
incorporate elements of causality missing from data-driven ML approaches. In
this work, we demonstrate a computational approach that integrates mechanistic
models with phenomenological and ML models to rapidly predict the
temperature-dependent yield strength of high entropy alloys (HEAs) that form in
the single-phase face-centered cubic (FCC) structure. Our main contribution is
in establishing a quantitative relationship between the HEA compositions and
temperature-dependent elastic constants. This allows us to improve the
treatment of elastic constant mismatch to the solid solution strengthening
effect in the mechanistic model, which is important for reliable prediction of
yield strength as a function of temperature in single-phase FCC-based HEAs. We
accomplish this by combining Bayesian inference with ensemble ML methods. The
outcome is a probability distribution of elastic constants which, when
propagated through the mechanistic model, yields a prediction of
temperature-dependent yield strength, along with the uncertainties. The
predicted yield strength shows good agreement with published experimental data,
giving us confidence in applying the developed approach for the rapid search of
novel FCC-based HEAs with excellent yield strength at various temperatures.

### Title: Inference-Based Quantum Sensing
* Paper ID: 2206.09919v1
* Paper URL: [http://arxiv.org/abs/2206.09919v1](http://arxiv.org/abs/2206.09919v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In a standard Quantum Sensing (QS) task one aims at estimating an unknown
parameter $\theta$, encoded into an $n$-qubit probe state, via measurements of
the system. The success of this task hinges on the ability to correlate changes
in the parameter to changes in the system response $\mathcal{R}(\theta)$ (i.e.,
changes in the measurement outcomes). For simple cases the form of
$\mathcal{R}(\theta)$ is known, but the same cannot be said for realistic
scenarios, as no general closed-form expression exists. In this work we present
an inference-based scheme for QS. We show that, for a general class of unitary
families of encoding, $\mathcal{R}(\theta)$ can be fully characterized by only
measuring the system response at $2n+1$ parameters. In turn, this allows us to
infer the value of an unknown parameter given the measured response, as well as
to determine the sensitivity of the sensing scheme, which characterizes its
overall performance. We show that inference error is, with high probability,
smaller than $\delta$, if one measures the system response with a number of
shots that scales only as $\Omega(\log^3(n)/\delta^2)$. Furthermore, the
framework presented can be broadly applied as it remains valid for arbitrary
probe states and measurement schemes, and, even holds in the presence of
quantum noise. We also discuss how to extend our results beyond unitary
families. Finally, to showcase our method we implement it for a QS task on real
quantum hardware, and in numerical simulations.

### Title: Learning Optimal Flows for Non-Equilibrium Importance Sampling
* Paper ID: 2206.09908v1
* Paper URL: [http://arxiv.org/abs/2206.09908v1](http://arxiv.org/abs/2206.09908v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Many applications in computational sciences and statistical inference require
the computation of expectations with respect to complex high-dimensional
distributions with unknown normalization constants, as well as the estimation
of these constants. Here we develop a method to perform these calculations
based on generating samples from a simple base distribution, transporting them
along the flow generated by a velocity field, and performing averages along
these flowlines. This non-equilibrium importance sampling (NEIS) strategy is
straightforward to implement, and can be used for calculations with arbitrary
target distributions. On the theory side we discuss how to tailor the velocity
field to the target and establish general conditions under which the proposed
estimator is a perfect estimator, with zero-variance. We also draw connections
between NEIS and approaches based on mapping a base distribution onto a target
via a transport map. On the computational side we show how to use deep learning
to represent the velocity field by a neural network and train it towards the
zero variance optimum. These results are illustrated numerically on high
dimensional examples, where we show that training the velocity field can
decrease the variance of the NEIS estimator by up to 6 order of magnitude
compared to a vanilla estimator. We also show that NEIS performs better on
these examples than Neal's annealed importance sampling (AIS).

### Title: Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world
* Paper ID: 2206.09889v1
* Paper URL: [http://arxiv.org/abs/2206.09889v1](http://arxiv.org/abs/2206.09889v1)
* Updated Date: 2022-06-20
* Code URL: [https://github.com/facebookresearch/nocturne](https://github.com/facebookresearch/nocturne)
* Summary: We introduce \textit{Nocturne}, a new 2D driving simulator for investigating
multi-agent coordination under partial observability. The focus of Nocturne is
to enable research into inference and theory of mind in real-world multi-agent
settings without the computational overhead of computer vision and feature
extraction from images. Agents in this simulator only observe an obstructed
view of the scene, mimicking human visual sensing constraints. Unlike existing
benchmarks that are bottlenecked by rendering human-like observations directly
using a camera input, Nocturne uses efficient intersection methods to compute a
vectorized set of visible features in a C++ back-end, allowing the simulator to
run at $2000+$ steps-per-second. Using open-source trajectory and map data, we
construct a simulator to load and replay arbitrary trajectories and scenes from
real-world driving data. Using this environment, we benchmark
reinforcement-learning and imitation-learning agents and demonstrate that the
agents are quite far from human-level coordination ability and deviate
significantly from the expert trajectories.

### Title: Efficient Pricing and Calibration of High-Dimensional Basket Options
* Paper ID: 2206.09877v1
* Paper URL: [http://arxiv.org/abs/2206.09877v1](http://arxiv.org/abs/2206.09877v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: This paper studies equity basket options -- i.e., multi-dimensional
derivatives whose payoffs depend on the value of a weighted sum of the
underlying stocks -- and develops a new and innovative approach to ensure
consistency between options on individual stocks and on the index comprising
them. Specifically, we show how to resolve a well-known problem that when
individual constituent distributions of an equity index are inferred from the
single-stock option markets and combined in a multi-dimensional
local/stochastic volatility model, the resulting basket option prices will not
generate a skew matching that of the options on the equity index corresponding
to the basket. To address this ``insufficient skewness'', we proceed in two
steps. First, we propose an ``effective'' local volatility model by mapping the
general multi-dimensional basket onto a collection of marginal distributions.
Second, we build a multivariate dependence structure between all the marginal
distributions assuming a jump-diffusion model for the effective projection
parameters, and show how to calibrate the basket to the index smile. Numerical
tests and calibration exercises demonstrate an excellent fit for a basket of as
many as 30 stocks with fast calculation time.

### Title: Sparse inference of the human hematopoietic system from heterogeneous and partially observed genomic data
* Paper ID: 2206.09863v1
* Paper URL: [http://arxiv.org/abs/2206.09863v1](http://arxiv.org/abs/2206.09863v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Hematopoiesis is the process of blood cell formation, through which
progenitor stem cells differentiate into mature forms, such as white and red
blood cells or mature platelets. While the precursors of the mature forms share
many regulatory pathways involving common cellular nuclear factors, specific
networks of regulation shape their fate towards one lineage or another. In this
study, we aim to analyse the complex regulatory network that drives the
formation of mature red blood cells and platelets from their common precursor.
To this aim, we develop a dedicated graphical model which we infer from the
latest RT-qPCR genomic data. The model also accounts for the effect of external
genomic data. A computationally efficient Expectation-Maximization algorithm
allows regularised network inference from the high-dimensional and often only
partially observed RT-qPCR data. A careful combination of alternating direction
method of multipliers algorithms allows achieving sparsity in the individual
lineage networks and a high sharing between these networks, together with the
detection of the associations between the membrane-bound receptors and the
nuclear factors. The approach will be implemented in the R package cglasso and
can be used in similar applications where network inference is conducted from
high-dimensional, heterogeneous and partially observed data.

### Title: Chemical abundances in Seyfert galaxies -IX. Helium abundance estimates
* Paper ID: 2206.09836v1
* Paper URL: [http://arxiv.org/abs/2206.09836v1](http://arxiv.org/abs/2206.09836v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: For the first time, the helium abundance relative to hydrogen (He/H), which
relied on direct measurements of the electron temperature, has been derived in
the narrow line regions (NLRs) from a local sample of Seyfert 2 nuclei. In view
of this, optical emission line intensities [$3000 \: < \lambda \: < \: 7000$]
of 65 local Seyfert 2 nuclei ($z \: < \: 0.2$), taken from Sloan Digital Sky
Survey Data Release 15 and additional compilation from the literature, were
considered. We used photoionization model grid to derive an Ionization
Correction Factor (ICF) for the neutral helium. The application of this ICF
indicates that the NLRs of Seyfert 2 present a neutral helium fraction of
$\sim$50 per cent in relation to the total helium abundance. We find that
Seyfert 2 nuclei present helium abundance ranging from 0.60 to 2.50 times the
solar value, while $\sim$85 per cent of the sample present over-solar abundance
values. The derived (He/H)-(O/H) abundance relation from the Seyfert 2 is
stepper than that of star-forming regions (SFs) and this difference could be
due to excess of helium injected into the Interstellar Medium by the winds of
Wolf Rayet stars. From a regression to zero metallicity, by using Seyfert 2
estimates combined with SFs estimates, we obtained a primordial helium mass
fraction $Y_{\rm p}$=0.2441$\pm$0.0037, a value in good agreement with the one
inferred from the temperature fluctuations of the cosmic microwave background
by the Planck Collaboration, i.e. $Y_{\rm p}$=0.2471$\pm$0.0003

### Title: Double soft-thresholded model for multi-group scalar on vector-valued image regression
* Paper ID: 2206.09819v1
* Paper URL: [http://arxiv.org/abs/2206.09819v1](http://arxiv.org/abs/2206.09819v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In this paper, we develop a novel spatial variable selection method for
scalar on vector-valued image regression in a multi-group setting. Here,
'vector-valued image' refers to the imaging datasets that contain vector-valued
information at each pixel/voxel location, such as in RGB color images,
multimodal medical images, DTI imaging, etc. The focus of this work is to
identify the spatial locations in the image having an important effect on the
scalar outcome measure. Specifically, the overall effect of each voxel is of
interest. We thus develop a novel shrinkage prior by soft-thresholding the
\ell_2 norm of a latent multivariate Gaussian process. It will allow us to
estimate sparse and piecewise-smooth spatially varying vector-valued regression
coefficient functions. Furthermore, motivated by our clinical application, the
regression effect is decomposed into shared and group-specific parts, where a
double soft-thresholding based prior using ST2N is introduced. For posterior
inference, an efficient MCMC algorithm is developed. We establish the posterior
contraction rate for parameter estimation and consistency for variable
selection of the proposed Bayesian model, assuming that the true regression
coefficients are Holder smooth. Finally, we demonstrate the better performance
in simulations and illustrate in an ADNI dataset for modeling MMSE scores based
on DTI-based vector-valued imaging markers.

### Title: Statistical Inference for Large-dimensional Tensor Factor Model by Weighted/Unweighted Projection
* Paper ID: 2206.09800v1
* Paper URL: [http://arxiv.org/abs/2206.09800v1](http://arxiv.org/abs/2206.09800v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Tensor Factor Models (TFM) are appealing dimension reduction tools for
high-order large-dimensional time series, and have wide applications in
economics, finance and medical imaging. Two types of TFM have been proposed in
the literature, essentially based on the Tucker or CP decomposition of tensors.
In this paper, we propose a projection estimator for the Tucker-decomposition
based TFM, and provide its least-square interpretation which parallels to the
least-square interpretation of the Principal Component Analysis (PCA) for the
vector factor model. The projection technique simultaneously reduce the
dimensionality and the magnitudes of the idiosyncratic error matrix, thus
leading to an increase of signal-to-noise ratio. We derive a faster convergence
rate of the projection estimator than that of the naive PCA-based estimator,
under mild conditions which allow the idiosyncratic noise to have weak
cross-correlations and weak autocorrelations. Further motivated by the
least-squares interpretation, we propose a robust version by utilizing a
Huber-loss function, which leads to an iterative weighted projection technique.
Extensive numerical studies are conducted to investigate the empirical
performance of the proposed (weighted) projection estimator relative to the
sate-of-the-art ones. The simulation results shows that the projection
estimator performs better than the non-projection estimators, and the weighted
projection estimator performs much better than the existing ones in the
heavy-tailed case. We are still working on the theoretical analysis of the
weighted projection estimator.

### Title: Sufficient Conditions for the Joined Set of Solutions of the Overdetermined Interval System of Linear Algebraic Equations Membership to Only One Orthant
* Paper ID: 2206.09773v1
* Paper URL: [http://arxiv.org/abs/2206.09773v1](http://arxiv.org/abs/2206.09773v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Interval systems of linear algebraic equations (ISLAE) are considered in the
context of constructing of linear models according to data with interval
uncertainty. Sufficient conditions for boundedness and convexity of an
admissible domain (AD) of ISLAE and its belonging to only one orthant of an
$n$-dimensional space are proposed, which can be verified in polynomial time by
the methods of computational linear algebra. In this case, AD ISLAE turns out
to be a convex bounded polyhedron, entirely lying in the corresponding ortant.
These properties of AD ISLAE allow, firstly, to find solutions to the
corresponding ISLAE in polynomial time by linear programming methods (while
finding a solution to ISLAE of a general form is an NP-hard problem). Secondly,
the coefficients of the linear model obtained by solving the corresponding
ISLAE have an analogue of the significance property of the coefficient of the
linear model, since the coefficients of the linear model do not change their
sign within the limits of the AD. The formulation and proof of the
corresponding theorem are presented. The error estimation and convergence of an
arbitrary solution of ISLAE to the normal solution of a hypothetical exact
system of linear algebraic equations are also investigated. An illustrative
numerical example is given.

### Title: Multiband Delay Estimation for Localization Using a Two-Stage Global Estimation Scheme
* Paper ID: 2206.09751v1
* Paper URL: [http://arxiv.org/abs/2206.09751v1](http://arxiv.org/abs/2206.09751v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The time of arrival (TOA)-based localization techniques, which need to
estimate the delay of the line-of-sight (LoS) path, have been widely employed
in location-aware networks. To achieve a high-accuracy delay estimation, a
number of multiband-based algorithms have been proposed recently, which exploit
the channel state information (CSI) measurements over multiple non-contiguous
frequency bands. However, to the best of our knowledge, there still lacks an
efficient scheme that fully exploits the multiband gains when the phase
distortion factors caused by hardware imperfections are considered, due to that
the associated multi-parameter estimation problem contains many local optimums
and the existing algorithms can easily get stuck in a "bad" local optimum. To
address these issues, we propose a novel two-stage global estimation (TSGE)
scheme for multiband delay estimation. In the coarse stage, we exploit the
group sparsity structure of the multiband channel and propose a Turbo Bayesian
inference (Turbo-BI) algorithm to achieve a good initial delay estimation based
on a coarse signal model, which is transformed from the original multiband
signal model by absorbing the carrier frequency terms. The estimation problem
derived from the coarse signal model contains less local optimums and thus a
more stable estimation can be achieved than directly using the original signal
model. Then in the refined stage, with the help of coarse estimation results to
narrow down the search range, we perform a global delay estimation using a
particle swarm optimization-least square (PSO-LS) algorithm based on a refined
multiband signal model to exploit the multiband gains to further improve the
estimation accuracy. Simulation results show that the proposed TSGE
significantly outperforms the benchmarks with comparative computational
complexity.

### Title: Variable eddy viscosities in the atmospheric boundary layer from ageostrophic wind-speed profiles
* Paper ID: 2206.09712v1
* Paper URL: [http://arxiv.org/abs/2206.09712v1](http://arxiv.org/abs/2206.09712v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We generate explicit height-dependent eddy viscosity coefficients in the
Ekman layer from convex wind speed profiles. The solutions we obtain are
parameterized in terms of the relative deflection angle between the wind
directions at the top and bottom of the flow, as well as the geostrophic wind
speed and a velocity scale we interpret as the transfer rate of horizontal
momentum in the vertical direction. The solutions may be used to infer the
thickness of the Ekman layer for a variety of deflection angles different from
deflection angle of the classic Ekman spiral.

### Title: Great Expectations: Unsupervised Inference of Suspense, Surprise and Salience in Storytelling
* Paper ID: 2206.09708v1
* Paper URL: [http://arxiv.org/abs/2206.09708v1](http://arxiv.org/abs/2206.09708v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Stories interest us not because they are a sequence of mundane and
predictable events but because they have drama and tension. Crucial to creating
dramatic and exciting stories are surprise and suspense. The thesis trains a
series of deep learning models via only reading stories, a self-supervised (or
unsupervised) system. Narrative theory methods (rules and procedures) are
applied to the knowledge built into deep learning models to directly infer
salience, surprise, and salience in stories. Extensions add memory and external
knowledge from story plots and from Wikipedia to infer salience on novels such
as Great Expectations and plays such as Macbeth. Other work adapts the models
as a planning system for generating original stories.
  The thesis finds that applying the narrative theory to deep learning models
can align with the typical reader. In follow-up work, the insights could help
improve computer models for tasks such as automatic story writing and
assistance for writing, summarising or editing stories. Moreover, the approach
of applying narrative theory to the inherent qualities built in a system that
learns itself (self-supervised) from reading from books, watching videos, and
listening to audio is much cheaper and more adaptable to other domains and
tasks. Progress is swift in improving self-supervised systems. As such, the
thesis's relevance is that applying domain expertise with these systems may be
a more productive approach for applying machine learning in many areas of
interest.

### Title: Perihelion Activity of (3200) Phaethon Is Not Dusty
* Paper ID: 2206.09704v1
* Paper URL: [http://arxiv.org/abs/2206.09704v1](http://arxiv.org/abs/2206.09704v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We present an analysis of asteroid (3200) Phaethon using coronagraphic
observations from 2008 to 2022 by the COR2 cameras onboard the twin Solar
TErrestrial RElations Observatory (STEREO) spacecraft. Although the asteroid
cannot be confidently detected in individual images, we managed to spot it in
image stacks combined from the same sets of perihelion observations, yet only
when observed at low phase angles ($\lesssim$30\deg) but not at large phase
angles ($\gtrsim$150\deg). The lack of a strong forward-scattering enhancement
that is expected for dust grains having sizes comparable to transmitted
wavelengths thereby implies that the perihelion activity of Phaethon is highly
unlikely to be relevant to the ejection of dust grains as suggested by earlier
studies based on STEREO's HI-1 observations. Assuming the observed activity of
Phaethon is caused by dust ejection will lead to an insurmountable discrepancy
in the inferred amount of dust no less than an order of magnitude between the
HI-1 and COR2 observations. Rather, we speculate that the perihelion activity
is caused by sodium and/or iron emissions, the former of which may have become
transmittable due to an ageing effect of the HI-1 cameras. The modelled
emission flux is qualitatively similar to the HI-1 observations in the case
where the peak of the atomic production rate is delayed by $\sim$1 day from
perihelion. We encourage future observations of Phaethon at small heliocentric
distances to verify our conjecture.

### Title: Prevalence of short-lived radioactive isotopes across exoplanetary systems inferred from polluted white dwarfs
* Paper ID: 2206.09675v1
* Paper URL: [http://arxiv.org/abs/2206.09675v1](http://arxiv.org/abs/2206.09675v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In the Solar System short-lived radioisotopes, such as 26Al, played a crucial
role during the formation planetary bodies by providing a significant
additional source of heat. Notably, this led to early and large-scale melting
and iron core formation in planetesimals and their loss of volatile elements,
such as hydrogen and carbon. In the context exoplanetary systems, therefore,
the prevalence of short-lived radioisotopes is key to interpreting the observed
bulk volatile budget and atmospheric diversity among low-mass exoplanets. White
dwarfs that have accreted planetary material provide a unique means to infer
the frequency of iron core formation in extrasolar planetesimals, and hence the
ubiquity of planetary systems forming with high short-lived radioisotope
abundances. Here, we devise a quantitative method to infer the fraction of
planetary systems enriched with shortlived radionuclides upon planetesimal
formation from white dwarf data. We argue that the current evidence from white
dwarfs point towards a significant fraction of exo-planetesimals having formed
an iron core. Although the data may be explained by the accretion of exo-moon
or Pluto-sized bodies that were able to differentiate due to gravitational
potential energy release, our results suggest that the most likely explanation
for the prevalence of differentiated material among polluted white dwarfs is
that the Solar System is not unusual in being enriched in 26Al. The models
presented here suggest a ubiquitous pathway for the enrichment of exoplanetary
systems by short-lived radioisotopes, disfavouring short-lived radioisotope
enrichment scenarios relying on rare chance encounters with single nearby
supernovae, Wolf-Rayet or AGB stars.

### Title: Adaptive Domain Interest Network for Multi-domain Recommendation
* Paper ID: 2206.09672v1
* Paper URL: [http://arxiv.org/abs/2206.09672v1](http://arxiv.org/abs/2206.09672v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Industrial recommender systems usually hold data from multiple business
scenarios and are expected to provide recommendation services for these
scenarios simultaneously. In the retrieval step, the topK high-quality items
selected from a large number of corpus usually need to be various for multiple
scenarios. Take Alibaba display advertising system for example, not only
because the behavior patterns of Taobao users are diverse, but also
differentiated scenarios' bid prices assigned by advertisers vary
significantly. Traditional methods either train models for each scenario
separately, ignoring the cross-domain overlapping of user groups and items, or
simply mix all samples and maintain a shared model which makes it difficult to
capture significant diversities between scenarios. In this paper, we present
Adaptive Domain Interest network that adaptively handles the commonalities and
diversities across scenarios, making full use of multi-scenarios data during
training. Then the proposed method is able to improve the performance of each
business domain by giving various topK candidates for different scenarios
during online inference. Specifically, our proposed ADI models the
commonalities and diversities for different domains by shared networks and
domain-specific networks, respectively. In addition, we apply the
domain-specific batch normalization and design the domain interest adaptation
layer for feature-level domain adaptation. A self training strategy is also
incorporated to capture label-level connections across domains.ADI has been
deployed in the display advertising system of Alibaba, and obtains 1.8%
improvement on advertising revenue.

### Title: Benchmarking Constraint Inference in Inverse Reinforcement Learning
* Paper ID: 2206.09670v1
* Paper URL: [http://arxiv.org/abs/2206.09670v1](http://arxiv.org/abs/2206.09670v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: When deploying Reinforcement Learning (RL) agents into a physical system, we
must ensure that these agents are well aware of the underlying constraints. In
many real-world problems, however, the constraints followed by expert agents
(e.g., humans) are often hard to specify mathematically and unknown to the RL
agents. To tackle these issues, Constraint Inverse Reinforcement Learning
(CIRL) considers the formalism of Constrained Markov Decision Processes (CMDPs)
and estimates constraints from expert demonstrations by learning a constraint
function. As an emerging research topic, CIRL does not have common benchmarks,
and previous works tested their algorithms with hand-crafted environments
(e.g., grid worlds). In this paper, we construct a CIRL benchmark in the
context of two major application domains: robot control and autonomous driving.
We design relevant constraints for each environment and empirically study the
ability of different algorithms to recover those constraints based on expert
trajectories that respect those constraints. To handle stochastic dynamics, we
propose a variational approach that infers constraint distributions, and we
demonstrate its performance by comparing it with other CIRL baselines on our
benchmark. The benchmark, including the information for reproducing the
performance of CIRL algorithms, is publicly available at
https://github.com/Guiliang/CIRL-benchmarks-public

### Title: Sampling Efficient Deep Reinforcement Learning through Preference-Guided Stochastic Exploration
* Paper ID: 2206.09627v1
* Paper URL: [http://arxiv.org/abs/2206.09627v1](http://arxiv.org/abs/2206.09627v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Massive practical works addressed by Deep Q-network (DQN) algorithm have
indicated that stochastic policy, despite its simplicity, is the most
frequently used exploration approach. However, most existing stochastic
exploration approaches either explore new actions heuristically regardless of
Q-values or inevitably introduce bias into the learning process to couple the
sampling with Q-values. In this paper, we propose a novel preference-guided
$\epsilon$-greedy exploration algorithm that can efficiently learn the action
distribution in line with the landscape of Q-values for DQN without introducing
additional bias. Specifically, we design a dual architecture consisting of two
branches, one of which is a copy of DQN, namely the Q-branch. The other branch,
which we call the preference branch, learns the action preference that the DQN
implicit follows. We theoretically prove that the policy improvement theorem
holds for the preference-guided $\epsilon$-greedy policy and experimentally
show that the inferred action preference distribution aligns with the landscape
of corresponding Q-values. Consequently, preference-guided $\epsilon$-greedy
exploration motivates the DQN agent to take diverse actions, i.e., actions with
larger Q-values can be sampled more frequently whereas actions with smaller
Q-values still have a chance to be explored, thus encouraging the exploration.
We assess the proposed method with four well-known DQN variants in nine
different environments. Extensive results confirm the superiority of our
proposed method in terms of performance and convergence speed.
  Index Terms- Preference-guided exploration, stochastic policy, data
efficiency, deep reinforcement learning, deep Q-learning.

### Title: Distortion-Aware Network Pruning and Feature Reuse for Real-time Video Segmentation
* Paper ID: 2206.09604v1
* Paper URL: [http://arxiv.org/abs/2206.09604v1](http://arxiv.org/abs/2206.09604v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Real-time video segmentation is a crucial task for many real-world
applications such as autonomous driving and robot control. Since
state-of-the-art semantic segmentation models are often too heavy for real-time
applications despite their impressive performance, researchers have proposed
lightweight architectures with speed-accuracy trade-offs, achieving real-time
speed at the expense of reduced accuracy. In this paper, we propose a novel
framework to speed up any architecture with skip-connections for real-time
vision tasks by exploiting the temporal locality in videos. Specifically, at
the arrival of each frame, we transform the features from the previous frame to
reuse them at specific spatial bins. We then perform partial computation of the
backbone network on the regions of the current frame that captures temporal
differences between the current and previous frame. This is done by dynamically
dropping out residual blocks using a gating mechanism which decides which
blocks to drop based on inter-frame distortion. We validate our
Spatial-Temporal Mask Generator (STMG) on video semantic segmentation
benchmarks with multiple backbone networks, and show that our method largely
speeds up inference with minimal loss of accuracy.

### Title: Examining the Robustness of Spiking Neural Networks on Non-ideal Memristive Crossbars
* Paper ID: 2206.09599v1
* Paper URL: [http://arxiv.org/abs/2206.09599v1](http://arxiv.org/abs/2206.09599v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Spiking Neural Networks (SNNs) have recently emerged as the low-power
alternative to Artificial Neural Networks (ANNs) owing to their asynchronous,
sparse, and binary information processing. To improve the energy-efficiency and
throughput, SNNs can be implemented on memristive crossbars where
Multiply-and-Accumulate (MAC) operations are realized in the analog domain
using emerging Non-Volatile-Memory (NVM) devices. Despite the compatibility of
SNNs with memristive crossbars, there is little attention to study on the
effect of intrinsic crossbar non-idealities and stochasticity on the
performance of SNNs. In this paper, we conduct a comprehensive analysis of the
robustness of SNNs on non-ideal crossbars. We examine SNNs trained via learning
algorithms such as, surrogate gradient and ANN-SNN conversion. Our results show
that repetitive crossbar computations across multiple time-steps induce error
accumulation, resulting in a huge performance drop during SNN inference. We
further show that SNNs trained with a smaller number of time-steps achieve
better accuracy when deployed on memristive crossbars.

### Title: Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data
* Paper ID: 2206.09591v1
* Paper URL: [http://arxiv.org/abs/2206.09591v1](http://arxiv.org/abs/2206.09591v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Domain adaptive text classification is a challenging problem for the
large-scale pretrained language models because they often require expensive
additional labeled data to adapt to new domains. Existing works usually fails
to leverage the implicit relationships among words across domains. In this
paper, we propose a novel method, called Domain Adaptation with Structured
Knowledge (DASK), to enhance domain adaptation by exploiting word-level
semantic relationships. DASK first builds a knowledge graph to capture the
relationship between pivot terms (domain-independent words) and non-pivot terms
in the target domain. Then during training, DASK injects pivot-related
knowledge graph information into source domain texts. For the downstream task,
these knowledge-injected texts are fed into a BERT variant capable of
processing knowledge-injected textual data. Thanks to the knowledge injection,
our model learns domain-invariant features for non-pivots according to their
relationships with pivots. DASK ensures the pivots to have domain-invariant
behaviors by dynamically inferring via the polarity scores of candidate pivots
during training with pseudo-labels. We validate DASK on a wide range of
cross-domain sentiment classification tasks and observe up to 2.9% absolute
performance improvement over baselines for 20 different domain pairs. Code will
be made available at https://github.com/hikaru-nara/DASK.

### Title: Deep Random Vortex Method for Simulation and Inference of Navier-Stokes Equations
* Paper ID: 2206.09571v1
* Paper URL: [http://arxiv.org/abs/2206.09571v1](http://arxiv.org/abs/2206.09571v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Navier-Stokes equations are significant partial differential equations that
describe the motion of fluids such as liquids and air. Due to the importance of
Navier-Stokes equations, the development on efficient numerical schemes is
important for both science and engineer. Recently, with the development of AI
techniques, several approaches have been designed to integrate deep neural
networks in simulating and inferring the fluid dynamics governed by
incompressible Navier-Stokes equations, which can accelerate the simulation or
inferring process in a mesh-free and differentiable way. In this paper, we
point out that the capability of existing deep Navier-Stokes informed methods
is limited to handle non-smooth or fractional equations, which are two critical
situations in reality. To this end, we propose the \emph{Deep Random Vortex
Method} (DRVM), which combines the neural network with a random vortex dynamics
system equivalent to the Navier-Stokes equation. Specifically, the random
vortex dynamics motivates a Monte Carlo based loss function for training the
neural network, which avoids the calculation of derivatives through
auto-differentiation. Therefore, DRVM not only can efficiently solve
Navier-Stokes equations involving rough path, non-differentiable initial
conditions and fractional operators, but also inherits the mesh-free and
differentiable benefits of the deep-learning-based solver. We conduct
experiments on the Cauchy problem, parametric solver learning, and the inverse
problem of both 2-d and 3-d incompressible Navier-Stokes equations. The
proposed method achieves accurate results for simulation and inference of
Navier-Stokes equations. Especially for the cases that include singular initial
conditions, DRVM significantly outperforms existing PINN method.

### Title: nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models
* Paper ID: 2206.09557v1
* Paper URL: [http://arxiv.org/abs/2206.09557v1](http://arxiv.org/abs/2206.09557v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The recent advance of self-supervised learning associated with the
Transformer architecture enables natural language processing (NLP) to exhibit
extremely low perplexity. Such powerful models demand ever-increasing model
size, and thus, large amounts of computations and memory footprints. In this
paper, we propose an efficient inference framework for large-scale generative
language models. As the key to reducing model size, we quantize weights by a
non-uniform quantization method. Then, quantized matrix multiplications are
accelerated by our proposed kernel, called nuQmm, which allows a wide trade-off
between compression ratio and accuracy. Our proposed nuQmm reduces the latency
of not only each GPU but also the entire inference of large LMs because a high
compression ratio (by low-bit quantization) mitigates the minimum required
number of GPUs. We demonstrate that nuQmm can accelerate the inference speed of
the GPT-3 (175B) model by about 14.4 times and save energy consumption by 93%.

### Title: Capturing and Inferring Dense Full-Body Human-Scene Contact
* Paper ID: 2206.09553v1
* Paper URL: [http://arxiv.org/abs/2206.09553v1](http://arxiv.org/abs/2206.09553v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Inferring human-scene contact (HSC) is the first step toward understanding
how humans interact with their surroundings. While detecting 2D human-object
interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed
significant progress, reasoning about 3D human-scene contact from a single
image is still challenging. Existing HSC detection methods consider only a few
types of predefined contact, often reduce body and scene to a small number of
primitives, and even overlook image evidence. To predict human-scene contact
from a single image, we address the limitations above from both data and
algorithmic perspectives. We capture a new dataset called RICH for "Real
scenes, Interaction, Contact and Humans." RICH contains multiview
outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies
captured using markerless motion capture, 3D body scans, and high resolution 3D
scene scans. A key feature of RICH is that it also contains accurate
vertex-level contact labels on the body. Using RICH, we train a network that
predicts dense body-scene contacts from a single RGB image. Our key insight is
that regions in contact are always occluded so the network needs the ability to
explore the whole image for evidence. We use a transformer to learn such
non-local relationships and propose a new Body-Scene contact TRansfOrmer
(BSTRO). Very few methods explore 3D contact; those that do focus on the feet
only, detect foot contact as a post-processing step, or infer contact from body
pose without looking at the scene. To our knowledge, BSTRO is the first method
to directly estimate 3D body-scene contact from a single image. We demonstrate
that BSTRO significantly outperforms the prior art. The code and dataset are
available at https://rich.is.tue.mpg.de.

### Title: Dynamic Message Propagation Network for RGB-D Salient Object Detection
* Paper ID: 2206.09552v1
* Paper URL: [http://arxiv.org/abs/2206.09552v1](http://arxiv.org/abs/2206.09552v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: This paper presents a novel deep neural network framework for RGB-D salient
object detection by controlling the message passing between the RGB images and
depth maps on the feature level and exploring the long-range semantic contexts
and geometric information on both RGB and depth features to infer salient
objects. To achieve this, we formulate a dynamic message propagation (DMP)
module with the graph neural networks and deformable convolutions to
dynamically learn the context information and to automatically predict filter
weights and affinity matrices for message propagation control. We further embed
this module into a Siamese-based network to process the RGB image and depth map
respectively and design a multi-level feature fusion (MFF) module to explore
the cross-level information between the refined RGB and depth features.
Compared with 17 state-of-the-art methods on six benchmark datasets for RGB-D
salient object detection, experimental results show that our method outperforms
all the others, both quantitatively and visually.

### Title: Robust One Round Federated Learning with Predictive Space Bayesian Inference
* Paper ID: 2206.09526v1
* Paper URL: [http://arxiv.org/abs/2206.09526v1](http://arxiv.org/abs/2206.09526v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Making predictions robust is an important challenge. A separate challenge in
federated learning (FL) is to reduce the number of communication rounds,
particularly since doing so reduces performance in heterogeneous data settings.
To tackle both issues, we take a Bayesian perspective on the problem of
learning a global model. We show how the global predictive posterior can be
approximated using client predictive posteriors. This is unlike other works
which aggregate the local model space posteriors into the global model space
posterior, and are susceptible to high approximation errors due to the
posterior's high dimensional multimodal nature. In contrast, our method
performs the aggregation on the predictive posteriors, which are typically
easier to approximate owing to the low-dimensionality of the output space. We
present an algorithm based on this idea, which performs MCMC sampling at each
client to obtain an estimate of the local posterior, and then aggregates these
in one round to obtain a global ensemble model. Through empirical evaluation on
several classification and regression tasks, we show that despite using one
round of communication, the method is competitive with other FL techniques, and
outperforms them on heterogeneous settings. The code is publicly available at
https://github.com/hasanmohsin/FedPredSpace_1Round.

### Title: Multiple Testing Framework for Out-of-Distribution Detection
* Paper ID: 2206.09522v1
* Paper URL: [http://arxiv.org/abs/2206.09522v1](http://arxiv.org/abs/2206.09522v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We study the problem of Out-of-Distribution (OOD) detection, that is,
detecting whether a learning algorithm's output can be trusted at inference
time. While a number of tests for OOD detection have been proposed in prior
work, a formal framework for studying this problem is lacking. We propose a
definition for the notion of OOD that includes both the input distribution and
the learning algorithm, which provides insights for the construction of
powerful tests for OOD detection. We propose a multiple hypothesis testing
inspired procedure to systematically combine any number of different statistics
from the learning algorithm using conformal p-values. We further provide strong
guarantees on the probability of incorrectly classifying an in-distribution
sample as OOD. In our experiments, we find that threshold-based tests proposed
in prior work perform well in specific settings, but not uniformly well across
different types of OOD instances. In contrast, our proposed method that
combines multiple statistics performs uniformly well across different datasets
and neural networks.

