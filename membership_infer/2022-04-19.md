### Title: Computational Adaptation of XR Interfaces Through Interaction Simulation
* Paper ID: 2204.09162v1
* Paper URL: [http://arxiv.org/abs/2204.09162v1](http://arxiv.org/abs/2204.09162v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Adaptive and intelligent user interfaces have been proposed as a critical
component of a successful extended reality (XR) system. In particular, a
predictive system can make inferences about a user and provide them with
task-relevant recommendations or adaptations. However, we believe such adaptive
interfaces should carefully consider the overall \emph{cost} of interactions to
better address uncertainty of predictions. In this position paper, we discuss a
computational approach to adapt XR interfaces, with the goal of improving user
experience and performance. Our novel model, applied to menu selection tasks,
simulates user interactions by considering both cognitive and motor costs. In
contrast to greedy algorithms that adapt based on predictions alone, our model
holistically accounts for costs and benefits of adaptations towards adapting
the interface and providing optimal recommendations to the user.

### Title: DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation
* Paper ID: 2204.09149v1
* Paper URL: [http://arxiv.org/abs/2204.09149v1](http://arxiv.org/abs/2204.09149v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Task-oriented dialogue generation is challenging since the underlying
knowledge is often dynamic and effectively incorporating knowledge into the
learning process is hard. It is particularly challenging to generate both
human-like and informative responses in this setting. Recent research primarily
focused on various knowledge distillation methods where the underlying
relationship between the facts in a knowledge base is not effectively captured.
In this paper, we go one step further and demonstrate how the structural
information of a knowledge graph can improve the system's inference
capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue
system that effectively incorporates knowledge into a language model. Our
proposed system views relational knowledge as a knowledge graph and introduces
(1) a structure-aware knowledge embedding technique, and (2) a knowledge
graph-weighted attention masking strategy to facilitate the system selecting
relevant information during the dialogue generation. An empirical evaluation
demonstrates the effectiveness of DialoKG over state-of-the-art methods on
several standard benchmark datasets.

### Title: ALBETO and DistilBETO: Lightweight Spanish Language Models
* Paper ID: 2204.09145v1
* Paper URL: [http://arxiv.org/abs/2204.09145v1](http://arxiv.org/abs/2204.09145v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In recent years there have been considerable advances in pre-trained language
models, where non-English language versions have also been made available. Due
to their increasing use, many lightweight versions of these models (with
reduced parameters) have also been released to speed up training and inference
times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for
languages other than English are still scarce. In this paper we present ALBETO
and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained
exclusively on Spanish corpora. We train several versions of ALBETO ranging
from 5M to 223M parameters and one of DistilBETO with 67M parameters. We
evaluate our models in the GLUES benchmark that includes various natural
language understanding tasks in Spanish. The results show that our lightweight
models achieve competitive results to those of BETO (Spanish-BERT) despite
having fewer parameters. More specifically, our larger ALBETO model outperforms
all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.
However, BETO remains unbeaten for POS and NER. As a further contribution, all
models are publicly available to the community for future research.

### Title: X-ray observation of the Roche-lobe filling white dwarf plus hot subdwarf system ZTF J213056.71+442046.5
* Paper ID: 2204.09127v1
* Paper URL: [http://arxiv.org/abs/2204.09127v1](http://arxiv.org/abs/2204.09127v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: ZTF J213056.71+442046.5 is the prototype of a small class of recently
discovered compact binaries composed of a white dwarf and a hot subdwarf that
fills its Roche-lobe. Its orbital period of only 39 min is the shortest known
for the objects in this class. Evidence for a high orbital inclination (i=86
deg) and for the presence of an accretion disk has been inferred from a
detailed modeling of its optical photometric and spectroscopic data. We report
the results of an XMM-Newton observation carried out on 2021 January 7. ZTF
J213056.71+442046.5 was clearly detected by the Optical Monitor, which showed a
periodic variability in the UV band (200-400 nm), with a light curve similar to
that seen at longer wavelengths. Despite accretion on the white dwarf at an
estimated rate of the order of 10^{-9} M_sun/yr, no X-rays were detected with
the EPIC instrument, with a limit of ~10^{30} erg/s on the 0.2-12 keV
luminosity. We discuss possible explanations for the lack of a strong X-ray
emission from this system.

### Title: Mobility Analysis Workflow (MAW): An accessible, interoperable, and reproducible container system for processing raw mobile data
* Paper ID: 2204.09125v1
* Paper URL: [http://arxiv.org/abs/2204.09125v1](http://arxiv.org/abs/2204.09125v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Mobility analysis, or understanding and modeling of people's mobility
patterns in terms of when, where, and how people move from one place to
another, is fundamentally important as such information is the basis for
large-scale investment decisions on the nation's multi-modal transportation
infrastructure. Recent rise of using passively generated mobile data from
mobile devices have raised questions on using such data for capturing the
mobility patterns of a population because: 1) there is a great variety of
different kinds of mobile data and their respective properties are unknown; and
2) data pre-processing and analysis methods are often not explicitly reported.
The high stakes involved with mobility analysis and issues associated with the
passively generated mobile data call for mobility analysis (including data,
methods and results) to be accessible to all, interoperable across different
computing systems, reproducible and reusable by others. In this study, a
container system named Mobility Analysis Workflow (MAW) that integrates data,
methods and results, is developed. Built upon the containerization technology,
MAW allows its users to easily create, configure, modify, execute and share
their methods and results in the form of Docker containers. Tools for
operationalizing MAW are also developed and made publicly available on GitHub.
One use case of MAW is the comparative analysis for the impacts of different
pre-processing and mobility analysis methods on inferred mobility patterns.
This study finds that different pre-processing and analysis methods do have
impacts on the resulting mobility patterns. The creation of MAW and a better
understanding of the relationship between data, methods and resulting mobility
patterns as facilitated by MAW represent an important first step toward
promoting reproducibility and reusability in mobility analysis with
passively-generated data.

### Title: An improved central limit theorem and fast convergence rates for entropic transportation costs
* Paper ID: 2204.09105v1
* Paper URL: [http://arxiv.org/abs/2204.09105v1](http://arxiv.org/abs/2204.09105v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We prove a central limit theorem for the entropic transportation cost between
subgaussian probability measures, centered at the population cost. This is the
first result which allows for asymptotically valid inference for entropic
optimal transport between measures which are not necessarily discrete. In the
compactly supported case, we complement these results with new, faster,
convergence rates for the expected entropic transportation cost between
empirical measures. Our proof is based on strengthening convergence results for
dual solutions to the entropic optimal transport problem.

### Title: AutoField: Automating Feature Selection in Deep Recommender Systems
* Paper ID: 2204.09078v1
* Paper URL: [http://arxiv.org/abs/2204.09078v1](http://arxiv.org/abs/2204.09078v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Feature quality has an impactful effect on recommendation performance.
Thereby, feature selection is a critical process in developing deep
learning-based recommender systems. Most existing deep recommender systems,
however, focus on designing sophisticated neural networks, while neglecting the
feature selection process. Typically, they just feed all possible features into
their proposed deep architectures, or select important features manually by
human experts. The former leads to non-trivial embedding parameters and extra
inference time, while the latter requires plenty of expert knowledge and human
labor effort. In this work, we propose an AutoML framework that can adaptively
select the essential feature fields in an automatic manner. Specifically, we
first design a differentiable controller network, which is capable of
automatically adjusting the probability of selecting a particular feature
field; then, only selected feature fields are utilized to retrain the deep
recommendation model. Extensive experiments on three benchmark datasets
demonstrate the effectiveness of our framework. We conduct further experiments
to investigate its properties, including the transferability, key components,
and parameter sensitivity.

### Title: Substructures in protoplanetary disks imprinted by compact planetary systems
* Paper ID: 2204.09074v1
* Paper URL: [http://arxiv.org/abs/2204.09074v1](http://arxiv.org/abs/2204.09074v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The substructures observed in protoplanetary disks may be the signposts of
embedded planets carving gaps or creating vortices. The inferred masses of
these planets often fall in the Jovian regime despite their low abundance
compared to lower-mass planets, partly because previous works often assume that
a single substructure (a gap or vortex) is caused by a single planet. In this
work, we study the possible imprints of compact systems composed of
Neptune-like planets ($\sim10-30\;M_\oplus$) and show that long-standing
vortices are a prevalent outcome when their inter-planetary separation ($\Delta
a$) falls below $\sim8$ times $H_{{\rm p}}$ -- the average disk's scale height
at the planets locations. In simulations where a single planet is unable to
produce long-lived vortices, two-planet systems can preserve them for at least
$5,000$ orbits in two regimes: i) fully-shared density gaps with elongated
vortices around the stable Lagrange points $L_4$ and $L_5$ for the most compact
planet pairs ($\Delta a \lesssim 4.6\; H_{{\rm p}}$); ii) partially-shared gaps
for more widely spaced planets ($\Delta a \sim 4.6 - 8\;H_{{\rm p}}$) forming
vortices in a density ring between the planets through the Rossby wave
instability. The latter case can produce vortices with a wide range of aspect
ratios down to $\sim3$ and can occur for planets captured into the 3:2 (2:1)
mean-motion resonances for disk's aspects ratios of $h\gtrsim 0.033$ ($h\gtrsim
0.057$). We suggest that their long lifetimes are sustained by the interaction
of spiral density waves launched by the neighboring planets. Overall, our
results show that distinguishing imprint of compact systems with Neptune-mass
planets are long-lived vortices inside the density gaps, which in turn are
shallower than single-planet gaps for a fixed gap width.

### Title: Live Fast, Die $α$-Enhanced: The Mass-Metallicity-$α$ Relation of the Milky Way's Disrupted Dwarf Galaxies
* Paper ID: 2204.09057v1
* Paper URL: [http://arxiv.org/abs/2204.09057v1](http://arxiv.org/abs/2204.09057v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The Milky Way's satellite galaxies ("surviving dwarfs") have been studied for
decades as unique probes of chemical evolution in the low-mass regime. Here we
extend such studies to the "disrupted dwarfs", whose debris constitutes the
stellar halo. We present abundances ([Fe/H], [$\alpha$/Fe]) and stellar masses
for nine disrupted dwarfs with $M_{\star}\approx10^{6}-10^{9}M_{\odot}$ from
the H3 Survey (Sagittarius, $Gaia$-Sausage-Enceladus, Helmi Streams, Sequoia,
Wukong/LMS-1, Cetus, Thamnos, I'itoi, Orphan/Chenab). The surviving and
disrupted dwarfs are chemically distinct: at fixed mass, the disrupted dwarfs
are systematically metal-poor and $\alpha$-enhanced. The disrupted dwarfs
define a mass-metallicity relation (MZR) with a similar slope as the $z=0$ MZR
followed by the surviving dwarfs, but offset to lower metallicities by
$\Delta$[Fe/H]$\approx0.3-0.4$ dex. Dwarfs with larger offsets from the $z=0$
MZR are more $\alpha$-enhanced. In simulations as well as observations,
galaxies with higher $\Delta$[Fe/H] formed at higher redshifts -- exploiting
this, we infer the disrupted dwarfs have typical star-formation truncation
redshifts of $z_{\rm{trunc}}{\sim}1-2$. We compare the chemically inferred
$z_{\rm{trunc}}$ with dynamically inferred accretion redshifts and find almost
all dwarfs are quenched only after accretion. The differences between disrupted
and surviving dwarfs are likely because the disrupted dwarfs assembled their
mass rapidly, at higher redshifts, and within denser dark matter halos that
formed closer to the Galaxy. Our results place novel archaeological constraints
on low-mass galaxies inaccessible to direct high-$z$ studies: (i) the redshift
evolution of the MZR along parallel tracks but offset to lower metallicities
extends to $M_{\star}\approx10^{6}-10^{9}M_{\odot}$; (ii) galaxies at
$z\approx2-3$ are $\alpha$-enhanced with [$\alpha$/Fe]$\approx0.4$.

### Title: A stochastic Stein Variational Newton method
* Paper ID: 2204.09039v1
* Paper URL: [http://arxiv.org/abs/2204.09039v1](http://arxiv.org/abs/2204.09039v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Stein variational gradient descent (SVGD) is a general-purpose
optimization-based sampling algorithm that has recently exploded in popularity,
but is limited by two issues: it is known to produce biased samples, and it can
be slow to converge on complicated distributions. A recently proposed
stochastic variant of SVGD (sSVGD) addresses the first issue, producing
unbiased samples by incorporating a special noise into the SVGD dynamics such
that asymptotic convergence is guaranteed. Meanwhile, Stein variational Newton
(SVN), a Newton-like extension of SVGD, dramatically accelerates the
convergence of SVGD by incorporating Hessian information into the dynamics, but
also produces biased samples. In this paper we derive, and provide a practical
implementation of, a stochastic variant of SVN (sSVN) which is both
asymptotically correct and converges rapidly. We demonstrate the effectiveness
of our algorithm on a difficult class of test problems -- the Hybrid Rosenbrock
density -- and show that sSVN converges using three orders of magnitude fewer
gradient evaluations of the log likelihood than its stochastic SVGD
counterpart. Our results show that sSVN is a promising approach to accelerating
high-precision Bayesian inference tasks with modest-dimension,
$d\sim\mathcal{O}(10)$.

### Title: A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets
* Paper ID: 2204.08997v1
* Paper URL: [http://arxiv.org/abs/2204.08997v1](http://arxiv.org/abs/2204.08997v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this paper, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. A new large medical dialogue dataset with multi-level fine-grained
annotations is introduced and five independent tasks are established, including
named entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies.

### Title: Metappearance: Meta-Learning for Visual Appearance Reproduction
* Paper ID: 2204.08993v1
* Paper URL: [http://arxiv.org/abs/2204.08993v1](http://arxiv.org/abs/2204.08993v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: There currently are two main approaches to reproducing visual appearance
using Machine Learning (ML): The first is training models that generalize over
different instances of a problem, e.g., different images from a dataset. Such
models learn priors over the data corpus and use this knowledge to provide fast
inference with little input, often as a one-shot operation. However, this
generality comes at the cost of fidelity, as such methods often struggle to
achieve the final quality required. The second approach does not train a model
that generalizes across the data, but overfits to a single instance of a
problem, e.g., a flash image of a material. This produces detailed and
high-quality results, but requires time-consuming training and is, as mere
non-linear function fitting, unable to exploit previous experience. Techniques
such as fine-tuning or auto-decoders combine both approaches but are sequential
and rely on per-exemplar optimization. We suggest to combine both techniques
end-to-end using meta-learning: We over-fit onto a single problem instance in
an inner loop, while also learning how to do so efficiently in an outer-loop
that builds intuition over many optimization runs. We demonstrate this concept
to be versatile and efficient, applying it to RGB textures, Bi-directional
Reflectance Distribution Functions (BRDFs), or Spatially-varying BRDFs
(svBRDFs).

### Title: CPU- and GPU-based Distributed Sampling in Dirichlet Process Mixtures for Large-scale Analysis
* Paper ID: 2204.08988v1
* Paper URL: [http://arxiv.org/abs/2204.08988v1](http://arxiv.org/abs/2204.08988v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In the realm of unsupervised learning, Bayesian nonparametric mixture models,
exemplified by the Dirichlet Process Mixture Model (DPMM), provide a principled
approach for adapting the complexity of the model to the data. Such models are
particularly useful in clustering tasks where the number of clusters is
unknown. Despite their potential and mathematical elegance, however, DPMMs have
yet to become a mainstream tool widely adopted by practitioners. This is
arguably due to a misconception that these models scale poorly as well as the
lack of high-performance (and user-friendly) software tools that can handle
large datasets efficiently. In this paper we bridge this practical gap by
proposing a new, easy-to-use, statistical software package for scalable DPMM
inference. More concretely, we provide efficient and easily-modifiable
implementations for high-performance distributed sampling-based inference in
DPMMs where the user is free to choose between either a multiple-machine,
multiple-core, CPU implementation (written in Julia) and a multiple-stream GPU
implementation (written in CUDA/C++). Both the CPU and GPU implementations come
with a common (and optional) python wrapper, providing the user with a single
point of entry with the same interface. On the algorithmic side, our
implementations leverage a leading DPMM sampler from (Chang and Fisher III,
2013). While Chang and Fisher III's implementation (written in MATLAB/C++) used
only CPU and was designed for a single multi-core machine, the packages we
proposed here distribute the computations efficiently across either multiple
multi-core machines or across mutiple GPU streams. This leads to speedups,
alleviates memory and storage limitations, and lets us fit DPMMs to
significantly larger datasets and of higher dimensionality than was possible
previously by either (Chang and Fisher III, 2013) or other DPMM methods.

### Title: Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data
* Paper ID: 2204.08932v1
* Paper URL: [http://arxiv.org/abs/2204.08932v1](http://arxiv.org/abs/2204.08932v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Deep neural network (DNN) suffers from catastrophic forgetting when learning
incrementally, which greatly limits its applications. Although maintaining a
handful of samples (called `exemplars`) of each task could alleviate forgetting
to some extent, existing methods are still limited by the small number of
exemplars since these exemplars are too few to carry enough task-specific
knowledge, and therefore the forgetting remains. To overcome this problem, we
propose to `imagine` diverse counterparts of given exemplars referring to the
abundant semantic-irrelevant information from unlabeled data. Specifically, we
develop a learnable feature generator to diversify exemplars by adaptively
generating diverse counterparts of exemplars based on semantic information from
exemplars and semantically-irrelevant information from unlabeled data. We
introduce semantic contrastive learning to enforce the generated samples to be
semantic consistent with exemplars and perform semanticdecoupling contrastive
learning to encourage diversity of generated samples. The diverse generated
samples could effectively prevent DNN from forgetting when learning new tasks.
Our method does not bring any extra inference cost and outperforms
state-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a
clear margin.

### Title: Inferring Epidemics from Multiple Dependent Data via Pseudo-Marginal Methods
* Paper ID: 2204.08901v1
* Paper URL: [http://arxiv.org/abs/2204.08901v1](http://arxiv.org/abs/2204.08901v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Health-policy planning requires evidence on the burden that epidemics place
on healthcare systems. Multiple, often dependent, datasets provide a noisy and
fragmented signal from the unobserved epidemic process including transmission
and severity dynamics. This paper explores important challenges to the use of
state-space models for epidemic inference when multiple dependent datasets are
analysed. We propose a new semi-stochastic model that exploits deterministic
approximations for large-scale transmission dynamics while retaining
stochasticity in the occurrence and reporting of relatively rare severe events.
This model is suitable for many real-time situations including large seasonal
epidemics and pandemics. Within this context, we develop algorithms to provide
exact parameter inference and test them via simulation. Finally, we apply our
joint model and the proposed algorithm to several surveillance data on the
2017-18 influenza epidemic in England to reconstruct transmission dynamics and
estimate the daily new influenza infections as well as severity indicators as
the case-hospitalisation risk and the hospital-intensive care risk.

### Title: Towards Efficient Single Image Dehazing and Desnowing
* Paper ID: 2204.08899v1
* Paper URL: [http://arxiv.org/abs/2204.08899v1](http://arxiv.org/abs/2204.08899v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Removing adverse weather conditions like rain, fog, and snow from images is a
challenging problem. Although the current recovery algorithms targeting a
specific condition have made impressive progress, it is not flexible enough to
deal with various degradation types. We propose an efficient and compact image
restoration network named DAN-Net (Degradation-Adaptive Neural Network) to
address this problem, which consists of multiple compact expert networks with
one adaptive gated neural. A single expert network efficiently addresses
specific degradation in nasty winter scenes relying on the compact architecture
and three novel components. Based on the Mixture of Experts strategy, DAN-Net
captures degradation information from each input image to adaptively modulate
the outputs of task-specific expert networks to remove various adverse winter
weather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural
Network to estimate gated attention maps of the input image, while different
task-specific experts with the same topology are jointly dispatched to process
the degraded image. Such novel image restoration pipeline handles different
types of severe weather scenes effectively and efficiently. It also enjoys the
benefit of coordinate boosting in which the whole network outperforms each
expert trained without coordination.
  Extensive experiments demonstrate that the presented manner outperforms the
state-of-the-art single-task methods on image quality and has better inference
efficiency. Furthermore, we have collected the first real-world winter scenes
dataset to evaluate winter image restoration methods, which contains various
hazy and snowy images snapped in winter. Both the dataset and source code will
be publicly available.

### Title: Using a Semantic Knowledge Base to Improve the Management of Security Reports in Industrial DevOps Projects
* Paper ID: 2204.08888v1
* Paper URL: [http://arxiv.org/abs/2204.08888v1](http://arxiv.org/abs/2204.08888v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Integrating security activities into the software development lifecycle to
detect security flaws is essential for any project. These activities produce
reports that must be managed and looped back to project stakeholders like
developers to enable security improvements. This so-called Feedback Loop is a
crucial part of any project and is required by various industrial security
standards and models. However, the operation of this loop presents a variety of
challenges. These challenges range from ensuring that feedback data is of
sufficient quality over providing different stakeholders with the information
they need to the enormous effort to manage the reports. In this paper, we
propose a novel approach for treating findings from security activity reports
as belief in a Knowledge Base (KB). By utilizing continuous logical inferences,
we derive information necessary for practitioners and address existing
challenges in the industry. This approach is currently evaluated in industrial
DevOps projects, using data from continuous security testing.

### Title: Revisiting the evidences for spectral anomalies in distant blazars: new data on the photon-ALP mixing
* Paper ID: 2204.08865v1
* Paper URL: [http://arxiv.org/abs/2204.08865v1](http://arxiv.org/abs/2204.08865v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We re-examine possible dependencies on redshift of the spectral parameters of
blazars observed at very-high energies (VHEs) with Imaging Atmospheric
Cherenkov telescopes (IACTs). This is relevant to assess potential effects with
the source distance of the photon to axion-like particle (ALP) mixing, that
would deeply affect the propagation of VHE photons across the Universe. We
focus our spectral analysis on 38 BL Lac objects (32 high-peaked and 6
intermediate-peaked) up to redshift $z\simeq 0.5$, and a small sample of 5 Flat
Spectrum Radio Quasars up to $z=1$ treated independently to increase the
redshift baseline. The 78 independent spectra of these sources are first of all
carefully corrected for the gamma-gamma interaction with photons of the
Extragalactic Background Light, that are responsible for the major
redshift-dependent opacity effect. Then, the corrected spectra are fitted with
simple power-laws to infer the intrinsic spectral indices $\Gamma_{\rm em}$ at
VHE, to test the assumption that such spectral properties are set by the local
rather than the global cosmological environment. We find some systematic
anti-correlations with redshift of $\Gamma_{\rm em}$ that might indicate,
although with low-significance, a spectral anomaly potentially requiring a
revision of the photon propagation process. More conclusive tests with higher
statistical significance will require the observational improvements offered by
the forthcoming new generation of Cherenkov arrays (CTA, ASTRI, LHAASO).

### Title: Effects of spatial resolution on inferences of atmospheric quantities from simulations
* Paper ID: 2204.08849v1
* Paper URL: [http://arxiv.org/abs/2204.08849v1](http://arxiv.org/abs/2204.08849v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Small scale processes are thought to be important for the dynamics of the
solar atmosphere. While numerical resolution fundamentally limits their
inclusion in MHD simulations, real observations at the same nominal resolution
should still contain imprints of sub-resolution effects. This means that the
synthetic observables from a simulation of given resolution might not be
directly comparable to real observables at the same resolution. It is thus of
interest to investigate how inferences based on synthetic spectra from
simulations with different numerical resolutions compare, and whether these
differences persist after the spectra have been spatially degraded to a common
resolution. We aim to compare synthetic spectra obtained from realistic 3D
radiative magnetohydrodynamic (rMHD) simulations run at three different
numerical resolutions from the same initial atmosphere, using very simple
methods for inferring line-of-sight velocities and magnetic fields.
Additionally we examine how the differing spatial resolution impacts the
results retrieved from the STiC inversion code. We find that while the simple
inferences for all three simulations reveal the same large-scale tendencies,
the higher resolutions yield more fine-grained structures and more extreme
line-of-sight velocities/magnetic fields in concentrated spots even after
spatial smearing. We also see indications that the imprints of sub-resolution
effects on the degraded spectra result in systematic errors in the inversions,
and that these errors increase with the amount of sub-resolution effects
included. Fortunately, however, we find that including successively more
sub-resolution yields smaller additional effects; i.e. there is a clear trend
of diminishing importance for progressively finer sub-resolution effects.

### Title: Compressed Empirical Measures (in finite dimensions)
* Paper ID: 2204.08847v1
* Paper URL: [http://arxiv.org/abs/2204.08847v1](http://arxiv.org/abs/2204.08847v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We study approaches for compressing the empirical measure in the context of
finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context,
the empirical measure is contained within a natural convex set and can be
approximated using convex optimization methods. Such an approximation gives
under certain conditions rise to a coreset of data points. A key quantity that
controls how large such a coreset has to be is the size of the largest ball
around the empirical measure that is contained within the empirical convex set.
The bulk of our work is concerned with deriving high probability lower bounds
on the size of such a ball under various conditions. We complement this
derivation of the lower bound by developing techniques that allow us to apply
the compression approach to concrete inference problems such as kernel ridge
regression. We conclude with a construction of an infinite dimensional RKHS for
which the compression is poor, highlighting some of the difficulties one faces
when trying to move to infinite dimensional RKHSs.

### Title: Quantum Bayesian Statistical Inference
* Paper ID: 2204.08845v1
* Paper URL: [http://arxiv.org/abs/2204.08845v1](http://arxiv.org/abs/2204.08845v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In this work a quantum analogue of Bayesian statistical inference is
considered. Based on the notion of instrument, we propose a sequential
measurement scheme from which observations needed for statistical inference are
obtained. We further put forward a quantum analogue of Bayes rule, which states
how the prior normal state of a quantum system updates under those
observations. We next generalize the fundamental notions and results of
Bayesian statistics according to the quantum Bayes rule. It is also note that
our theory retains the classical one as its special case. Finally, we
investigate the limit of posterior normal state as the number of observations
tends to infinity.

### Title: Detect-and-describe: Joint learning framework for detection and description of objects
* Paper ID: 2204.08828v1
* Paper URL: [http://arxiv.org/abs/2204.08828v1](http://arxiv.org/abs/2204.08828v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Traditional object detection answers two questions; "what" (what the object
is?) and "where" (where the object is?). "what" part of the object detection
can be fine-grained further i.e. "what type", "what shape" and "what material"
etc. This results in the shifting of the object detection tasks to the object
description paradigm. Describing an object provides additional detail that
enables us to understand the characteristics and attributes of the object
("plastic boat" not just boat, "glass bottle" not just bottle). This additional
information can implicitly be used to gain insight into unseen objects (e.g.
unknown object is "metallic", "has wheels"), which is not possible in
traditional object detection. In this paper, we present a new approach to
simultaneously detect objects and infer their attributes, we call it Detect and
Describe (DaD) framework. DaD is a deep learning-based approach that extends
object detection to object attribute prediction as well. We train our model on
aPascal train set and evaluate our approach on aPascal test set. We achieve
97.0% in Area Under the Receiver Operating Characteristic Curve (AUC) for
object attributes prediction on aPascal test set. We also show qualitative
results for object attribute prediction on unseen objects, which demonstrate
the effectiveness of our approach for describing unknown objects.

### Title: Ariel stellar characterisation: I -- homogeneous stellar parameters of 187 FGK planet host stars Description and validation of the method
* Paper ID: 2204.08825v1
* Paper URL: [http://arxiv.org/abs/2204.08825v1](http://arxiv.org/abs/2204.08825v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In 2020 the European Space Agency selected Ariel as the next mission to join
the space fleet of observatories to study planets outside our Solar System.
Ariel will be devoted to the characterisation of a thousand planetary
atmospheres, for understanding what exoplanets are made of, how they formed and
how they evolve. To achieve the last two goals all planets need to be studied
within the context of their own host stars, which in turn have to be analysed
with the same technique, in a uniform way. We present the spectro-photometric
method we have developed to infer the atmospheric parameters of the known host
stars in the Tier 1 of the Ariel Reference Sample. Our method is based on an
iterative approach, which combines spectral analysis, the determination of the
surface gravity from {\em Gaia} data, and the determination of stellar masses
from isochrone fitting. We validated our approach with the analysis of a
control sample, composed by members of three open clusters with well-known ages
and metallicities. We measured effective temperature, Teff, surface gravity,
logg, and the metallicity, [Fe/H], of 187 F-G-K stars within the Ariel
Reference Sample. We presented the general properties of the sample, including
their kinematics which allows us to separate them between thin and thick disc
populations. A homogeneous determination of the parameters of the host stars is
fundamental in the study of the stars themselves and their planetary systems.
Our analysis systematically improves agreement with theoretical models and
decreases uncertainties in the mass estimate (from 0.21+/-0.30 to 0.10+/-0.02
M_sun), providing useful data for the Ariel consortium and the astronomical
community at large.

### Title: RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology
* Paper ID: 2204.08810v1
* Paper URL: [http://arxiv.org/abs/2204.08810v1](http://arxiv.org/abs/2204.08810v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Although traditional symbolic reasoning methods are highly interpretable,
their application in knowledge graph link prediction is limited due to their
low computational efficiency. In this paper, we propose a new neural symbolic
reasoning method: RNNCTPs, which improves computational efficiency by
re-filtering the knowledge selection of Conditional Theorem Provers (CTPs), and
is less sensitive to the embedding size parameter. RNNCTPs are divided into
relation selectors and predictors. The relation selectors are trained
efficiently and interpretably, so that the whole model can dynamically generate
knowledge for the inference of the predictor. In all four datasets, the method
shows competitive performance against traditional methods on the link
prediction task, and can have higher applicability to the selection of datasets
relative to CTPs.

### Title: What powers the wind from the black hole accretion disc in GRO J1655-40?
* Paper ID: 2204.08802v1
* Paper URL: [http://arxiv.org/abs/2204.08802v1](http://arxiv.org/abs/2204.08802v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Black hole accretion discs can produce powerful outflowing plasma (disc
winds), seen as blue-shifted absorption lines in stellar and supermassive
systems. These winds in Quasars have an essential role in controlling galaxy
formation across cosmic time, but there is no consensus on how these are
physically launched. A single unique observation of a stellar-mass black hole
GRO J1655-40 showed the high wind density estimated from an absorption line
from the metastable level of Fe xxii and ruled out X-ray heating
(thermal-radiative wind) for the low observed luminosity. This left magnetic
driving as the only viable mechanism, motivating unified models of magnetic
winds in both binaries and Quasars. Here we reanalyse these data using a
photoionisation code that includes the contribution of radiative cascades and
collisions in populating the metastable level. The effect of radiative cascades
reduces the inferred wind density by orders of magnitude. The derived column is
also optically thick, so the source is intrinsically more luminous than
observed. We show that a thermal-radiative wind model calculated from a
radiation hydrodynamic simulation matches well with the data. We revisit the
previous magnetic wind solution and show that this is also optically thick.
Hence, it requires a larger source luminosity, and it struggles to reproduce
the overall ion population at the required density (both new and old). These
results remove the requirement for a magnetic wind in these data and remove the
basis of the self-similar unified magnetic wind models extrapolated to Quasar
outflows.

### Title: Programmable heralded linear optical generation of two-qubit states
* Paper ID: 2204.08788v1
* Paper URL: [http://arxiv.org/abs/2204.08788v1](http://arxiv.org/abs/2204.08788v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We have investigated the heralded generation of two-qubit dual-rail-encoded
states by programmable linear optics. Two types of schemes generating the
states from four single photons, which is the minimal possible to accomplish
the task, have been considered. The schemes have different detection patterns
heralding successful generation events, namely, one-mode heralding, in which
the two auxiliary photons are detected in one mode, and two-mode heralding, in
which single photons are detected in each of the two modes simultaneously. We
have shown that the dependence of the schemes' success probabilities on the
target state's degree of entanglement are essentially different. In particular,
one-mode heralding yields better efficiency for highly-entangled states, if the
programmable interferometers can explore the full space of the unitary transfer
matrices,. It is reversed in case of weakly-entangled states where two-mode
heralding is better. We have found a minimal decomposition of the scheme with
two-mode heralding that is programmed by one variable phase shift. We infer
that the linear optical schemes designed specifically for generation of
two-qubit states are more efficient than schemes implementing gate-based
circuits with known two-qubit linear optical gates. Our results yield
substantial reduction of physical resources needed to generate two-qubit
dual-rail-encoded photonic states.

### Title: IndicXNLI: Evaluating Multilingual Inference for Indian Languages
* Paper ID: 2204.08776v1
* Paper URL: [http://arxiv.org/abs/2204.08776v1](http://arxiv.org/abs/2204.08776v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: While Indic NLP has made rapid advances recently in terms of the availability
of corpora and pre-trained models, benchmark datasets on standard NLU tasks are
limited. To this end, we introduce IndicXNLI, an NLI dataset for 11 Indic
languages. It has been created by high-quality machine translation of the
original English XNLI dataset and our analysis attests to the quality of
IndicXNLI. By finetuning different pre-trained LMs on this IndicXNLI, we
analyze various cross-lingual transfer techniques with respect to the impact of
the choice of language models, languages, multi-linguality, mix-language input,
etc. These experiments provide us with useful insights into the behaviour of
pre-trained models for a diverse set of languages.

### Title: Near K-Edge Photoionization and Photoabsorption of Singly, Doubly, and Triply Charged Silicon Ions
* Paper ID: 2204.08750v1
* Paper URL: [http://arxiv.org/abs/2204.08750v1](http://arxiv.org/abs/2204.08750v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Experimental and theoretical results are presented for double, triple, and
quadruple photoionization of Si$^+$ and Si$^{2+}$ ions and for double
photoionization of Si$^{3+}$ ions by a single photon. The experiments employed
the photon-ion merged-beams technique at a synchrotron light source. The
experimental photon-energy range 1835--1900 eV comprises resonances associated
with the excitation of a $1s$ electron to higher subshells and subsequent
autoionization. Energies, widths, and strengths of these resonances are
extracted from high-resolution photoionization measurements, and the core-hole
lifetime of K-shell ionized neutral silicon is inferred. In addition,
theoretical cross sections for photoabsorption and multiple photoionization
were obtained from large-scale Multi-Configuration Dirac-Hartree-Fock (MCDHF)
calculations. The present calculations agree with the experiment much better
than previously published theoretical results. The importance of an accurate
energy calibration of laboratory data is pointed out. The present benchmark
results are particularly useful for discriminating between silicon absorption
in the gaseous and in the solid component (dust grains) of the interstellar
medium.

### Title: Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing
* Paper ID: 2204.08734v1
* Paper URL: [http://arxiv.org/abs/2204.08734v1](http://arxiv.org/abs/2204.08734v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Deep learning (DL) techniques are proven effective in many challenging tasks,
and become widely-adopted in practice. However, previous work has shown that DL
libraries, the basis of building and executing DL models, contain bugs and can
cause severe consequences. Unfortunately, existing testing approaches still
cannot comprehensively exercise DL libraries. They utilize existing trained
models and only detect bugs in model inference phase. In this work we propose
Muffin to address these issues. To this end, Muffin applies a
specifically-designed model fuzzing approach, which allows it to generate
diverse DL models to explore the target library, instead of relying only on
existing trained models. Muffin makes differential testing feasible in the
model training phase by tailoring a set of metrics to measure the
inconsistencies between different DL libraries. In this way, Muffin can best
exercise the library code to detect more bugs. To evaluate the effectiveness of
Muffin, we conduct experiments on three widely-used DL libraries. The results
demonstrate that Muffin can detect 39 new bugs in the latest release versions
of popular DL libraries, including Tensorflow, CNTK, and Theano.

### Title: On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules
* Paper ID: 2204.08653v1
* Paper URL: [http://arxiv.org/abs/2204.08653v1](http://arxiv.org/abs/2204.08653v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently
used in software engineering as models pre-trained on large source code
corpora. Their knowledge is transferred to downstream tasks (e.g. code clone
detection) via fine-tuning. In natural language processing (NLP), other
alternatives for transferring the knowledge of PTLMs are explored through using
adapters, compact, parameter efficient modules inserted in the layers of the
PTLM. Although adapters are known to facilitate adapting to many downstream
tasks compared to fine-tuning the model that require retraining all of the
models' parameters -- which owes to the adapters' plug and play nature and
being parameter efficient -- their usage in software engineering is not
explored.
  Here, we explore the knowledge transfer using adapters and based on the
Naturalness Hypothesis proposed by Hindle et. al \cite{hindle2016naturalness}.
Thus, studying the bimodality of adapters for two tasks of cloze test and code
clone detection, compared to their benchmarks from the CodeXGLUE platform.
These adapters are trained using programming languages and are inserted in a
PTLM that is pre-trained on English corpora (N-PTLM). Three programming
languages, C/C++, Python, and Java, are studied along with extensive
experiments on the best setup used for adapters. Improving the results of the
N-PTLM confirms the success of the adapters in knowledge transfer to software
engineering, which sometimes are in par with or exceed the results of a PTLM
trained on source code; while being more efficient in terms of the number of
parameters, memory usage, and inference time. Our results can open new
directions to build smaller models for more software engineering tasks. We open
source all the scripts and the trained adapters.

### Title: LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation
* Paper ID: 2204.08649v1
* Paper URL: [http://arxiv.org/abs/2204.08649v1](http://arxiv.org/abs/2204.08649v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The rapid growth of biomedical literature poses a significant challenge for
curation and interpretation. This has become more evident during the COVID-19
pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,
has accumulated over 180,000 articles with millions of accesses. Approximately
10,000 new articles are added to LitCovid every month. A main curation task in
LitCovid is topic annotation where an article is assigned with up to eight
topics, e.g., Treatment and Diagnosis. The annotated topics have been widely
used both in LitCovid (e.g., accounting for ~18% of total uses) and downstream
studies such as network generation. However, it has been a primary curation
bottleneck due to the nature of the task and the rapid literature growth. This
study proposes LITMC-BERT, a transformer-based multi-label classification
method in biomedical literature. It uses a shared transformer backbone for all
the labels while also captures label-specific features and the correlations
between label pairs. We compare LITMC-BERT with three baseline models on two
datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the
current best results, respectively, and only requires ~18% of the inference
time than the Binary BERT baseline. The related datasets and models are
available via https://github.com/ncbi/ml-transformer.

### Title: "Flux+Mutability": A Conditional Generative Approach to One-Class Classification and Anomaly Detection
* Paper ID: 2204.08609v1
* Paper URL: [http://arxiv.org/abs/2204.08609v1](http://arxiv.org/abs/2204.08609v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Anomaly Detection is becoming increasingly popular within the experimental
physics community. At experiments such as the Large Hadron Collider, anomaly
detection is at the forefront of finding new physics beyond the Standard Model.
This paper details the implementation of a novel Machine Learning architecture,
called Flux+Mutability, which combines cutting-edge conditional generative
models with clustering algorithms. In the `flux' stage we learn the
distribution of a reference class. The `mutability' stage at inference
addresses if data significantly deviates from the reference class. We
demonstrate the validity of our approach and its connection to multiple
problems spanning from one-class classification to anomaly detection. In
particular, we apply our method to the isolation of neutral showers in an
electromagnetic calorimeter and show its performance in detecting anomalous
dijets events from standard QCD background. This approach limits assumptions on
the reference sample and remains agnostic to the complementary class of objects
of a given problem. We describe the possibility of dynamically generating a
reference population and defining selection criteria via quantile cuts.
Remarkably this flexible architecture can be deployed for a wide range of
problems, and applications like multi-class classification or data quality
control are left for further exploration.

### Title: G2GT: Retrosynthesis Prediction with Graph to Graph Attention Neural Network and Self-Training
* Paper ID: 2204.08608v1
* Paper URL: [http://arxiv.org/abs/2204.08608v1](http://arxiv.org/abs/2204.08608v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Retrosynthesis prediction is one of the fundamental challenges in organic
chemistry and related fields. The goal is to find reactants molecules that can
synthesize product molecules. To solve this task, we propose a new
graph-to-graph transformation model, G2GT, in which the graph encoder and graph
decoder are built upon the standard transformer structure. We also show that
self-training, a powerful data augmentation method that utilizes unlabeled
molecule data, can significantly improve the model's performance. Inspired by
the reaction type label and ensemble learning, we proposed a novel weak
ensemble method to enhance diversity. We combined beam search, nucleus, and
top-k sampling methods to further improve inference diversity and proposed a
simple ranking algorithm to retrieve the final top-10 results. We achieved new
state-of-the-art results on both the USPTO-50K dataset, with top1 accuracy of
54%, and the larger data set USPTO-full, with top1 accuracy of 50%, and
competitive top-10 results.

### Title: Bounds on skewness and kurtosis of steady state currents
* Paper ID: 2204.08421v2
* Paper URL: [http://arxiv.org/abs/2204.08421v2](http://arxiv.org/abs/2204.08421v2)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Current fluctuations are a powerful tool to unreaveal the underlying physics
of the observed transport process. This work discusses some general properties
of the third and the fourth cumulant of the current (skewness and kurtosis)
related to dynamics and thermodynamics of a transport setup. Specifically,
several distinct bounds on these quanities are either analytically derived or
numerically conjectured, which are applicable to: 1) noninteracting fermionic
systems, 2) nonintereacting bosonic systems, 3) thermally driven classical
Markovian systems, 4) unicyclic Markovian networks. Finally, it is demonstrated
that violation of the obtained bounds can be used to infer the presence of
dynamical channel blockade in multilevel quantum dots, electron pairing in
normal metal-superconductor junctions or coherent dynamics and interactions in
quantum dot molecules even close to equilibrium, when the shot noise - a
standard signature of these phenomena - is masked by the thermal noise.

