### Title: DC-driven positive streamer coronas in airflow
* Paper ID: 2204.10947v1
* Paper URL: [http://arxiv.org/abs/2204.10947v1](http://arxiv.org/abs/2204.10947v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: An experimental study of the effect of laminar airflow on positive
self-pulsating streamer coronas in a needle-to-plate geometry is presented. The
experiments are performed in an open return wind tunnel with winds up to 30 m/s
orthogonal to the needle. The experimental data is presented in terms of
statistical properties of the discharge, inferred from high resolution, large
sample-size current waveforms. The key properties of the current pulsations,
namely inter-pulse period, peak current, deposited energy, and pulse width are
analyzed as a function of wind speed and applied DC voltage. All parameters
increase in dispersion with wind speed. The mean of the inter-pulse period
decreases with wind speed and the mean pulsation frequency increases. The peak
currents and energies per pulsation have a general tendency to decrease in
magnitude but also higher-current, higher-energy, streamer bursts are observed.
At low wind speeds, streamers preferentially propagate in the downwind
direction but, as the wind speed is increased, more streamers can propagate
upwind. Synchronized imaging reveals correlations between the streamer
direction and the electrical characteristics.

### Title: Bayesian Spatiotemporal Modeling for Inverse Problems
* Paper ID: 2204.10929v1
* Paper URL: [http://arxiv.org/abs/2204.10929v1](http://arxiv.org/abs/2204.10929v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Inverse problems with spatiotemporal observations are ubiquitous in
scientific studies and engineering applications. In these spatiotemporal
inverse problems, observed multivariate time series are used to infer
parameters of physical or biological interests. Traditional solutions for these
problems often ignore the spatial or temporal correlations in the data (static
model), or simply model the data summarized over time (time-averaged model). In
either case, the data information that contains the spatiotemporal interactions
is not fully utilized for parameter learning, which leads to insufficient
modeling in these problems. In this paper, we apply Bayesian models based on
spatiotemporal Gaussian processes (STGP) to the inverse problems with
spatiotemporal data and show that the spatial and temporal information provides
more effective parameter estimation and uncertainty quantification (UQ). We
demonstrate the merit of Bayesian spatiotemporal modeling for inverse problems
compared with traditional static and time-averaged approaches using a
time-dependent advection-diffusion partial different equation (PDE) and three
chaotic ordinary differential equations (ODE). We also provide theoretic
justification for the superiority of spatiotemporal modeling to fit the
trajectories even it appears cumbersome (e.g. for chaotic dynamics).

### Title: Your Echos are Heard: Tracking, Profiling, and Ad Targeting in the Amazon Smart Speaker Ecosystem
* Paper ID: 2204.10920v1
* Paper URL: [http://arxiv.org/abs/2204.10920v1](http://arxiv.org/abs/2204.10920v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Smart speakers collect voice input that can be used to infer sensitive
information about users. Given a number of egregious privacy breaches, there is
a clear unmet need for greater transparency and control over data collection,
sharing, and use by smart speaker platforms as well as third party skills
supported on them. To bridge the gap, we build an auditing framework that
leverages online advertising to measure data collection, its usage, and its
sharing by the smart speaker platforms. We evaluate our framework on the Amazon
smart speaker ecosystem. Our results show that Amazon and third parties
(including advertising and tracking services) collect smart speaker interaction
data; where Amazon shares it with as many as 41 advertising partners. We find
that Amazon processes voice data to infer user interests and uses it to serve
targeted ads on-platform (Echo devices) as well as off-platform (web). Smart
speaker interaction leads to as much as 30X higher ad bids from advertisers.
Finally, we find that Amazon's and skills' operational practices are often
inconsistent with their privacy policies

### Title: Identity Preserving Loss for Learned Image Compression
* Paper ID: 2204.10869v1
* Paper URL: [http://arxiv.org/abs/2204.10869v1](http://arxiv.org/abs/2204.10869v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Deep learning model inference on embedded devices is challenging due to the
limited availability of computation resources. A popular alternative is to
perform model inference on the cloud, which requires transmitting images from
the embedded device to the cloud. Image compression techniques are commonly
employed in such cloud-based architectures to reduce transmission latency over
low bandwidth networks. This work proposes an end-to-end image compression
framework that learns domain-specific features to achieve higher compression
ratios than standard HEVC/JPEG compression techniques while maintaining
accuracy on downstream tasks (e.g., recognition). Our framework does not
require fine-tuning of the downstream task, which allows us to drop-in any
off-the-shelf downstream task model without retraining. We choose faces as an
application domain due to the ready availability of datasets and off-the-shelf
recognition models as representative downstream tasks. We present a novel
Identity Preserving Reconstruction (IPR) loss function which achieves
Bits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression
for LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,
respectively, while maintaining parity in recognition accuracy. The superior
compression ratio is achieved as the model learns to retain the domain-specific
features (e.g., facial features) while sacrificing details in the background.
Furthermore, images reconstructed by our proposed compression model are robust
to changes in downstream model architectures. We show at-par recognition
performance on the LFW dataset with an unseen recognition model while retaining
a lower BPP value of ~38% of CRF-23 HEVC compression.

### Title: How Sampling Impacts the Robustness of Stochastic Neural Networks
* Paper ID: 2204.10839v1
* Paper URL: [http://arxiv.org/abs/2204.10839v1](http://arxiv.org/abs/2204.10839v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Stochastic neural networks (SNNs) are random functions and predictions are
gained by averaging over multiple realizations of this random function.
Consequently, an adversarial attack is calculated based on one set of samples
and applied to the prediction defined by another set of samples. In this paper
we analyze robustness in this setting by deriving a sufficient condition for
the given prediction process to be robust against the calculated attack. This
allows us to identify the factors that lead to an increased robustness of SNNs
and helps to explain the impact of the variance and the amount of samples.
Among other things, our theoretical analysis gives insights into (i) why
increasing the amount of samples drawn for the estimation of adversarial
examples increases the attack's strength, (ii) why decreasing sample size
during inference hardly influences the robustness, and (iii) why a higher
prediction variance between realizations relates to a higher robustness. We
verify the validity of our theoretical findings by an extensive empirical
analysis.

### Title: S2AMP: A High-Coverage Dataset of Scholarly Mentorship Inferred from Publications
* Paper ID: 2204.10838v1
* Paper URL: [http://arxiv.org/abs/2204.10838v1](http://arxiv.org/abs/2204.10838v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Mentorship is a critical component of academia, but is not as visible as
publications, citations, grants, and awards. Despite the importance of studying
the quality and impact of mentorship, there are few large representative
mentorship datasets available. We contribute two datasets to the study of
mentorship. The first has 300,000 ground truth academic mentor-mentee pairs
obtained from multiple diverse, manually-curated sources, and linked to the
Semantic Scholar (S2) knowledge graph. We use this dataset to train an accurate
classifier for predicting mentorship relations from bibliographic features,
achieving a held-out area under the ROC curve of 0.96. Our second dataset is
formed by applying the classifier to the complete co-authorship graph of S2.
The result is an inferred graph with 137 million weighted mentorship edges
among 24 million nodes. We release this first-of-its-kind dataset to the
community to help accelerate the study of scholarly mentorship:
\url{https://github.com/allenai/S2AMP-data}

### Title: Bayesian operator inference for data-driven reduced-order modeling
* Paper ID: 2204.10829v1
* Paper URL: [http://arxiv.org/abs/2204.10829v1](http://arxiv.org/abs/2204.10829v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/Willcox-Research-Group/ROM-OpInf-Combustion-2D](https://github.com/Willcox-Research-Group/ROM-OpInf-Combustion-2D)
* Summary: This work proposes a Bayesian inference method for the reduced-order modeling
of time-dependent systems. Informed by the structure of governing equations,
the task of learning a reduced-order model from data is posed as a Bayesian
inversion problem with Gaussian prior and likelihood. The operators defining
the reduced-order model, rather than being chosen deterministically, are
characterized probabilistically as posterior Gaussian distributions. This
embeds uncertainty into the reduced-order model, and hence the predictions
subsequently issued by the reduced-order model are endowed with uncertainty.
The learned reduced-order models are computationally efficient, which enables
Monte Carlo sampling over the posterior distributions of reduced-order
operators. Furthermore, the proposed Bayesian framework provides a statistical
interpretation of the Tikhonov regularization incorporated in the operator
inference, and the empirical Bayes approach of maximum marginal likelihood
suggests a selection algorithm for the regularization hyperparameters. The
proposed method is demonstrated by two examples: the compressible Euler
equations with noise-corrupted observations, and a single-injector combustion
process.

### Title: Constructing dynamic residential energy lifestyles using Latent Dirichlet Allocation
* Paper ID: 2204.10770v1
* Paper URL: [http://arxiv.org/abs/2204.10770v1](http://arxiv.org/abs/2204.10770v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: The rapid expansion of Advanced Meter Infrastructure (AMI) has dramatically
altered the energy information landscape. However, our ability to use this
information to generate actionable insights about residential electricity
demand remains limited. In this research, we propose and test a new framework
for understanding residential electricity demand by using a dynamic energy
lifestyles approach that is iterative and highly extensible. To obtain energy
lifestyles, we develop a novel approach that applies Latent Dirichlet
Allocation (LDA), a method commonly used for inferring the latent topical
structure of text data, to extract a series of latent household energy
attributes. By doing so, we provide a new perspective on household electricity
consumption where each household is characterized by a mixture of energy
attributes that form the building blocks for identifying a sparse collection of
energy lifestyles. We examine this approach by running experiments on one year
of hourly smart meter data from 60,000 households and we extract six energy
attributes that describe general daily use patterns. We then use clustering
techniques to derive six distinct energy lifestyle profiles from energy
attribute proportions. Our lifestyle approach is also flexible to varying time
interval lengths, and we test our lifestyle approach seasonally (Autumn,
Winter, Spring, and Summer) to track energy lifestyle dynamics within and
across households and find that around 73% of households manifest multiple
lifestyles across a year. These energy lifestyles are then compared to
different energy use characteristics, and we discuss their practical
applications for demand response program design and lifestyle change analysis.

### Title: The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models
* Paper ID: 2204.10759v1
* Paper URL: [http://arxiv.org/abs/2204.10759v1](http://arxiv.org/abs/2204.10759v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/cassidylaidlaw/boltzmann-policy-distribution](https://github.com/cassidylaidlaw/boltzmann-policy-distribution)
* Summary: Models of human behavior for prediction and collaboration tend to fall into
two categories: ones that learn from large amounts of data via imitation
learning, and ones that assume human behavior to be noisily-optimal for some
reward function. The former are very useful, but only when it is possible to
gather a lot of human data in the target environment and distribution. The
advantage of the latter type, which includes Boltzmann rationality, is the
ability to make accurate predictions in new environments without extensive data
when humans are actually close to optimal. However, these models fail when
humans exhibit systematic suboptimality, i.e. when their deviations from
optimal behavior are not independent, but instead consistent over time. Our key
insight is that systematic suboptimality can be modeled by predicting policies,
which couple action choices over time, instead of trajectories. We introduce
the Boltzmann policy distribution (BPD), which serves as a prior over human
policies and adapts via Bayesian inference to capture systematic deviations by
observing human actions during a single episode. The BPD is difficult to
compute and represent because policies lie in a high-dimensional continuous
space, but we leverage tools from generative and sequence models to enable
efficient sampling and inference. We show that the BPD enables prediction of
human behavior and human-AI collaboration equally as well as imitation
learning-based human models while using far less data.

### Title: Machine Learning methods to estimate observational properties of galaxy clusters in large volume cosmological N-body simulations
* Paper ID: 2204.10751v1
* Paper URL: [http://arxiv.org/abs/2204.10751v1](http://arxiv.org/abs/2204.10751v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: In this paper we study the applicability of a set of supervised machine
learning (ML) models specifically trained to infer observed related properties
of the baryonic component (stars and gas) from a set of features of dark matter
only cluster-size halos. The training set is built from THE THREE HUNDRED
project which consists of a series of zoomed hydrodynamical simulations of
cluster-size regions extracted from the 1 Gpc volume MultiDark dark-matter only
simulation (MDPL2). We use as target variables a set of baryonic properties for
the intra cluster gas and stars derived from the hydrodynamical simulations and
correlate them with the properties of the dark matter halos from the MDPL2
N-body simulation. The different ML models are trained from this database and
subsequently used to infer the same baryonic properties for the whole range of
cluster-size halos identified in the MDPL2. We also test the robustness of the
predictions of the models against mass resolution of the dark matter halos and
conclude that their inferred baryonic properties are rather insensitive to
their DM properties which are resolved with almost an order of magnitude
smaller number of particles. We conclude that the ML models presented in this
paper can be used as an accurate and computationally efficient tool for
populating cluster-size halos with observational related baryonic properties in
large volume N-body simulations making them more valuable for comparison with
full sky galaxy cluster surveys at different wavelengths. We make the best ML
trained model publicly available.

### Title: Comparing Bayes factors and hierarchical inference for testing general relativity with gravitational waves
* Paper ID: 2204.10742v1
* Paper URL: [http://arxiv.org/abs/2204.10742v1](http://arxiv.org/abs/2204.10742v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: In the context of testing general relativity with gravitational waves,
constraints obtained with multiple events are typically combined either through
a hierarchical formalism or though a combined multiplicative Bayes factor. We
show that the well-known dependence of Bayes factors on the analysis priors in
regions of the parameter space without likelihood support can lead to strong
confidence in favor of incorrect conclusions when one employs the
multiplicative Bayes factor. Bayes factors $\mathcal{O}(1)$ are ambivalent as
they depend sensitively on the analysis priors, which are rarely set in a
principled way; additionally, combined Bayes factors $>\mathcal{O}(10^3)$ can
be obtained in favor of the incorrect conclusion depending on the analysis
priors when many $\mathcal{O}(1)$ Bayes factors are multiplied, and
specifically when the priors are much wider than the underlying population. The
hierarchical analysis that instead infers the ensemble distribution of the
individual beyond-general-relativity constraints does not suffer from this
problem, and generically converges to favor the correct conclusion. Rather than
a naive multiplication, a more reliable Bayes factor can be computed from the
hierarchical analysis. We present a number of toy models showing that the
practice of multiplying Bayes Factors can lead to incorrect conclusions.

### Title: Group sequential methods for interim monitoring of randomized clinical trials with time-lagged outcome
* Paper ID: 2204.10739v1
* Paper URL: [http://arxiv.org/abs/2204.10739v1](http://arxiv.org/abs/2204.10739v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: The primary analysis in two-arm clinical trials usually involves inference on
a scalar treatment effect parameter; e.g., depending on the outcome, the
difference of treatment-specific means, risk difference, risk ratio, or odds
ratio. Most clinical trials are monitored for the possibility of early
stopping. Because ordinarily the outcome on any given subject can be
ascertained only after some time lag, at the time of an interim analysis, among
the subjects already enrolled, the outcome is known for only a subset and is
effectively censored for those who have not been enrolled sufficiently long for
it to be observed. Typically, the interim analysis is based only on the data
from subjects for whom the outcome has been ascertained. A goal of an interim
analysis is to stop the trial as soon as the evidence is strong enough to do
so, suggesting that the analysis ideally should make the most efficient use of
all available data, thus including information on censoring as well as other
baseline and time-dependent covariates in a principled way. A general group
sequential framework is proposed for clinical trials with a time-lagged
outcome. Treatment effect estimators that take account of censoring and
incorporate covariate information at an interim analysis are derived using
semiparametric theory and are demonstrated to lead to stronger evidence for
early stopping than standard approaches. The associated test statistics are
shown to have the independent increments structure, so that standard software
can be used to obtain stopping boundaries.

### Title: SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across Drone and Satellite
* Paper ID: 2204.10704v1
* Paper URL: [http://arxiv.org/abs/2204.10704v1](http://arxiv.org/abs/2204.10704v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/Reza-Zhu/SUES-200-Benchmark](https://github.com/Reza-Zhu/SUES-200-Benchmark)
* Summary: The purpose of cross-view image matching is to match images acquired from the
different platforms of the same target scene and then help positioning system
to infer the location of the target scene. With the rapid development of drone
technology, how to help Drone positioning or navigation through cross-view
matching technology has become a challenging research topic. However, the
accuracy of current cross-view matching models is still low, mainly because the
existing public datasets do not include the differences in images obtained by
drones at different heights, and the types of scenes are relatively
homogeneous, which makes the models unable to adapt to complex and changing
scenes. We propose a new cross-view dataset, SUES-200, to address these
issues.SUES-200 contains images acquired by the drone at four flight heights
and the corresponding satellite view images under the same target scene. To our
knowledge, SUES-200 is the first dataset that considers the differences
generated by aerial photography of drones at different flight heights. In
addition, we build a pipeline for efficient training testing and evaluation of
cross-view matching models. Then, we comprehensively evaluate the performance
of feature extractors with different CNN architectures on SUES-200 through an
evaluation system for cross-view matching models and propose a robust baseline
model. The experimental results show that SUES-200 can help the model learn
features with high discrimination at different heights. Evaluating indicators
of the matching system improves as the drone flight height gets higher because
the drone camera pose and the surrounding environment have less influence on
aerial photography.

### Title: Bayesian mixed-effect models for independent dynamic social network data
* Paper ID: 2204.10676v1
* Paper URL: [http://arxiv.org/abs/2204.10676v1](http://arxiv.org/abs/2204.10676v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Relational event or time-stamped social network data have become increasingly
available over the years. Accordingly, statistical methods for such data have
also surfaced. These techniques are based on log-linear models of the rates of
interactions in a social network via actor covariates and network statistics.
Particularly, the use of survival analysis concepts has stimulated the
development of powerful methods over the past decade. These models mainly focus
on the analysis of single networks. To date, there are few models that can deal
with multiple relational event networks jointly. In this paper, we propose a
new Bayesian hierarchical model for multiple relational event sequences. This
approach allows inferences at the actor level, which are useful in
understanding which effects guide actors' preferences in social interactions.
We also present Bayes factors for hypothesis testing in this class of models.
In addition, a new Bayes factor to test random-effect structures is developed.
In this test, we let the prior be determined by the data, alleviating the issue
of employing improper priors in Bayes factors and thus preventing the use of
ad-hoc choices in absence of prior information. We use data of classroom
interactions among high school students to illustrate the proposed methods.

### Title: Unknown Face Presentation Attack Detection via Localised Learning of Multiple Kernels
* Paper ID: 2204.10675v1
* Paper URL: [http://arxiv.org/abs/2204.10675v1](http://arxiv.org/abs/2204.10675v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: The paper studies face spoofing, a.k.a. presentation attack detection (PAD)
in the demanding scenarios of unknown types of attack. While earlier studies
have revealed the benefits of ensemble methods, and in particular, a multiple
kernel learning approach to the problem, one limitation of such techniques is
that they typically treat the entire observation space similarly and ignore any
variability and local structure inherent to the data. This work studies this
aspect of the face presentation attack detection problem in relation to
multiple kernel learning in a one-class setting to benefit from intrinsic local
structure in bona fide face samples. More concretely, inspired by the success
of the one-class Fisher null formalism, we formulate a convex localised
multiple kernel learning algorithm by imposing a joint matrix-norm constraint
on the collection of local kernel weights and infer locally adaptive weights
for zero-shot one-class unseen attack detection.
  We present a theoretical study of the proposed localised MKL algorithm using
Rademacher complexities to characterise its generalisation capability and
demonstrate the advantages of the proposed technique over some other options.
An assessment of the proposed approach on general object image datasets
illustrates its efficacy for abnormality and novelty detection while the
results of the experiments on face PAD datasets verifies its potential in
detecting unknown/unseen face presentation attacks.

### Title: An Upper Bound of the Information Flow From Children to Parent Node on Trees
* Paper ID: 2204.10618v1
* Paper URL: [http://arxiv.org/abs/2204.10618v1](http://arxiv.org/abs/2204.10618v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We consider the transmission of a state from the root of a tree towards its
leaves, assuming that each transmission occurs through a noisy channel. The
states at the leaves are observed, while at deeper nodes we can compute the
likelihood of each state given the observation. In this sense, information
flows from child nodes towards the parent node. Here we find an upper bound of
this children-to-parent information flow. To do so, first we introduce a new
measure of information, the memory vector, whose norm quantifies whether all
states have the same likelihood. Then we find conditions such that the norm of
the memory vector at the parent node can be linearly bounded by the sum of
norms at the child nodes. We also describe the reconstruction problem of
estimating the ancestral state at the root given the observation at the leaves.
We infer sufficient conditions under which the original state at the root
cannot be confidently reconstructed using the observed leaves, assuming that
the number of levels from the root to the leaves is large.

### Title: Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors
* Paper ID: 2204.10603v1
* Paper URL: [http://arxiv.org/abs/2204.10603v1](http://arxiv.org/abs/2204.10603v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/mabaorui/onsurfaceprior](https://github.com/mabaorui/onsurfaceprior)
* Summary: It is an important task to reconstruct surfaces from 3D point clouds. Current
methods are able to reconstruct surfaces by learning Signed Distance Functions
(SDFs) from single point clouds without ground truth signed distances or point
normals. However, they require the point clouds to be dense, which dramatically
limits their performance in real applications. To resolve this issue, we
propose to reconstruct highly accurate surfaces from sparse point clouds with
an on-surface prior. We train a neural network to learn SDFs via projecting
queries onto the surface represented by the sparse point cloud. Our key idea is
to infer signed distances by pushing both the query projections to be on the
surface and the projection distance to be the minimum. To achieve this, we
train a neural network to capture the on-surface prior to determine whether a
point is on a sparse point cloud or not, and then leverage it as a
differentiable function to learn SDFs from unseen sparse point cloud. Our
method can learn SDFs from a single sparse point cloud without ground truth
signed distances or point normals. Our numerical evaluation under widely used
benchmarks demonstrates that our method achieves state-of-the-art
reconstruction accuracy, especially for sparse point clouds.

### Title: Notip: Non-parametric True Discovery Proportion estimation for brain imaging
* Paper ID: 2204.10572v1
* Paper URL: [http://arxiv.org/abs/2204.10572v1](http://arxiv.org/abs/2204.10572v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Cluster-level inference procedures are widely used for brain mapping. These
methods compare the size of clusters obtained by thresholding brain maps to an
upper bound under the global null hypothesis, computed using Random Field
Theory or permutations. However, the guarantees obtained by this type of
inference - i.e. at least one voxel is truly activated in the cluster - are not
informative with regards to the strength of the signal therein. There is thus a
need for methods to assess the amount of signal within clusters; yet such
methods have to take into account that clusters are defined based on the data,
which creates circularity in the inference scheme. This has motivated the use
of post hoc estimates that allow statistically valid estimation of the
proportion of activated voxels in clusters. In the context of fMRI data, the
All-Resolutions Inference framework introduced in [24] provides post hoc
estimates of the proportion of activated voxels. However, this method relies on
parametric threshold families, which results in conservative inference. In this
paper, we leverage randomization methods to adapt to data characteristics and
obtain tighter false discovery control. We obtain Notip: a powerful,
non-parametric method that yields statistically valid estimation of the
proportion of activated voxels in data-derived clusters. Numerical experiments
demonstrate substantial power gains compared with state-of-the-art methods on
36 fMRI datasets. The conditions under which the proposed method brings
benefits are also discussed.

### Title: Privacy-preserving Social Distance Monitoring on Microcontrollers with Low-Resolution Infrared Sensors and CNNs
* Paper ID: 2204.10541v1
* Paper URL: [http://arxiv.org/abs/2204.10541v1](http://arxiv.org/abs/2204.10541v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Low-resolution infrared (IR) array sensors offer a low-cost, low-power, and
privacy-preserving alternative to optical cameras and smartphones/wearables for
social distance monitoring in indoor spaces, permitting the recognition of
basic shapes, without revealing the personal details of individuals. In this
work, we demonstrate that an accurate detection of social distance violations
can be achieved processing the raw output of a 8x8 IR array sensor with a
small-sized Convolutional Neural Network (CNN). Furthermore, the CNN can be
executed directly on a Microcontroller (MCU)-based sensor node.
  With results on a newly collected open dataset, we show that our best CNN
achieves 86.3% balanced accuracy, significantly outperforming the 61% achieved
by a state-of-the-art deterministic algorithm. Changing the architectural
parameters of the CNN, we obtain a rich Pareto set of models, spanning
70.5-86.3% accuracy and 0.18-75k parameters. Deployed on a STM32L476RG MCU,
these models have a latency of 0.73-5.33ms, with an energy consumption per
inference of 9.38-68.57{\mu}J.

### Title: Energy-efficient and Privacy-aware Social Distance Monitoring with Low-resolution Infrared Sensors and Adaptive Inference
* Paper ID: 2204.10539v1
* Paper URL: [http://arxiv.org/abs/2204.10539v1](http://arxiv.org/abs/2204.10539v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Low-resolution infrared (IR) Sensors combined with machine learning (ML) can
be leveraged to implement privacy-preserving social distance monitoring
solutions in indoor spaces. However, the need of executing these applications
on Internet of Things (IoT) edge nodes makes energy consumption critical. In
this work, we propose an energy-efficient adaptive inference solution
consisting of the cascade of a simple wake-up trigger and a 8-bit quantized
Convolutional Neural Network (CNN), which is only invoked for
difficult-to-classify frames. Deploying such adaptive system on a IoT
Microcontroller, we show that, when processing the output of a 8x8
low-resolution IR sensor, we are able to reduce the energy consumption by
37-57% with respect to a static CNN-based approach, with an accuracy drop of
less than 2% (83% balanced accuracy).

### Title: Fourier Imager Network (FIN): A deep neural network for hologram reconstruction with superior external generalization
* Paper ID: 2204.10533v1
* Paper URL: [http://arxiv.org/abs/2204.10533v1](http://arxiv.org/abs/2204.10533v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Deep learning-based image reconstruction methods have achieved remarkable
success in phase recovery and holographic imaging. However, the generalization
of their image reconstruction performance to new types of samples never seen by
the network remains a challenge. Here we introduce a deep learning framework,
termed Fourier Imager Network (FIN), that can perform end-to-end phase recovery
and image reconstruction from raw holograms of new types of samples, exhibiting
unprecedented success in external generalization. FIN architecture is based on
spatial Fourier transform modules that process the spatial frequencies of its
inputs using learnable filters and a global receptive field. Compared with
existing convolutional deep neural networks used for hologram reconstruction,
FIN exhibits superior generalization to new types of samples, while also being
much faster in its image inference speed, completing the hologram
reconstruction task in ~0.04 s per 1 mm^2 of the sample area. We experimentally
validated the performance of FIN by training it using human lung tissue samples
and blindly testing it on human prostate, salivary gland tissue and Pap smear
samples, proving its superior external generalization and image reconstruction
speed. Beyond holographic microscopy and quantitative phase imaging, FIN and
the underlying neural network architecture might open up various new
opportunities to design broadly generalizable deep learning models in
computational imaging and machine vision fields.

### Title: End-to-end symbolic regression with transformers
* Paper ID: 2204.10532v1
* Paper URL: [http://arxiv.org/abs/2204.10532v1](http://arxiv.org/abs/2204.10532v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Symbolic regression, the task of predicting the mathematical expression of a
function from the observation of its values, is a difficult task which usually
involves a two-step procedure: predicting the "skeleton" of the expression up
to the choice of numerical constants, then fitting the constants by optimizing
a non-convex loss function. The dominant approach is genetic programming, which
evolves candidates by iterating this subroutine a large number of times. Neural
networks have recently been tasked to predict the correct skeleton in a single
try, but remain much less powerful. In this paper, we challenge this two-step
procedure, and task a Transformer to directly predict the full mathematical
expression, constants included. One can subsequently refine the predicted
constants by feeding them to the non-convex optimizer as an informed
initialization. We present ablations to show that this end-to-end approach
yields better results, sometimes even without the refinement step. We evaluate
our model on problems from the SRBench benchmark and show that our model
approaches the performance of state-of-the-art genetic programming with several
orders of magnitude faster inference.

### Title: LiDetector: License Incompatibility Detection for Open Source Software
* Paper ID: 2204.10502v1
* Paper URL: [http://arxiv.org/abs/2204.10502v1](http://arxiv.org/abs/2204.10502v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Open-source software (OSS) licenses dictate the conditions which should be
followed to reuse, distribute, and modify the software. Apart from widely-used
licenses such as the MIT License, developers are also allowed to customize
their own licenses (called custom licenses), whose descriptions are more
flexible. The presence of such various licenses imposes challenges to
understanding licenses and their compatibility. To avoid financial and legal
risks, it is essential to ensure license compatibility when integrating
third-party packages or reusing code accompanied with licenses. In this work,
we propose LiDetector, an effective tool that extracts and interprets OSS
licenses (including both official licenses and custom licenses), and detects
license incompatibility among these licenses. Specifically, LiDetector
introduces a learning-based method to automatically identify meaningful license
terms from an arbitrary license and employs Probabilistic Context-Free Grammar
(PCFG) to infer rights and obligations for incompatibility detection.
Experiments demonstrate that LiDetector outperforms existing methods with
93.28% precision for term identification, and 91.09% accuracy for right and
obligation inference, and can effectively detect incompatibility with a 10.06%
FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale
empirical study on 1,846 projects reveals that 72.91% of the projects are
suffering from license incompatibility, including popular ones such as the MIT
License and the Apache License. We highlighted lessons learned from the
perspectives of different stakeholders and made all related data and the
replication package publicly available to facilitate follow-up research.

### Title: Adversarial Estimators
* Paper ID: 2204.10495v1
* Paper URL: [http://arxiv.org/abs/2204.10495v1](http://arxiv.org/abs/2204.10495v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We develop an asymptotic theory of adversarial estimators (`A-estimators').
Like maximum-likelihood-type estimators (`M-estimators'), both the estimator
and estimand are defined as the critical points of a sample and population
average respectively. A-estimators generalize M-estimators, as their objective
is maximized by one set of parameters and minimized by another. The
continuous-updating Generalized Method of Moments estimator, popular in
econometrics and causal inference, is among the earliest members of this class
which distinctly falls outside the M-estimation framework. Since the recent
success of Generative Adversarial Networks, A-estimators received considerable
attention in both machine learning and causal inference contexts, where a
flexible adversary can remove the need for researchers to manually specify
which features of a problem are important. We present general results
characterizing the convergence rates of A-estimators under both point-wise and
partial identification, and derive the asymptotic root-n normality for plug-in
estimates of smooth functionals of their parameters. All unknown parameters may
contain functions which are approximated via sieves. While the results apply
generally, we provide easily verifiable, low-level conditions for the case
where the sieves correspond to (deep) neural networks. Our theory also yields
the asymptotic normality of general functionals of neural network M-estimators
(as a special case), overcoming technical issues previously identified by the
literature. We examine a variety of A-estimators proposed across econometrics
and machine learning and use our theory to derive novel statistical results for
each of them. Embedding distinct A-estimators into the same framework, we
notice interesting connections among them, providing intuition and formal
justification for their recent success in practical applications.

### Title: Gene Function Prediction with Gene Interaction Networks: A Context Graph Kernel Approach
* Paper ID: 2204.10473v1
* Paper URL: [http://arxiv.org/abs/2204.10473v1](http://arxiv.org/abs/2204.10473v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Predicting gene functions is a challenge for biologists in the post genomic
era. Interactions among genes and their products compose networks that can be
used to infer gene functions. Most previous studies adopt a linkage assumption,
i.e., they assume that gene interactions indicate functional similarities
between connected genes. In this study, we propose to use a gene's context
graph, i.e., the gene interaction network associated with the focal gene, to
infer its functions. In a kernel-based machine-learning framework, we design a
context graph kernel to capture the information in context graphs. Our
experimental study on a testbed of p53-related genes demonstrates the advantage
of using indirect gene interactions and shows the empirical superiority of the
proposed approach over linkage-assumption-based methods, such as the algorithm
to minimize inconsistent connected genes and diffusion kernels.

### Title: Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering
* Paper ID: 2204.10448v1
* Paper URL: [http://arxiv.org/abs/2204.10448v1](http://arxiv.org/abs/2204.10448v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/yujungheo/kbvqa-public](https://github.com/yujungheo/kbvqa-public)
* Summary: Knowledge-based visual question answering (QA) aims to answer a question
which requires visually-grounded external knowledge beyond image content
itself. Answering complex questions that require multi-hop reasoning under weak
supervision is considered as a challenging problem since i) no supervision is
given to the reasoning process and ii) high-order semantics of multi-hop
knowledge facts need to be captured. In this paper, we introduce a concept of
hypergraph to encode high-level semantics of a question and a knowledge base,
and to learn high-order associations between them. The proposed model,
Hypergraph Transformer, constructs a question hypergraph and a query-aware
knowledge hypergraph, and infers an answer by encoding inter-associations
between two hypergraphs and intra-associations in both hypergraph itself.
Extensive experiments on two knowledge-based visual QA and two knowledge-based
textual QA demonstrate the effectiveness of our method, especially for
multi-hop reasoning problem. Our source code is available at
https://github.com/yujungheo/kbvqa-public.

### Title: Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
* Paper ID: 2204.09204v2
* Paper URL: [http://arxiv.org/abs/2204.09204v2](http://arxiv.org/abs/2204.09204v2)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.

