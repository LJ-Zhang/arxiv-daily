# membership infer

## 04-20

### Title: A stochastic Stein Variational Newton method
* Paper ID: 2204.09039v1
* Paper URL: [http://arxiv.org/abs/2204.09039v1](http://arxiv.org/abs/2204.09039v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/leviyevalex/ssvn](https://github.com/leviyevalex/ssvn)
* Summary: Stein variational gradient descent (SVGD) is a general-purpose
optimization-based sampling algorithm that has recently exploded in popularity,
but is limited by two issues: it is known to produce biased samples, and it can
be slow to converge on complicated distributions. A recently proposed
stochastic variant of SVGD (sSVGD) addresses the first issue, producing
unbiased samples by incorporating a special noise into the SVGD dynamics such
that asymptotic convergence is guaranteed. Meanwhile, Stein variational Newton
(SVN), a Newton-like extension of SVGD, dramatically accelerates the
convergence of SVGD by incorporating Hessian information into the dynamics, but
also produces biased samples. In this paper we derive, and provide a practical
implementation of, a stochastic variant of SVN (sSVN) which is both
asymptotically correct and converges rapidly. We demonstrate the effectiveness
of our algorithm on a difficult class of test problems -- the Hybrid Rosenbrock
density -- and show that sSVN converges using three orders of magnitude fewer
gradient evaluations of the log likelihood than its stochastic SVGD
counterpart. Our results show that sSVN is a promising approach to accelerating
high-precision Bayesian inference tasks with modest-dimension,
$d\sim\mathcal{O}(10)$.

### Title: A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets
* Paper ID: 2204.08997v1
* Paper URL: [http://arxiv.org/abs/2204.08997v1](http://arxiv.org/abs/2204.08997v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/lemuria-wchen/imcs21](https://github.com/lemuria-wchen/imcs21)
* Summary: In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this paper, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. A new large medical dialogue dataset with multi-level fine-grained
annotations is introduced and five independent tasks are established, including
named entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies.

### Title: Metappearance: Meta-Learning for Visual Appearance Reproduction
* Paper ID: 2204.08993v1
* Paper URL: [http://arxiv.org/abs/2204.08993v1](http://arxiv.org/abs/2204.08993v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: There currently are two main approaches to reproducing visual appearance
using Machine Learning (ML): The first is training models that generalize over
different instances of a problem, e.g., different images from a dataset. Such
models learn priors over the data corpus and use this knowledge to provide fast
inference with little input, often as a one-shot operation. However, this
generality comes at the cost of fidelity, as such methods often struggle to
achieve the final quality required. The second approach does not train a model
that generalizes across the data, but overfits to a single instance of a
problem, e.g., a flash image of a material. This produces detailed and
high-quality results, but requires time-consuming training and is, as mere
non-linear function fitting, unable to exploit previous experience. Techniques
such as fine-tuning or auto-decoders combine both approaches but are sequential
and rely on per-exemplar optimization. We suggest to combine both techniques
end-to-end using meta-learning: We over-fit onto a single problem instance in
an inner loop, while also learning how to do so efficiently in an outer-loop
that builds intuition over many optimization runs. We demonstrate this concept
to be versatile and efficient, applying it to RGB textures, Bi-directional
Reflectance Distribution Functions (BRDFs), or Spatially-varying BRDFs
(svBRDFs).

### Title: CPU- and GPU-based Distributed Sampling in Dirichlet Process Mixtures for Large-scale Analysis
* Paper ID: 2204.08988v1
* Paper URL: [http://arxiv.org/abs/2204.08988v1](http://arxiv.org/abs/2204.08988v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/bgu-cs-vil/dpmmsubclusters_gpu](https://github.com/bgu-cs-vil/dpmmsubclusters_gpu)
* Summary: In the realm of unsupervised learning, Bayesian nonparametric mixture models,
exemplified by the Dirichlet Process Mixture Model (DPMM), provide a principled
approach for adapting the complexity of the model to the data. Such models are
particularly useful in clustering tasks where the number of clusters is
unknown. Despite their potential and mathematical elegance, however, DPMMs have
yet to become a mainstream tool widely adopted by practitioners. This is
arguably due to a misconception that these models scale poorly as well as the
lack of high-performance (and user-friendly) software tools that can handle
large datasets efficiently. In this paper we bridge this practical gap by
proposing a new, easy-to-use, statistical software package for scalable DPMM
inference. More concretely, we provide efficient and easily-modifiable
implementations for high-performance distributed sampling-based inference in
DPMMs where the user is free to choose between either a multiple-machine,
multiple-core, CPU implementation (written in Julia) and a multiple-stream GPU
implementation (written in CUDA/C++). Both the CPU and GPU implementations come
with a common (and optional) python wrapper, providing the user with a single
point of entry with the same interface. On the algorithmic side, our
implementations leverage a leading DPMM sampler from (Chang and Fisher III,
2013). While Chang and Fisher III's implementation (written in MATLAB/C++) used
only CPU and was designed for a single multi-core machine, the packages we
proposed here distribute the computations efficiently across either multiple
multi-core machines or across mutiple GPU streams. This leads to speedups,
alleviates memory and storage limitations, and lets us fit DPMMs to
significantly larger datasets and of higher dimensionality than was possible
previously by either (Chang and Fisher III, 2013) or other DPMM methods.

### Title: Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data
* Paper ID: 2204.08932v1
* Paper URL: [http://arxiv.org/abs/2204.08932v1](http://arxiv.org/abs/2204.08932v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/TOM-tym/Learn-to-Imagine](https://github.com/TOM-tym/Learn-to-Imagine)
* Summary: Deep neural network (DNN) suffers from catastrophic forgetting when learning
incrementally, which greatly limits its applications. Although maintaining a
handful of samples (called `exemplars`) of each task could alleviate forgetting
to some extent, existing methods are still limited by the small number of
exemplars since these exemplars are too few to carry enough task-specific
knowledge, and therefore the forgetting remains. To overcome this problem, we
propose to `imagine` diverse counterparts of given exemplars referring to the
abundant semantic-irrelevant information from unlabeled data. Specifically, we
develop a learnable feature generator to diversify exemplars by adaptively
generating diverse counterparts of exemplars based on semantic information from
exemplars and semantically-irrelevant information from unlabeled data. We
introduce semantic contrastive learning to enforce the generated samples to be
semantic consistent with exemplars and perform semanticdecoupling contrastive
learning to encourage diversity of generated samples. The diverse generated
samples could effectively prevent DNN from forgetting when learning new tasks.
Our method does not bring any extra inference cost and outperforms
state-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a
clear margin.

### Title: Inferring Epidemics from Multiple Dependent Data via Pseudo-Marginal Methods
* Paper ID: 2204.08901v1
* Paper URL: [http://arxiv.org/abs/2204.08901v1](http://arxiv.org/abs/2204.08901v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Health-policy planning requires evidence on the burden that epidemics place
on healthcare systems. Multiple, often dependent, datasets provide a noisy and
fragmented signal from the unobserved epidemic process including transmission
and severity dynamics. This paper explores important challenges to the use of
state-space models for epidemic inference when multiple dependent datasets are
analysed. We propose a new semi-stochastic model that exploits deterministic
approximations for large-scale transmission dynamics while retaining
stochasticity in the occurrence and reporting of relatively rare severe events.
This model is suitable for many real-time situations including large seasonal
epidemics and pandemics. Within this context, we develop algorithms to provide
exact parameter inference and test them via simulation. Finally, we apply our
joint model and the proposed algorithm to several surveillance data on the
2017-18 influenza epidemic in England to reconstruct transmission dynamics and
estimate the daily new influenza infections as well as severity indicators as
the case-hospitalisation risk and the hospital-intensive care risk.

### Title: Towards Efficient Single Image Dehazing and Desnowing
* Paper ID: 2204.08899v1
* Paper URL: [http://arxiv.org/abs/2204.08899v1](http://arxiv.org/abs/2204.08899v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Removing adverse weather conditions like rain, fog, and snow from images is a
challenging problem. Although the current recovery algorithms targeting a
specific condition have made impressive progress, it is not flexible enough to
deal with various degradation types. We propose an efficient and compact image
restoration network named DAN-Net (Degradation-Adaptive Neural Network) to
address this problem, which consists of multiple compact expert networks with
one adaptive gated neural. A single expert network efficiently addresses
specific degradation in nasty winter scenes relying on the compact architecture
and three novel components. Based on the Mixture of Experts strategy, DAN-Net
captures degradation information from each input image to adaptively modulate
the outputs of task-specific expert networks to remove various adverse winter
weather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural
Network to estimate gated attention maps of the input image, while different
task-specific experts with the same topology are jointly dispatched to process
the degraded image. Such novel image restoration pipeline handles different
types of severe weather scenes effectively and efficiently. It also enjoys the
benefit of coordinate boosting in which the whole network outperforms each
expert trained without coordination.
  Extensive experiments demonstrate that the presented manner outperforms the
state-of-the-art single-task methods on image quality and has better inference
efficiency. Furthermore, we have collected the first real-world winter scenes
dataset to evaluate winter image restoration methods, which contains various
hazy and snowy images snapped in winter. Both the dataset and source code will
be publicly available.

### Title: Using a Semantic Knowledge Base to Improve the Management of Security Reports in Industrial DevOps Projects
* Paper ID: 2204.08888v1
* Paper URL: [http://arxiv.org/abs/2204.08888v1](http://arxiv.org/abs/2204.08888v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Integrating security activities into the software development lifecycle to
detect security flaws is essential for any project. These activities produce
reports that must be managed and looped back to project stakeholders like
developers to enable security improvements. This so-called Feedback Loop is a
crucial part of any project and is required by various industrial security
standards and models. However, the operation of this loop presents a variety of
challenges. These challenges range from ensuring that feedback data is of
sufficient quality over providing different stakeholders with the information
they need to the enormous effort to manage the reports. In this paper, we
propose a novel approach for treating findings from security activity reports
as belief in a Knowledge Base (KB). By utilizing continuous logical inferences,
we derive information necessary for practitioners and address existing
challenges in the industry. This approach is currently evaluated in industrial
DevOps projects, using data from continuous security testing.

### Title: Revisiting the evidences for spectral anomalies in distant blazars: new data on the photon-ALP mixing
* Paper ID: 2204.08865v1
* Paper URL: [http://arxiv.org/abs/2204.08865v1](http://arxiv.org/abs/2204.08865v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We re-examine possible dependencies on redshift of the spectral parameters of
blazars observed at very-high energies (VHEs) with Imaging Atmospheric
Cherenkov telescopes (IACTs). This is relevant to assess potential effects with
the source distance of the photon to axion-like particle (ALP) mixing, that
would deeply affect the propagation of VHE photons across the Universe. We
focus our spectral analysis on 38 BL Lac objects (32 high-peaked and 6
intermediate-peaked) up to redshift $z\simeq 0.5$, and a small sample of 5 Flat
Spectrum Radio Quasars up to $z=1$ treated independently to increase the
redshift baseline. The 78 independent spectra of these sources are first of all
carefully corrected for the gamma-gamma interaction with photons of the
Extragalactic Background Light, that are responsible for the major
redshift-dependent opacity effect. Then, the corrected spectra are fitted with
simple power-laws to infer the intrinsic spectral indices $\Gamma_{\rm em}$ at
VHE, to test the assumption that such spectral properties are set by the local
rather than the global cosmological environment. We find some systematic
anti-correlations with redshift of $\Gamma_{\rm em}$ that might indicate,
although with low-significance, a spectral anomaly potentially requiring a
revision of the photon propagation process. More conclusive tests with higher
statistical significance will require the observational improvements offered by
the forthcoming new generation of Cherenkov arrays (CTA, ASTRI, LHAASO).

### Title: Effects of spatial resolution on inferences of atmospheric quantities from simulations
* Paper ID: 2204.08849v1
* Paper URL: [http://arxiv.org/abs/2204.08849v1](http://arxiv.org/abs/2204.08849v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Small scale processes are thought to be important for the dynamics of the
solar atmosphere. While numerical resolution fundamentally limits their
inclusion in MHD simulations, real observations at the same nominal resolution
should still contain imprints of sub-resolution effects. This means that the
synthetic observables from a simulation of given resolution might not be
directly comparable to real observables at the same resolution. It is thus of
interest to investigate how inferences based on synthetic spectra from
simulations with different numerical resolutions compare, and whether these
differences persist after the spectra have been spatially degraded to a common
resolution. We aim to compare synthetic spectra obtained from realistic 3D
radiative magnetohydrodynamic (rMHD) simulations run at three different
numerical resolutions from the same initial atmosphere, using very simple
methods for inferring line-of-sight velocities and magnetic fields.
Additionally we examine how the differing spatial resolution impacts the
results retrieved from the STiC inversion code. We find that while the simple
inferences for all three simulations reveal the same large-scale tendencies,
the higher resolutions yield more fine-grained structures and more extreme
line-of-sight velocities/magnetic fields in concentrated spots even after
spatial smearing. We also see indications that the imprints of sub-resolution
effects on the degraded spectra result in systematic errors in the inversions,
and that these errors increase with the amount of sub-resolution effects
included. Fortunately, however, we find that including successively more
sub-resolution yields smaller additional effects; i.e. there is a clear trend
of diminishing importance for progressively finer sub-resolution effects.

### Title: Compressed Empirical Measures (in finite dimensions)
* Paper ID: 2204.08847v1
* Paper URL: [http://arxiv.org/abs/2204.08847v1](http://arxiv.org/abs/2204.08847v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We study approaches for compressing the empirical measure in the context of
finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context,
the empirical measure is contained within a natural convex set and can be
approximated using convex optimization methods. Such an approximation gives
under certain conditions rise to a coreset of data points. A key quantity that
controls how large such a coreset has to be is the size of the largest ball
around the empirical measure that is contained within the empirical convex set.
The bulk of our work is concerned with deriving high probability lower bounds
on the size of such a ball under various conditions. We complement this
derivation of the lower bound by developing techniques that allow us to apply
the compression approach to concrete inference problems such as kernel ridge
regression. We conclude with a construction of an infinite dimensional RKHS for
which the compression is poor, highlighting some of the difficulties one faces
when trying to move to infinite dimensional RKHSs.

### Title: Quantum Bayesian Statistical Inference
* Paper ID: 2204.08845v1
* Paper URL: [http://arxiv.org/abs/2204.08845v1](http://arxiv.org/abs/2204.08845v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In this work a quantum analogue of Bayesian statistical inference is
considered. Based on the notion of instrument, we propose a sequential
measurement scheme from which observations needed for statistical inference are
obtained. We further put forward a quantum analogue of Bayes rule, which states
how the prior normal state of a quantum system updates under those
observations. We next generalize the fundamental notions and results of
Bayesian statistics according to the quantum Bayes rule. It is also note that
our theory retains the classical one as its special case. Finally, we
investigate the limit of posterior normal state as the number of observations
tends to infinity.

### Title: Detect-and-describe: Joint learning framework for detection and description of objects
* Paper ID: 2204.08828v1
* Paper URL: [http://arxiv.org/abs/2204.08828v1](http://arxiv.org/abs/2204.08828v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Traditional object detection answers two questions; "what" (what the object
is?) and "where" (where the object is?). "what" part of the object detection
can be fine-grained further i.e. "what type", "what shape" and "what material"
etc. This results in the shifting of the object detection tasks to the object
description paradigm. Describing an object provides additional detail that
enables us to understand the characteristics and attributes of the object
("plastic boat" not just boat, "glass bottle" not just bottle). This additional
information can implicitly be used to gain insight into unseen objects (e.g.
unknown object is "metallic", "has wheels"), which is not possible in
traditional object detection. In this paper, we present a new approach to
simultaneously detect objects and infer their attributes, we call it Detect and
Describe (DaD) framework. DaD is a deep learning-based approach that extends
object detection to object attribute prediction as well. We train our model on
aPascal train set and evaluate our approach on aPascal test set. We achieve
97.0% in Area Under the Receiver Operating Characteristic Curve (AUC) for
object attributes prediction on aPascal test set. We also show qualitative
results for object attribute prediction on unseen objects, which demonstrate
the effectiveness of our approach for describing unknown objects.

### Title: Ariel stellar characterisation: I -- homogeneous stellar parameters of 187 FGK planet host stars Description and validation of the method
* Paper ID: 2204.08825v1
* Paper URL: [http://arxiv.org/abs/2204.08825v1](http://arxiv.org/abs/2204.08825v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In 2020 the European Space Agency selected Ariel as the next mission to join
the space fleet of observatories to study planets outside our Solar System.
Ariel will be devoted to the characterisation of a thousand planetary
atmospheres, for understanding what exoplanets are made of, how they formed and
how they evolve. To achieve the last two goals all planets need to be studied
within the context of their own host stars, which in turn have to be analysed
with the same technique, in a uniform way. We present the spectro-photometric
method we have developed to infer the atmospheric parameters of the known host
stars in the Tier 1 of the Ariel Reference Sample. Our method is based on an
iterative approach, which combines spectral analysis, the determination of the
surface gravity from {\em Gaia} data, and the determination of stellar masses
from isochrone fitting. We validated our approach with the analysis of a
control sample, composed by members of three open clusters with well-known ages
and metallicities. We measured effective temperature, Teff, surface gravity,
logg, and the metallicity, [Fe/H], of 187 F-G-K stars within the Ariel
Reference Sample. We presented the general properties of the sample, including
their kinematics which allows us to separate them between thin and thick disc
populations. A homogeneous determination of the parameters of the host stars is
fundamental in the study of the stars themselves and their planetary systems.
Our analysis systematically improves agreement with theoretical models and
decreases uncertainties in the mass estimate (from 0.21+/-0.30 to 0.10+/-0.02
M_sun), providing useful data for the Ariel consortium and the astronomical
community at large.

### Title: RNNCTPs: A Neural Symbolic Reasoning Method Using Dynamic Knowledge Partitioning Technology
* Paper ID: 2204.08810v1
* Paper URL: [http://arxiv.org/abs/2204.08810v1](http://arxiv.org/abs/2204.08810v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Although traditional symbolic reasoning methods are highly interpretable,
their application in knowledge graph link prediction is limited due to their
low computational efficiency. In this paper, we propose a new neural symbolic
reasoning method: RNNCTPs, which improves computational efficiency by
re-filtering the knowledge selection of Conditional Theorem Provers (CTPs), and
is less sensitive to the embedding size parameter. RNNCTPs are divided into
relation selectors and predictors. The relation selectors are trained
efficiently and interpretably, so that the whole model can dynamically generate
knowledge for the inference of the predictor. In all four datasets, the method
shows competitive performance against traditional methods on the link
prediction task, and can have higher applicability to the selection of datasets
relative to CTPs.

### Title: What powers the wind from the black hole accretion disc in GRO J1655-40?
* Paper ID: 2204.08802v1
* Paper URL: [http://arxiv.org/abs/2204.08802v1](http://arxiv.org/abs/2204.08802v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Black hole accretion discs can produce powerful outflowing plasma (disc
winds), seen as blue-shifted absorption lines in stellar and supermassive
systems. These winds in Quasars have an essential role in controlling galaxy
formation across cosmic time, but there is no consensus on how these are
physically launched. A single unique observation of a stellar-mass black hole
GRO J1655-40 showed the high wind density estimated from an absorption line
from the metastable level of Fe xxii and ruled out X-ray heating
(thermal-radiative wind) for the low observed luminosity. This left magnetic
driving as the only viable mechanism, motivating unified models of magnetic
winds in both binaries and Quasars. Here we reanalyse these data using a
photoionisation code that includes the contribution of radiative cascades and
collisions in populating the metastable level. The effect of radiative cascades
reduces the inferred wind density by orders of magnitude. The derived column is
also optically thick, so the source is intrinsically more luminous than
observed. We show that a thermal-radiative wind model calculated from a
radiation hydrodynamic simulation matches well with the data. We revisit the
previous magnetic wind solution and show that this is also optically thick.
Hence, it requires a larger source luminosity, and it struggles to reproduce
the overall ion population at the required density (both new and old). These
results remove the requirement for a magnetic wind in these data and remove the
basis of the self-similar unified magnetic wind models extrapolated to Quasar
outflows.

### Title: Programmable heralded linear optical generation of two-qubit states
* Paper ID: 2204.08788v1
* Paper URL: [http://arxiv.org/abs/2204.08788v1](http://arxiv.org/abs/2204.08788v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We have investigated the heralded generation of two-qubit dual-rail-encoded
states by programmable linear optics. Two types of schemes generating the
states from four single photons, which is the minimal possible to accomplish
the task, have been considered. The schemes have different detection patterns
heralding successful generation events, namely, one-mode heralding, in which
the two auxiliary photons are detected in one mode, and two-mode heralding, in
which single photons are detected in each of the two modes simultaneously. We
have shown that the dependence of the schemes' success probabilities on the
target state's degree of entanglement are essentially different. In particular,
one-mode heralding yields better efficiency for highly-entangled states, if the
programmable interferometers can explore the full space of the unitary transfer
matrices,. It is reversed in case of weakly-entangled states where two-mode
heralding is better. We have found a minimal decomposition of the scheme with
two-mode heralding that is programmed by one variable phase shift. We infer
that the linear optical schemes designed specifically for generation of
two-qubit states are more efficient than schemes implementing gate-based
circuits with known two-qubit linear optical gates. Our results yield
substantial reduction of physical resources needed to generate two-qubit
dual-rail-encoded photonic states.

### Title: IndicXNLI: Evaluating Multilingual Inference for Indian Languages
* Paper ID: 2204.08776v1
* Paper URL: [http://arxiv.org/abs/2204.08776v1](http://arxiv.org/abs/2204.08776v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/divyanshuaggarwal/indicxnli](https://github.com/divyanshuaggarwal/indicxnli)
* Summary: While Indic NLP has made rapid advances recently in terms of the availability
of corpora and pre-trained models, benchmark datasets on standard NLU tasks are
limited. To this end, we introduce IndicXNLI, an NLI dataset for 11 Indic
languages. It has been created by high-quality machine translation of the
original English XNLI dataset and our analysis attests to the quality of
IndicXNLI. By finetuning different pre-trained LMs on this IndicXNLI, we
analyze various cross-lingual transfer techniques with respect to the impact of
the choice of language models, languages, multi-linguality, mix-language input,
etc. These experiments provide us with useful insights into the behaviour of
pre-trained models for a diverse set of languages.

### Title: Near K-Edge Photoionization and Photoabsorption of Singly, Doubly, and Triply Charged Silicon Ions
* Paper ID: 2204.08750v1
* Paper URL: [http://arxiv.org/abs/2204.08750v1](http://arxiv.org/abs/2204.08750v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Experimental and theoretical results are presented for double, triple, and
quadruple photoionization of Si$^+$ and Si$^{2+}$ ions and for double
photoionization of Si$^{3+}$ ions by a single photon. The experiments employed
the photon-ion merged-beams technique at a synchrotron light source. The
experimental photon-energy range 1835--1900 eV comprises resonances associated
with the excitation of a $1s$ electron to higher subshells and subsequent
autoionization. Energies, widths, and strengths of these resonances are
extracted from high-resolution photoionization measurements, and the core-hole
lifetime of K-shell ionized neutral silicon is inferred. In addition,
theoretical cross sections for photoabsorption and multiple photoionization
were obtained from large-scale Multi-Configuration Dirac-Hartree-Fock (MCDHF)
calculations. The present calculations agree with the experiment much better
than previously published theoretical results. The importance of an accurate
energy calibration of laboratory data is pointed out. The present benchmark
results are particularly useful for discriminating between silicon absorption
in the gaseous and in the solid component (dust grains) of the interstellar
medium.

### Title: Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing
* Paper ID: 2204.08734v1
* Paper URL: [http://arxiv.org/abs/2204.08734v1](http://arxiv.org/abs/2204.08734v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Deep learning (DL) techniques are proven effective in many challenging tasks,
and become widely-adopted in practice. However, previous work has shown that DL
libraries, the basis of building and executing DL models, contain bugs and can
cause severe consequences. Unfortunately, existing testing approaches still
cannot comprehensively exercise DL libraries. They utilize existing trained
models and only detect bugs in model inference phase. In this work we propose
Muffin to address these issues. To this end, Muffin applies a
specifically-designed model fuzzing approach, which allows it to generate
diverse DL models to explore the target library, instead of relying only on
existing trained models. Muffin makes differential testing feasible in the
model training phase by tailoring a set of metrics to measure the
inconsistencies between different DL libraries. In this way, Muffin can best
exercise the library code to detect more bugs. To evaluate the effectiveness of
Muffin, we conduct experiments on three widely-used DL libraries. The results
demonstrate that Muffin can detect 39 new bugs in the latest release versions
of popular DL libraries, including Tensorflow, CNTK, and Theano.

### Title: Emu: A Case Study for TDI-like Imaging for Infrared Observation from Space
* Paper ID: 2204.08713v1
* Paper URL: [http://arxiv.org/abs/2204.08713v1](http://arxiv.org/abs/2204.08713v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: A wide-field zenith-looking telescope operating in a mode similar to
Time-Delay-Integration (TDI) or drift scan imaging can perform an infrared sky
survey without active pointing control but it requires a high-speed, low-noise
infrared detector. Operating from a hosted payload platform on the
International Space Station (ISS), the Emu space telescope employs the
paradigm-changing properties of the Leonardo SAPHIRA electron avalanche
photodiode array to provide powerful new observations of cool stars at the
critical water absorption wavelength (1.4 {\mu}m) largely inaccessible to
ground-based telescopes due to the Earth's own atmosphere. Cool stars,
especially those of spectral-type M, are important probes across contemporary
astrophysics, from the formation history of the Galaxy to the formation of
rocky exoplanets. Main sequence M-dwarf stars are the most abundant stars in
the Galaxy and evolved M-giant stars are some of the most distant stars that
can be individually observed. The Emu sky survey will deliver critical stellar
properties of these cool stars by inferring oxygen abundances via measurement
of the water absorption band strength at 1.4 {\mu}m. Here we present the
TDI-like imaging capability of Emu mission, its science objectives, instrument
details and simulation results.

### Title: On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules
* Paper ID: 2204.08653v1
* Paper URL: [http://arxiv.org/abs/2204.08653v1](http://arxiv.org/abs/2204.08653v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/fardfh-lab/NL-Code-Adapter](https://github.com/fardfh-lab/NL-Code-Adapter)
* Summary: Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently
used in software engineering as models pre-trained on large source code
corpora. Their knowledge is transferred to downstream tasks (e.g. code clone
detection) via fine-tuning. In natural language processing (NLP), other
alternatives for transferring the knowledge of PTLMs are explored through using
adapters, compact, parameter efficient modules inserted in the layers of the
PTLM. Although adapters are known to facilitate adapting to many downstream
tasks compared to fine-tuning the model that require retraining all of the
models' parameters -- which owes to the adapters' plug and play nature and
being parameter efficient -- their usage in software engineering is not
explored.
  Here, we explore the knowledge transfer using adapters and based on the
Naturalness Hypothesis proposed by Hindle et. al \cite{hindle2016naturalness}.
Thus, studying the bimodality of adapters for two tasks of cloze test and code
clone detection, compared to their benchmarks from the CodeXGLUE platform.
These adapters are trained using programming languages and are inserted in a
PTLM that is pre-trained on English corpora (N-PTLM). Three programming
languages, C/C++, Python, and Java, are studied along with extensive
experiments on the best setup used for adapters. Improving the results of the
N-PTLM confirms the success of the adapters in knowledge transfer to software
engineering, which sometimes are in par with or exceed the results of a PTLM
trained on source code; while being more efficient in terms of the number of
parameters, memory usage, and inference time. Our results can open new
directions to build smaller models for more software engineering tasks. We open
source all the scripts and the trained adapters.

### Title: LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation
* Paper ID: 2204.08649v1
* Paper URL: [http://arxiv.org/abs/2204.08649v1](http://arxiv.org/abs/2204.08649v1)
* Updated Date: 2022-04-19
* Code URL: [https://github.com/ncbi/ml-transformer](https://github.com/ncbi/ml-transformer)
* Summary: The rapid growth of biomedical literature poses a significant challenge for
curation and interpretation. This has become more evident during the COVID-19
pandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,
has accumulated over 180,000 articles with millions of accesses. Approximately
10,000 new articles are added to LitCovid every month. A main curation task in
LitCovid is topic annotation where an article is assigned with up to eight
topics, e.g., Treatment and Diagnosis. The annotated topics have been widely
used both in LitCovid (e.g., accounting for ~18% of total uses) and downstream
studies such as network generation. However, it has been a primary curation
bottleneck due to the nature of the task and the rapid literature growth. This
study proposes LITMC-BERT, a transformer-based multi-label classification
method in biomedical literature. It uses a shared transformer backbone for all
the labels while also captures label-specific features and the correlations
between label pairs. We compare LITMC-BERT with three baseline models on two
datasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the
current best results, respectively, and only requires ~18% of the inference
time than the Binary BERT baseline. The related datasets and models are
available via https://github.com/ncbi/ml-transformer.

### Title: "Flux+Mutability": A Conditional Generative Approach to One-Class Classification and Anomaly Detection
* Paper ID: 2204.08609v1
* Paper URL: [http://arxiv.org/abs/2204.08609v1](http://arxiv.org/abs/2204.08609v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Anomaly Detection is becoming increasingly popular within the experimental
physics community. At experiments such as the Large Hadron Collider, anomaly
detection is at the forefront of finding new physics beyond the Standard Model.
This paper details the implementation of a novel Machine Learning architecture,
called Flux+Mutability, which combines cutting-edge conditional generative
models with clustering algorithms. In the `flux' stage we learn the
distribution of a reference class. The `mutability' stage at inference
addresses if data significantly deviates from the reference class. We
demonstrate the validity of our approach and its connection to multiple
problems spanning from one-class classification to anomaly detection. In
particular, we apply our method to the isolation of neutral showers in an
electromagnetic calorimeter and show its performance in detecting anomalous
dijets events from standard QCD background. This approach limits assumptions on
the reference sample and remains agnostic to the complementary class of objects
of a given problem. We describe the possibility of dynamically generating a
reference population and defining selection criteria via quantile cuts.
Remarkably this flexible architecture can be deployed for a wide range of
problems, and applications like multi-class classification or data quality
control are left for further exploration.

### Title: G2GT: Retrosynthesis Prediction with Graph to Graph Attention Neural Network and Self-Training
* Paper ID: 2204.08608v1
* Paper URL: [http://arxiv.org/abs/2204.08608v1](http://arxiv.org/abs/2204.08608v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Retrosynthesis prediction is one of the fundamental challenges in organic
chemistry and related fields. The goal is to find reactants molecules that can
synthesize product molecules. To solve this task, we propose a new
graph-to-graph transformation model, G2GT, in which the graph encoder and graph
decoder are built upon the standard transformer structure. We also show that
self-training, a powerful data augmentation method that utilizes unlabeled
molecule data, can significantly improve the model's performance. Inspired by
the reaction type label and ensemble learning, we proposed a novel weak
ensemble method to enhance diversity. We combined beam search, nucleus, and
top-k sampling methods to further improve inference diversity and proposed a
simple ranking algorithm to retrieve the final top-10 results. We achieved new
state-of-the-art results on both the USPTO-50K dataset, with top1 accuracy of
54%, and the larger data set USPTO-full, with top1 accuracy of 50%, and
competitive top-10 results.

### Title: Adaptive Noisy Data Augmentation for Regularized Estimation and Inference in Generalized Linear Models
* Paper ID: 2204.08574v1
* Paper URL: [http://arxiv.org/abs/2204.08574v1](http://arxiv.org/abs/2204.08574v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We propose the AdaPtive Noise Augmentation (PANDA) procedure to regularize
the estimation and inference of generalized linear models (GLMs). PANDA
iteratively optimizes the objective function given noise augmented data until
convergence to obtain the regularized model estimates. The augmented noises are
designed to achieve various regularization effects, including $l_0$, bridge
(lasso and ridge included), elastic net, adaptive lasso, and SCAD, as well as
group lasso and fused ridge. We examine the tail bound of the noise-augmented
loss function and establish the almost sure convergence of the noise-augmented
loss function and its minimizer to the expected penalized loss function and its
minimizer, respectively. We derive the asymptotic distributions for the
regularized parameters, based on which, inferences can be obtained
simultaneously with variable selection. PANDA exhibits ensemble learning
behaviors that help further decrease the generalization error. Computationally,
PANDA is easy to code, leveraging existing software for implementing GLMs,
without resorting to complicated optimization techniques. We demonstrate the
superior or similar performance of PANDA against the existing approaches of the
same type of regularizers in simulated and real-life data. We show that the
inferences through PANDA achieve nominal or near-nominal coverage and are far
more efficient compared to a popular existing post-selection procedure.

### Title: Imagination-Augmented Natural Language Understanding
* Paper ID: 2204.08535v1
* Paper URL: [http://arxiv.org/abs/2204.08535v1](http://arxiv.org/abs/2204.08535v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.

### Title: How large are the monomers of dust aggregates in planet-forming disks?: Insights from quantitative optical and near-infrared polarimetry
* Paper ID: 2204.08506v1
* Paper URL: [http://arxiv.org/abs/2204.08506v1](http://arxiv.org/abs/2204.08506v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Context: The size of the constituent particles (monomers) of dust aggregates
is one of the most uncertain parameters directly affecting collisional growth
of aggregates in planet-forming disks. Despite its importance, the monomer size
has not yet been meaningfully constrained by disk observations. Aims: We
attempt to derive the monomer size from optical and near-infrared (IR)
polarimetric observations of planet-forming disks. Methods: We perform a
comprehensive parameter survey on the degree of linear polarization of light
scattered by dust aggregates, using an exact numerical method called the
$T$-matrix method. We investigate the effect of the monomer size, aggregate
size, porosity, and composition on the degree of polarization. The obtained
results are then compared with observed polarization fractions of several
planet-forming disks at optical and near-IR wavelengths. Results: It is shown
that the degree of polarization of aggregates depends sensitively on the
monomer size unless the monomer size parameter is smaller than one or two.
Comparing the simulation results with the disk observations, we find that the
monomer radius is no greater than $0.4~\mu$m. The inferred monomer size is
therefore similar to subunit sizes of the solar system dust aggregates and the
maximum size of interstellar grains. Conclusions: Optical and near-IR
quantitative polarimetry will provide observational grounds on the initial
conditions for dust coagulation and thereby planetesimal formation in
planet-forming disks.

### Title: Neural Space-filling Curves
* Paper ID: 2204.08453v1
* Paper URL: [http://arxiv.org/abs/2204.08453v1](http://arxiv.org/abs/2204.08453v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We present Neural Space-filling Curves (SFCs), a data-driven approach to
infer a context-based scan order for a set of images. Linear ordering of pixels
forms the basis for many applications such as video scrambling, compression,
and auto-regressive models that are used in generative modeling for images.
Existing algorithms resort to a fixed scanning algorithm such as Raster scan or
Hilbert scan. Instead, our work learns a spatially coherent linear ordering of
pixels from the dataset of images using a graph-based neural network. The
resulting Neural SFC is optimized for an objective suitable for the downstream
task when the image is traversed along with the scan line order. We show the
advantage of using Neural SFCs in downstream applications such as image
compression. Code and additional results will be made available at
https://hywang66.github.io/publication/neuralsfc.

### Title: Constraining runaway dilaton models using joint gravitational-wave and electromagnetic observations
* Paper ID: 2204.08445v1
* Paper URL: [http://arxiv.org/abs/2204.08445v1](http://arxiv.org/abs/2204.08445v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: With the advent of gravitational-wave astronomy it has now been possible to
constrain modified theories of gravity that were invoked to explain the dark
energy. In a class of dilaton models, distances to cosmic sources inferred from
electromagnetic and gravitational wave observations would differ due to the
presence of a friction term. In such theories, the ratio of the Newton's
constant to the fine structure constant varies with time. In this paper we
explore the degree to which it will be possible to test such models. If
collocated sources (e.g. supernovae and binary neutron star mergers), but not
necessarily multimessengers, can be identified by electromagnetic telescopes
and gravitational-wave detectors one can probe if light and gravitational
radiation are subject to the same laws of propagation over cosmological
distances. This helps in constraining the variation of Newton's constant
relative to fine-structure constant. The next generation of gravitational wave
detectors, such as the Cosmic Explorer and Einstein Telescope, in tandem with
the Vera Rubin Observatory and gamma ray observatories such as the Fermi Space
Observatory will be able to detect or constrain such variations at the level of
a few parts in 100. We apply this method to GW170817 with distances inferred by
the LIGO and Virgo detectors and the observed Kilonova.

### Title: Bounds on skewness and kurtosis of steady state currents
* Paper ID: 2204.08421v2
* Paper URL: [http://arxiv.org/abs/2204.08421v2](http://arxiv.org/abs/2204.08421v2)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Current fluctuations are a powerful tool to unreaveal the underlying physics
of the observed transport process. This work discusses some general properties
of the third and the fourth cumulant of the current (skewness and kurtosis)
related to dynamics and thermodynamics of a transport setup. Specifically,
several distinct bounds on these quanities are either analytically derived or
numerically conjectured, which are applicable to: 1) noninteracting fermionic
systems, 2) nonintereacting bosonic systems, 3) thermally driven classical
Markovian systems, 4) unicyclic Markovian networks. Finally, it is demonstrated
that violation of the obtained bounds can be used to infer the presence of
dynamical channel blockade in multilevel quantum dots, electron pairing in
normal metal-superconductor junctions or coherent dynamics and interactions in
quantum dot molecules even close to equilibrium, when the shot noise - a
standard signature of these phenomena - is masked by the thermal noise.

### Title: Temporally Efficient Vision Transformer for Video Instance Segmentation
* Paper ID: 2204.08412v1
* Paper URL: [http://arxiv.org/abs/2204.08412v1](http://arxiv.org/abs/2204.08412v1)
* Updated Date: 2022-04-18
* Code URL: [https://github.com/hustvl/tevit](https://github.com/hustvl/tevit)
* Summary: Recently vision transformer has achieved tremendous success on image-level
visual recognition tasks. To effectively and efficiently model the crucial
temporal information within a video clip, we propose a Temporally Efficient
Vision Transformer (TeViT) for video instance segmentation (VIS). Different
from previous transformer-based VIS methods, TeViT is nearly convolution-free,
which contains a transformer backbone and a query-based video instance
segmentation head. In the backbone stage, we propose a nearly parameter-free
messenger shift mechanism for early temporal context fusion. In the head
stages, we propose a parameter-shared spatiotemporal query interaction
mechanism to build the one-to-one correspondence between video instances and
queries. Thus, TeViT fully utilizes both framelevel and instance-level temporal
context information and obtains strong temporal modeling capacity with
negligible extra computational cost. On three widely adopted VIS benchmarks,
i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains
state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with
68.9 FPS on YouTube-VIS-2019. Code is available at
https://github.com/hustvl/TeViT.

### Title: Dynamic Network Adaptation at Inference
* Paper ID: 2204.08400v1
* Paper URL: [http://arxiv.org/abs/2204.08400v1](http://arxiv.org/abs/2204.08400v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Machine learning (ML) inference is a real-time workload that must comply with
strict Service Level Objectives (SLOs), including latency and accuracy targets.
Unfortunately, ensuring that SLOs are not violated in inference-serving systems
is challenging due to inherent model accuracy-latency tradeoffs, SLO diversity
across and within application domains, evolution of SLOs over time,
unpredictable query patterns, and co-location interference. In this paper, we
observe that neural networks exhibit high degrees of per-input activation
sparsity during inference. . Thus, we propose SLO-Aware Neural Networks which
dynamically drop out nodes per-inference query, thereby tuning the amount of
computation performed, according to specified SLO optimization targets and
machine utilization. SLO-Aware Neural Networks achieve average speedups of
$1.3-56.7\times$ with little to no accuracy loss (less than 0.3%). When
accuracy constrained, SLO-Aware Neural Networks are able to serve a range of
accuracy targets at low latency with the same trained model. When latency
constrained, SLO-Aware Neural Networks can proactively alleviate latency
degradation from co-location interference while maintaining high accuracy to
meet latency constraints.

### Title: Fast and Memory-Efficient Network Towards Efficient Image Super-Resolution
* Paper ID: 2204.08397v1
* Paper URL: [http://arxiv.org/abs/2204.08397v1](http://arxiv.org/abs/2204.08397v1)
* Updated Date: 2022-04-18
* Code URL: [https://github.com/nju-jet/fmen](https://github.com/nju-jet/fmen)
* Summary: Runtime and memory consumption are two important aspects for efficient image
super-resolution (EISR) models to be deployed on resource-constrained devices.
Recent advances in EISR exploit distillation and aggregation strategies with
plenty of channel split and concatenation operations to make full use of
limited hierarchical features. In contrast, sequential network operations avoid
frequently accessing preceding states and extra nodes, and thus are beneficial
to reducing the memory consumption and runtime overhead. Following this idea,
we design our lightweight network backbone by mainly stacking multiple highly
optimized convolution and activation layers and decreasing the usage of feature
fusion. We propose a novel sequential attention branch, where every pixel is
assigned an important factor according to local and global contexts, to enhance
high-frequency details. In addition, we tailor the residual block for EISR and
propose an enhanced residual block (ERB) to further accelerate the network
inference. Finally, combining all the above techniques, we construct a fast and
memory-efficient network (FMEN) and its small version FMEN-S, which runs 33%
faster and reduces 74% memory consumption compared with the state-of-the-art
EISR model: E-RFDN, the champion in AIM 2020 efficient super-resolution
challenge. Besides, FMEN-S achieves the lowest memory consumption and the
second shortest runtime in NTIRE 2022 challenge on efficient super-resolution.
Code is available at https://github.com/NJU-Jet/FMEN.

### Title: StableMoE: Stable Routing Strategy for Mixture of Experts
* Paper ID: 2204.08396v1
* Paper URL: [http://arxiv.org/abs/2204.08396v1](http://arxiv.org/abs/2204.08396v1)
* Updated Date: 2022-04-18
* Code URL: [https://github.com/hunter-ddm/stablemoe](https://github.com/hunter-ddm/stablemoe)
* Summary: The Mixture-of-Experts (MoE) technique can scale up the model size of
Transformers with an affordable computational overhead. We point out that
existing learning-to-route MoE methods suffer from the routing fluctuation
issue, i.e., the target expert of the same input may change along with
training, but only one expert will be activated for the input during inference.
The routing fluctuation tends to harm sample efficiency because the same input
updates different experts but only one is finally used. In this paper, we
propose StableMoE with two training stages to address the routing fluctuation
problem. In the first training stage, we learn a balanced and cohesive routing
strategy and distill it into a lightweight router decoupled from the backbone
model. In the second training stage, we utilize the distilled router to
determine the token-to-expert assignment and freeze it for a stable routing
strategy. We validate our method on language modeling and multilingual machine
translation. The results show that StableMoE outperforms existing MoE methods
in terms of both convergence speed and performance.

### Title: Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes
* Paper ID: 2204.08356v1
* Paper URL: [http://arxiv.org/abs/2204.08356v1](http://arxiv.org/abs/2204.08356v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: This paper considers the problem of inference in cluster randomized
experiments when cluster sizes are non-ignorable. Here, by a cluster randomized
experiment, we mean one in which treatment is assigned at the level of the
cluster; by non-ignorable cluster sizes we mean that "large" clusters and
"small" clusters may be heterogeneous, and, in particular, the effects of the
treatment may vary across clusters of differing sizes. In order to permit this
sort of flexibility, we consider a sampling framework in which cluster sizes
themselves are random. In this way, our analysis departs from earlier analyses
of cluster randomized experiments in which cluster sizes are treated as
non-random. We distinguish between two different parameters of interest: the
equally-weighted cluster-level average treatment effect, and the size-weighted
cluster-level average treatment effect. For each parameter, we provide methods
for inference in an asymptotic framework where the number of clusters tends to
infinity and treatment is assigned using simple random sampling. We
additionally permit the experimenter to sample only a subset of the units
within each cluster rather than the entire cluster and demonstrate the
implications of such sampling for some commonly used estimators. A small
simulation study shows the practical relevance of our theoretical results.

### Title: Using Statistical Emulation and Knowledge of Grain-Surface Diffusion for Bayesian Inference of Reaction Rate Parameters: An Application to a Glycine Network
* Paper ID: 2204.08347v1
* Paper URL: [http://arxiv.org/abs/2204.08347v1](http://arxiv.org/abs/2204.08347v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: There exists much uncertainty surrounding interstellar grain-surface
chemistry. One of the major reaction mechanisms is grain-surface diffusion for
which the the binding energy parameter for each species needs to be known.
However, these values vary significantly across the literature which can lead
to debate as to whether or not a particular reaction takes place via diffusion.
In this work we employ Bayesian inference to use available ice abundances to
estimate the reaction rates of the reactions in a chemical network that
produces glycine. Using this we estimate the binding energy of a variety of
important species in the network, by assuming that the reactions take place via
diffusion. We use our understanding of the diffusion mechanism to reduce the
dimensionality of the inference problem from 49 to 14, by demonstrating that
reactions can be separated into classes. This dimensionality reduction makes
the problem computationally feasible. A neural network statistical emulator is
used to also help accelerate the Bayesian inference process substantially.
  The binding energies of most of the diffusive species of interest are found
to match some of the disparate literature values, with the exceptions of atomic
and diatomic hydrogen. The discrepancies with these two species are related to
limitations of the physical and chemical model. However, the use of a dummy
reaction of the form H + X -> HX is found to somewhat reduce the discrepancy
with the binding energy of atomic hydrogen. Using the inferred binding energies
in the full gas-grain version of UCLCHEM results in almost all the molecular
abundances being recovered.

### Title: StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts
* Paper ID: 2204.08292v1
* Paper URL: [http://arxiv.org/abs/2204.08292v1](http://arxiv.org/abs/2204.08292v1)
* Updated Date: 2022-04-18
* Code URL: [https://github.com/ZhengxiangShi/StepGame](https://github.com/ZhengxiangShi/StepGame)
* Summary: Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.

### Title: Precursors of fatty alcohols in the ISM: Discovery of n-propanol
* Paper ID: 2204.08267v1
* Paper URL: [http://arxiv.org/abs/2204.08267v1](http://arxiv.org/abs/2204.08267v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Theories on the origins of life propose that early cell membranes were
synthesized from amphiphilic molecules simpler than phospholipids such as fatty
alcohols. The discovery in the interstellar medium (ISM) of ethanolamine, the
simplest phospholipid head group, raises the question whether simple
amphiphilic molecules are also synthesized in space. We investigate whether
precursors of fatty alcohols are present in the ISM. For this, we have carried
out a spectral survey at 7, 3, 2 and 1 mm toward the Giant Molecular Cloud
G+0.693-0.027 located in the Galactic Center using the IRAM 30m and Yebes 40m
telescopes. Here, we report the detection in the ISM of the primary alcohol
n-propanol (in both conformers Ga-n-C3H7OH and Aa-n-C3H7OH), a precursor of
fatty alcohols. The derived column densities of n-propanol are (5.5+-0.4)x10^13
cm^-2 for the Ga conformer and (3.4+-0.3)x10^13 cm^-2 for the Aa conformer,
which imply molecular abundances of (4.1+-0.3)x10^-10 for Ga-n-C3H7OH and of
(2.5+-0.2)x10^-10 for Aa-n-C3H7OH. We also searched for the AGa conformer of
n-butanol (AGa-n-C4H9OH) without success yielding an upper limit to its
abundance of <4.1x10^-11. The inferred CH3OH:C2H5OH:C3H7OH:C4H9OH abundance
ratios go as 1:0.04:0.006:<0.0004 toward G+0.693-0.027, i.e. they decrease
roughly by one order of magnitude for increasing complexity. We also report the
detection of both syn and anti conformers of vinyl alcohol, with column
densities of (1.11+-0.08)x10^14 cm^-2 and (1.3+-0.4)x10^13 cm^-2, and
abundances of (8.2+-0.6)x10^-10 and (9.6+-3.0)x10^-11, respectively. The
detection of n-propanol, together with the recent discovery of ethanolamine in
the ISM, opens the possibility that precursors of lipids according to theories
of the origin of life, could have been brought to Earth from outer space.

### Title: High-resolution ALMA observations of transition disk candidates in Lupus
* Paper ID: 2204.08225v1
* Paper URL: [http://arxiv.org/abs/2204.08225v1](http://arxiv.org/abs/2204.08225v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Transition disks with small inner dust cavities are interesting targets for
the study of disk clearing mechanisms. Such disks have been identified through
a deficit in the infrared part of their SED, but spatially resolved millimeter
imaging is required to confirm the presence of an inner dust cavity. We use
high-resolution ALMA observations of 30 mas resolution in Band 6 continuum and
$^{12}$CO 2--1 emission of 10 transition disk candidates in the Lupus star
forming region, in order to confirm the presence of inner dust cavities and
infer the responsible mechanism. The continuum data are analyzed using
visibility modeling and the SEDs are compared with radiative transfer models.
Out of the six transition disk candidates selected from their SED, only one
disk revealed an inner dust cavity of 4 au in radius. Three of the other disks
are highly inclined, which limits the detectability of an inner dust cavity but
it is also demonstrated to be the possible cause for the infrared deficit in
their SED. The two remaining SED-selected disks are very compact, with dust
radii of only $\sim$3 au. From the four candidates selected from low-resolution
images, three new transition disks with large inner cavities $>$20 au are
identified, bringing the total number of transition disks with large cavities
in Lupus to 13. SED-selected transition disks with small cavities are biased
towards highly inclined and compact disks, which casts doubt on the use of
their occurrence rates in estimating dispersal timescales of photoevaporation.
Using newly derived disk dust masses and radii, we re-evaluate the
size-luminosity and $M_{\rm dust}-M_{\rm star}$ relations. These relations can
be understood if the bright disks are dominated by disks with substructure
whereas faint disks are dominated by drift-dominated disks. (Abridged)

### Title: Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge
* Paper ID: 2204.08189v1
* Paper URL: [http://arxiv.org/abs/2204.08189v1](http://arxiv.org/abs/2204.08189v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Adversarial example attack endangers the mobile edge systems such as vehicles
and drones that adopt deep neural networks for visual sensing. This paper
presents {\em Sardino}, an active and dynamic defense approach that renews the
inference ensemble at run time to develop security against the adaptive
adversary who tries to exfiltrate the ensemble and construct the corresponding
effective adversarial examples. By applying consistency check and data fusion
on the ensemble's predictions, Sardino can detect and thwart adversarial
inputs. Compared with the training-based ensemble renewal, we use HyperNet to
achieve {\em one million times} acceleration and per-frame ensemble renewal
that presents the highest level of difficulty to the prerequisite exfiltration
attacks. Moreover, the robustness of the renewed ensembles against adversarial
examples is enhanced with adversarial learning for the HyperNet. We design a
run-time planner that maximizes the ensemble size in favor of security while
maintaining the processing frame rate. Beyond adversarial examples, Sardino can
also address the issue of out-of-distribution inputs effectively. This paper
presents extensive evaluation of Sardino's performance in counteracting
adversarial examples and applies it to build a real-time car-borne traffic sign
recognition system. Live on-road tests show the built system's effectiveness in
maintaining frame rate and detecting out-of-distribution inputs due to the
false positives of a preceding YOLO-based traffic sign detector.

### Title: Robust End-to-end Speaker Diarization with Generic Neural Clustering
* Paper ID: 2204.08164v1
* Paper URL: [http://arxiv.org/abs/2204.08164v1](http://arxiv.org/abs/2204.08164v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: End-to-end speaker diarization approaches have shown exceptional performance
over the traditional modular approaches. To further improve the performance of
the end-to-end speaker diarization for real speech recordings, recently works
have been proposed which integrate unsupervised clustering algorithms with the
end-to-end neural diarization models. However, these methods have a number of
drawbacks: 1) The unsupervised clustering algorithms cannot leverage the
supervision from the available datasets; 2) The K-means-based unsupervised
algorithms that are explored often suffer from the constraint violation
problem; 3) There is unavoidable mismatch between the supervised training and
the unsupervised inference. In this paper, a robust generic neural clustering
approach is proposed that can be integrated with any chunk-level predictor to
accomplish a fully supervised end-to-end speaker diarization model. Also, by
leveraging the sequence modelling ability of a recurrent neural network, the
proposed neural clustering approach can dynamically estimate the number of
speakers during inference. Experimental show that when integrating an
attractor-based chunk-level predictor, the proposed neural clustering approach
can yield better Diarization Error Rate (DER) than the constrained
K-means-based clustering approaches under the mismatched conditions.

