### Title: Understanding and Preventing Capacity Loss in Reinforcement Learning
* Paper ID: 2204.09560v1
* Paper URL: [http://arxiv.org/abs/2204.09560v1](http://arxiv.org/abs/2204.09560v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The reinforcement learning (RL) problem is rife with sources of
non-stationarity, making it a notoriously difficult problem domain for the
application of neural networks. We identify a mechanism by which non-stationary
prediction targets can prevent learning progress in deep RL agents:
\textit{capacity loss}, whereby networks trained on a sequence of target values
lose their ability to quickly update their predictions over time. We
demonstrate that capacity loss occurs in a range of RL agents and environments,
and is particularly damaging to performance in sparse-reward tasks. We then
present a simple regularizer, Initial Feature Regularization (InFeR), that
mitigates this phenomenon by regressing a subspace of features towards its
value at initialization, leading to significant performance improvements in
sparse-reward environments such as Montezuma's Revenge. We conclude that
preventing capacity loss is crucial to enable agents to maximally benefit from
the learning signals they obtain throughout the entire training trajectory.

### Title: Generalizing to the Future: Mitigating Entity Bias in Fake News Detection
* Paper ID: 2204.09484v1
* Paper URL: [http://arxiv.org/abs/2204.09484v1](http://arxiv.org/abs/2204.09484v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/ictmcg/endef-sigir2022](https://github.com/ictmcg/endef-sigir2022)
* Summary: The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.

### Title: A Generalisable Data Fusion Framework to Infer Mode of Transport Using Mobile Phone Data
* Paper ID: 2204.09482v1
* Paper URL: [http://arxiv.org/abs/2204.09482v1](http://arxiv.org/abs/2204.09482v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Cities often lack up-to-date data analytics to evaluate and implement
transport planning interventions to achieve sustainability goals, as
traditional data sources are expensive, infrequent, and suffer from data
latency. Mobile phone data provide an inexpensive source of geospatial
information to capture human mobility at unprecedented geographic and temporal
granularity. This paper proposes a method to estimate updated mode of
transportation usage in a city, with novel usage of mobile phone application
traces to infer previously hard to detect modes, such as bikes and
ride-hailing/taxi. By using data fusion and matrix factorisation, we integrate
socioeconomic and demographic attributes of the local resident population into
the model. We tested the method in a case study of Santiago (Chile), and found
that changes from 2012 to 2020 in mode of transportation inferred by the method
are coherent with expectations from domain knowledge and the literature, such
as ride-hailing trips replacing mass transport.

### Title: On the relative asymptotic expressivity of inference frameworks
* Paper ID: 2204.09457v1
* Paper URL: [http://arxiv.org/abs/2204.09457v1](http://arxiv.org/abs/2204.09457v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Let $\sigma$ be a first-order signature and let $\mathbf{W}_n$ be the set of
all $\sigma$-structures with domain $[n] = \{1, \ldots, n\}$. We can think of
each structure in $\mathbf{W}_n$ as representing a "possible (state of the)
world". By an inference framework we mean a class $\mathbf{F}$ of pairs
$(\mathbb{P}, L)$, where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$
and each $\mathbb{P}_n$ is a probability distribution on $\mathbb{W}_n$, and
$L$ is a logic with truth values in the unit interval $[0, 1]$.
  From the point of view of probabilistic and logical expressivity one may
consider an inference framework as optimal if it allows any pair $(\mathbb{P},
L)$ where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$ is a sequence of
probability distributions on $\mathbb{W}_n$ and $L$ is a logic. But from the
point of view of using a pair $(\mathbb{P}, L)$ from such an inference
framework for making inferences on $\mathbb{W}_n$ when $n$ is large we face the
problem of computational complexity. This motivates looking for an "optimal"
trade-off (in a given context) between expressivity and computational
efficiency.
  We define a notion that an inference framework is "asymptotically at least as
expressive" as another inference framework. This relation is a preorder and we
describe a (strict) partial order on the equivalence classes of some inference
frameworks that in our opinion are natural in the context of machine learning
and artificial intelligence. The results have bearing on issues concerning
efficient learning and probabilistic inference, but are also new instances of
results in finite model theory about "almost sure elimination" of extra
syntactic features (e.g quantifiers) beyond the connectives. Often such a
result has a logical convergence law as a corollary.

### Title: Effective Goal-oriented 6G Communications: the Energy-aware Edge Inferencing Case
* Paper ID: 2204.09447v1
* Paper URL: [http://arxiv.org/abs/2204.09447v1](http://arxiv.org/abs/2204.09447v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Currently, the world experiences an unprecedentedly increasing generation of
application data, from sensor measurements to video streams, thanks to the
extreme connectivity capability provided by 5G networks. Going beyond 5G
technology, such data aim to be ingested by Artificial Intelligence (AI)
functions instantiated in the network to facilitate informed decisions,
essential for the operation of applications, such as automated driving and
factory automation. Nonetheless, while computing platforms hosting Machine
Learning (ML) models are ever powerful, their energy footprint is a key
impeding factor towards realizing a wireless network as a sustainable
intelligent platform. Focusing on a beyond 5G wireless network, overlaid by a
Multi-access Edge Computing (MEC) infrastructure with inferencing capabilities,
our paper tackles the problem of energy-aware dependable inference by
considering inference effectiveness as value of a goal that needs to be
accomplished by paying the minimum price in energy consumption. Both
MEC-assisted standalone and ensemble inference options are evaluated. It is
shown that, for some system scenarios, goal effectiveness above 84% is achieved
and sustained even by relaxing communication reliability requirements by one
decimal digit, while enjoying a device radio energy consumption reduction of
almost 23% at the same time. Also, ensemble inference is shown to improve
system-wide energy efficiency and even achieve higher goal effectiveness, as
compared to the standalone case for some system parameterizations.

### Title: GIMO: Gaze-Informed Human Motion Prediction in Context
* Paper ID: 2204.09443v1
* Paper URL: [http://arxiv.org/abs/2204.09443v1](http://arxiv.org/abs/2204.09443v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with eye gaze that serves as a surrogate
for inferring human intent. By employing inertial sensors for motion capture,
our data collection is not tied to specific scenes, which further boosts the
motion dynamics observed from our subjects. We perform an extensive study of
the benefits of leveraging eye gaze for ego-centric human motion prediction
with various state-of-the-art architectures. Moreover, to realize the full
potential of gaze, we propose a novel network architecture that enables
bidirectional communication between the gaze and motion branches. Our network
achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from the gaze and the denoised gaze
feature modulated by the motion. The proposed dataset and our network
implementation will be publicly available.

### Title: Multi-Auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation
* Paper ID: 2204.09422v1
* Paper URL: [http://arxiv.org/abs/2204.09422v1](http://arxiv.org/abs/2204.09422v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Recommending appropriate tags to items can facilitate content organization,
retrieval, consumption and other applications, where hybrid tag recommender
systems have been utilized to integrate collaborative information and content
information for better recommendations. In this paper, we propose a
multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for
tag recommendation, which couples item collaborative information and item
multi-auxiliary information, i.e., content and social graph, by defining a
generative process. Specifically, the model learns deep latent embeddings from
different item auxiliary information using variational auto-encoders (VAE),
which could form a generative distribution over each auxiliary information by
introducing a latent variable parameterized by deep neural network. Moreover,
to recommend tags for new items, item multi-auxiliary latent embeddings are
utilized as a surrogate through the item decoder for predicting recommendation
probabilities of each tag, where reconstruction losses are added in the
training phase to constrict the generation for feedback predictions via
different auxiliary embeddings. In addition, an inductive variational graph
auto-encoder is designed where new item nodes could be inferred in the test
phase, such that item social embeddings could be exploited for new items.
Extensive experiments on MovieLens and citeulike datasets demonstrate the
effectiveness of our method.

### Title: Distance and age of the massive stellar cluster Westerlund 1. I. Parallax method using Gaia-EDR3
* Paper ID: 2204.09414v1
* Paper URL: [http://arxiv.org/abs/2204.09414v1](http://arxiv.org/abs/2204.09414v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Westerlund 1 (Wd 1) is one of the most massive young star clusters in the
Milky Way. Although relevant for star formation and evolution, its fundamental
parameters are not yet very well constrained. Our goal is to derive an accurate
distance and provide constraints on the cluster age. We used the photometric
and astrometric information available in the Gaia Early Data Release 3
(Gaia-EDR3) to infer its distance of 4.06$^{+0.36}_{-0.34}$ kpc. Modelling of
the eclipsing binary system W36 reported in Paper II led to the distance of
4.34$\pm$0.25 kpc, in agreement with the Gaia-EDR3 distance and, therefore,
validating the parallax zero-point correction approach appropriate for red
objects. By taking advantage of another two recent distance determinations
using the Gaia-EDR3, we obtained a weighted mean distance for the cluster as
d$_{\rm wd1}$=4.23$^{+0.15}_{-0.13}$ kpc ($m-M$=13.13$^{+0.08}_{-0.07}$ mag),
which has an unprecedented accuracy of 4\%. We adopted recent Geneva
evolutionary tracks for supra-solar metallicity objects to infer the age of the
faintest RSG source from Wd 1, leading to a cluster age of 11.0$\pm$0.5 Myr, in
excellent agreement with recent work by Beasor \& Davies (10.4$^{+1.3}_{-1.2}$
Myr) based on MIST evolutionary models. The age of W36 was reported to be
3.5$\pm$0.5 Myr in Paper II, supporting recent claims of a temporal spread of
several Myr for the star-forming process within Wd 1 instead of a monolithic
starburst scenario.

### Title: Euclid detectability of pair instability supernovae in binary population synthesis models consistent with merging binary black holes
* Paper ID: 2204.09402v1
* Paper URL: [http://arxiv.org/abs/2204.09402v1](http://arxiv.org/abs/2204.09402v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We infer the expected detection number of pair instability supernovae (PISNe)
during the operation of the Euclid space telescope, based on two binary
population models that are consistent with binary black holes (BHs) observed by
gravitational waves. The two models consider different PISN criteria depending
on the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. The fiducial and
$3\sigma$ models adopt the standard and $3\sigma$-smaller $^{12}$C$(\alpha,
\gamma)^{16}$O reaction rate, which predicts that stars with helium core masses
$65-135 M_\odot$ and $90-180 M_\odot$ cause PISNe, respectively. Our fiducial
model predicts that Euclid detects several Type I or hydrogen-poor PISNe. For
the $3\sigma$ model, detection of $\sim 1$ Type I PISN by Euclid is expected if
the stellar mass distribution extends to $M_{\max} \sim 600 M_\odot$, but the
expected number becomes significantly smaller if $M_{\max} \sim 300 M_\odot$.
Thus, we may be able to prove or distinguish the fiducial and $3\sigma$ models
by the observed PISN rate. This will help us to constrain the origin of binary
BHs and the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. PISN ejecta mass
estimates from light curves and spectra obtained by follow-up observations
would also be important to constrain the $^{12}$C$(\alpha, \gamma)^{16}$O
reaction rate.

### Title: Electrostatic Dust Ejection From Asteroid (3200) Phaethon With the Aid of Mobile Alkali Ions at Perihelion
* Paper ID: 2204.09385v1
* Paper URL: [http://arxiv.org/abs/2204.09385v1](http://arxiv.org/abs/2204.09385v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The asteroid (3200) Phaethon is known to be the parent body of the Geminids,
although meteor showers are commonly associated with the activity of periodic
comets. What is most peculiar to the asteroid is its comet-like activity in the
ejection of micrometer-sized dust particles at every perihelion passage, while
the activity of the asteroid has never been identified outside the
near-perihelion zone at $0.14~\mathrm{au}$ from the Sun. From the theoretical
point of view, we argue that the activity of the asteroid is well explained by
the electrostatic lofting of micrometer-sized dust particles with the aid of
mobile alkali ions at high temperatures. The mass-loss rates of
micrometer-sized particles from the asteroid in our model is entirely
consistent with the values inferred from visible observations of Phaethon's
dust tail. For millimeter-sized particles, we predict three orders of
magnitudes higher mass-loss rates, which could also account for the total mass
of the Geminid meteoroid stream by the electrostatic lofting mechanism.

### Title: A Variational Autoencoder for Heterogeneous Temporal and Longitudinal Data
* Paper ID: 2204.09369v1
* Paper URL: [http://arxiv.org/abs/2204.09369v1](http://arxiv.org/abs/2204.09369v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model's efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.

### Title: Multi-Component Imaging of the Fermi Gamma-ray Sky in the Spatio-spectral Domain
* Paper ID: 2204.09360v1
* Paper URL: [http://arxiv.org/abs/2204.09360v1](http://arxiv.org/abs/2204.09360v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We perform two distinct spatio-spectral reconstructions of the gamma-ray sky
in the range of 0.56-316 GeV based on Fermi Large Area Telescope (LAT) data.
Both describe the sky brightness to be composed of a diffuse-emission and a
point-source component. The first model requires minimal assumptions and
provides a template-free reconstruction as a reference. It makes use of spatial
and spectral correlations to distinguish between the different components. The
second model is physics-informed and further differentiates between diffuse
emission of hadronic and leptonic origin. For this, we assume parametric, but
spatially varying energy spectra to distinguish between the processes and use
thermal Galactic dust observations to indicate the preferred sites of hadronic
interactions. To account for instrumental effects we model the point-spread,
the energy dispersion, and the exposure of the telescope throughout the
observation. The reconstruction problem is formulated as a Bayesian inference
task, that is solved by variational inference. We show decompositions of the
Gamma-ray flux into diffuse and point-like emissions, and of the diffuse
emissions into multiple physically motivated components. The diffuse
decomposition provides an unprecedented view of the Galactic leptonic diffuse
emission. It shows the Fermi bubbles and their spectral variations in high
fidelity and other areas exhibiting strong cosmic ray electron contents, such
as a thick disk in the inner Galaxy and outflow regions. Furthermore, we report
a hard spectrum gamma ray arc in the northern outer bubble co-spatial with the
reported X-ray arc by the eROSITA collaboration. All our spatio-spectral sky
reconstructions and their uncertainty quantification are publicly available.

### Title: Inferring entropy production in anharmonic Brownian gyrators
* Paper ID: 2204.09283v1
* Paper URL: [http://arxiv.org/abs/2204.09283v1](http://arxiv.org/abs/2204.09283v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A non-vanishing entropy production rate is one of the defining
characteristics of any non-equilibrium system, and several techniques exist to
determine this quantity directly from experimental data. The short-time
inference scheme, derived from the thermodynamic uncertainty relation, is a
recent addition to the list of these techniques. Here we apply this scheme to
quantify the entropy production rate in a class of microscopic heat engine
models called Brownian gyrators. In particular, we consider models with
anharmonic confining potentials. In these cases, the dynamical equations are
indelibly non-linear, and the exact dependences of the entropy production rate
on the model parameters are unknown. Our results demonstrate that the
short-time inference scheme can efficiently determine these dependencies from a
moderate amount of trajectory data. Furthermore, the results show that the
non-equilibrium properties of the gyrator model with anharmonic confining
potentials are considerably different from its harmonic counterpart -
especially in set-ups leading to a non-equilibrium dynamics and the resulting
gyration patterns.

### Title: A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond
* Paper ID: 2204.09269v1
* Paper URL: [http://arxiv.org/abs/2204.09269v1](http://arxiv.org/abs/2204.09269v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.

### Title: The effect of spatial resolution on magnetic field modeling and helicity computation
* Paper ID: 2204.09267v1
* Paper URL: [http://arxiv.org/abs/2204.09267v1](http://arxiv.org/abs/2204.09267v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Nonlinear force-free (NLFF) modeling is regularly used in order to indirectly
infer the 3D geometry of the coronal magnetic field, not accessible on a
regular basis by means of direct measurements otherwise. We study the effect of
binning in time series NLFF modeling of individual active regions (ARs) in
order to quantify the effect of a different underlying spatial resolution on
the quality of modeling as well as on the derived physical parameters. We apply
an optimization method to sequences of SDO/HMI vector magnetogram data at three
different spatial resolutions for three solar ARs to obtain nine NLFF model
time series. From the NLFF models, we deduce active-region magnetic fluxes,
electric currents, magnetic energies and relative helicities, and analyze those
with respect to the underlying spatial resolution. We calculate various metrics
to quantify the quality of the derived NLFF models and apply a Helmholtz
decomposition to characterize solenoidal errors. At a given spatial resolution,
the quality of NLFF modeling is different for different ARs, as well as varies
along of the individual model time series. For a given AR, modeling at a given
spatial resolution is not necessarily of superior quality compared to that
performed at different spatial resolutions at all time instances of a NLFF
model time series. Generally, the NLFF model quality tends to be higher at
reduced spatial resolution with the solenoidal quality being the ultimate cause
for systematic variations in model-deduced physical quantities.
Optimization-based modeling based on binned SDO/HMI vector data delivers
magnetic energies and helicity estimates different by $\lesssim$30\%, given
that concise checks ensure the physical plausibility and high solenoidal
quality of the tested model. Spatial-resolution induced differences are
relatively small compared to that arising from other sources of uncertainty.

### Title: Details in BeiDou-G2: Past and Present
* Paper ID: 2204.09258v1
* Paper URL: [http://arxiv.org/abs/2204.09258v1](http://arxiv.org/abs/2204.09258v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: In January 2022, the defunct satellite BeiDou-G2 was pulled out of
geostationary orbit by Shijian-21 to a graveyard orbit. For safe docking and
operation, it was necessary to determine the rotation state in advance. In this
paper, we show the evolution of the rotation of the BeiDou-G2 satellite based
on the photometry observation data for the past 10 years. The rotational speed
of BeiDou-G2 was found to be annual oscillation, mainly due to the solar
radiation. Based on the evolution of BeiDou-G2's rotation speed and its orbit,
we confirmed that in the last 10 years, the satellite had six abnormal events.
These abnormal events were mainly due to the increase in the rotation speed
caused by suspected fuel leakages. Additionally, the abnormal events included
one collision in 2012, which was inferred to be the trigger of the fuel
leakages in the following years. No rotational speed abnormalities occurred
again after 2017, probably due to the complete release of the residual fuel.
The parameters and the propagating models after one incidence of solar panel
damage in 2014 and one fragment in 2016 were believed to be able to satisfy the
accuracy requirements of the rotation state well at the moment of docking,
which was ultimately confirmed by Shijian-21.

### Title: Compositional Semantics and Inference System for Temporal Order based on Japanese CCG
* Paper ID: 2204.09245v1
* Paper URL: [http://arxiv.org/abs/2204.09245v1](http://arxiv.org/abs/2204.09245v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Natural Language Inference (NLI) is the task of determining whether a premise
entails a hypothesis. NLI with temporal order is a challenging task because
tense and aspect are complex linguistic phenomena involving interactions with
temporal adverbs and temporal connectives. To tackle this, temporal and
aspectual inference has been analyzed in various ways in the field of formal
semantics. However, a Japanese NLI system for temporal order based on the
analysis of formal semantics has not been sufficiently developed. We present a
logic-based NLI system that considers temporal order in Japanese based on
compositional semantics via Combinatory Categorial Grammar (CCG) syntactic
analysis. Our system performs inference involving temporal order by using
axioms for temporal relations and automated theorem provers. We evaluate our
system by experimenting with Japanese NLI datasets that involve temporal order.
We show that our system outperforms previous logic-based systems as well as
current deep learning-based models.

### Title: Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers
* Paper ID: 2204.09225v1
* Paper URL: [http://arxiv.org/abs/2204.09225v1](http://arxiv.org/abs/2204.09225v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: How to identify and characterize functional brain networks (BN) is
fundamental to gain system-level insights into the mechanisms of brain
organizational architecture. Current functional magnetic resonance (fMRI)
analysis highly relies on prior knowledge of specific patterns in either
spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain.
In addition, most approaches aim to find group-wise common functional networks,
individual-specific functional networks have been rarely studied. In this work,
we propose a novel Twin-Transformers framework to simultaneously infer common
and individual functional networks in both spatial and temporal space, in a
self-supervised manner. The first transformer takes space-divided information
as input and generates spatial features, while the second transformer takes
time-related information as input and outputs temporal features. The spatial
and temporal features are further separated into common and individual ones via
interactions (weights sharing) and constraints between the two transformers. We
applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI
dataset and identified multiple common brain networks, including both
task-related and resting-state networks (e.g., default mode network).
Interestingly, we also successfully recovered a set of individual-specific
networks that are not related to task stimulus and only exist at the individual
level.

### Title: Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
* Paper ID: 2204.09204v1
* Paper URL: [http://arxiv.org/abs/2204.09204v1](http://arxiv.org/abs/2204.09204v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.

### Title: Functional Calibration under Non-Probability Survey Sampling
* Paper ID: 2204.09193v1
* Paper URL: [http://arxiv.org/abs/2204.09193v1](http://arxiv.org/abs/2204.09193v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-probability sampling is prevailing in survey sampling, but ignoring its
selection bias leads to erroneous inferences. We offer a unified nonparametric
calibration method to estimate the sampling weights for a non-probability
sample by calibrating functions of auxiliary variables in a reproducing kernel
Hilbert space. The consistency and the limiting distribution of the proposed
estimator are established, and the corresponding variance estimator is also
investigated. Compared with existing works, the proposed method is more robust
since no parametric assumption is made for the selection mechanism of the
non-probability sample. Numerical results demonstrate that the proposed method
outperforms its competitors, especially when the model is misspecified. The
proposed method is applied to analyze the average total cholesterol of Korean
citizens based on a non-probability sample from the National Health Insurance
Sharing Service and a reference probability sample from the Korea National
Health and Nutrition Examination Survey.

### Title: Emu: A Case Study for TDI-like Imaging for Infrared Observation from Space
* Paper ID: 2204.08713v2
* Paper URL: [http://arxiv.org/abs/2204.08713v2](http://arxiv.org/abs/2204.08713v2)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A wide-field zenith-looking telescope operating in a mode similar to
Time-Delay-Integration (TDI) or drift scan imaging can perform an infrared sky
survey without active pointing control but it requires a high-speed, low-noise
infrared detector. Operating from a hosted payload platform on the
International Space Station (ISS), the Emu space telescope employs the
paradigm-changing properties of the Leonardo SAPHIRA electron avalanche
photodiode array to provide powerful new observations of cool stars at the
critical water absorption wavelength (1.4 $\mu$m) largely inaccessible to
ground-based telescopes due to the Earth's own atmosphere. Cool stars,
especially those of spectral-type M, are important probes across contemporary
astrophysics, from the formation history of the Galaxy to the formation of
rocky exoplanets. Main sequence M-dwarf stars are the most abundant stars in
the Galaxy and evolved M-giant stars are some of the most distant stars that
can be individually observed. The Emu sky survey will deliver critical stellar
properties of these cool stars by inferring oxygen abundances via measurement
of the water absorption band strength at 1.4 $\mu$m. Here we present the
TDI-like imaging capability of Emu mission, its science objectives, instrument
details and simulation results.

### Title: Multiply-and-Fire (MNF): An Event-driven Sparse Neural Network Accelerator
* Paper ID: 2204.09797v1
* Paper URL: [http://arxiv.org/abs/2204.09797v1](http://arxiv.org/abs/2204.09797v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Machine learning, particularly deep neural network inference, has become a
vital workload for many computing systems, from data centers and HPC systems to
edge-based computing. As advances in sparsity have helped improve the
efficiency of AI acceleration, there is a continued need for improved system
efficiency for both high-performance and system-level acceleration.
  This work takes a unique look at sparsity with an event (or
activation-driven) approach to ANN acceleration that aims to minimize useless
work, improve utilization, and increase performance and energy efficiency. Our
analytical and experimental results show that this event-driven solution
presents a new direction to enable highly efficient AI inference for both CNN
and MLP workloads.
  This work demonstrates state-of-the-art energy efficiency and performance
centring on activation-based sparsity and a highly-parallel dataflow method
that improves the overall functional unit utilization (at 30 fps). This work
enhances energy efficiency over a state-of-the-art solution by 1.46$\times$.
Taken together, this methodology presents a novel, new direction to achieve
high-efficiency, high-performance designs for next-generation AI acceleration
platforms.

### Title: Estimating optimal individualized treatment rules with multistate processes
* Paper ID: 2204.09785v1
* Paper URL: [http://arxiv.org/abs/2204.09785v1](http://arxiv.org/abs/2204.09785v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Multistate process data are common in studies of chronic diseases such as
cancer. These data are ideal for precision medicine purposes as they can be
leveraged to improve more refined health outcomes, compared to standard
survival outcomes, as well as incorporate patient preferences regarding
quantity versus quality of life. However, there are currently no methods for
the estimation of optimal individualized treatment rules with such data. In
this article, we propose a nonparametric outcome weighted learning approach for
this problem in randomized clinical trial settings. The theoretical properties
of the proposed methods, including Fisher consistency and asymptotic normality
of the estimated expected outcome under the estimated optimal individualized
treatment rule, are rigorously established. A consistent closed-form variance
estimator is provided and methodology for the calculation of simultaneous
confidence intervals is proposed. Simulation studies show that the proposed
methodology and inference procedures work well even with small sample sizes and
high rates of right censoring. The methodology is illustrated using data from a
randomized clinical trial on the treatment of metastatic squamous-cell
carcinoma of the head and neck.

### Title: Time-based Self-supervised Learning for Wireless Capsule Endoscopy
* Paper ID: 2204.09773v1
* Paper URL: [http://arxiv.org/abs/2204.09773v1](http://arxiv.org/abs/2204.09773v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: State-of-the-art machine learning models, and especially deep learning ones,
are significantly data-hungry; they require vast amounts of manually labeled
samples to function correctly. However, in most medical imaging fields,
obtaining said data can be challenging. Not only the volume of data is a
problem, but also the imbalances within its classes; it is common to have many
more images of healthy patients than of those with pathology. Computer-aided
diagnostic systems suffer from these issues, usually over-designing their
models to perform accurately. This work proposes using self-supervised learning
for wireless endoscopy videos by introducing a custom-tailored method that does
not initially need labels or appropriate balance. We prove that using the
inferred inherent structure learned by our method, extracted from the temporal
axis, improves the detection rate on several domain-specific applications even
under severe imbalance.

### Title: A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines
* Paper ID: 2204.09772v1
* Paper URL: [http://arxiv.org/abs/2204.09772v1](http://arxiv.org/abs/2204.09772v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.

### Title: Inferring ice sheet damage models from limited observations using CRIKit: the Constitutive Relation Inference Toolkit
* Paper ID: 2204.09748v1
* Paper URL: [http://arxiv.org/abs/2204.09748v1](http://arxiv.org/abs/2204.09748v1)
* Updated Date: 2022-04-20
* Code URL: [https://gitlab.com/gbruer/ice-crikit](https://gitlab.com/gbruer/ice-crikit)
* Summary: We examine the prospect of learning ice sheet damage models from
observational data. Our approach, implemented in CRIKit (the Constitutive
Relation Inference Toolkit), is to model the material time derivative of damage
as a frame-invariant neural network, and to optimize the parameters of the
model from simulations of the flow of an ice dome. Using the model of Albrecht
and Levermann as the ground truth to generate synthetic observations, we
measure the difference of optimized neural network models from that model to
try to understand how well this process generates models that can then transfer
to other ice sheet simulations.
  The use of so-called "deep-learning" models for constitutive equations,
equations of state, sub-grid-scale processes, and other pointwise relations
that appear in systems of PDEs has been successful in other disciplines, yet
our inference setting has some confounding factors. The first is the type of
observations that are available: we compare the quality of the inferred models
when the loss of the numerical simulations includes observation misfits
throughout the ice, which is unobtainable in real settings, to losses that
include only combinations of surface and borehole observations. The second
confounding factor is the evolution of damage in an ice sheet, which is
advection dominated. The non-local effect of perturbations in a damage models
results in loss functions that have both many local minima and many parameter
configurations for which the system is unsolvable.
  Our experience suggests that basic neural networks have several deficiencies
that affect the quality of the optimized models. We suggest several approaches
to incorporating additional inductive biases into neural networks which may
lead to better performance in future work.

### Title: ColorCode: A Bayesian Approach to Augmentative and Alternative Communication with Two Buttons
* Paper ID: 2204.09745v1
* Paper URL: [http://arxiv.org/abs/2204.09745v1](http://arxiv.org/abs/2204.09745v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Many people with severely limited muscle control can only communicate through
augmentative and alternative communication (AAC) systems with a small number of
buttons. In this paper, we present the design for ColorCode, which is an AAC
system with two buttons that uses Bayesian inference to determine what the user
wishes to communicate. Our information-theoretic analysis of ColorCode
simulations shows that it is efficient in extracting information from the user,
even in the presence of errors, achieving nearly optimal error correction.
ColorCode is provided as open source software
(https://github.com/mrdaly/ColorCode).

### Title: A majorization-minimization algorithm for nonnegative binary matrix factorization
* Paper ID: 2204.09741v1
* Paper URL: [http://arxiv.org/abs/2204.09741v1](http://arxiv.org/abs/2204.09741v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: This paper tackles the problem of decomposing binary data using matrix
factorization. We consider the family of mean-parametrized Bernoulli models, a
class of generative models that are well suited for modeling binary data and
enables interpretability of the factors. We factorize the Bernoulli parameter
and consider an additional Beta prior on one of the factors to further improve
the model's expressive power. While similar models have been proposed in the
literature, they only exploit the Beta prior as a proxy to ensure a valid
Bernoulli parameter in a Bayesian setting; in practice it reduces to a uniform
or uninformative prior. Besides, estimation in these models has focused on
costly Bayesian inference. In this paper, we propose a simple yet very
efficient majorization-minimization algorithm for maximum a posteriori
estimation. Our approach leverages the Beta prior whose parameters can be tuned
to improve performance in matrix completion tasks. Experiments conducted on
three public binary datasets show that our approach offers an excellent
trade-off between prediction performance, computational complexity, and
interpretability.

### Title: Physical, subjective and analogical probability
* Paper ID: 2204.10159v1
* Paper URL: [http://arxiv.org/abs/2204.10159v1](http://arxiv.org/abs/2204.10159v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The aim of this paper is to show that the concept of probability is best
understood by dividing this concept into two different types of probability,
namely physical probability and analogical probability. Loosely speaking, a
physical probability is a probability that applies to the outcomes of an
experiment that have been judged as being equally likely on the basis of
physical symmetry. Physical probabilities are arguably in some sense
'objective' and possess all the standard properties of the concept of
probability. On the other hand, an analogical probability is defined by making
an analogy between the uncertainty surrounding an event of interest and the
uncertainty surrounding an event that has a physical probability. Analogical
probabilities are undeniably subjective probabilities and are not obliged to
have all the standard mathematical properties possessed by physical
probabilities, e.g. they may not have the property of additivity or obey the
standard definition of conditional probability. Nevertheless, analogical
probabilities have extra properties, which are not possessed by physical
probabilities, that assist in their direct elicitation, general derivation,
comparison and justification. More specifically, these properties facilitate
the application of analogical probability to real-world problems that can not
be adequately resolved by using only physical probability, e.g. probabilistic
inference about hypotheses on the basis of observed data. Careful definitions
are given of the concepts that are introduced and, where appropriate, examples
of the application of these concepts are presented for additional clarity.

### Title: Understanding and Preventing Capacity Loss in Reinforcement Learning
* Paper ID: 2204.09560v1
* Paper URL: [http://arxiv.org/abs/2204.09560v1](http://arxiv.org/abs/2204.09560v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The reinforcement learning (RL) problem is rife with sources of
non-stationarity, making it a notoriously difficult problem domain for the
application of neural networks. We identify a mechanism by which non-stationary
prediction targets can prevent learning progress in deep RL agents:
\textit{capacity loss}, whereby networks trained on a sequence of target values
lose their ability to quickly update their predictions over time. We
demonstrate that capacity loss occurs in a range of RL agents and environments,
and is particularly damaging to performance in sparse-reward tasks. We then
present a simple regularizer, Initial Feature Regularization (InFeR), that
mitigates this phenomenon by regressing a subspace of features towards its
value at initialization, leading to significant performance improvements in
sparse-reward environments such as Montezuma's Revenge. We conclude that
preventing capacity loss is crucial to enable agents to maximally benefit from
the learning signals they obtain throughout the entire training trajectory.

### Title: Generalizing to the Future: Mitigating Entity Bias in Fake News Detection
* Paper ID: 2204.09484v1
* Paper URL: [http://arxiv.org/abs/2204.09484v1](http://arxiv.org/abs/2204.09484v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/ictmcg/endef-sigir2022](https://github.com/ictmcg/endef-sigir2022)
* Summary: The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.

### Title: A Generalisable Data Fusion Framework to Infer Mode of Transport Using Mobile Phone Data
* Paper ID: 2204.09482v1
* Paper URL: [http://arxiv.org/abs/2204.09482v1](http://arxiv.org/abs/2204.09482v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Cities often lack up-to-date data analytics to evaluate and implement
transport planning interventions to achieve sustainability goals, as
traditional data sources are expensive, infrequent, and suffer from data
latency. Mobile phone data provide an inexpensive source of geospatial
information to capture human mobility at unprecedented geographic and temporal
granularity. This paper proposes a method to estimate updated mode of
transportation usage in a city, with novel usage of mobile phone application
traces to infer previously hard to detect modes, such as bikes and
ride-hailing/taxi. By using data fusion and matrix factorisation, we integrate
socioeconomic and demographic attributes of the local resident population into
the model. We tested the method in a case study of Santiago (Chile), and found
that changes from 2012 to 2020 in mode of transportation inferred by the method
are coherent with expectations from domain knowledge and the literature, such
as ride-hailing trips replacing mass transport.

### Title: On the relative asymptotic expressivity of inference frameworks
* Paper ID: 2204.09457v1
* Paper URL: [http://arxiv.org/abs/2204.09457v1](http://arxiv.org/abs/2204.09457v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Let $\sigma$ be a first-order signature and let $\mathbf{W}_n$ be the set of
all $\sigma$-structures with domain $[n] = \{1, \ldots, n\}$. We can think of
each structure in $\mathbf{W}_n$ as representing a "possible (state of the)
world". By an inference framework we mean a class $\mathbf{F}$ of pairs
$(\mathbb{P}, L)$, where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$
and each $\mathbb{P}_n$ is a probability distribution on $\mathbb{W}_n$, and
$L$ is a logic with truth values in the unit interval $[0, 1]$.
  From the point of view of probabilistic and logical expressivity one may
consider an inference framework as optimal if it allows any pair $(\mathbb{P},
L)$ where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$ is a sequence of
probability distributions on $\mathbb{W}_n$ and $L$ is a logic. But from the
point of view of using a pair $(\mathbb{P}, L)$ from such an inference
framework for making inferences on $\mathbb{W}_n$ when $n$ is large we face the
problem of computational complexity. This motivates looking for an "optimal"
trade-off (in a given context) between expressivity and computational
efficiency.
  We define a notion that an inference framework is "asymptotically at least as
expressive" as another inference framework. This relation is a preorder and we
describe a (strict) partial order on the equivalence classes of some inference
frameworks that in our opinion are natural in the context of machine learning
and artificial intelligence. The results have bearing on issues concerning
efficient learning and probabilistic inference, but are also new instances of
results in finite model theory about "almost sure elimination" of extra
syntactic features (e.g quantifiers) beyond the connectives. Often such a
result has a logical convergence law as a corollary.

### Title: Effective Goal-oriented 6G Communications: the Energy-aware Edge Inferencing Case
* Paper ID: 2204.09447v1
* Paper URL: [http://arxiv.org/abs/2204.09447v1](http://arxiv.org/abs/2204.09447v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Currently, the world experiences an unprecedentedly increasing generation of
application data, from sensor measurements to video streams, thanks to the
extreme connectivity capability provided by 5G networks. Going beyond 5G
technology, such data aim to be ingested by Artificial Intelligence (AI)
functions instantiated in the network to facilitate informed decisions,
essential for the operation of applications, such as automated driving and
factory automation. Nonetheless, while computing platforms hosting Machine
Learning (ML) models are ever powerful, their energy footprint is a key
impeding factor towards realizing a wireless network as a sustainable
intelligent platform. Focusing on a beyond 5G wireless network, overlaid by a
Multi-access Edge Computing (MEC) infrastructure with inferencing capabilities,
our paper tackles the problem of energy-aware dependable inference by
considering inference effectiveness as value of a goal that needs to be
accomplished by paying the minimum price in energy consumption. Both
MEC-assisted standalone and ensemble inference options are evaluated. It is
shown that, for some system scenarios, goal effectiveness above 84% is achieved
and sustained even by relaxing communication reliability requirements by one
decimal digit, while enjoying a device radio energy consumption reduction of
almost 23% at the same time. Also, ensemble inference is shown to improve
system-wide energy efficiency and even achieve higher goal effectiveness, as
compared to the standalone case for some system parameterizations.

### Title: Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware
* Paper ID: 2204.10183v1
* Paper URL: [http://arxiv.org/abs/2204.10183v1](http://arxiv.org/abs/2204.10183v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/bharathsudharsan/cnn_on_mcu](https://github.com/bharathsudharsan/cnn_on_mcu)
* Summary: The majority of IoT devices like smartwatches, smart plugs, HVAC controllers,
etc., are powered by hardware with a constrained specification (low memory,
clock speed and processor) which is insufficient to accommodate and execute
large, high-quality models. On such resource-constrained devices, manufacturers
still manage to provide attractive functionalities (to boost sales) by
following the traditional approach of programming IoT devices/products to
collect and transmit data (image, audio, sensor readings, etc.) to their
cloud-based ML analytics platforms. For decades, this online approach has been
facing issues such as compromised data streams, non-real-time analytics due to
latency, bandwidth constraints, costly subscriptions, recent privacy issues
raised by users and the GDPR guidelines, etc. In this paper, to enable
ultra-fast and accurate AI-based offline analytics on resource-constrained IoT
devices, we present an end-to-end multi-component model optimization sequence
and open-source its implementation. Researchers and developers can use our
optimization sequence to optimize high memory, computation demanding models in
multiple aspects in order to produce small size, low latency, low-power
consuming models that can comfortably fit and execute on resource-constrained
hardware. The experimental results show that our optimization components can
produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more
accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our
optimization sequence is generic and can be applied to any state-of-the-art
models trained for anomaly detection, predictive maintenance, robotics, voice
recognition, and machine vision.

### Title: GIMO: Gaze-Informed Human Motion Prediction in Context
* Paper ID: 2204.09443v1
* Paper URL: [http://arxiv.org/abs/2204.09443v1](http://arxiv.org/abs/2204.09443v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with eye gaze that serves as a surrogate
for inferring human intent. By employing inertial sensors for motion capture,
our data collection is not tied to specific scenes, which further boosts the
motion dynamics observed from our subjects. We perform an extensive study of
the benefits of leveraging eye gaze for ego-centric human motion prediction
with various state-of-the-art architectures. Moreover, to realize the full
potential of gaze, we propose a novel network architecture that enables
bidirectional communication between the gaze and motion branches. Our network
achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from the gaze and the denoised gaze
feature modulated by the motion. The proposed dataset and our network
implementation will be publicly available.

### Title: Multi-Auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation
* Paper ID: 2204.09422v1
* Paper URL: [http://arxiv.org/abs/2204.09422v1](http://arxiv.org/abs/2204.09422v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Recommending appropriate tags to items can facilitate content organization,
retrieval, consumption and other applications, where hybrid tag recommender
systems have been utilized to integrate collaborative information and content
information for better recommendations. In this paper, we propose a
multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for
tag recommendation, which couples item collaborative information and item
multi-auxiliary information, i.e., content and social graph, by defining a
generative process. Specifically, the model learns deep latent embeddings from
different item auxiliary information using variational auto-encoders (VAE),
which could form a generative distribution over each auxiliary information by
introducing a latent variable parameterized by deep neural network. Moreover,
to recommend tags for new items, item multi-auxiliary latent embeddings are
utilized as a surrogate through the item decoder for predicting recommendation
probabilities of each tag, where reconstruction losses are added in the
training phase to constrict the generation for feedback predictions via
different auxiliary embeddings. In addition, an inductive variational graph
auto-encoder is designed where new item nodes could be inferred in the test
phase, such that item social embeddings could be exploited for new items.
Extensive experiments on MovieLens and citeulike datasets demonstrate the
effectiveness of our method.

### Title: Distance and age of the massive stellar cluster Westerlund 1. I. Parallax method using Gaia-EDR3
* Paper ID: 2204.09414v1
* Paper URL: [http://arxiv.org/abs/2204.09414v1](http://arxiv.org/abs/2204.09414v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Westerlund 1 (Wd 1) is one of the most massive young star clusters in the
Milky Way. Although relevant for star formation and evolution, its fundamental
parameters are not yet very well constrained. Our goal is to derive an accurate
distance and provide constraints on the cluster age. We used the photometric
and astrometric information available in the Gaia Early Data Release 3
(Gaia-EDR3) to infer its distance of 4.06$^{+0.36}_{-0.34}$ kpc. Modelling of
the eclipsing binary system W36 reported in Paper II led to the distance of
4.34$\pm$0.25 kpc, in agreement with the Gaia-EDR3 distance and, therefore,
validating the parallax zero-point correction approach appropriate for red
objects. By taking advantage of another two recent distance determinations
using the Gaia-EDR3, we obtained a weighted mean distance for the cluster as
d$_{\rm wd1}$=4.23$^{+0.15}_{-0.13}$ kpc ($m-M$=13.13$^{+0.08}_{-0.07}$ mag),
which has an unprecedented accuracy of 4\%. We adopted recent Geneva
evolutionary tracks for supra-solar metallicity objects to infer the age of the
faintest RSG source from Wd 1, leading to a cluster age of 11.0$\pm$0.5 Myr, in
excellent agreement with recent work by Beasor \& Davies (10.4$^{+1.3}_{-1.2}$
Myr) based on MIST evolutionary models. The age of W36 was reported to be
3.5$\pm$0.5 Myr in Paper II, supporting recent claims of a temporal spread of
several Myr for the star-forming process within Wd 1 instead of a monolithic
starburst scenario.

### Title: Euclid detectability of pair instability supernovae in binary population synthesis models consistent with merging binary black holes
* Paper ID: 2204.09402v1
* Paper URL: [http://arxiv.org/abs/2204.09402v1](http://arxiv.org/abs/2204.09402v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We infer the expected detection number of pair instability supernovae (PISNe)
during the operation of the Euclid space telescope, based on two binary
population models that are consistent with binary black holes (BHs) observed by
gravitational waves. The two models consider different PISN criteria depending
on the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. The fiducial and
$3\sigma$ models adopt the standard and $3\sigma$-smaller $^{12}$C$(\alpha,
\gamma)^{16}$O reaction rate, which predicts that stars with helium core masses
$65-135 M_\odot$ and $90-180 M_\odot$ cause PISNe, respectively. Our fiducial
model predicts that Euclid detects several Type I or hydrogen-poor PISNe. For
the $3\sigma$ model, detection of $\sim 1$ Type I PISN by Euclid is expected if
the stellar mass distribution extends to $M_{\max} \sim 600 M_\odot$, but the
expected number becomes significantly smaller if $M_{\max} \sim 300 M_\odot$.
Thus, we may be able to prove or distinguish the fiducial and $3\sigma$ models
by the observed PISN rate. This will help us to constrain the origin of binary
BHs and the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. PISN ejecta mass
estimates from light curves and spectra obtained by follow-up observations
would also be important to constrain the $^{12}$C$(\alpha, \gamma)^{16}$O
reaction rate.

### Title: Electrostatic Dust Ejection From Asteroid (3200) Phaethon With the Aid of Mobile Alkali Ions at Perihelion
* Paper ID: 2204.09385v1
* Paper URL: [http://arxiv.org/abs/2204.09385v1](http://arxiv.org/abs/2204.09385v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The asteroid (3200) Phaethon is known to be the parent body of the Geminids,
although meteor showers are commonly associated with the activity of periodic
comets. What is most peculiar to the asteroid is its comet-like activity in the
ejection of micrometer-sized dust particles at every perihelion passage, while
the activity of the asteroid has never been identified outside the
near-perihelion zone at $0.14~\mathrm{au}$ from the Sun. From the theoretical
point of view, we argue that the activity of the asteroid is well explained by
the electrostatic lofting of micrometer-sized dust particles with the aid of
mobile alkali ions at high temperatures. The mass-loss rates of
micrometer-sized particles from the asteroid in our model is entirely
consistent with the values inferred from visible observations of Phaethon's
dust tail. For millimeter-sized particles, we predict three orders of
magnitudes higher mass-loss rates, which could also account for the total mass
of the Geminid meteoroid stream by the electrostatic lofting mechanism.

### Title: A Variational Autoencoder for Heterogeneous Temporal and Longitudinal Data
* Paper ID: 2204.09369v1
* Paper URL: [http://arxiv.org/abs/2204.09369v1](http://arxiv.org/abs/2204.09369v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model's efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.

### Title: Multi-Component Imaging of the Fermi Gamma-ray Sky in the Spatio-spectral Domain
* Paper ID: 2204.09360v1
* Paper URL: [http://arxiv.org/abs/2204.09360v1](http://arxiv.org/abs/2204.09360v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We perform two distinct spatio-spectral reconstructions of the gamma-ray sky
in the range of 0.56-316 GeV based on Fermi Large Area Telescope (LAT) data.
Both describe the sky brightness to be composed of a diffuse-emission and a
point-source component. The first model requires minimal assumptions and
provides a template-free reconstruction as a reference. It makes use of spatial
and spectral correlations to distinguish between the different components. The
second model is physics-informed and further differentiates between diffuse
emission of hadronic and leptonic origin. For this, we assume parametric, but
spatially varying energy spectra to distinguish between the processes and use
thermal Galactic dust observations to indicate the preferred sites of hadronic
interactions. To account for instrumental effects we model the point-spread,
the energy dispersion, and the exposure of the telescope throughout the
observation. The reconstruction problem is formulated as a Bayesian inference
task, that is solved by variational inference. We show decompositions of the
Gamma-ray flux into diffuse and point-like emissions, and of the diffuse
emissions into multiple physically motivated components. The diffuse
decomposition provides an unprecedented view of the Galactic leptonic diffuse
emission. It shows the Fermi bubbles and their spectral variations in high
fidelity and other areas exhibiting strong cosmic ray electron contents, such
as a thick disk in the inner Galaxy and outflow regions. Furthermore, we report
a hard spectrum gamma ray arc in the northern outer bubble co-spatial with the
reported X-ray arc by the eROSITA collaboration. All our spatio-spectral sky
reconstructions and their uncertainty quantification are publicly available.

### Title: Inferring entropy production in anharmonic Brownian gyrators
* Paper ID: 2204.09283v1
* Paper URL: [http://arxiv.org/abs/2204.09283v1](http://arxiv.org/abs/2204.09283v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A non-vanishing entropy production rate is one of the defining
characteristics of any non-equilibrium system, and several techniques exist to
determine this quantity directly from experimental data. The short-time
inference scheme, derived from the thermodynamic uncertainty relation, is a
recent addition to the list of these techniques. Here we apply this scheme to
quantify the entropy production rate in a class of microscopic heat engine
models called Brownian gyrators. In particular, we consider models with
anharmonic confining potentials. In these cases, the dynamical equations are
indelibly non-linear, and the exact dependences of the entropy production rate
on the model parameters are unknown. Our results demonstrate that the
short-time inference scheme can efficiently determine these dependencies from a
moderate amount of trajectory data. Furthermore, the results show that the
non-equilibrium properties of the gyrator model with anharmonic confining
potentials are considerably different from its harmonic counterpart -
especially in set-ups leading to a non-equilibrium dynamics and the resulting
gyration patterns.

### Title: A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond
* Paper ID: 2204.09269v1
* Paper URL: [http://arxiv.org/abs/2204.09269v1](http://arxiv.org/abs/2204.09269v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.

### Title: The effect of spatial resolution on magnetic field modeling and helicity computation
* Paper ID: 2204.09267v1
* Paper URL: [http://arxiv.org/abs/2204.09267v1](http://arxiv.org/abs/2204.09267v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Nonlinear force-free (NLFF) modeling is regularly used in order to indirectly
infer the 3D geometry of the coronal magnetic field, not accessible on a
regular basis by means of direct measurements otherwise. We study the effect of
binning in time series NLFF modeling of individual active regions (ARs) in
order to quantify the effect of a different underlying spatial resolution on
the quality of modeling as well as on the derived physical parameters. We apply
an optimization method to sequences of SDO/HMI vector magnetogram data at three
different spatial resolutions for three solar ARs to obtain nine NLFF model
time series. From the NLFF models, we deduce active-region magnetic fluxes,
electric currents, magnetic energies and relative helicities, and analyze those
with respect to the underlying spatial resolution. We calculate various metrics
to quantify the quality of the derived NLFF models and apply a Helmholtz
decomposition to characterize solenoidal errors. At a given spatial resolution,
the quality of NLFF modeling is different for different ARs, as well as varies
along of the individual model time series. For a given AR, modeling at a given
spatial resolution is not necessarily of superior quality compared to that
performed at different spatial resolutions at all time instances of a NLFF
model time series. Generally, the NLFF model quality tends to be higher at
reduced spatial resolution with the solenoidal quality being the ultimate cause
for systematic variations in model-deduced physical quantities.
Optimization-based modeling based on binned SDO/HMI vector data delivers
magnetic energies and helicity estimates different by $\lesssim$30\%, given
that concise checks ensure the physical plausibility and high solenoidal
quality of the tested model. Spatial-resolution induced differences are
relatively small compared to that arising from other sources of uncertainty.

### Title: Details in BeiDou-G2: Past and Present
* Paper ID: 2204.09258v1
* Paper URL: [http://arxiv.org/abs/2204.09258v1](http://arxiv.org/abs/2204.09258v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: In January 2022, the defunct satellite BeiDou-G2 was pulled out of
geostationary orbit by Shijian-21 to a graveyard orbit. For safe docking and
operation, it was necessary to determine the rotation state in advance. In this
paper, we show the evolution of the rotation of the BeiDou-G2 satellite based
on the photometry observation data for the past 10 years. The rotational speed
of BeiDou-G2 was found to be annual oscillation, mainly due to the solar
radiation. Based on the evolution of BeiDou-G2's rotation speed and its orbit,
we confirmed that in the last 10 years, the satellite had six abnormal events.
These abnormal events were mainly due to the increase in the rotation speed
caused by suspected fuel leakages. Additionally, the abnormal events included
one collision in 2012, which was inferred to be the trigger of the fuel
leakages in the following years. No rotational speed abnormalities occurred
again after 2017, probably due to the complete release of the residual fuel.
The parameters and the propagating models after one incidence of solar panel
damage in 2014 and one fragment in 2016 were believed to be able to satisfy the
accuracy requirements of the rotation state well at the moment of docking,
which was ultimately confirmed by Shijian-21.

### Title: Compositional Semantics and Inference System for Temporal Order based on Japanese CCG
* Paper ID: 2204.09245v1
* Paper URL: [http://arxiv.org/abs/2204.09245v1](http://arxiv.org/abs/2204.09245v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Natural Language Inference (NLI) is the task of determining whether a premise
entails a hypothesis. NLI with temporal order is a challenging task because
tense and aspect are complex linguistic phenomena involving interactions with
temporal adverbs and temporal connectives. To tackle this, temporal and
aspectual inference has been analyzed in various ways in the field of formal
semantics. However, a Japanese NLI system for temporal order based on the
analysis of formal semantics has not been sufficiently developed. We present a
logic-based NLI system that considers temporal order in Japanese based on
compositional semantics via Combinatory Categorial Grammar (CCG) syntactic
analysis. Our system performs inference involving temporal order by using
axioms for temporal relations and automated theorem provers. We evaluate our
system by experimenting with Japanese NLI datasets that involve temporal order.
We show that our system outperforms previous logic-based systems as well as
current deep learning-based models.

### Title: Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers
* Paper ID: 2204.09225v1
* Paper URL: [http://arxiv.org/abs/2204.09225v1](http://arxiv.org/abs/2204.09225v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: How to identify and characterize functional brain networks (BN) is
fundamental to gain system-level insights into the mechanisms of brain
organizational architecture. Current functional magnetic resonance (fMRI)
analysis highly relies on prior knowledge of specific patterns in either
spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain.
In addition, most approaches aim to find group-wise common functional networks,
individual-specific functional networks have been rarely studied. In this work,
we propose a novel Twin-Transformers framework to simultaneously infer common
and individual functional networks in both spatial and temporal space, in a
self-supervised manner. The first transformer takes space-divided information
as input and generates spatial features, while the second transformer takes
time-related information as input and outputs temporal features. The spatial
and temporal features are further separated into common and individual ones via
interactions (weights sharing) and constraints between the two transformers. We
applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI
dataset and identified multiple common brain networks, including both
task-related and resting-state networks (e.g., default mode network).
Interestingly, we also successfully recovered a set of individual-specific
networks that are not related to task stimulus and only exist at the individual
level.

### Title: Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
* Paper ID: 2204.09204v1
* Paper URL: [http://arxiv.org/abs/2204.09204v1](http://arxiv.org/abs/2204.09204v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.

### Title: Functional Calibration under Non-Probability Survey Sampling
* Paper ID: 2204.09193v1
* Paper URL: [http://arxiv.org/abs/2204.09193v1](http://arxiv.org/abs/2204.09193v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-probability sampling is prevailing in survey sampling, but ignoring its
selection bias leads to erroneous inferences. We offer a unified nonparametric
calibration method to estimate the sampling weights for a non-probability
sample by calibrating functions of auxiliary variables in a reproducing kernel
Hilbert space. The consistency and the limiting distribution of the proposed
estimator are established, and the corresponding variance estimator is also
investigated. Compared with existing works, the proposed method is more robust
since no parametric assumption is made for the selection mechanism of the
non-probability sample. Numerical results demonstrate that the proposed method
outperforms its competitors, especially when the model is misspecified. The
proposed method is applied to analyze the average total cholesterol of Korean
citizens based on a non-probability sample from the National Health Insurance
Sharing Service and a reference probability sample from the Korea National
Health and Nutrition Examination Survey.

### Title: Multiply-and-Fire (MNF): An Event-driven Sparse Neural Network Accelerator
* Paper ID: 2204.09797v1
* Paper URL: [http://arxiv.org/abs/2204.09797v1](http://arxiv.org/abs/2204.09797v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Machine learning, particularly deep neural network inference, has become a
vital workload for many computing systems, from data centers and HPC systems to
edge-based computing. As advances in sparsity have helped improve the
efficiency of AI acceleration, there is a continued need for improved system
efficiency for both high-performance and system-level acceleration.
  This work takes a unique look at sparsity with an event (or
activation-driven) approach to ANN acceleration that aims to minimize useless
work, improve utilization, and increase performance and energy efficiency. Our
analytical and experimental results show that this event-driven solution
presents a new direction to enable highly efficient AI inference for both CNN
and MLP workloads.
  This work demonstrates state-of-the-art energy efficiency and performance
centring on activation-based sparsity and a highly-parallel dataflow method
that improves the overall functional unit utilization (at 30 fps). This work
enhances energy efficiency over a state-of-the-art solution by 1.46$\times$.
Taken together, this methodology presents a novel, new direction to achieve
high-efficiency, high-performance designs for next-generation AI acceleration
platforms.

### Title: Estimating optimal individualized treatment rules with multistate processes
* Paper ID: 2204.09785v1
* Paper URL: [http://arxiv.org/abs/2204.09785v1](http://arxiv.org/abs/2204.09785v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Multistate process data are common in studies of chronic diseases such as
cancer. These data are ideal for precision medicine purposes as they can be
leveraged to improve more refined health outcomes, compared to standard
survival outcomes, as well as incorporate patient preferences regarding
quantity versus quality of life. However, there are currently no methods for
the estimation of optimal individualized treatment rules with such data. In
this article, we propose a nonparametric outcome weighted learning approach for
this problem in randomized clinical trial settings. The theoretical properties
of the proposed methods, including Fisher consistency and asymptotic normality
of the estimated expected outcome under the estimated optimal individualized
treatment rule, are rigorously established. A consistent closed-form variance
estimator is provided and methodology for the calculation of simultaneous
confidence intervals is proposed. Simulation studies show that the proposed
methodology and inference procedures work well even with small sample sizes and
high rates of right censoring. The methodology is illustrated using data from a
randomized clinical trial on the treatment of metastatic squamous-cell
carcinoma of the head and neck.

### Title: Time-based Self-supervised Learning for Wireless Capsule Endoscopy
* Paper ID: 2204.09773v1
* Paper URL: [http://arxiv.org/abs/2204.09773v1](http://arxiv.org/abs/2204.09773v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: State-of-the-art machine learning models, and especially deep learning ones,
are significantly data-hungry; they require vast amounts of manually labeled
samples to function correctly. However, in most medical imaging fields,
obtaining said data can be challenging. Not only the volume of data is a
problem, but also the imbalances within its classes; it is common to have many
more images of healthy patients than of those with pathology. Computer-aided
diagnostic systems suffer from these issues, usually over-designing their
models to perform accurately. This work proposes using self-supervised learning
for wireless endoscopy videos by introducing a custom-tailored method that does
not initially need labels or appropriate balance. We prove that using the
inferred inherent structure learned by our method, extracted from the temporal
axis, improves the detection rate on several domain-specific applications even
under severe imbalance.

### Title: A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines
* Paper ID: 2204.09772v1
* Paper URL: [http://arxiv.org/abs/2204.09772v1](http://arxiv.org/abs/2204.09772v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.

### Title: Inferring ice sheet damage models from limited observations using CRIKit: the Constitutive Relation Inference Toolkit
* Paper ID: 2204.09748v1
* Paper URL: [http://arxiv.org/abs/2204.09748v1](http://arxiv.org/abs/2204.09748v1)
* Updated Date: 2022-04-20
* Code URL: [https://gitlab.com/gbruer/ice-crikit](https://gitlab.com/gbruer/ice-crikit)
* Summary: We examine the prospect of learning ice sheet damage models from
observational data. Our approach, implemented in CRIKit (the Constitutive
Relation Inference Toolkit), is to model the material time derivative of damage
as a frame-invariant neural network, and to optimize the parameters of the
model from simulations of the flow of an ice dome. Using the model of Albrecht
and Levermann as the ground truth to generate synthetic observations, we
measure the difference of optimized neural network models from that model to
try to understand how well this process generates models that can then transfer
to other ice sheet simulations.
  The use of so-called "deep-learning" models for constitutive equations,
equations of state, sub-grid-scale processes, and other pointwise relations
that appear in systems of PDEs has been successful in other disciplines, yet
our inference setting has some confounding factors. The first is the type of
observations that are available: we compare the quality of the inferred models
when the loss of the numerical simulations includes observation misfits
throughout the ice, which is unobtainable in real settings, to losses that
include only combinations of surface and borehole observations. The second
confounding factor is the evolution of damage in an ice sheet, which is
advection dominated. The non-local effect of perturbations in a damage models
results in loss functions that have both many local minima and many parameter
configurations for which the system is unsolvable.
  Our experience suggests that basic neural networks have several deficiencies
that affect the quality of the optimized models. We suggest several approaches
to incorporating additional inductive biases into neural networks which may
lead to better performance in future work.

### Title: ColorCode: A Bayesian Approach to Augmentative and Alternative Communication with Two Buttons
* Paper ID: 2204.09745v1
* Paper URL: [http://arxiv.org/abs/2204.09745v1](http://arxiv.org/abs/2204.09745v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/mrdaly/colorcode](https://github.com/mrdaly/colorcode)
* Summary: Many people with severely limited muscle control can only communicate through
augmentative and alternative communication (AAC) systems with a small number of
buttons. In this paper, we present the design for ColorCode, which is an AAC
system with two buttons that uses Bayesian inference to determine what the user
wishes to communicate. Our information-theoretic analysis of ColorCode
simulations shows that it is efficient in extracting information from the user,
even in the presence of errors, achieving nearly optimal error correction.
ColorCode is provided as open source software
(https://github.com/mrdaly/ColorCode).

### Title: A majorization-minimization algorithm for nonnegative binary matrix factorization
* Paper ID: 2204.09741v1
* Paper URL: [http://arxiv.org/abs/2204.09741v1](http://arxiv.org/abs/2204.09741v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: This paper tackles the problem of decomposing binary data using matrix
factorization. We consider the family of mean-parametrized Bernoulli models, a
class of generative models that are well suited for modeling binary data and
enables interpretability of the factors. We factorize the Bernoulli parameter
and consider an additional Beta prior on one of the factors to further improve
the model's expressive power. While similar models have been proposed in the
literature, they only exploit the Beta prior as a proxy to ensure a valid
Bernoulli parameter in a Bayesian setting; in practice it reduces to a uniform
or uninformative prior. Besides, estimation in these models has focused on
costly Bayesian inference. In this paper, we propose a simple yet very
efficient majorization-minimization algorithm for maximum a posteriori
estimation. Our approach leverages the Beta prior whose parameters can be tuned
to improve performance in matrix completion tasks. Experiments conducted on
three public binary datasets show that our approach offers an excellent
trade-off between prediction performance, computational complexity, and
interpretability.

### Title: Physical, subjective and analogical probability
* Paper ID: 2204.10159v1
* Paper URL: [http://arxiv.org/abs/2204.10159v1](http://arxiv.org/abs/2204.10159v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The aim of this paper is to show that the concept of probability is best
understood by dividing this concept into two different types of probability,
namely physical probability and analogical probability. Loosely speaking, a
physical probability is a probability that applies to the outcomes of an
experiment that have been judged as being equally likely on the basis of
physical symmetry. Physical probabilities are arguably in some sense
'objective' and possess all the standard properties of the concept of
probability. On the other hand, an analogical probability is defined by making
an analogy between the uncertainty surrounding an event of interest and the
uncertainty surrounding an event that has a physical probability. Analogical
probabilities are undeniably subjective probabilities and are not obliged to
have all the standard mathematical properties possessed by physical
probabilities, e.g. they may not have the property of additivity or obey the
standard definition of conditional probability. Nevertheless, analogical
probabilities have extra properties, which are not possessed by physical
probabilities, that assist in their direct elicitation, general derivation,
comparison and justification. More specifically, these properties facilitate
the application of analogical probability to real-world problems that can not
be adequately resolved by using only physical probability, e.g. probabilistic
inference about hypotheses on the basis of observed data. Careful definitions
are given of the concepts that are introduced and, where appropriate, examples
of the application of these concepts are presented for additional clarity.

### Title: Understanding and Preventing Capacity Loss in Reinforcement Learning
* Paper ID: 2204.09560v1
* Paper URL: [http://arxiv.org/abs/2204.09560v1](http://arxiv.org/abs/2204.09560v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The reinforcement learning (RL) problem is rife with sources of
non-stationarity, making it a notoriously difficult problem domain for the
application of neural networks. We identify a mechanism by which non-stationary
prediction targets can prevent learning progress in deep RL agents:
\textit{capacity loss}, whereby networks trained on a sequence of target values
lose their ability to quickly update their predictions over time. We
demonstrate that capacity loss occurs in a range of RL agents and environments,
and is particularly damaging to performance in sparse-reward tasks. We then
present a simple regularizer, Initial Feature Regularization (InFeR), that
mitigates this phenomenon by regressing a subspace of features towards its
value at initialization, leading to significant performance improvements in
sparse-reward environments such as Montezuma's Revenge. We conclude that
preventing capacity loss is crucial to enable agents to maximally benefit from
the learning signals they obtain throughout the entire training trajectory.

### Title: Generalizing to the Future: Mitigating Entity Bias in Fake News Detection
* Paper ID: 2204.09484v1
* Paper URL: [http://arxiv.org/abs/2204.09484v1](http://arxiv.org/abs/2204.09484v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/ictmcg/endef-sigir2022](https://github.com/ictmcg/endef-sigir2022)
* Summary: The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.

### Title: A Generalisable Data Fusion Framework to Infer Mode of Transport Using Mobile Phone Data
* Paper ID: 2204.09482v1
* Paper URL: [http://arxiv.org/abs/2204.09482v1](http://arxiv.org/abs/2204.09482v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Cities often lack up-to-date data analytics to evaluate and implement
transport planning interventions to achieve sustainability goals, as
traditional data sources are expensive, infrequent, and suffer from data
latency. Mobile phone data provide an inexpensive source of geospatial
information to capture human mobility at unprecedented geographic and temporal
granularity. This paper proposes a method to estimate updated mode of
transportation usage in a city, with novel usage of mobile phone application
traces to infer previously hard to detect modes, such as bikes and
ride-hailing/taxi. By using data fusion and matrix factorisation, we integrate
socioeconomic and demographic attributes of the local resident population into
the model. We tested the method in a case study of Santiago (Chile), and found
that changes from 2012 to 2020 in mode of transportation inferred by the method
are coherent with expectations from domain knowledge and the literature, such
as ride-hailing trips replacing mass transport.

### Title: On the relative asymptotic expressivity of inference frameworks
* Paper ID: 2204.09457v1
* Paper URL: [http://arxiv.org/abs/2204.09457v1](http://arxiv.org/abs/2204.09457v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Let $\sigma$ be a first-order signature and let $\mathbf{W}_n$ be the set of
all $\sigma$-structures with domain $[n] = \{1, \ldots, n\}$. We can think of
each structure in $\mathbf{W}_n$ as representing a "possible (state of the)
world". By an inference framework we mean a class $\mathbf{F}$ of pairs
$(\mathbb{P}, L)$, where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$
and each $\mathbb{P}_n$ is a probability distribution on $\mathbb{W}_n$, and
$L$ is a logic with truth values in the unit interval $[0, 1]$.
  From the point of view of probabilistic and logical expressivity one may
consider an inference framework as optimal if it allows any pair $(\mathbb{P},
L)$ where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$ is a sequence of
probability distributions on $\mathbb{W}_n$ and $L$ is a logic. But from the
point of view of using a pair $(\mathbb{P}, L)$ from such an inference
framework for making inferences on $\mathbb{W}_n$ when $n$ is large we face the
problem of computational complexity. This motivates looking for an "optimal"
trade-off (in a given context) between expressivity and computational
efficiency.
  We define a notion that an inference framework is "asymptotically at least as
expressive" as another inference framework. This relation is a preorder and we
describe a (strict) partial order on the equivalence classes of some inference
frameworks that in our opinion are natural in the context of machine learning
and artificial intelligence. The results have bearing on issues concerning
efficient learning and probabilistic inference, but are also new instances of
results in finite model theory about "almost sure elimination" of extra
syntactic features (e.g quantifiers) beyond the connectives. Often such a
result has a logical convergence law as a corollary.

### Title: Effective Goal-oriented 6G Communications: the Energy-aware Edge Inferencing Case
* Paper ID: 2204.09447v1
* Paper URL: [http://arxiv.org/abs/2204.09447v1](http://arxiv.org/abs/2204.09447v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Currently, the world experiences an unprecedentedly increasing generation of
application data, from sensor measurements to video streams, thanks to the
extreme connectivity capability provided by 5G networks. Going beyond 5G
technology, such data aim to be ingested by Artificial Intelligence (AI)
functions instantiated in the network to facilitate informed decisions,
essential for the operation of applications, such as automated driving and
factory automation. Nonetheless, while computing platforms hosting Machine
Learning (ML) models are ever powerful, their energy footprint is a key
impeding factor towards realizing a wireless network as a sustainable
intelligent platform. Focusing on a beyond 5G wireless network, overlaid by a
Multi-access Edge Computing (MEC) infrastructure with inferencing capabilities,
our paper tackles the problem of energy-aware dependable inference by
considering inference effectiveness as value of a goal that needs to be
accomplished by paying the minimum price in energy consumption. Both
MEC-assisted standalone and ensemble inference options are evaluated. It is
shown that, for some system scenarios, goal effectiveness above 84% is achieved
and sustained even by relaxing communication reliability requirements by one
decimal digit, while enjoying a device radio energy consumption reduction of
almost 23% at the same time. Also, ensemble inference is shown to improve
system-wide energy efficiency and even achieve higher goal effectiveness, as
compared to the standalone case for some system parameterizations.

### Title: Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware
* Paper ID: 2204.10183v1
* Paper URL: [http://arxiv.org/abs/2204.10183v1](http://arxiv.org/abs/2204.10183v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/bharathsudharsan/cnn_on_mcu](https://github.com/bharathsudharsan/cnn_on_mcu)
* Summary: The majority of IoT devices like smartwatches, smart plugs, HVAC controllers,
etc., are powered by hardware with a constrained specification (low memory,
clock speed and processor) which is insufficient to accommodate and execute
large, high-quality models. On such resource-constrained devices, manufacturers
still manage to provide attractive functionalities (to boost sales) by
following the traditional approach of programming IoT devices/products to
collect and transmit data (image, audio, sensor readings, etc.) to their
cloud-based ML analytics platforms. For decades, this online approach has been
facing issues such as compromised data streams, non-real-time analytics due to
latency, bandwidth constraints, costly subscriptions, recent privacy issues
raised by users and the GDPR guidelines, etc. In this paper, to enable
ultra-fast and accurate AI-based offline analytics on resource-constrained IoT
devices, we present an end-to-end multi-component model optimization sequence
and open-source its implementation. Researchers and developers can use our
optimization sequence to optimize high memory, computation demanding models in
multiple aspects in order to produce small size, low latency, low-power
consuming models that can comfortably fit and execute on resource-constrained
hardware. The experimental results show that our optimization components can
produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more
accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our
optimization sequence is generic and can be applied to any state-of-the-art
models trained for anomaly detection, predictive maintenance, robotics, voice
recognition, and machine vision.

### Title: GIMO: Gaze-Informed Human Motion Prediction in Context
* Paper ID: 2204.09443v1
* Paper URL: [http://arxiv.org/abs/2204.09443v1](http://arxiv.org/abs/2204.09443v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with eye gaze that serves as a surrogate
for inferring human intent. By employing inertial sensors for motion capture,
our data collection is not tied to specific scenes, which further boosts the
motion dynamics observed from our subjects. We perform an extensive study of
the benefits of leveraging eye gaze for ego-centric human motion prediction
with various state-of-the-art architectures. Moreover, to realize the full
potential of gaze, we propose a novel network architecture that enables
bidirectional communication between the gaze and motion branches. Our network
achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from the gaze and the denoised gaze
feature modulated by the motion. The proposed dataset and our network
implementation will be publicly available.

### Title: Multi-Auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation
* Paper ID: 2204.09422v1
* Paper URL: [http://arxiv.org/abs/2204.09422v1](http://arxiv.org/abs/2204.09422v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Recommending appropriate tags to items can facilitate content organization,
retrieval, consumption and other applications, where hybrid tag recommender
systems have been utilized to integrate collaborative information and content
information for better recommendations. In this paper, we propose a
multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for
tag recommendation, which couples item collaborative information and item
multi-auxiliary information, i.e., content and social graph, by defining a
generative process. Specifically, the model learns deep latent embeddings from
different item auxiliary information using variational auto-encoders (VAE),
which could form a generative distribution over each auxiliary information by
introducing a latent variable parameterized by deep neural network. Moreover,
to recommend tags for new items, item multi-auxiliary latent embeddings are
utilized as a surrogate through the item decoder for predicting recommendation
probabilities of each tag, where reconstruction losses are added in the
training phase to constrict the generation for feedback predictions via
different auxiliary embeddings. In addition, an inductive variational graph
auto-encoder is designed where new item nodes could be inferred in the test
phase, such that item social embeddings could be exploited for new items.
Extensive experiments on MovieLens and citeulike datasets demonstrate the
effectiveness of our method.

### Title: Distance and age of the massive stellar cluster Westerlund 1. I. Parallax method using Gaia-EDR3
* Paper ID: 2204.09414v1
* Paper URL: [http://arxiv.org/abs/2204.09414v1](http://arxiv.org/abs/2204.09414v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Westerlund 1 (Wd 1) is one of the most massive young star clusters in the
Milky Way. Although relevant for star formation and evolution, its fundamental
parameters are not yet very well constrained. Our goal is to derive an accurate
distance and provide constraints on the cluster age. We used the photometric
and astrometric information available in the Gaia Early Data Release 3
(Gaia-EDR3) to infer its distance of 4.06$^{+0.36}_{-0.34}$ kpc. Modelling of
the eclipsing binary system W36 reported in Paper II led to the distance of
4.34$\pm$0.25 kpc, in agreement with the Gaia-EDR3 distance and, therefore,
validating the parallax zero-point correction approach appropriate for red
objects. By taking advantage of another two recent distance determinations
using the Gaia-EDR3, we obtained a weighted mean distance for the cluster as
d$_{\rm wd1}$=4.23$^{+0.15}_{-0.13}$ kpc ($m-M$=13.13$^{+0.08}_{-0.07}$ mag),
which has an unprecedented accuracy of 4\%. We adopted recent Geneva
evolutionary tracks for supra-solar metallicity objects to infer the age of the
faintest RSG source from Wd 1, leading to a cluster age of 11.0$\pm$0.5 Myr, in
excellent agreement with recent work by Beasor \& Davies (10.4$^{+1.3}_{-1.2}$
Myr) based on MIST evolutionary models. The age of W36 was reported to be
3.5$\pm$0.5 Myr in Paper II, supporting recent claims of a temporal spread of
several Myr for the star-forming process within Wd 1 instead of a monolithic
starburst scenario.

### Title: Euclid detectability of pair instability supernovae in binary population synthesis models consistent with merging binary black holes
* Paper ID: 2204.09402v1
* Paper URL: [http://arxiv.org/abs/2204.09402v1](http://arxiv.org/abs/2204.09402v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We infer the expected detection number of pair instability supernovae (PISNe)
during the operation of the Euclid space telescope, based on two binary
population models that are consistent with binary black holes (BHs) observed by
gravitational waves. The two models consider different PISN criteria depending
on the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. The fiducial and
$3\sigma$ models adopt the standard and $3\sigma$-smaller $^{12}$C$(\alpha,
\gamma)^{16}$O reaction rate, which predicts that stars with helium core masses
$65-135 M_\odot$ and $90-180 M_\odot$ cause PISNe, respectively. Our fiducial
model predicts that Euclid detects several Type I or hydrogen-poor PISNe. For
the $3\sigma$ model, detection of $\sim 1$ Type I PISN by Euclid is expected if
the stellar mass distribution extends to $M_{\max} \sim 600 M_\odot$, but the
expected number becomes significantly smaller if $M_{\max} \sim 300 M_\odot$.
Thus, we may be able to prove or distinguish the fiducial and $3\sigma$ models
by the observed PISN rate. This will help us to constrain the origin of binary
BHs and the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. PISN ejecta mass
estimates from light curves and spectra obtained by follow-up observations
would also be important to constrain the $^{12}$C$(\alpha, \gamma)^{16}$O
reaction rate.

### Title: Electrostatic Dust Ejection From Asteroid (3200) Phaethon With the Aid of Mobile Alkali Ions at Perihelion
* Paper ID: 2204.09385v1
* Paper URL: [http://arxiv.org/abs/2204.09385v1](http://arxiv.org/abs/2204.09385v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The asteroid (3200) Phaethon is known to be the parent body of the Geminids,
although meteor showers are commonly associated with the activity of periodic
comets. What is most peculiar to the asteroid is its comet-like activity in the
ejection of micrometer-sized dust particles at every perihelion passage, while
the activity of the asteroid has never been identified outside the
near-perihelion zone at $0.14~\mathrm{au}$ from the Sun. From the theoretical
point of view, we argue that the activity of the asteroid is well explained by
the electrostatic lofting of micrometer-sized dust particles with the aid of
mobile alkali ions at high temperatures. The mass-loss rates of
micrometer-sized particles from the asteroid in our model is entirely
consistent with the values inferred from visible observations of Phaethon's
dust tail. For millimeter-sized particles, we predict three orders of
magnitudes higher mass-loss rates, which could also account for the total mass
of the Geminid meteoroid stream by the electrostatic lofting mechanism.

### Title: A Variational Autoencoder for Heterogeneous Temporal and Longitudinal Data
* Paper ID: 2204.09369v1
* Paper URL: [http://arxiv.org/abs/2204.09369v1](http://arxiv.org/abs/2204.09369v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model's efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.

### Title: Multi-Component Imaging of the Fermi Gamma-ray Sky in the Spatio-spectral Domain
* Paper ID: 2204.09360v1
* Paper URL: [http://arxiv.org/abs/2204.09360v1](http://arxiv.org/abs/2204.09360v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We perform two distinct spatio-spectral reconstructions of the gamma-ray sky
in the range of 0.56-316 GeV based on Fermi Large Area Telescope (LAT) data.
Both describe the sky brightness to be composed of a diffuse-emission and a
point-source component. The first model requires minimal assumptions and
provides a template-free reconstruction as a reference. It makes use of spatial
and spectral correlations to distinguish between the different components. The
second model is physics-informed and further differentiates between diffuse
emission of hadronic and leptonic origin. For this, we assume parametric, but
spatially varying energy spectra to distinguish between the processes and use
thermal Galactic dust observations to indicate the preferred sites of hadronic
interactions. To account for instrumental effects we model the point-spread,
the energy dispersion, and the exposure of the telescope throughout the
observation. The reconstruction problem is formulated as a Bayesian inference
task, that is solved by variational inference. We show decompositions of the
Gamma-ray flux into diffuse and point-like emissions, and of the diffuse
emissions into multiple physically motivated components. The diffuse
decomposition provides an unprecedented view of the Galactic leptonic diffuse
emission. It shows the Fermi bubbles and their spectral variations in high
fidelity and other areas exhibiting strong cosmic ray electron contents, such
as a thick disk in the inner Galaxy and outflow regions. Furthermore, we report
a hard spectrum gamma ray arc in the northern outer bubble co-spatial with the
reported X-ray arc by the eROSITA collaboration. All our spatio-spectral sky
reconstructions and their uncertainty quantification are publicly available.

### Title: Inferring entropy production in anharmonic Brownian gyrators
* Paper ID: 2204.09283v1
* Paper URL: [http://arxiv.org/abs/2204.09283v1](http://arxiv.org/abs/2204.09283v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A non-vanishing entropy production rate is one of the defining
characteristics of any non-equilibrium system, and several techniques exist to
determine this quantity directly from experimental data. The short-time
inference scheme, derived from the thermodynamic uncertainty relation, is a
recent addition to the list of these techniques. Here we apply this scheme to
quantify the entropy production rate in a class of microscopic heat engine
models called Brownian gyrators. In particular, we consider models with
anharmonic confining potentials. In these cases, the dynamical equations are
indelibly non-linear, and the exact dependences of the entropy production rate
on the model parameters are unknown. Our results demonstrate that the
short-time inference scheme can efficiently determine these dependencies from a
moderate amount of trajectory data. Furthermore, the results show that the
non-equilibrium properties of the gyrator model with anharmonic confining
potentials are considerably different from its harmonic counterpart -
especially in set-ups leading to a non-equilibrium dynamics and the resulting
gyration patterns.

### Title: A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond
* Paper ID: 2204.09269v1
* Paper URL: [http://arxiv.org/abs/2204.09269v1](http://arxiv.org/abs/2204.09269v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/litterbrother-xiao/overview-of-non-autoregressive-applications](https://github.com/litterbrother-xiao/overview-of-non-autoregressive-applications)
* Summary: Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.

### Title: The effect of spatial resolution on magnetic field modeling and helicity computation
* Paper ID: 2204.09267v1
* Paper URL: [http://arxiv.org/abs/2204.09267v1](http://arxiv.org/abs/2204.09267v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Nonlinear force-free (NLFF) modeling is regularly used in order to indirectly
infer the 3D geometry of the coronal magnetic field, not accessible on a
regular basis by means of direct measurements otherwise. We study the effect of
binning in time series NLFF modeling of individual active regions (ARs) in
order to quantify the effect of a different underlying spatial resolution on
the quality of modeling as well as on the derived physical parameters. We apply
an optimization method to sequences of SDO/HMI vector magnetogram data at three
different spatial resolutions for three solar ARs to obtain nine NLFF model
time series. From the NLFF models, we deduce active-region magnetic fluxes,
electric currents, magnetic energies and relative helicities, and analyze those
with respect to the underlying spatial resolution. We calculate various metrics
to quantify the quality of the derived NLFF models and apply a Helmholtz
decomposition to characterize solenoidal errors. At a given spatial resolution,
the quality of NLFF modeling is different for different ARs, as well as varies
along of the individual model time series. For a given AR, modeling at a given
spatial resolution is not necessarily of superior quality compared to that
performed at different spatial resolutions at all time instances of a NLFF
model time series. Generally, the NLFF model quality tends to be higher at
reduced spatial resolution with the solenoidal quality being the ultimate cause
for systematic variations in model-deduced physical quantities.
Optimization-based modeling based on binned SDO/HMI vector data delivers
magnetic energies and helicity estimates different by $\lesssim$30\%, given
that concise checks ensure the physical plausibility and high solenoidal
quality of the tested model. Spatial-resolution induced differences are
relatively small compared to that arising from other sources of uncertainty.

### Title: Details in BeiDou-G2: Past and Present
* Paper ID: 2204.09258v1
* Paper URL: [http://arxiv.org/abs/2204.09258v1](http://arxiv.org/abs/2204.09258v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: In January 2022, the defunct satellite BeiDou-G2 was pulled out of
geostationary orbit by Shijian-21 to a graveyard orbit. For safe docking and
operation, it was necessary to determine the rotation state in advance. In this
paper, we show the evolution of the rotation of the BeiDou-G2 satellite based
on the photometry observation data for the past 10 years. The rotational speed
of BeiDou-G2 was found to be annual oscillation, mainly due to the solar
radiation. Based on the evolution of BeiDou-G2's rotation speed and its orbit,
we confirmed that in the last 10 years, the satellite had six abnormal events.
These abnormal events were mainly due to the increase in the rotation speed
caused by suspected fuel leakages. Additionally, the abnormal events included
one collision in 2012, which was inferred to be the trigger of the fuel
leakages in the following years. No rotational speed abnormalities occurred
again after 2017, probably due to the complete release of the residual fuel.
The parameters and the propagating models after one incidence of solar panel
damage in 2014 and one fragment in 2016 were believed to be able to satisfy the
accuracy requirements of the rotation state well at the moment of docking,
which was ultimately confirmed by Shijian-21.

### Title: Compositional Semantics and Inference System for Temporal Order based on Japanese CCG
* Paper ID: 2204.09245v1
* Paper URL: [http://arxiv.org/abs/2204.09245v1](http://arxiv.org/abs/2204.09245v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Natural Language Inference (NLI) is the task of determining whether a premise
entails a hypothesis. NLI with temporal order is a challenging task because
tense and aspect are complex linguistic phenomena involving interactions with
temporal adverbs and temporal connectives. To tackle this, temporal and
aspectual inference has been analyzed in various ways in the field of formal
semantics. However, a Japanese NLI system for temporal order based on the
analysis of formal semantics has not been sufficiently developed. We present a
logic-based NLI system that considers temporal order in Japanese based on
compositional semantics via Combinatory Categorial Grammar (CCG) syntactic
analysis. Our system performs inference involving temporal order by using
axioms for temporal relations and automated theorem provers. We evaluate our
system by experimenting with Japanese NLI datasets that involve temporal order.
We show that our system outperforms previous logic-based systems as well as
current deep learning-based models.

### Title: Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers
* Paper ID: 2204.09225v1
* Paper URL: [http://arxiv.org/abs/2204.09225v1](http://arxiv.org/abs/2204.09225v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: How to identify and characterize functional brain networks (BN) is
fundamental to gain system-level insights into the mechanisms of brain
organizational architecture. Current functional magnetic resonance (fMRI)
analysis highly relies on prior knowledge of specific patterns in either
spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain.
In addition, most approaches aim to find group-wise common functional networks,
individual-specific functional networks have been rarely studied. In this work,
we propose a novel Twin-Transformers framework to simultaneously infer common
and individual functional networks in both spatial and temporal space, in a
self-supervised manner. The first transformer takes space-divided information
as input and generates spatial features, while the second transformer takes
time-related information as input and outputs temporal features. The spatial
and temporal features are further separated into common and individual ones via
interactions (weights sharing) and constraints between the two transformers. We
applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI
dataset and identified multiple common brain networks, including both
task-related and resting-state networks (e.g., default mode network).
Interestingly, we also successfully recovered a set of individual-specific
networks that are not related to task stimulus and only exist at the individual
level.

### Title: Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
* Paper ID: 2204.09204v1
* Paper URL: [http://arxiv.org/abs/2204.09204v1](http://arxiv.org/abs/2204.09204v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.

### Title: Functional Calibration under Non-Probability Survey Sampling
* Paper ID: 2204.09193v1
* Paper URL: [http://arxiv.org/abs/2204.09193v1](http://arxiv.org/abs/2204.09193v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-probability sampling is prevailing in survey sampling, but ignoring its
selection bias leads to erroneous inferences. We offer a unified nonparametric
calibration method to estimate the sampling weights for a non-probability
sample by calibrating functions of auxiliary variables in a reproducing kernel
Hilbert space. The consistency and the limiting distribution of the proposed
estimator are established, and the corresponding variance estimator is also
investigated. Compared with existing works, the proposed method is more robust
since no parametric assumption is made for the selection mechanism of the
non-probability sample. Numerical results demonstrate that the proposed method
outperforms its competitors, especially when the model is misspecified. The
proposed method is applied to analyze the average total cholesterol of Korean
citizens based on a non-probability sample from the National Health Insurance
Sharing Service and a reference probability sample from the Korea National
Health and Nutrition Examination Survey.

### Title: Multiply-and-Fire (MNF): An Event-driven Sparse Neural Network Accelerator
* Paper ID: 2204.09797v1
* Paper URL: [http://arxiv.org/abs/2204.09797v1](http://arxiv.org/abs/2204.09797v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Machine learning, particularly deep neural network inference, has become a
vital workload for many computing systems, from data centers and HPC systems to
edge-based computing. As advances in sparsity have helped improve the
efficiency of AI acceleration, there is a continued need for improved system
efficiency for both high-performance and system-level acceleration.
  This work takes a unique look at sparsity with an event (or
activation-driven) approach to ANN acceleration that aims to minimize useless
work, improve utilization, and increase performance and energy efficiency. Our
analytical and experimental results show that this event-driven solution
presents a new direction to enable highly efficient AI inference for both CNN
and MLP workloads.
  This work demonstrates state-of-the-art energy efficiency and performance
centring on activation-based sparsity and a highly-parallel dataflow method
that improves the overall functional unit utilization (at 30 fps). This work
enhances energy efficiency over a state-of-the-art solution by 1.46$\times$.
Taken together, this methodology presents a novel, new direction to achieve
high-efficiency, high-performance designs for next-generation AI acceleration
platforms.

### Title: Estimating optimal individualized treatment rules with multistate processes
* Paper ID: 2204.09785v1
* Paper URL: [http://arxiv.org/abs/2204.09785v1](http://arxiv.org/abs/2204.09785v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Multistate process data are common in studies of chronic diseases such as
cancer. These data are ideal for precision medicine purposes as they can be
leveraged to improve more refined health outcomes, compared to standard
survival outcomes, as well as incorporate patient preferences regarding
quantity versus quality of life. However, there are currently no methods for
the estimation of optimal individualized treatment rules with such data. In
this article, we propose a nonparametric outcome weighted learning approach for
this problem in randomized clinical trial settings. The theoretical properties
of the proposed methods, including Fisher consistency and asymptotic normality
of the estimated expected outcome under the estimated optimal individualized
treatment rule, are rigorously established. A consistent closed-form variance
estimator is provided and methodology for the calculation of simultaneous
confidence intervals is proposed. Simulation studies show that the proposed
methodology and inference procedures work well even with small sample sizes and
high rates of right censoring. The methodology is illustrated using data from a
randomized clinical trial on the treatment of metastatic squamous-cell
carcinoma of the head and neck.

### Title: Time-based Self-supervised Learning for Wireless Capsule Endoscopy
* Paper ID: 2204.09773v1
* Paper URL: [http://arxiv.org/abs/2204.09773v1](http://arxiv.org/abs/2204.09773v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: State-of-the-art machine learning models, and especially deep learning ones,
are significantly data-hungry; they require vast amounts of manually labeled
samples to function correctly. However, in most medical imaging fields,
obtaining said data can be challenging. Not only the volume of data is a
problem, but also the imbalances within its classes; it is common to have many
more images of healthy patients than of those with pathology. Computer-aided
diagnostic systems suffer from these issues, usually over-designing their
models to perform accurately. This work proposes using self-supervised learning
for wireless endoscopy videos by introducing a custom-tailored method that does
not initially need labels or appropriate balance. We prove that using the
inferred inherent structure learned by our method, extracted from the temporal
axis, improves the detection rate on several domain-specific applications even
under severe imbalance.

### Title: A Hierarchical Bayesian Approach to Inverse Reinforcement Learning with Symbolic Reward Machines
* Paper ID: 2204.09772v1
* Paper URL: [http://arxiv.org/abs/2204.09772v1](http://arxiv.org/abs/2204.09772v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A misspecified reward can degrade sample efficiency and induce undesired
behaviors in reinforcement learning (RL) problems. We propose symbolic reward
machines for incorporating high-level task knowledge when specifying the reward
signals. Symbolic reward machines augment existing reward machine formalism by
allowing transitions to carry predicates and symbolic reward outputs. This
formalism lends itself well to inverse reinforcement learning, whereby the key
challenge is determining appropriate assignments to the symbolic values from a
few expert demonstrations. We propose a hierarchical Bayesian approach for
inferring the most likely assignments such that the concretized reward machine
can discriminate expert demonstrated trajectories from other trajectories with
high accuracy. Experimental results show that learned reward machines can
significantly improve training efficiency for complex RL tasks and generalize
well across different task environment configurations.

### Title: Inferring ice sheet damage models from limited observations using CRIKit: the Constitutive Relation Inference Toolkit
* Paper ID: 2204.09748v1
* Paper URL: [http://arxiv.org/abs/2204.09748v1](http://arxiv.org/abs/2204.09748v1)
* Updated Date: 2022-04-20
* Code URL: [https://gitlab.com/gbruer/ice-crikit](https://gitlab.com/gbruer/ice-crikit)
* Summary: We examine the prospect of learning ice sheet damage models from
observational data. Our approach, implemented in CRIKit (the Constitutive
Relation Inference Toolkit), is to model the material time derivative of damage
as a frame-invariant neural network, and to optimize the parameters of the
model from simulations of the flow of an ice dome. Using the model of Albrecht
and Levermann as the ground truth to generate synthetic observations, we
measure the difference of optimized neural network models from that model to
try to understand how well this process generates models that can then transfer
to other ice sheet simulations.
  The use of so-called "deep-learning" models for constitutive equations,
equations of state, sub-grid-scale processes, and other pointwise relations
that appear in systems of PDEs has been successful in other disciplines, yet
our inference setting has some confounding factors. The first is the type of
observations that are available: we compare the quality of the inferred models
when the loss of the numerical simulations includes observation misfits
throughout the ice, which is unobtainable in real settings, to losses that
include only combinations of surface and borehole observations. The second
confounding factor is the evolution of damage in an ice sheet, which is
advection dominated. The non-local effect of perturbations in a damage models
results in loss functions that have both many local minima and many parameter
configurations for which the system is unsolvable.
  Our experience suggests that basic neural networks have several deficiencies
that affect the quality of the optimized models. We suggest several approaches
to incorporating additional inductive biases into neural networks which may
lead to better performance in future work.

### Title: ColorCode: A Bayesian Approach to Augmentative and Alternative Communication with Two Buttons
* Paper ID: 2204.09745v1
* Paper URL: [http://arxiv.org/abs/2204.09745v1](http://arxiv.org/abs/2204.09745v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/mrdaly/colorcode](https://github.com/mrdaly/colorcode)
* Summary: Many people with severely limited muscle control can only communicate through
augmentative and alternative communication (AAC) systems with a small number of
buttons. In this paper, we present the design for ColorCode, which is an AAC
system with two buttons that uses Bayesian inference to determine what the user
wishes to communicate. Our information-theoretic analysis of ColorCode
simulations shows that it is efficient in extracting information from the user,
even in the presence of errors, achieving nearly optimal error correction.
ColorCode is provided as open source software
(https://github.com/mrdaly/ColorCode).

### Title: A majorization-minimization algorithm for nonnegative binary matrix factorization
* Paper ID: 2204.09741v1
* Paper URL: [http://arxiv.org/abs/2204.09741v1](http://arxiv.org/abs/2204.09741v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: This paper tackles the problem of decomposing binary data using matrix
factorization. We consider the family of mean-parametrized Bernoulli models, a
class of generative models that are well suited for modeling binary data and
enables interpretability of the factors. We factorize the Bernoulli parameter
and consider an additional Beta prior on one of the factors to further improve
the model's expressive power. While similar models have been proposed in the
literature, they only exploit the Beta prior as a proxy to ensure a valid
Bernoulli parameter in a Bayesian setting; in practice it reduces to a uniform
or uninformative prior. Besides, estimation in these models has focused on
costly Bayesian inference. In this paper, we propose a simple yet very
efficient majorization-minimization algorithm for maximum a posteriori
estimation. Our approach leverages the Beta prior whose parameters can be tuned
to improve performance in matrix completion tasks. Experiments conducted on
three public binary datasets show that our approach offers an excellent
trade-off between prediction performance, computational complexity, and
interpretability.

### Title: Physical, subjective and analogical probability
* Paper ID: 2204.10159v1
* Paper URL: [http://arxiv.org/abs/2204.10159v1](http://arxiv.org/abs/2204.10159v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The aim of this paper is to show that the concept of probability is best
understood by dividing this concept into two different types of probability,
namely physical probability and analogical probability. Loosely speaking, a
physical probability is a probability that applies to the outcomes of an
experiment that have been judged as being equally likely on the basis of
physical symmetry. Physical probabilities are arguably in some sense
'objective' and possess all the standard properties of the concept of
probability. On the other hand, an analogical probability is defined by making
an analogy between the uncertainty surrounding an event of interest and the
uncertainty surrounding an event that has a physical probability. Analogical
probabilities are undeniably subjective probabilities and are not obliged to
have all the standard mathematical properties possessed by physical
probabilities, e.g. they may not have the property of additivity or obey the
standard definition of conditional probability. Nevertheless, analogical
probabilities have extra properties, which are not possessed by physical
probabilities, that assist in their direct elicitation, general derivation,
comparison and justification. More specifically, these properties facilitate
the application of analogical probability to real-world problems that can not
be adequately resolved by using only physical probability, e.g. probabilistic
inference about hypotheses on the basis of observed data. Careful definitions
are given of the concepts that are introduced and, where appropriate, examples
of the application of these concepts are presented for additional clarity.

### Title: Understanding and Preventing Capacity Loss in Reinforcement Learning
* Paper ID: 2204.09560v1
* Paper URL: [http://arxiv.org/abs/2204.09560v1](http://arxiv.org/abs/2204.09560v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The reinforcement learning (RL) problem is rife with sources of
non-stationarity, making it a notoriously difficult problem domain for the
application of neural networks. We identify a mechanism by which non-stationary
prediction targets can prevent learning progress in deep RL agents:
\textit{capacity loss}, whereby networks trained on a sequence of target values
lose their ability to quickly update their predictions over time. We
demonstrate that capacity loss occurs in a range of RL agents and environments,
and is particularly damaging to performance in sparse-reward tasks. We then
present a simple regularizer, Initial Feature Regularization (InFeR), that
mitigates this phenomenon by regressing a subspace of features towards its
value at initialization, leading to significant performance improvements in
sparse-reward environments such as Montezuma's Revenge. We conclude that
preventing capacity loss is crucial to enable agents to maximally benefit from
the learning signals they obtain throughout the entire training trajectory.

### Title: Generalizing to the Future: Mitigating Entity Bias in Fake News Detection
* Paper ID: 2204.09484v1
* Paper URL: [http://arxiv.org/abs/2204.09484v1](http://arxiv.org/abs/2204.09484v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/ictmcg/endef-sigir2022](https://github.com/ictmcg/endef-sigir2022)
* Summary: The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.

### Title: A Generalisable Data Fusion Framework to Infer Mode of Transport Using Mobile Phone Data
* Paper ID: 2204.09482v1
* Paper URL: [http://arxiv.org/abs/2204.09482v1](http://arxiv.org/abs/2204.09482v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Cities often lack up-to-date data analytics to evaluate and implement
transport planning interventions to achieve sustainability goals, as
traditional data sources are expensive, infrequent, and suffer from data
latency. Mobile phone data provide an inexpensive source of geospatial
information to capture human mobility at unprecedented geographic and temporal
granularity. This paper proposes a method to estimate updated mode of
transportation usage in a city, with novel usage of mobile phone application
traces to infer previously hard to detect modes, such as bikes and
ride-hailing/taxi. By using data fusion and matrix factorisation, we integrate
socioeconomic and demographic attributes of the local resident population into
the model. We tested the method in a case study of Santiago (Chile), and found
that changes from 2012 to 2020 in mode of transportation inferred by the method
are coherent with expectations from domain knowledge and the literature, such
as ride-hailing trips replacing mass transport.

### Title: On the relative asymptotic expressivity of inference frameworks
* Paper ID: 2204.09457v1
* Paper URL: [http://arxiv.org/abs/2204.09457v1](http://arxiv.org/abs/2204.09457v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Let $\sigma$ be a first-order signature and let $\mathbf{W}_n$ be the set of
all $\sigma$-structures with domain $[n] = \{1, \ldots, n\}$. We can think of
each structure in $\mathbf{W}_n$ as representing a "possible (state of the)
world". By an inference framework we mean a class $\mathbf{F}$ of pairs
$(\mathbb{P}, L)$, where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$
and each $\mathbb{P}_n$ is a probability distribution on $\mathbb{W}_n$, and
$L$ is a logic with truth values in the unit interval $[0, 1]$.
  From the point of view of probabilistic and logical expressivity one may
consider an inference framework as optimal if it allows any pair $(\mathbb{P},
L)$ where $\mathbb{P} = (\mathbb{P}_n : n = 1, 2, 3, \ldots)$ is a sequence of
probability distributions on $\mathbb{W}_n$ and $L$ is a logic. But from the
point of view of using a pair $(\mathbb{P}, L)$ from such an inference
framework for making inferences on $\mathbb{W}_n$ when $n$ is large we face the
problem of computational complexity. This motivates looking for an "optimal"
trade-off (in a given context) between expressivity and computational
efficiency.
  We define a notion that an inference framework is "asymptotically at least as
expressive" as another inference framework. This relation is a preorder and we
describe a (strict) partial order on the equivalence classes of some inference
frameworks that in our opinion are natural in the context of machine learning
and artificial intelligence. The results have bearing on issues concerning
efficient learning and probabilistic inference, but are also new instances of
results in finite model theory about "almost sure elimination" of extra
syntactic features (e.g quantifiers) beyond the connectives. Often such a
result has a logical convergence law as a corollary.

### Title: Effective Goal-oriented 6G Communications: the Energy-aware Edge Inferencing Case
* Paper ID: 2204.09447v1
* Paper URL: [http://arxiv.org/abs/2204.09447v1](http://arxiv.org/abs/2204.09447v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Currently, the world experiences an unprecedentedly increasing generation of
application data, from sensor measurements to video streams, thanks to the
extreme connectivity capability provided by 5G networks. Going beyond 5G
technology, such data aim to be ingested by Artificial Intelligence (AI)
functions instantiated in the network to facilitate informed decisions,
essential for the operation of applications, such as automated driving and
factory automation. Nonetheless, while computing platforms hosting Machine
Learning (ML) models are ever powerful, their energy footprint is a key
impeding factor towards realizing a wireless network as a sustainable
intelligent platform. Focusing on a beyond 5G wireless network, overlaid by a
Multi-access Edge Computing (MEC) infrastructure with inferencing capabilities,
our paper tackles the problem of energy-aware dependable inference by
considering inference effectiveness as value of a goal that needs to be
accomplished by paying the minimum price in energy consumption. Both
MEC-assisted standalone and ensemble inference options are evaluated. It is
shown that, for some system scenarios, goal effectiveness above 84% is achieved
and sustained even by relaxing communication reliability requirements by one
decimal digit, while enjoying a device radio energy consumption reduction of
almost 23% at the same time. Also, ensemble inference is shown to improve
system-wide energy efficiency and even achieve higher goal effectiveness, as
compared to the standalone case for some system parameterizations.

### Title: Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware
* Paper ID: 2204.10183v1
* Paper URL: [http://arxiv.org/abs/2204.10183v1](http://arxiv.org/abs/2204.10183v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/bharathsudharsan/cnn_on_mcu](https://github.com/bharathsudharsan/cnn_on_mcu)
* Summary: The majority of IoT devices like smartwatches, smart plugs, HVAC controllers,
etc., are powered by hardware with a constrained specification (low memory,
clock speed and processor) which is insufficient to accommodate and execute
large, high-quality models. On such resource-constrained devices, manufacturers
still manage to provide attractive functionalities (to boost sales) by
following the traditional approach of programming IoT devices/products to
collect and transmit data (image, audio, sensor readings, etc.) to their
cloud-based ML analytics platforms. For decades, this online approach has been
facing issues such as compromised data streams, non-real-time analytics due to
latency, bandwidth constraints, costly subscriptions, recent privacy issues
raised by users and the GDPR guidelines, etc. In this paper, to enable
ultra-fast and accurate AI-based offline analytics on resource-constrained IoT
devices, we present an end-to-end multi-component model optimization sequence
and open-source its implementation. Researchers and developers can use our
optimization sequence to optimize high memory, computation demanding models in
multiple aspects in order to produce small size, low latency, low-power
consuming models that can comfortably fit and execute on resource-constrained
hardware. The experimental results show that our optimization components can
produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more
accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our
optimization sequence is generic and can be applied to any state-of-the-art
models trained for anomaly detection, predictive maintenance, robotics, voice
recognition, and machine vision.

### Title: GIMO: Gaze-Informed Human Motion Prediction in Context
* Paper ID: 2204.09443v1
* Paper URL: [http://arxiv.org/abs/2204.09443v1](http://arxiv.org/abs/2204.09443v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with eye gaze that serves as a surrogate
for inferring human intent. By employing inertial sensors for motion capture,
our data collection is not tied to specific scenes, which further boosts the
motion dynamics observed from our subjects. We perform an extensive study of
the benefits of leveraging eye gaze for ego-centric human motion prediction
with various state-of-the-art architectures. Moreover, to realize the full
potential of gaze, we propose a novel network architecture that enables
bidirectional communication between the gaze and motion branches. Our network
achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from the gaze and the denoised gaze
feature modulated by the motion. The proposed dataset and our network
implementation will be publicly available.

### Title: Multi-Auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation
* Paper ID: 2204.09422v1
* Paper URL: [http://arxiv.org/abs/2204.09422v1](http://arxiv.org/abs/2204.09422v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Recommending appropriate tags to items can facilitate content organization,
retrieval, consumption and other applications, where hybrid tag recommender
systems have been utilized to integrate collaborative information and content
information for better recommendations. In this paper, we propose a
multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for
tag recommendation, which couples item collaborative information and item
multi-auxiliary information, i.e., content and social graph, by defining a
generative process. Specifically, the model learns deep latent embeddings from
different item auxiliary information using variational auto-encoders (VAE),
which could form a generative distribution over each auxiliary information by
introducing a latent variable parameterized by deep neural network. Moreover,
to recommend tags for new items, item multi-auxiliary latent embeddings are
utilized as a surrogate through the item decoder for predicting recommendation
probabilities of each tag, where reconstruction losses are added in the
training phase to constrict the generation for feedback predictions via
different auxiliary embeddings. In addition, an inductive variational graph
auto-encoder is designed where new item nodes could be inferred in the test
phase, such that item social embeddings could be exploited for new items.
Extensive experiments on MovieLens and citeulike datasets demonstrate the
effectiveness of our method.

### Title: Distance and age of the massive stellar cluster Westerlund 1. I. Parallax method using Gaia-EDR3
* Paper ID: 2204.09414v1
* Paper URL: [http://arxiv.org/abs/2204.09414v1](http://arxiv.org/abs/2204.09414v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Westerlund 1 (Wd 1) is one of the most massive young star clusters in the
Milky Way. Although relevant for star formation and evolution, its fundamental
parameters are not yet very well constrained. Our goal is to derive an accurate
distance and provide constraints on the cluster age. We used the photometric
and astrometric information available in the Gaia Early Data Release 3
(Gaia-EDR3) to infer its distance of 4.06$^{+0.36}_{-0.34}$ kpc. Modelling of
the eclipsing binary system W36 reported in Paper II led to the distance of
4.34$\pm$0.25 kpc, in agreement with the Gaia-EDR3 distance and, therefore,
validating the parallax zero-point correction approach appropriate for red
objects. By taking advantage of another two recent distance determinations
using the Gaia-EDR3, we obtained a weighted mean distance for the cluster as
d$_{\rm wd1}$=4.23$^{+0.15}_{-0.13}$ kpc ($m-M$=13.13$^{+0.08}_{-0.07}$ mag),
which has an unprecedented accuracy of 4\%. We adopted recent Geneva
evolutionary tracks for supra-solar metallicity objects to infer the age of the
faintest RSG source from Wd 1, leading to a cluster age of 11.0$\pm$0.5 Myr, in
excellent agreement with recent work by Beasor \& Davies (10.4$^{+1.3}_{-1.2}$
Myr) based on MIST evolutionary models. The age of W36 was reported to be
3.5$\pm$0.5 Myr in Paper II, supporting recent claims of a temporal spread of
several Myr for the star-forming process within Wd 1 instead of a monolithic
starburst scenario.

### Title: Euclid detectability of pair instability supernovae in binary population synthesis models consistent with merging binary black holes
* Paper ID: 2204.09402v1
* Paper URL: [http://arxiv.org/abs/2204.09402v1](http://arxiv.org/abs/2204.09402v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We infer the expected detection number of pair instability supernovae (PISNe)
during the operation of the Euclid space telescope, based on two binary
population models that are consistent with binary black holes (BHs) observed by
gravitational waves. The two models consider different PISN criteria depending
on the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. The fiducial and
$3\sigma$ models adopt the standard and $3\sigma$-smaller $^{12}$C$(\alpha,
\gamma)^{16}$O reaction rate, which predicts that stars with helium core masses
$65-135 M_\odot$ and $90-180 M_\odot$ cause PISNe, respectively. Our fiducial
model predicts that Euclid detects several Type I or hydrogen-poor PISNe. For
the $3\sigma$ model, detection of $\sim 1$ Type I PISN by Euclid is expected if
the stellar mass distribution extends to $M_{\max} \sim 600 M_\odot$, but the
expected number becomes significantly smaller if $M_{\max} \sim 300 M_\odot$.
Thus, we may be able to prove or distinguish the fiducial and $3\sigma$ models
by the observed PISN rate. This will help us to constrain the origin of binary
BHs and the $^{12}$C$(\alpha, \gamma)^{16}$O reaction rate. PISN ejecta mass
estimates from light curves and spectra obtained by follow-up observations
would also be important to constrain the $^{12}$C$(\alpha, \gamma)^{16}$O
reaction rate.

### Title: Electrostatic Dust Ejection From Asteroid (3200) Phaethon With the Aid of Mobile Alkali Ions at Perihelion
* Paper ID: 2204.09385v1
* Paper URL: [http://arxiv.org/abs/2204.09385v1](http://arxiv.org/abs/2204.09385v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The asteroid (3200) Phaethon is known to be the parent body of the Geminids,
although meteor showers are commonly associated with the activity of periodic
comets. What is most peculiar to the asteroid is its comet-like activity in the
ejection of micrometer-sized dust particles at every perihelion passage, while
the activity of the asteroid has never been identified outside the
near-perihelion zone at $0.14~\mathrm{au}$ from the Sun. From the theoretical
point of view, we argue that the activity of the asteroid is well explained by
the electrostatic lofting of micrometer-sized dust particles with the aid of
mobile alkali ions at high temperatures. The mass-loss rates of
micrometer-sized particles from the asteroid in our model is entirely
consistent with the values inferred from visible observations of Phaethon's
dust tail. For millimeter-sized particles, we predict three orders of
magnitudes higher mass-loss rates, which could also account for the total mass
of the Geminid meteoroid stream by the electrostatic lofting mechanism.

### Title: A Variational Autoencoder for Heterogeneous Temporal and Longitudinal Data
* Paper ID: 2204.09369v1
* Paper URL: [http://arxiv.org/abs/2204.09369v1](http://arxiv.org/abs/2204.09369v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model's efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.

### Title: Multi-Component Imaging of the Fermi Gamma-ray Sky in the Spatio-spectral Domain
* Paper ID: 2204.09360v1
* Paper URL: [http://arxiv.org/abs/2204.09360v1](http://arxiv.org/abs/2204.09360v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We perform two distinct spatio-spectral reconstructions of the gamma-ray sky
in the range of 0.56-316 GeV based on Fermi Large Area Telescope (LAT) data.
Both describe the sky brightness to be composed of a diffuse-emission and a
point-source component. The first model requires minimal assumptions and
provides a template-free reconstruction as a reference. It makes use of spatial
and spectral correlations to distinguish between the different components. The
second model is physics-informed and further differentiates between diffuse
emission of hadronic and leptonic origin. For this, we assume parametric, but
spatially varying energy spectra to distinguish between the processes and use
thermal Galactic dust observations to indicate the preferred sites of hadronic
interactions. To account for instrumental effects we model the point-spread,
the energy dispersion, and the exposure of the telescope throughout the
observation. The reconstruction problem is formulated as a Bayesian inference
task, that is solved by variational inference. We show decompositions of the
Gamma-ray flux into diffuse and point-like emissions, and of the diffuse
emissions into multiple physically motivated components. The diffuse
decomposition provides an unprecedented view of the Galactic leptonic diffuse
emission. It shows the Fermi bubbles and their spectral variations in high
fidelity and other areas exhibiting strong cosmic ray electron contents, such
as a thick disk in the inner Galaxy and outflow regions. Furthermore, we report
a hard spectrum gamma ray arc in the northern outer bubble co-spatial with the
reported X-ray arc by the eROSITA collaboration. All our spatio-spectral sky
reconstructions and their uncertainty quantification are publicly available.

### Title: Inferring entropy production in anharmonic Brownian gyrators
* Paper ID: 2204.09283v1
* Paper URL: [http://arxiv.org/abs/2204.09283v1](http://arxiv.org/abs/2204.09283v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: A non-vanishing entropy production rate is one of the defining
characteristics of any non-equilibrium system, and several techniques exist to
determine this quantity directly from experimental data. The short-time
inference scheme, derived from the thermodynamic uncertainty relation, is a
recent addition to the list of these techniques. Here we apply this scheme to
quantify the entropy production rate in a class of microscopic heat engine
models called Brownian gyrators. In particular, we consider models with
anharmonic confining potentials. In these cases, the dynamical equations are
indelibly non-linear, and the exact dependences of the entropy production rate
on the model parameters are unknown. Our results demonstrate that the
short-time inference scheme can efficiently determine these dependencies from a
moderate amount of trajectory data. Furthermore, the results show that the
non-equilibrium properties of the gyrator model with anharmonic confining
potentials are considerably different from its harmonic counterpart -
especially in set-ups leading to a non-equilibrium dynamics and the resulting
gyration patterns.

### Title: A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond
* Paper ID: 2204.09269v1
* Paper URL: [http://arxiv.org/abs/2204.09269v1](http://arxiv.org/abs/2204.09269v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/litterbrother-xiao/overview-of-non-autoregressive-applications](https://github.com/litterbrother-xiao/overview-of-non-autoregressive-applications)
* Summary: Non-autoregressive (NAR) generation, which is first proposed in neural
machine translation (NMT) to speed up inference, has attracted much attention
in both machine learning and natural language processing communities. While NAR
generation can significantly accelerate inference speed for machine
translation, the speedup comes at the cost of sacrificed translation accuracy
compared to its counterpart, auto-regressive (AR) generation. In recent years,
many new models and algorithms have been designed/proposed to bridge the
accuracy gap between NAR generation and AR generation. In this paper, we
conduct a systematic survey with comparisons and discussions of various
non-autoregressive translation (NAT) models from different aspects.
Specifically, we categorize the efforts of NAT into several groups, including
data manipulation, modeling methods, training criterion, decoding algorithms,
and the benefit from pre-trained models. Furthermore, we briefly review other
applications of NAR models beyond machine translation, such as dialogue
generation, text summarization, grammar error correction, semantic parsing,
speech synthesis, and automatic speech recognition. In addition, we also
discuss potential directions for future exploration, including releasing the
dependency of KD, dynamic length prediction, pre-training for NAR, and wider
applications, etc. We hope this survey can help researchers capture the latest
progress in NAR generation, inspire the design of advanced NAR models and
algorithms, and enable industry practitioners to choose appropriate solutions
for their applications. The web page of this survey is at
\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.

### Title: The effect of spatial resolution on magnetic field modeling and helicity computation
* Paper ID: 2204.09267v1
* Paper URL: [http://arxiv.org/abs/2204.09267v1](http://arxiv.org/abs/2204.09267v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Nonlinear force-free (NLFF) modeling is regularly used in order to indirectly
infer the 3D geometry of the coronal magnetic field, not accessible on a
regular basis by means of direct measurements otherwise. We study the effect of
binning in time series NLFF modeling of individual active regions (ARs) in
order to quantify the effect of a different underlying spatial resolution on
the quality of modeling as well as on the derived physical parameters. We apply
an optimization method to sequences of SDO/HMI vector magnetogram data at three
different spatial resolutions for three solar ARs to obtain nine NLFF model
time series. From the NLFF models, we deduce active-region magnetic fluxes,
electric currents, magnetic energies and relative helicities, and analyze those
with respect to the underlying spatial resolution. We calculate various metrics
to quantify the quality of the derived NLFF models and apply a Helmholtz
decomposition to characterize solenoidal errors. At a given spatial resolution,
the quality of NLFF modeling is different for different ARs, as well as varies
along of the individual model time series. For a given AR, modeling at a given
spatial resolution is not necessarily of superior quality compared to that
performed at different spatial resolutions at all time instances of a NLFF
model time series. Generally, the NLFF model quality tends to be higher at
reduced spatial resolution with the solenoidal quality being the ultimate cause
for systematic variations in model-deduced physical quantities.
Optimization-based modeling based on binned SDO/HMI vector data delivers
magnetic energies and helicity estimates different by $\lesssim$30\%, given
that concise checks ensure the physical plausibility and high solenoidal
quality of the tested model. Spatial-resolution induced differences are
relatively small compared to that arising from other sources of uncertainty.

### Title: Compositional Semantics and Inference System for Temporal Order based on Japanese CCG
* Paper ID: 2204.09245v1
* Paper URL: [http://arxiv.org/abs/2204.09245v1](http://arxiv.org/abs/2204.09245v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Natural Language Inference (NLI) is the task of determining whether a premise
entails a hypothesis. NLI with temporal order is a challenging task because
tense and aspect are complex linguistic phenomena involving interactions with
temporal adverbs and temporal connectives. To tackle this, temporal and
aspectual inference has been analyzed in various ways in the field of formal
semantics. However, a Japanese NLI system for temporal order based on the
analysis of formal semantics has not been sufficiently developed. We present a
logic-based NLI system that considers temporal order in Japanese based on
compositional semantics via Combinatory Categorial Grammar (CCG) syntactic
analysis. Our system performs inference involving temporal order by using
axioms for temporal relations and automated theorem provers. We evaluate our
system by experimenting with Japanese NLI datasets that involve temporal order.
We show that our system outperforms previous logic-based systems as well as
current deep learning-based models.

### Title: Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers
* Paper ID: 2204.09225v1
* Paper URL: [http://arxiv.org/abs/2204.09225v1](http://arxiv.org/abs/2204.09225v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: How to identify and characterize functional brain networks (BN) is
fundamental to gain system-level insights into the mechanisms of brain
organizational architecture. Current functional magnetic resonance (fMRI)
analysis highly relies on prior knowledge of specific patterns in either
spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain.
In addition, most approaches aim to find group-wise common functional networks,
individual-specific functional networks have been rarely studied. In this work,
we propose a novel Twin-Transformers framework to simultaneously infer common
and individual functional networks in both spatial and temporal space, in a
self-supervised manner. The first transformer takes space-divided information
as input and generates spatial features, while the second transformer takes
time-related information as input and outputs temporal features. The spatial
and temporal features are further separated into common and individual ones via
interactions (weights sharing) and constraints between the two transformers. We
applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI
dataset and identified multiple common brain networks, including both
task-related and resting-state networks (e.g., default mode network).
Interestingly, we also successfully recovered a set of individual-specific
networks that are not related to task stimulus and only exist at the individual
level.

### Title: Functional Calibration under Non-Probability Survey Sampling
* Paper ID: 2204.09193v1
* Paper URL: [http://arxiv.org/abs/2204.09193v1](http://arxiv.org/abs/2204.09193v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Non-probability sampling is prevailing in survey sampling, but ignoring its
selection bias leads to erroneous inferences. We offer a unified nonparametric
calibration method to estimate the sampling weights for a non-probability
sample by calibrating functions of auxiliary variables in a reproducing kernel
Hilbert space. The consistency and the limiting distribution of the proposed
estimator are established, and the corresponding variance estimator is also
investigated. Compared with existing works, the proposed method is more robust
since no parametric assumption is made for the selection mechanism of the
non-probability sample. Numerical results demonstrate that the proposed method
outperforms its competitors, especially when the model is misspecified. The
proposed method is applied to analyze the average total cholesterol of Korean
citizens based on a non-probability sample from the National Health Insurance
Sharing Service and a reference probability sample from the Korea National
Health and Nutrition Examination Survey.

