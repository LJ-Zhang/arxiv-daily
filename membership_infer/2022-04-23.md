### Title: Learning and Inference in Sparse Coding Models with Langevin Dynamics
* Paper ID: 2204.11150v1
* Paper URL: [http://arxiv.org/abs/2204.11150v1](http://arxiv.org/abs/2204.11150v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: We describe a stochastic, dynamical system capable of inference and learning
in a probabilistic latent variable model. The most challenging problem in such
models - sampling the posterior distribution over latent variables - is
proposed to be solved by harnessing natural sources of stochasticity inherent
in electronic and neural systems. We demonstrate this idea for a sparse coding
model by deriving a continuous-time equation for inferring its latent variables
via Langevin dynamics. The model parameters are learned by simultaneously
evolving according to another continuous-time equation, thus bypassing the need
for digital accumulators or a global clock. Moreover we show that Langevin
dynamics lead to an efficient procedure for sampling from the posterior
distribution in the 'L0 sparse' regime, where latent variables are encouraged
to be set to zero as opposed to having a small L1 norm. This allows the model
to properly incorporate the notion of sparsity rather than having to resort to
a relaxed version of sparsity to make optimization tractable. Simulations of
the proposed dynamical system on both synthetic and natural image datasets
demonstrate that the model is capable of probabilistically correct inference,
enabling learning of the dictionary as well as parameters of the prior.

### Title: Gabor is Enough: Interpretable Deep Denoising with a Gabor Synthesis Dictionary Prior
* Paper ID: 2204.11146v1
* Paper URL: [http://arxiv.org/abs/2204.11146v1](http://arxiv.org/abs/2204.11146v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Image processing neural networks, natural and artificial, have a long history
with orientation-selectivity, often described mathematically as Gabor filters.
Gabor-like filters have been observed in the early layers of CNN classifiers
and even throughout low-level image processing networks. In this work, we take
this observation to the extreme and explicitly constrain the filters of a
natural-image denoising CNN to be learned 2D real Gabor filters. Surprisingly,
we find that the proposed network (GDLNet) can achieve near state-of-the-art
denoising performance amongst popular fully convolutional neural networks, with
only a fraction of the learned parameters. We further verify that this
parameterization maintains the noise-level generalization (training vs.
inference mismatch) characteristics of the base network, and investigate the
contribution of individual Gabor filter parameters to the performance of the
denoiser. We present positive findings for the interpretation of dictionary
learning networks as performing accelerated sparse-coding via the importance of
untied learned scale parameters between network layers. Our network's success
suggests that representations used by low-level image processing CNNs can be as
simple and interpretable as Gabor filterbanks.

### Title: On the randomness and correlation in the trajectories of alpha particle emitted from ${}^{241}$Am: Statistical inference based on information entropy
* Paper ID: 2204.11124v1
* Paper URL: [http://arxiv.org/abs/2204.11124v1](http://arxiv.org/abs/2204.11124v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Most particle detectors are based on the hypothesis that particles are
emitted randomly upon nuclear decay. In the present work, we tested the
hypothesis of the existence of correlation in the random trajectories of alpha
particles emitted from ${}^{241}$Am source and the null hypothesis of random
trajectories. The trajectories were clued through the registration of track in
a solid-state nuclear track detector. The experimental parameters were
optimized to identify the possible sources of correlation in the track
registration and the detector conditions upon exposure and etching to avoid
misleading results. The optimization included authentication of linearity in
registration efficiency with exposure time to prevent coalescence of registered
tracks. The statistical inference processes were based upon adaptive quadrates
analysis of the spatial data, and entropy and divergence analysis of the
quadrate data together with the null hypothesis of Poisson distribution of
random trajectories. The clustering and dispersion analysis were performed with
central deviation tendency, empirical K-function, radial distribution analysis,
and proximity Analysis. Results showed a pattern of gained information within
the registered tracks that may be attributed to being a consequence of the
structure of the source.

### Title: Power Enhancement and Phase Transitions for Global Testing of the Mixed Membership Stochastic Block Model
* Paper ID: 2204.11109v1
* Paper URL: [http://arxiv.org/abs/2204.11109v1](http://arxiv.org/abs/2204.11109v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: The mixed-membership stochastic block model (MMSBM) is a common model for
social networks. Given an $n$-node symmetric network generated from a
$K$-community MMSBM, we would like to test $K=1$ versus $K>1$. We first study
the degree-based $\chi^2$ test and the orthodox Signed Quadrilateral (oSQ)
test. These two statistics estimate an order-2 polynomial and an order-4
polynomial of a "signal" matrix, respectively. We derive the asymptotic null
distribution and power for both tests. However, for each test, there exists a
parameter regime where its power is unsatisfactory. It motivates us to propose
a power enhancement (PE) test to combine the strengths of both tests. We show
that the PE test has a tractable null distribution and improves the power of
both tests. To assess the optimality of PE, we consider a randomized setting,
where the $n$ membership vectors are independently drawn from a distribution on
the standard simplex. We show that the success of global testing is governed by
a quantity $\beta_n(K,P,h)$, which depends on the community structure matrix
$P$ and the mean vector $h$ of memberships. For each given $(K, P, h)$, a test
is called $\textit{ optimal}$ if it distinguishes two hypotheses when
$\beta_n(K, P,h)\to\infty$. A test is called $\textit{optimally adaptive}$ if
it is optimal for all $(K, P, h)$. We show that the PE test is optimally
adaptive, while many existing tests are only optimal for some particular $(K,
P, h)$, hence, not optimally adaptive.

### Title: The SCORE normalization, especially for highly heterogeneous network and text data
* Paper ID: 2204.11097v1
* Paper URL: [http://arxiv.org/abs/2204.11097v1](http://arxiv.org/abs/2204.11097v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: SCORE was introduced as a spectral approach to network community detection.
Since many networks have severe degree heterogeneity, the ordinary spectral
clustering (OSC) approach to community detection may perform unsatisfactorily.
SCORE alleviates the effect of degree heterogeneity by introducing a new
normalization idea in the spectral domain and makes OSC more effective. SCORE
is easy to use and computationally fast. It adapts easily to new directions and
sees an increasing interest in practice. In this paper, we review the basics of
SCORE, the adaption of SCORE to network mixed membership estimation and topic
modeling, and the application of SCORE in real data, including two datasets on
the publications of statisticians. We also review the theoretical 'ideology'
underlying SCORE. We show that in the spectral domain, SCORE converts a
simplicial cone to a simplex, and provides a simple and direct link between the
simplex and network memberships. SCORE attains an exponential rate and a sharp
phase transition in community detection, and achieves optimal rates in mixed
membership estimation and topic modeling.

### Title: The effects of a magnetar engine on the gamma-ray burst-associated supernovae: Application to double-peaked SN 2006aj
* Paper ID: 2204.11092v1
* Paper URL: [http://arxiv.org/abs/2204.11092v1](http://arxiv.org/abs/2204.11092v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: A millisecond magnetar engine has been widely suggested to exist in gamma-ray
burst (GRB) phenomena, in view of its substantial influences on the GRB
afterglow emission. In this paper, we investigate the effects of the magnetar
engine on the supernova (SN) emission which is associated with long GRBs and,
specifically, confront the model with the observational data of SN 2006aj/GRB
060218. SN 2006aj is featured by its remarkable double-peaked
ultraviolet-optical (UV-opt) light curves. By fitting these light curves, we
demonstrate that the first peak can be well accounted for by the breakout
emission of the shock driven by the magnetar wind, while the primary supernova
emission is also partly powered by the energy injection from the magnetar. The
magnetic field strength of the magnetar is constrained to be $\sim 10^{15}$ G,
which is in good agreement with the common results inferred from the afterglow
emission of long GRBs. In more detail, it is further suggested that the UV
excess in the late emission of the supernova could also be due to the leakage
of the non-thermal emission of the pulsar wind nebula (PWN), if some special
conditions can be satisfied. The consistency between the model and the SN
2006aj observation indicates that the magnetar engine is likely to be
ubiquitous in the GRB phenomena and even further intensify their connection
with the phenomena of superluminous supernovae.

### Title: On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation
* Paper ID: 2204.11091v1
* Paper URL: [http://arxiv.org/abs/2204.11091v1](http://arxiv.org/abs/2204.11091v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/xiaxin1998/od-rec](https://github.com/xiaxin1998/od-rec)
* Summary: Modern recommender systems operate in a fully server-based fashion. To cater
to millions of users, the frequent model maintaining and the high-speed
processing for concurrent user requests are required, which comes at the cost
of a huge carbon footprint. Meanwhile, users need to upload their behavior data
even including the immediate environmental context to the server, raising the
public concern about privacy. On-device recommender systems circumvent these
two issues with cost-conscious settings and local inference. However, due to
the limited memory and computing resources, on-device recommender systems are
confronted with two fundamental challenges: (1) how to reduce the size of
regular models to fit edge devices? (2) how to retain the original capacity?
Previous research mostly adopts tensor decomposition techniques to compress the
regular recommendation model with limited compression ratio so as to avoid
drastic performance degradation. In this paper, we explore ultra-compact models
for next-item recommendation, by loosing the constraint of dimensionality
consistency in tensor decomposition. Meanwhile, to compensate for the capacity
loss caused by compression, we develop a self-supervised knowledge distillation
framework which enables the compressed model (student) to distill the essential
information lying in the raw data, and improves the long-tail item
recommendation through an embedding-recombination strategy with the original
model (teacher). The extensive experiments on two benchmarks demonstrate that,
with 30x model size reduction, the compressed model almost comes with no
accuracy loss, and even outperforms its uncompressed counterpart in most cases.

### Title: Emerging population of gap-opening planets around type-A stars -- Long-term evolution of the forming planets around HD 163296
* Paper ID: 2204.11086v1
* Paper URL: [http://arxiv.org/abs/2204.11086v1](http://arxiv.org/abs/2204.11086v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: The presence of forming planets embedded in their protoplanetary disks has
been inferred from the detection of multiring structures in such disks. Most of
these suspected planets are undetectable by direct imaging observations at
current measurement sensitivities. Inward migration and accretion might make
these putative planets accessible to the Doppler method, but the actual extent
of growth and orbital evolution remains unconstrained.
  Under the premise that the gaps in the disk around HD 163296 originate from
new-born planets, we investigate if and under which circumstances the
gap-opening planets could represent progenitors of the exoplanet population
detected around A-type stars. In particular, we study the dependence of final
planetary masses and orbital parameters on the viscosity of the disk. The
evolution of the embedded planets was simulated throughout the disk lifetime
and up to 100 Myr after the dispersal of the disk, taking the evolving disk
structure and a likely range of disk lifetimes into account.
  We find that the final configuration of the planets is largely determined by
the $\alpha$ viscosity parameter of the disk and less dependent on the choice
for the disk lifetime and the initial planetary parameters. If we assume that
planets such as those in HD 163296 evolve to form the observed exoplanet
population of A-type stars, a $\alpha$ parameter on the order of $3.16 \times
10^{-4} \lesssim \alpha \lesssim 10^{-3}$ is required for the disks to induce
sufficiently high migration rates. Depending on whether or not future direct
imaging surveys will uncover a larger number of planets with $m_\mathrm{pl}
\lesssim 3 M_\mathrm{Jup}$ and $a_\mathrm{pl} \gtrsim 10 \mathrm{AU}$ we expect
the $\alpha$ parameter to be at the lower or upper end of this range, always
under the assumption that such disks indeed harbor wide orbit planets.

### Title: Smart App Attack: Hacking Deep Learning Models in Android Apps
* Paper ID: 2204.11075v1
* Paper URL: [http://arxiv.org/abs/2204.11075v1](http://arxiv.org/abs/2204.11075v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/jinxhy/smartappattack](https://github.com/jinxhy/smartappattack)
* Summary: On-device deep learning is rapidly gaining popularity in mobile applications.
Compared to offloading deep learning from smartphones to the cloud, on-device
deep learning enables offline model inference while preserving user privacy.
However, such mechanisms inevitably store models on users' smartphones and may
invite adversarial attacks as they are accessible to attackers. Due to the
characteristic of the on-device model, most existing adversarial attacks cannot
be directly applied for on-device models. In this paper, we introduce a
grey-box adversarial attack framework to hack on-device models by crafting
highly similar binary classification models based on identified transfer
learning approaches and pre-trained models from TensorFlow Hub. We evaluate the
attack effectiveness and generality in terms of four different settings
including pre-trained models, datasets, transfer learning approaches and
adversarial attack algorithms. The results demonstrate that the proposed
attacks remain effective regardless of different settings, and significantly
outperform state-of-the-art baselines. We further conduct an empirical study on
real-world deep learning mobile apps collected from Google Play. Among 53 apps
adopting transfer learning, we find that 71.7\% of them can be successfully
attacked, which includes popular ones in medicine, automation, and finance
categories with critical usage scenarios. The results call for the awareness
and actions of deep learning mobile app developers to secure the on-device
models. The code of this work is available at
https://github.com/Jinxhy/SmartAppAttack

### Title: Dimension Reduction for time series with Variational AutoEncoders
* Paper ID: 2204.11060v1
* Paper URL: [http://arxiv.org/abs/2204.11060v1](http://arxiv.org/abs/2204.11060v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: In this work, we explore dimensionality reduction techniques for univariate
and multivariate time series data. We especially conduct a comparison between
wavelet decomposition and convolutional variational autoencoders for dimension
reduction. We show that variational autoencoders are a good option for reducing
the dimension of high dimensional data like ECG. We make these comparisons on a
real world, publicly available, ECG dataset that has lots of variability and
use the reconstruction error as the metric. We then explore the robustness of
these models with noisy data whether for training or inference. These tests are
intended to reflect the problems that exist in real-world time series data and
the VAE was robust to both tests.

### Title: Investigating Neural Architectures by Synthetic Dataset Design
* Paper ID: 2204.11045v1
* Paper URL: [http://arxiv.org/abs/2204.11045v1](http://arxiv.org/abs/2204.11045v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/AdrienCourtois/neural-networks-properties](https://github.com/AdrienCourtois/neural-networks-properties)
* Summary: Recent years have seen the emergence of many new neural network structures
(architectures and layers). To solve a given task, a network requires a certain
set of abilities reflected in its structure. The required abilities depend on
each task. There is so far no systematic study of the real capacities of the
proposed neural structures. The question of what each structure can and cannot
achieve is only partially answered by its performance on common benchmarks.
Indeed, natural data contain complex unknown statistical cues. It is therefore
impossible to know what cues a given neural structure is taking advantage of in
such data. In this work, we sketch a methodology to measure the effect of each
structure on a network's ability, by designing ad hoc synthetic datasets. Each
dataset is tailored to assess a given ability and is reduced to its simplest
form: each input contains exactly the amount of information needed to solve the
task. We illustrate our methodology by building three datasets to evaluate each
of the three following network properties: a) the ability to link local cues to
distant inferences, b) the translation covariance and c) the ability to group
pixels with the same characteristics and share information among them. Using a
first simplified depth estimation dataset, we pinpoint a serious nonlocal
deficit of the U-Net. We then evaluate how to resolve this limitation by
embedding its structure with nonlocal layers, which allow computing complex
features with long-range dependencies. Using a second dataset, we compare
different positional encoding methods and use the results to further improve
the U-Net on the depth estimation task. The third introduced dataset serves to
demonstrate the need for self-attention-like mechanisms for resolving more
realistic depth estimation tasks.

### Title: Compact star merger events with stars composed of interacting strange quark matter
* Paper ID: 2204.11034v1
* Paper URL: [http://arxiv.org/abs/2204.11034v1](http://arxiv.org/abs/2204.11034v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: We investigate the properties of stars participating in double compact star
merger events considering interacting model of stable strange quark matter. We
model the matter making it compatible with the recent astrophysical
observations of compact star mass-radius and gravitational wave events. In this
context we consider modified MIT bag model and vector bag model with and
without self interaction. We find new upper bound on tidal deformability of
$1.4~M_\odot$ strange star corresponding to the upper bound of effective tidal
deformability inferred from gravitational wave event. Range of compactness of
$1.4~M_\odot$ strange star is obtained as ${0.175}\leq{C_{1.4}}\leq{0.199}$.
Radius range of $1.5M_\odot$ primary star is deduced to be
${10.57}\leq{R_{1.5}}\leq{12.04}$ km, following stringent GW170817 constraints.
GW190425 constraints provide with upper limit on radius of $1.7$ solar mass
strange star that it should be less than $13.41$ $\text{km}$.

### Title: Federated Geometric Monte Carlo Clustering to Counter Non-IID Datasets
* Paper ID: 2204.11017v1
* Paper URL: [http://arxiv.org/abs/2204.11017v1](http://arxiv.org/abs/2204.11017v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Federated learning allows clients to collaboratively train models on datasets
that are acquired in different locations and that cannot be exchanged because
of their size or regulations. Such collected data is increasingly
non-independent and non-identically distributed (non-IID), negatively affecting
training accuracy. Previous works tried to mitigate the effects of non-IID
datasets on training accuracy, focusing mainly on non-IID labels, however
practical datasets often also contain non-IID features. To address both non-IID
labels and features, we propose FedGMCC, a novel framework where a central
server aggregates client models that it can cluster together. FedGMCC
clustering relies on a Monte Carlo procedure that samples the output space of
client models, infers their position in the weight space on a loss manifold and
computes their geometric connection via an affine curve parametrization.
FedGMCC aggregates connected models along their path connectivity to produce a
richer global model, incorporating knowledge of all connected client models.
FedGMCC outperforms FedAvg and FedProx in terms of convergence rates on the
EMNIST62 and a genomic sequence classification datasets (by up to +63%).
FedGMCC yields an improved accuracy (+4%) on the genomic dataset with respect
to CFL, in high non-IID feature space settings and label incongruency.

### Title: Surface Reconstruction from Point Clouds by Learning Predictive Context Priors
* Paper ID: 2204.11015v1
* Paper URL: [http://arxiv.org/abs/2204.11015v1](http://arxiv.org/abs/2204.11015v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/mabaorui/predictablecontextprior](https://github.com/mabaorui/predictablecontextprior)
* Summary: Surface reconstruction from point clouds is vital for 3D computer vision.
State-of-the-art methods leverage large datasets to first learn local context
priors that are represented as neural network-based signed distance functions
(SDFs) with some parameters encoding the local contexts. To reconstruct a
surface at a specific query location at inference time, these methods then
match the local reconstruction target by searching for the best match in the
local prior space (by optimizing the parameters encoding the local context) at
the given query location. However, this requires the local context prior to
generalize to a wide variety of unseen target regions, which is hard to
achieve. To resolve this issue, we introduce Predictive Context Priors by
learning Predictive Queries for each specific point cloud at inference time.
Specifically, we first train a local context prior using a large point cloud
dataset similar to previous techniques. For surface reconstruction at inference
time, however, we specialize the local context prior into our Predictive
Context Prior by learning Predictive Queries, which predict adjusted spatial
query locations as displacements of the original locations. This leads to a
global SDF that fits the specific point cloud the best. Intuitively, the query
prediction enables us to flexibly search the learned local context prior over
the entire prior space, rather than being restricted to the fixed query
locations, and this improves the generalizability. Our method does not require
ground truth signed distances, normals, or any additional procedure of signed
distance fusion across overlapping regions. Our experimental results in surface
reconstruction for single shapes or complex scenes show significant
improvements over the state-of-the-art under widely used benchmarks.

### Title: Discriminative Feature Learning Framework with Gradient Preference for Anomaly Detection
* Paper ID: 2204.11014v1
* Paper URL: [http://arxiv.org/abs/2204.11014v1](http://arxiv.org/abs/2204.11014v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Unsupervised representation learning has been extensively employed in anomaly
detection, achieving impressive performance. Extracting valuable feature
vectors that can remarkably improve the performance of anomaly detection are
essential in unsupervised representation learning. To this end, we propose a
novel discriminative feature learning framework with gradient preference for
anomaly detection. Specifically, we firstly design a gradient preference based
selector to store powerful feature points in space and then construct a feature
repository, which alleviate the interference of redundant feature vectors and
improve inference efficiency. To overcome the looseness of feature vectors,
secondly, we present a discriminative feature learning with center constrain to
map the feature repository to a compact subspace, so that the anomalous samples
are more distinguishable from the normal ones. Moreover, our method can be
easily extended to anomaly localization. Extensive experiments on popular
industrial and medical anomaly detection datasets demonstrate our proposed
framework can achieve competitive results in both anomaly detection and
localization. More important, our method outperforms the state-of-the-art in
few shot anomaly detection.

### Title: An Efficient Approach for Optimizing the Cost-effective Individualized Treatment Rule Using Conditional Random Forest
* Paper ID: 2204.10971v1
* Paper URL: [http://arxiv.org/abs/2204.10971v1](http://arxiv.org/abs/2204.10971v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/crystalxur/efficientceitr](https://github.com/crystalxur/efficientceitr)
* Summary: Evidence from observational studies has become increasingly important for
supporting healthcare policy making via cost-effectiveness (CE) analyses.
Similar as in comparative effectiveness studies, health economic evaluations
that consider subject-level heterogeneity produce individualized treatment
rules (ITRs) that are often more cost-effective than one-size-fits-all
treatment. Thus, it is of great interest to develop statistical tools for
learning such a cost-effective ITR (CE-ITR) under the causal inference
framework that allows proper handling of potential confounding and can be
applied to both trials and observational studies. In this paper, we use the
concept of net-monetary-benefit (NMB) to assess the trade-off between health
benefits and related costs. We estimate CE-ITR as a function of patients'
characteristics that, when implemented, optimizes the allocation of limited
healthcare resources by maximizing health gains while minimizing
treatment-related costs. We employ the conditional random forest approach and
identify the optimal CE-ITR using NMB-based classification algorithms, where
two partitioned estimators are proposed for the subject-specific weights to
effectively incorporate information from censored individuals. We conduct
simulation studies to evaluate the performance of our proposals. We apply our
top-performing algorithm to the NIH-funded Systolic Blood Pressure Intervention
Trial (SPRINT) to illustrate the CE gains of assigning customized intensive
blood pressure therapy.

### Title: When Doubly Robust Methods Meet Machine Learning for Estimating Treatment Effects from Real-World Data: A Comparative Study
* Paper ID: 2204.10969v1
* Paper URL: [http://arxiv.org/abs/2204.10969v1](http://arxiv.org/abs/2204.10969v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Observational cohort studies are increasingly being used for comparative
effectiveness research and to assess the safety of therapeutics. Recently,
various doubly robust methods have been proposed for average treatment effect
estimation by combining the treatment model and the outcome model via different
vehicles, such as matching, weighting, and regression. The key advantage of the
doubly robust estimators is that they require either the treatment model or the
outcome model to be correctly specified to obtain a consistent estimator of the
average treatment effect, and therefore lead to a more accurate and often more
precise inference. However, little work has been done to understand how doubly
robust estimators differ due to their unique strategies of using the treatment
and outcome models and how machine learning techniques can be combined with
these estimators to boost their performance. Also, little has been understood
about the challenges of covariates selection, overlapping of the covariate
distribution, and treatment effect heterogeneity on the performance of these
doubly robust estimators. Here we examine multiple popular doubly robust
methods in the categories of matching, weighting, or regression, and compare
their performance using different treatment and outcome modeling via extensive
simulations and a real-world application. We found that incorporating machine
learning with doubly robust estimators such as the targeted maximum likelihood
estimator outperforms. Practical guidance on how to apply doubly robust
estimators is provided.

### Title: Statistical inference of travelers' route choice preferences with system-level data
* Paper ID: 2204.10964v1
* Paper URL: [http://arxiv.org/abs/2204.10964v1](http://arxiv.org/abs/2204.10964v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Traditional network models encapsulate travel behavior among all
origin-destination pairs based on a simplified and generic utility function.
Typically, the utility function consists of travel time solely and its
coefficients are equated to estimates obtained from stated preference data.
While this modeling strategy is reasonable, the inherent sampling bias in
individual-level data may be further amplified over network flow aggregation,
leading to inaccurate flow estimates. This data must be collected from surveys
or travel diaries, which may be labor intensive, costly and limited to a small
time period. To address these limitations, this study extends classical
bi-level formulations to estimate travelers' utility functions with multiple
attributes using system-level data. We formulate a methodology grounded on
non-linear least squares to statistically infer travelers' utility function in
the network context using traffic counts, traffic speeds, traffic incidents and
sociodemographic information, among other attributes. The analysis of the
mathematical properties of the optimization problem and of its pseudo-convexity
motivate the use of normalized gradient descent. We also develop a hypothesis
test framework to examine statistical properties of the utility function
coefficients and to perform attributes selection. Experiments on synthetic data
show that the coefficients are consistently recovered and that hypothesis tests
are a reliable statistic to identify which attributes are determinants of
travelers' route choices. Besides, a series of Monte-Carlo experiments suggest
that statistical inference is robust to noise in the Origin-Destination matrix
and in the traffic counts, and to various levels of sensor coverage. The
methodology is also deployed at a large scale using real-world multi-source
data in Fresno, CA collected before and during the COVID-19 outbreak.

### Title: Local Gaussian process extrapolation for BART models with applications to causal inference
* Paper ID: 2204.10963v1
* Paper URL: [http://arxiv.org/abs/2204.10963v1](http://arxiv.org/abs/2204.10963v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Bayesian additive regression trees (BART) is a semi-parametric regression
model offering state-of-the-art performance on out-of-sample prediction.
Despite this success, standard implementations of BART typically provide
inaccurate prediction and overly narrow prediction intervals at points outside
the range of the training data. This paper proposes a novel extrapolation
strategy that grafts Gaussian processes to the leaf nodes in BART for
predicting points outside the range of the observed data. The new method is
compared to standard BART implementations and recent frequentist
resampling-based methods for predictive inference. We apply the new approach to
a challenging problem from causal inference, wherein for some regions of
predictor space, only treated or untreated units are observed (but not both).
In simulations studies, the new approach boasts superior performance compared
to popular alternatives, such as Jackknife+.

