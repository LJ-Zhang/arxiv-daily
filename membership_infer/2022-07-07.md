### Title: Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection
* Paper ID: 2207.03482v1
* Paper URL: [http://arxiv.org/abs/2207.03482v1](http://arxiv.org/abs/2207.03482v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV', 'cs.AI']
* Code URL: [https://github.com/hanoonaR/object-centric-ovd](https://github.com/hanoonaR/object-centric-ovd)
* Summary: Existing open-vocabulary object detectors typically enlarge their vocabulary
sizes by leveraging different forms of weak supervision. This helps generalize
to novel objects at inference. Two popular forms of weak-supervision used in
open-vocabulary detection (OVD) include pretrained CLIP model and image-level
supervision. We note that both these modes of supervision are not optimally
aligned for the detection task: CLIP is trained with image-text pairs and lacks
precise localization of objects while the image-level supervision has been used
with heuristics that do not accurately specify local object regions. In this
work, we propose to address this problem by performing object-centric alignment
of the language embeddings from the CLIP model. Furthermore, we visually ground
the objects with only image-level supervision using a pseudo-labeling process
that provides high-quality object proposals and helps expand the vocabulary
during training. We establish a bridge between the above two object-alignment
strategies via a novel weight transfer function that aggregates their
complimentary strengths. In essence, the proposed model seeks to minimize the
gap between object and image-centric representations in the OVD setting. On the
COCO benchmark, our proposed approach achieves 40.3 AP50 on novel classes, an
absolute 11.9 gain over the previous best performance.For LVIS, we surpass the
state-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall.
Code: https://bit.ly/3byZoQp.

### Title: Inferring Structural Parameters of Low-Surface-Brightness-Galaxies with Uncertainty Quantification using Bayesian Neural Networks
* Paper ID: 2207.03471v1
* Paper URL: [http://arxiv.org/abs/2207.03471v1](http://arxiv.org/abs/2207.03471v1)
* Updated Date: 2022-07-07
* Categories: ['astro-ph.IM', 'astro-ph.GA', 'cs.LG']
* Code URL: [https://github.com/dtanoglidis/bnn_lsbgs_icml](https://github.com/dtanoglidis/bnn_lsbgs_icml)
* Summary: Measuring the structural parameters (size, total brightness, light
concentration, etc.) of galaxies is a significant first step towards a
quantitative description of different galaxy populations. In this work, we
demonstrate that a Bayesian Neural Network (BNN) can be used for the inference,
with uncertainty quantification, of such morphological parameters from
simulated low-surface-brightness galaxy images. Compared to traditional
profile-fitting methods, we show that the uncertainties obtained using BNNs are
comparable in magnitude, well-calibrated, and the point estimates of the
parameters are closer to the true values. Our method is also significantly
faster, which is very important with the advent of the era of large galaxy
surveys and big data in astrophysics.

### Title: A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion
* Paper ID: 2207.03430v1
* Paper URL: [http://arxiv.org/abs/2207.03430v1](http://arxiv.org/abs/2207.03430v1)
* Updated Date: 2022-07-07
* Categories: ['eess.IV', 'cs.CV']
* Code URL: null
* Summary: Multi-modal medical image completion has been extensively applied to
alleviate the missing modality issue in a wealth of multi-modal diagnostic
tasks. However, for most existing synthesis methods, their inferences of
missing modalities can collapse into a deterministic mapping from the available
ones, ignoring the uncertainties inherent in the cross-modal relationships.
Here, we propose the Unified Multi-Modal Conditional Score-based Generative
Model (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in
modeling and stochastically sampling a target probability distribution, and
further extend SGM to cross-modal conditional synthesis for various
missing-modality configurations in a unified framework. Specifically, UMM-CSGM
employs a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn
a comprehensive set of cross-modal conditional distributions via conditional
diffusion and reverse generation in the complete modality space. In this way,
the generation process can be accurately conditioned by all available
information, and can fit all possible configurations of missing modalities in a
single network. Experiments on BraTS19 dataset show that the UMM-CSGM can more
reliably synthesize the heterogeneous enhancement and irregular area in
tumor-induced lesions for any missing modalities.

### Title: Domain Knowledge Driven 3D Dose Prediction Using Moment-Based Loss Function
* Paper ID: 2207.03414v1
* Paper URL: [http://arxiv.org/abs/2207.03414v1](http://arxiv.org/abs/2207.03414v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV']
* Code URL: null
* Summary: Dose volume histogram (DVH) metrics are widely accepted evaluation criteria
in the clinic. However, incorporating these metrics into deep learning dose
prediction models is challenging due to their non-convexity and
non-differentiability. We propose a novel moment-based loss function for
predicting 3D dose distribution for the challenging conventional lung intensity
modulated radiation therapy (IMRT) plans. The moment-based loss function is
convex and differentiable and can easily incorporate DVH metrics in any deep
learning framework without computational overhead. The moments can also be
customized to reflect the clinical priorities in 3D dose prediction. For
instance, using high-order moments allows better prediction in high-dose areas
for serial structures. We used a large dataset of 360 (240 for training, 50 for
validation and 70 for testing) conventional lung patients with 2Gy $\times$ 30
fractions to train the deep learning (DL) model using clinically treated plans
at our institution. We trained a UNet like CNN architecture using computed
tomography (CT), planning target volume (PTV) and organ-at-risk contours (OAR)
as input to infer corresponding voxel-wise 3D dose distribution. We evaluated
three different loss functions: (1) The popular Mean Absolute Error (MAE) Loss,
(2) the recently developed MAE + DVH Loss, and (3) the proposed MAE + Moments
Loss. The quality of the predictions was compared using different DVH metrics
as well as dose-score and DVH-score, recently introduced by the AAPM
knowledge-based planning grand challenge. Model with (MAE + Moment) loss
function outperformed the model with MAE loss by significantly improving the
DVH-score (11%, p$<$0.01) while having similar computational cost. It also
outperformed the model trained with (MAE+DVH) by significantly improving the
computational cost (48%) and the DVH-score (8%, p$<$0.01).

### Title: HE-PEx: Efficient Machine Learning under Homomorphic Encryption using Pruning, Permutation and Expansion
* Paper ID: 2207.03384v1
* Paper URL: [http://arxiv.org/abs/2207.03384v1](http://arxiv.org/abs/2207.03384v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CR', 'cs.LG']
* Code URL: null
* Summary: Privacy-preserving neural network (NN) inference solutions have recently
gained significant traction with several solutions that provide different
latency-bandwidth trade-offs. Of these, many rely on homomorphic encryption
(HE), a method of performing computations over encrypted data. However, HE
operations even with state-of-the-art schemes are still considerably slow
compared to their plaintext counterparts. Pruning the parameters of a NN model
is a well-known approach to improving inference latency. However, pruning
methods that are useful in the plaintext context may lend nearly negligible
improvement in the HE case, as has also been demonstrated in recent work.
  In this work, we propose a novel set of pruning methods that reduce the
latency and memory requirement, thus bringing the effectiveness of plaintext
pruning methods to HE. Crucially, our proposal employs two key techniques, viz.
permutation and expansion of the packed model weights, that enable pruning
significantly more ciphertexts and recuperating most of the accuracy loss,
respectively. We demonstrate the advantage of our method on fully connected
layers where the weights are packed using a recently proposed packing technique
called tile tensors, which allows executing deep NN inference in a
non-interactive mode. We evaluate our methods on various autoencoder
architectures and demonstrate that for a small mean-square reconstruction loss
of 1.5*10^{-5} on MNIST, we reduce the memory requirement and latency of
HE-enabled inference by 60%.

### Title: Challenges and Pitfalls of Bayesian Unlearning
* Paper ID: 2207.03227v1
* Paper URL: [http://arxiv.org/abs/2207.03227v1](http://arxiv.org/abs/2207.03227v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.AI', 'stat.ML']
* Code URL: null
* Summary: Machine unlearning refers to the task of removing a subset of training data,
thereby removing its contributions to a trained model. Approximate unlearning
are one class of methods for this task which avoid the need to retrain the
model from scratch on the retained data. Bayes' rule can be used to cast
approximate unlearning as an inference problem where the objective is to obtain
the updated posterior by dividing out the likelihood of deleted data. However
this has its own set of challenges as one often doesn't have access to the
exact posterior of the model parameters. In this work we examine the use of the
Laplace approximation and Variational Inference to obtain the updated
posterior. With a neural network trained for a regression task as the guiding
example, we draw insights on the applicability of Bayesian unlearning in
practical scenarios.

### Title: Harnessing Out-Of-Distribution Examples via Augmenting Content and Style
* Paper ID: 2207.03162v1
* Paper URL: [http://arxiv.org/abs/2207.03162v1](http://arxiv.org/abs/2207.03162v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG']
* Code URL: null
* Summary: Machine learning models are vulnerable to Out-Of-Distribution (OOD) examples,
such a problem has drawn much attention. However, current methods lack a full
understanding of different types of OOD data: there are benign OOD data that
can be properly adapted to enhance the learning performance, while other malign
OOD data would severely degenerate the classification result. To Harness OOD
data, this paper proposes HOOD method that can leverage the content and style
from each image instance to identify benign and malign OOD data. Particularly,
we design a variational inference framework to causally disentangle content and
style features by constructing a structural causal model. Subsequently, we
augment the content and style through an intervention process to produce malign
and benign OOD data, respectively. The benign OOD data contain novel styles but
hold our interested contents, and they can be leveraged to help train a
style-invariant model. In contrast, the malign OOD data inherit unknown
contents but carry familiar styles, by detecting them can improve model
robustness against deceiving anomalies. Thanks to the proposed novel
disentanglement and data augmentation techniques, HOOD can effectively deal
with OOD examples in unknown and open environments, whose effectiveness is
empirically validated in three typical OOD applications including OOD
detection, open-set semi-supervised learning, and open-set domain adaptation.

### Title: Equivariant Representation Learning via Class-Pose Decomposition
* Paper ID: 2207.03116v1
* Paper URL: [http://arxiv.org/abs/2207.03116v1](http://arxiv.org/abs/2207.03116v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'math.GR']
* Code URL: [https://github.com/equivariant-ml/equivariant-representation-learning](https://github.com/equivariant-ml/equivariant-representation-learning)
* Summary: We introduce a general method for learning representations that are
equivariant to symmetries of data. The central idea is to to decompose the
latent space in an invariant factor and the symmetry group itself. The
components semantically correspond to intrinsic data classes and poses
respectively. The learner is self-supervised and infers these semantics based
on relative symmetry information. The approach is motivated by theoretical
results from group theory and guarantees representations that are lossless,
interpretable and disentangled. We empirically investigate the approach via
experiments involving datasets with a variety of symmetries. Results show that
our representations capture the geometry of data and outperform other
equivariant representation learning frameworks.

### Title: An Additive Instance-Wise Approach to Multi-class Model Interpretation
* Paper ID: 2207.03113v1
* Paper URL: [http://arxiv.org/abs/2207.03113v1](http://arxiv.org/abs/2207.03113v1)
* Updated Date: 2022-07-07
* Categories: ['cs.LG', 'cs.AI']
* Code URL: [https://github.com/isvy08/aim](https://github.com/isvy08/aim)
* Summary: Interpretable machine learning offers insights into what factors drive a
certain prediction of a black-box system and whether to trust it for
high-stakes decisions or large-scale deployment. Existing methods mainly focus
on selecting explanatory input features, which follow either locally additive
or instance-wise approaches. Additive models use heuristically sampled
perturbations to learn instance-specific explainers sequentially. The process
is thus inefficient and susceptible to poorly-conditioned samples. Meanwhile,
instance-wise techniques directly learn local sampling distributions and can
leverage global information from other inputs. However, they can only interpret
single-class predictions and suffer from inconsistency across different
settings, due to a strict reliance on a pre-defined number of features
selected. This work exploits the strengths of both methods and proposes a
global framework for learning local explanations simultaneously for multiple
target classes. We also propose an adaptive inference strategy to determine the
optimal number of features for a specific instance. Our model explainer
significantly outperforms additive and instance-wise counterparts on
faithfulness while achieves high level of brevity on various data sets and
black-box model architectures.

### Title: Quantum Advantage in Variational Bayes Inference
* Paper ID: 2207.03104v1
* Paper URL: [http://arxiv.org/abs/2207.03104v1](http://arxiv.org/abs/2207.03104v1)
* Updated Date: 2022-07-07
* Categories: ['stat.ML', 'cond-mat.stat-mech', 'cs.LG', 'quant-ph']
* Code URL: null
* Summary: Variational Bayes (VB) inference algorithm is used widely to estimate both
the parameters and the unobserved hidden variables in generative statistical
models. The algorithm -- inspired by variational methods used in computational
physics -- is iterative and can get easily stuck in local minima, even when
classical techniques, such as deterministic annealing (DA), are used. We study
a variational Bayes (VB) inference algorithm based on a non-traditional quantum
annealing approach -- referred to as quantum annealing variational Bayes (QAVB)
inference -- and show that there is indeed a quantum advantage to QAVB over its
classical counterparts. In particular, we show that such better performance is
rooted in key concepts from quantum mechanics: (i) the ground state of the
Hamiltonian of a quantum system -- defined from the given variational Bayes
(VB) problem -- corresponds to an optimal solution for the minimization problem
of the variational free energy at very low temperatures; (ii) such a ground
state can be achieved by a technique paralleling the quantum annealing process;
and (iii) starting from this ground state, the optimal solution to the VB
problem can be achieved by increasing the heat bath temperature to unity, and
thereby avoiding local minima introduced by spontaneous symmetry breaking
observed in classical physics based VB algorithms. We also show that the update
equations of QAVB can be potentially implemented using $\lceil \log K \rceil$
qubits and $\mathcal{O} (K)$ operations per step. Thus, QAVB can match the time
complexity of existing VB algorithms, while delivering higher performance.

### Title: Backpropagation on Dynamical Networks
* Paper ID: 2207.03093v1
* Paper URL: [http://arxiv.org/abs/2207.03093v1](http://arxiv.org/abs/2207.03093v1)
* Updated Date: 2022-07-07
* Categories: ['math.DS', 'cs.LG', 'nlin.CD']
* Code URL: null
* Summary: Dynamical networks are versatile models that can describe a variety of
behaviours such as synchronisation and feedback. However, applying these models
in real world contexts is difficult as prior information pertaining to the
connectivity structure or local dynamics is often unknown and must be inferred
from time series observations of network states. Additionally, the influence of
coupling interactions between nodes further complicates the isolation of local
node dynamics. Given the architectural similarities between dynamical networks
and recurrent neural networks (RNN), we propose a network inference method
based on the backpropagation through time (BPTT) algorithm commonly used to
train recurrent neural networks. This method aims to simultaneously infer both
the connectivity structure and local node dynamics purely from observation of
node states. An approximation of local node dynamics is first constructed using
a neural network. This is alternated with an adapted BPTT algorithm to regress
corresponding network weights by minimising prediction errors of the dynamical
network based on the previously constructed local models until convergence is
achieved. This method was found to be succesful in identifying the connectivity
structure for coupled networks of Lorenz, Chua and FitzHugh-Nagumo oscillators.
Freerun prediction performance with the resulting local models and weights was
found to be comparable to the true system with noisy initial conditions. The
method is also extended to non-conventional network couplings such as
asymmetric negative coupling.

### Title: AV-Gaze: A Study on the Effectiveness of Audio Guided Visual Attention Estimation for Non-Profilic Faces
* Paper ID: 2207.03048v1
* Paper URL: [http://arxiv.org/abs/2207.03048v1](http://arxiv.org/abs/2207.03048v1)
* Updated Date: 2022-07-07
* Categories: ['cs.CV']
* Code URL: null
* Summary: In challenging real-life conditions such as extreme head-pose, occlusions,
and low-resolution images where the visual information fails to estimate visual
attention/gaze direction, audio signals could provide important and
complementary information. In this paper, we explore if audio-guided coarse
head-pose can further enhance visual attention estimation performance for
non-prolific faces. Since it is difficult to annotate audio signals for
estimating the head-pose of the speaker, we use off-the-shelf state-of-the-art
models to facilitate cross-modal weak-supervision. During the training phase,
the framework learns complementary information from synchronized audio-visual
modality. Our model can utilize any of the available modalities i.e. audio,
visual or audio-visual for task-specific inference. It is interesting to note
that, when AV-Gaze is tested on benchmark datasets with these specific
modalities, it achieves competitive results on multiple datasets, while being
highly adaptive towards challenging scenarios.

