### Title: Asymptotic Inference for Infinitely Imbalanced Logistic Regression
* Paper ID: 2204.13231v1
* Paper URL: [http://arxiv.org/abs/2204.13231v1](http://arxiv.org/abs/2204.13231v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: In this paper we extend the work of Owen (2007) by deriving a second order
expansion for the slope parameter in logistic regression, when the size of the
majority class is unbounded and the minority class is finite. More precisely,
we demonstrate that the second order term converges to a normal distribution
and explicitly compute its variance, which surprisingly once again depends only
on the mean of the minority class points and not their arrangement under mild
regularity assumptions. In the case that the majority class is normally
distributed, we illustrate that the variance of the the limiting slope depends
exponentially on the z-score of the average of the minority class's points with
respect to the majority class's distribution. We confirm our results by Monte
Carlo simulations.

### Title: Map-based cosmology inference with lognormal cosmic shear maps
* Paper ID: 2204.13216v1
* Paper URL: [http://arxiv.org/abs/2204.13216v1](http://arxiv.org/abs/2204.13216v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Most cosmic shear analyses to date have relied on summary statistics (e.g.
$\xi_+$ and $\xi_-$). These types of analyses are necessarily sub-optimal, as
the use of summary statistics is lossy. In this paper, we forward-model the
convergence field of the Universe as a lognormal random field conditioned on
the observed shear data. This new map-based inference framework enables us to
recover the joint posterior of the cosmological parameters and the convergence
field of the Universe. Our analysis properly accounts for the covariance in the
mass maps across tomographic bins, which significantly improves the fidelity of
the maps relative to single-bin reconstructions. We verify that applying our
inference pipeline to Gaussian random fields recovers posteriors that are in
excellent agreement with their analytical counterparts. At the resolution of
our maps -- and to the extent that the convergence field can be described by
the lognormal model -- our map posteriors allow us to reconstruct \it all \rm
summary statistics (including non-Gaussian statistics). We forecast that a
map-based inference analysis of LSST-Y10 data can improve cosmological
constraints in the $\sigma_8$--$\Omega_{\rm m}$ plane by $\approx 30\%$
relative to the currently standard cosmic shear analysis. This improvement
happens almost entirely along the $S_8=\sigma_8\Omega_{\rm m}^{1/2}$
directions, meaning map-based inference fails to significantly improve
constraints on $S_8$.

### Title: On randomization inference after inexact Mahalanobis matching
* Paper ID: 2204.13193v1
* Paper URL: [http://arxiv.org/abs/2204.13193v1](http://arxiv.org/abs/2204.13193v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: In observational causal inference, matched-pairs studies are often analyzed
using randomization tests. While intuitively appealing, these tests are not
formally justified by the randomness in the treatment assignment process (the
"design") unless all matches are exact. This paper asks whether these tests can
instead be justified by the random sampling of experimental units. We find that
under fairly restrictive sampling assumptions, the paired randomization test
based on a regression-adjusted test statistic may be asymptotically valid
despite inexact matching. We propose a new randomization test based on matching
with replacement that can be justified under weaker sampling assumptions.

### Title: SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks
* Paper ID: 2204.13153v1
* Paper URL: [http://arxiv.org/abs/2204.13153v1](http://arxiv.org/abs/2204.13153v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: This paper follows cognitive studies to investigate a graph representation
for sketches, where the information of strokes, i.e., parts of a sketch, are
encoded on vertices and information of inter-stroke on edges. The resultant
graph representation facilitates the training of a Graph Neural Networks for
classification tasks, and achieves accuracy and robustness comparable to the
state-of-the-art against translation and rotation attacks, as well as stronger
attacks on graph vertices and topologies, i.e., modifications and addition of
strokes, all without resorting to adversarial training. Prior studies on
sketches, e.g., graph transformers, encode control points of stroke on
vertices, which are not invariant to spatial transformations. In contrary, we
encode vertices and edges using pairwise distances among control points to
achieve invariance. Compared with existing generative sketch model for one-shot
classification, our method does not rely on run-time statistical inference.
Lastly, the proposed representation enables generation of novel sketches that
are structurally similar to while separable from the existing dataset.

### Title: What is in #P and what is not?
* Paper ID: 2204.13149v1
* Paper URL: [http://arxiv.org/abs/2204.13149v1](http://arxiv.org/abs/2204.13149v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: For several classical nonnegative integer functions, we investigate if they
are members of the counting complexity class #P or not. We prove #P membership
in surprising cases, and in other cases we prove non-membership, relying on
standard complexity assumptions or on oracle separations.
  We initiate the study of the polynomial closure properties of #P on affine
varieties, i.e., if all problem instances satisfy algebraic constraints. This
is directly linked to classical combinatorial proofs of algebraic identities
and inequalities. We investigate #TFNP and obtain oracle separations that prove
the strict inclusion of #P in all standard syntactic subclasses of #TFNP-1.

### Title: Proximal Causal Inference for Marginal Counterfactual Survival Curves
* Paper ID: 2204.13144v1
* Paper URL: [http://arxiv.org/abs/2204.13144v1](http://arxiv.org/abs/2204.13144v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Contrasting marginal counterfactual survival curves across treatment arms is
an effective and popular approach for inferring the causal effect of an
intervention on a right-censored time-to-event outcome. A key challenge to
drawing such inferences in observational settings is the possible existence of
unmeasured confounding, which may invalidate most commonly used methods that
assume no hidden confounding bias. In this paper, rather than making the
standard no unmeasured confounding assumption, we extend the recently proposed
proximal causal inference framework of Miao et al. (2018), Tchetgen et al.
(2020), Cui et al. (2020) to obtain nonparametric identification of a causal
survival contrast by leveraging observed covariates as imperfect proxies of
unmeasured confounders. Specifically, we develop a proximal inverse
probability-weighted (PIPW) estimator, the proximal analog of standard IPW,
which allows the observed data distribution for the time-to-event outcome to
remain completely unrestricted. PIPW estimation relies on a parametric model
for a so-called treatment confounding bridge function relating the treatment
process to confounding proxies. As a result, PIPW might be sensitive to model
misspecification. To improve robustness and efficiency, we also propose a
proximal doubly robust estimator and establish uniform consistency and
asymptotic normality of both estimators. We conduct extensive simulations to
examine the finite sample performance of our estimators, and proposed methods
are applied to a study evaluating the effectiveness of right heart
catheterization in the intensive care unit of critically ill patients.

### Title: Prospects for a flavour violating $Z^\prime$ explanation of $Δ a_{μ,e}$
* Paper ID: 2204.13134v1
* Paper URL: [http://arxiv.org/abs/2204.13134v1](http://arxiv.org/abs/2204.13134v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The apparent tensions emerging from the comparison of experimental data of
the anomalous magnetic moments of the muon and electron to the Standard Model
predictions ($\Delta a_{\mu,e}$) could be interpreted as a potential signal of
New Physics. Models encompassing a light vector boson have been known to offer
a satisfactory explanation to $\Delta a_{\mu}$, albeit subject to stringent
experimental constraints. Here we explore a minimal extension of the Standard
Model via a leptophilic vector boson $Z^\prime$, under the hypothesis of
strictly flavour-violating couplings of the latter to leptons. The most
constraining observables to this ad-hoc construction emerge from lepton flavour
universality violation (in $Z$ and $\tau$ decays) and from rare charged lepton
flavour violating transitions. Once these are accommodated, one can saturate
the tensions in $\Delta a_{\mu}$, but $\Delta a_{e}$ is predicted to be
Standard Model-like. We infer prospects for several observables, including
leptonic $Z$ decays and several charged lepton flavour violating processes. We
also discuss potential signatures of the considered $Z^\prime$ at a future muon
collider, emphasising the role of the $\mu^+\mu^- \to\tau^+\tau^- $
forward-backward asymmetry as a key probe of the model.

### Title: Accurate Model of the Projected Velocity Distribution of Galaxies in Dark Matter Halos
* Paper ID: 2204.13131v1
* Paper URL: [http://arxiv.org/abs/2204.13131v1](http://arxiv.org/abs/2204.13131v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: We present a percent-level accurate model of the line-of-sight velocity
distribution of galaxies around dark matter halos as a function of projected
radius and halo mass. The model is developed and tested using synthetic galaxy
catalogs generated with the UniverseMachine run on the Multi-Dark Planck 2
N-body simulations. The model decomposes the galaxies around a cluster into
three kinematically distinct classes: orbiting, infalling, and interloping
galaxies. We demonstrate that: 1) we can statistically distinguish between
these three types of galaxies using only projected line-of-sight velocity
information; 2) the halo edge radius inferred from the line-of-sight velocity
dispersion is an excellent proxy for the three-dimensional halo edge radius; 3)
we can accurately recover the full velocity dispersion profile for each of the
three populations of galaxies. Importantly, the velocity dispersion profiles of
the orbiting and infalling galaxies contain five independent parameters --
three distinct radial scales and two velocity dispersion amplitudes -- each of
which is correlated with mass. Thus, the velocity dispersion profile of galaxy
clusters has inherent redundancies that allow us to perform nontrivial
systematics check from a single data set. We discuss several potential
applications of our new model for detecting the edge radius and constraining
cosmology and astrophysics using upcoming spectroscopic surveys.

### Title: Quantum evolution of the Hawking state for black holes
* Paper ID: 2204.13126v1
* Paper URL: [http://arxiv.org/abs/2204.13126v1](http://arxiv.org/abs/2204.13126v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: We give a general description of the evolving quantum state of a
Schwarzschild black hole, in the quantum field theory approximation. Such a
time-dependent description is based on introducing a choice of time slices. We
in particular consider slices that smoothly cross the horizon, and introduction
of "stationary" such slices simplifies the analysis. This analysis goes beyond
standard derivations of Hawking radiation that focus on asymptotic excitations,
and in particular gives an evolving state that is regular at the horizon, with
no explicit transplanckian dependence, and that can in principle be generalized
to incorporate interacting fields. It is also expected to be useful in
connecting to information-theoretic investigation of black hole evolution. The
description of the evolving state depends on the choice of slices as well as
coordinates on the slices and mode bases; these choices give different
"pictures" analogous to that of Schr\"odinger. Evolution does have a simpler
appearance in an energy eigenbasis, but such a basis is also singular at the
horizon; evolution of regular modes has a more complicated appearance, whose
properties may be inferred by comparing with the energy eigenbasis. In a
regular description, Hawking quanta are produced in a black hole atmosphere, at
scales comparable to the horizon size. This approach is also argued to extend
to more general asymptotics, such as that of anti de Sitter space. In the
latter context, this analysis provides a description of the hamiltonian and
evolution of a black hole that may be compared to the large-$N$ dynamics of the
proposed dual CFT.

### Title: Moments for positivity: using Drell-Yan data to test positivity bounds and reverse-engineer new physics
* Paper ID: 2204.13121v1
* Paper URL: [http://arxiv.org/abs/2204.13121v1](http://arxiv.org/abs/2204.13121v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Moments of the leptonic angular distribution in the Drell-Yan process have
recently been shown to be sensitive probes of a specific class of dimension-8,
four-fermion operators in the Standard Model Effective Field Theory, involving
a pair of quarks and leptons. The same operators are also subject to positivity
bounds, when requiring the associated (unknown) UV completion to obey basic
principles of quantum field theory. We perform a phenomenological study to
quantify the sensitivity of the high-luminosity LHC to this set of operators
and, by extension, the positivity bounds. We further extend the angular basis
of moments and consider double differential information to improve the ability
to disentangle the different operators, leading to a sensitivity to new physics
scales up to 3 TeV. We use this information to explore the violation of
positivity at the LHC as a way to test the underlying principles of quantum
field theory. Finally, we present a case study which combines our results with
information from other (current and prospective) experiments, as well as the
positivity cone to infer the properties of possible tree-level UV completions.
The data lead to robust, model-independent lower bounds on the $M/\sqrt{g}$
combination of the particle mass and coupling, for states that couple to
right-handed leptons and/or up quarks.

### Title: FlowGNN: A Dataflow Architecture for Universal Graph Neural Network Inference via Multi-Queue Streaming
* Paper ID: 2204.13103v1
* Paper URL: [http://arxiv.org/abs/2204.13103v1](http://arxiv.org/abs/2204.13103v1)
* Updated Date: 2022-04-27
* Code URL: [https://github.com/sharc-lab/flowgnn](https://github.com/sharc-lab/flowgnn)
* Summary: Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to graph-related problems such as quantum chemistry,
drug discovery, and high energy physics. However, meeting demand for novel GNN
models and fast inference simultaneously is challenging because of the gap
between developing efficient accelerators and the rapid creation of new GNN
models. Prior art focuses on the acceleration of specific classes of GNNs, such
as Graph Convolutional Network (GCN), but lacks the generality to support a
wide range of existing or new GNN models. Meanwhile, most work rely on graph
pre-processing to exploit data locality, making them unsuitable for real-time
applications. To address these limitations, in this work, we propose a generic
dataflow architecture for GNN acceleration, named FlowGNN, which can flexibly
support the majority of message-passing GNNs. The contributions are three-fold.
First, we propose a novel and scalable dataflow architecture, which flexibly
supports a wide range of GNN models with message-passing mechanism. The
architecture features a configurable dataflow optimized for simultaneous
computation of node embedding, edge embedding, and message passing, which is
generally applicable to all models. We also propose a rich library of
model-specific components. Second, we deliver ultra-fast real-time GNN
inference without any graph pre-processing, making it agnostic to dynamically
changing graph structures. Third, we verify our architecture on the Xilinx
Alveo U50 FPGA board and measure the on-board end-to-end performance. We
achieve a speed-up of up to 51-254x against CPU (6226R) and 1.3-477x against
GPU (A6000) (with batch sizes 1 through 1024); we also outperform the SOTA GNN
accelerator I-GCN by 1.03x and 1.25x across two datasets. Our implementation
code and on-board measurement are publicly available on GitHub.

### Title: Variational Kalman Filtering with Hinf-Based Correction for Robust Bayesian Learning in High Dimensions
* Paper ID: 2204.13089v1
* Paper URL: [http://arxiv.org/abs/2204.13089v1](http://arxiv.org/abs/2204.13089v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: In this paper, we address the problem of convergence of sequential
variational inference filter (VIF) through the application of a robust
variational objective and Hinf-norm based correction for a linear Gaussian
system. As the dimension of state or parameter space grows, performing the full
Kalman update with the dense covariance matrix for a large scale system
requires increased storage and computational complexity, making it impractical.
The VIF approach, based on mean-field Gaussian variational inference, reduces
this burden through the variational approximation to the covariance usually in
the form of a diagonal covariance approximation. The challenge is to retain
convergence and correct for biases introduced by the sequential VIF steps. We
desire a framework that improves feasibility while still maintaining reasonable
proximity to the optimal Kalman filter as data is assimilated. To accomplish
this goal, a Hinf-norm based optimization perturbs the VIF covariance matrix to
improve robustness. This yields a novel VIF- Hinf recursion that employs
consecutive variational inference and Hinf based optimization steps. We explore
the development of this method and investigate a numerical example to
illustrate the effectiveness of the proposed filter.

### Title: Beryllium isotopic composition and Galactic cosmic ray propagation
* Paper ID: 2204.13085v1
* Paper URL: [http://arxiv.org/abs/2204.13085v1](http://arxiv.org/abs/2204.13085v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The isotopic composition of beryllium nuclei and its energy dependence encode
information of fundamental importance about the propagation of cosmic rays in
the Galaxy. The effects of decay on the spectrum of the unstable beryllium--10
isotope can be described introducing the average survival probability $P_{\rm
surv} (E_0)$ that can inferred from measurements of the isotopic ratio Be10/Be9
if one has sufficiently good knowledge of the nuclear fragmentation cross
sections that determine the isotopic composition of beryllium nuclei at
injection. The average survival probability can then be interpreted in terms of
propagation parameters, such as the cosmic ray average age, adopting a
theoretical framework for Galactic propagation. Recently the AMS02
Collaboration has presented preliminary measurements of the beryllium isotopic
composition that extend the observations to a broad energy range ($E_0 \simeq
0.7$-12 GeV/n) with small errors. In this work we discuss the average survival
probability that can be inferred from the preliminary AMS02 data, adopting
publically available models of the nuclear fragmentation cross sections, and
interpret the results in the framework of a simple diffusion model, This study
shows that the effects of decay decrease more slowly than the predictions,
resulting in an average cosmic ray age that increases with energy. An
alternative possibility is that the cosmic ray age distribution is broader than
in the models that are now commonly accepted, suggesting that the Galactic
confinement volume has a non trivial structure and is formed by an inner halo
contained in an extended one.

### Title: The outer orbit of the high-mass stellar triple system Herschel 36 determined with the VLTI
* Paper ID: 2204.13075v1
* Paper URL: [http://arxiv.org/abs/2204.13075v1](http://arxiv.org/abs/2204.13075v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Multiplicity is a ubiquitous characteristic of massive stars. Multiple
systems offer us a unique observational constraint on the formation of
high-mass systems. Herschel 36 A is a massive triple system composed of a close
binary (Ab1-Ab2) and an outer component (Aa). We measured the orbital motion of
the outer component of Herschel 36 A using infrared interferometry with the
AMBER and PIONIER instruments of ESO's Very Large Telescope Interferometer. Our
immediate aims are to constrain the masses of all components of this system and
to determine if the outer orbit is co-planar with the inner one. Reported
spectroscopic data for all three components of this system and our
interferometric data allow us to derive full orbital solutions for the outer
orbit Aa-Ab and the inner orbit Ab1-Ab2. For the first time, we derive the
absolute masses of mAa = 22.3 +/- 1.7 M_sun, mAb1 = 20.5 +/- 1.5 M_sun and mAb2
= 12.5 +/- 0.9 M_sun. Despite not being able to resolve the close binary
components, we infer the inclination of their orbit by imposing the same
parallax as the outer orbit. Inclinations derived from the inner and outer
orbits imply a modest difference of about 22 deg. between the orbital planes.
We discuss this result and the formation of Herschel 36 A in the context of
Core Accretion and Competitive Accretion models, which make different
predictions regarding the statistic of the relative orbital inclinations.

### Title: Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution
* Paper ID: 2204.13062v1
* Paper URL: [http://arxiv.org/abs/2204.13062v1](http://arxiv.org/abs/2204.13062v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Estimating the pose and shape of hands and objects under interaction finds
numerous applications including augmented and virtual reality. Existing
approaches for hand and object reconstruction require explicitly defined
physical constraints and known objects, which limits its application domains.
Our algorithm is agnostic to object models, and it learns the physical rules
governing hand-object interaction. This requires automatically inferring the
shapes and physical interaction of hands and (potentially unknown) objects. We
seek to approach this challenging problem by proposing a collaborative learning
strategy where two-branches of deep networks are learning from each other.
Specifically, we transfer hand mesh information to the object branch and vice
versa for the hand branch. The resulting optimisation (training) problem can be
unstable, and we address this via two strategies: (i) attention-guided graph
convolution which helps identify and focus on mutual occlusion and (ii)
unsupervised associative loss which facilitates the transfer of information
between the branches. Experiments using four widely-used benchmarks show that
our framework achieves beyond state-of-the-art accuracy in 3D pose estimation,
as well as recovers dense 3D hand and object shapes. Each technical component
above contributes meaningfully in the ablation study.

### Title: Dropout Inference with Non-Uniform Weight Scaling
* Paper ID: 2204.13047v1
* Paper URL: [http://arxiv.org/abs/2204.13047v1](http://arxiv.org/abs/2204.13047v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Dropout as regularization has been used extensively to prevent overfitting
for training neural networks. During training, units and their connections are
randomly dropped, which could be considered as sampling many different
submodels from the original model. At test time, weight scaling and Monte Carlo
approximation are two widely applied approaches to approximate the outputs.
Both approaches work well practically when all submodels are low-bias complex
learners. However, in this work, we demonstrate scenarios where some submodels
behave closer to high-bias models and a non-uniform weight scaling is a better
approximation for inference.

### Title: Extremal GloVe: Theoretically Accurate Distributed Word Embedding by Tail Inference
* Paper ID: 2204.13009v1
* Paper URL: [http://arxiv.org/abs/2204.13009v1](http://arxiv.org/abs/2204.13009v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Distributed word embeddings such as Word2Vec and GloVe have been widely
adopted in industrial context settings. Major technical applications of GloVe
include recommender systems and natural language processing. The fundamental
theory behind GloVe relies on the selection of a weighting function in the
weighted least squres formulation that computes the powered ratio of word
occurrence count and the maximum word count in the corpus. However, the initial
formulation of GloVe is not theoretically sound in two aspects, namely the
selection of the weighting function and its power exponent is ad-hoc. In this
paper, we utilize the theory of extreme value analysis and propose a
theoretically accurate version of GloVe. By reformulating the weighted least
squares loss function as the expected loss function and accurately choosing the
power exponent, we create a theoretically accurate version of GloVe. We
demonstrate the competitiveness of our algorithm and show that the initial
formulation of GloVe with the suggested optimal parameter can be viewed as a
special case of our paradigm.

### Title: Edge-PRUNE: Flexible Distributed Deep Learning Inference
* Paper ID: 2204.12947v1
* Paper URL: [http://arxiv.org/abs/2204.12947v1](http://arxiv.org/abs/2204.12947v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Collaborative deep learning inference between low-resource endpoint devices
and edge servers has received significant research interest in the last few
years. Such computation partitioning can help reducing endpoint device energy
consumption and improve latency, but equally importantly also contributes to
privacy-preserving of sensitive data. This paper describes Edge-PRUNE, a
flexible but light-weight computation framework for distributing machine
learning inference between edge servers and one or more client devices.
Compared to previous approaches, Edge-PRUNE is based on a formal dataflow
computing model, and is agnostic towards machine learning training frameworks,
offering at the same time wide support for leveraging deep learning
accelerators such as embedded GPUs. The experimental section of the paper
demonstrates the use and performance of Edge-PRUNE by image classification and
object tracking applications on two heterogeneous endpoint devices and an edge
server, over wireless and physical connections. Endpoint device inference time
for SSD-Mobilenet based object tracking, for example, is accelerated 5.8x by
collaborative inference.

### Title: Characterizing Visualization Insights through Entity-Based Interaction: An Exploratory Study
* Paper ID: 2204.12897v1
* Paper URL: [http://arxiv.org/abs/2204.12897v1](http://arxiv.org/abs/2204.12897v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: One of the primary purposes of visualization is to assist users in
discovering insights. While there has been much research in information
visualization aiming at complex data transformation and novel presentation
techniques, relatively little has been done to understand how users derive
insights through interactive visualization of data. This paper presents a
crowdsourced study with 158 participants investigating the relation between
entity-based interaction (an action + its target entity) and the resulting
insight. To this end, we generalized the interaction with an existing CO2
Explorer as entity-based interaction and enabled users to input notes and refer
to relevant entities to assist their narratives. We logged interactions of
users freely exploring the visualization and characterized their externalized
insights about the data. Using entity-based interactions and references to
infer insight characteristics (category, overview versus detail, and prior
knowledge), we found evidence that compared with interactions, entity
references improved insight characterization from slight/fair to fair/moderate
agreements. To interpret prediction outcomes, feature importance and
correlation analysis indicated that, e.g., detailed insights tended to have
more mouse-overs in the chart area and cite the vertical reference lines in the
line chart as evidence. We discuss study limitations and implications on
knowledge-assisted visualization, e.g., insight recommendations based on user
exploration.

### Title: Globular Clusters UVIT Legacy Survey (GlobULeS) I. FUV-optical Color-Magnitude Diagrams for Eight Globular Clusters
* Paper ID: 2204.12886v1
* Paper URL: [http://arxiv.org/abs/2204.12886v1](http://arxiv.org/abs/2204.12886v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: We present the first results of eight Globular Clusters (GCs) from the
AstroSat/UVIT Legacy Survey program GlobULeS based on the observations carried
out in two FUV filters (F148W and F169M). The FUV-optical and FUV-FUV
color-magnitude diagrams (CMDs) of GCs with the proper motion membership were
constructed by combining the UVIT data with HST UV Globular Cluster Survey
(HUGS) data for inner regions and Gaia Early Data Release (EDR3) for regions
outside the HST's field. We detect sources as faint as F148W $\sim$ 23.5~mag
which are classified based on their locations in CMDs by overlaying stellar
evolutionary models. The CMDs of 8 GCs are combined with the previous UVIT
studies of 3 GCs to create stacked FUV-optical CMDs to highlight the
features/peculiarities found in the different evolutionary sequences. The FUV
(F148W) detected stellar populations of 11 GCs comprises 2,816 Horizontal
Branch (HB) stars (190 Extreme HB candidates), 46 post-HB (pHB), 221 Blue
Straggler Stars (BSS), and 107 White Dwarf (WD) candidates. We note that the
blue HB color extension obtained from F148W$-$G color and the number of FUV
detected EHB candidates are strongly correlated with the maximum internal
Helium (He) variation within each GC, suggesting that the FUV-optical plane is
the most sensitive to He abundance variations in the HB. We discuss the
potential science cases that will be addressed using these catalogues including
HB morphologies, BSSs, pHB, and, WD stars.

### Title: A peculiarly long-duration gamma-ray burst from binary neutron star merger
* Paper ID: 2204.12771v1
* Paper URL: [http://arxiv.org/abs/2204.12771v1](http://arxiv.org/abs/2204.12771v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Some recent findings have shown that the duration of gamma-ray burst (GRB),
although crucially related to the GRB central engine time scale, is not
determinative in inferring the GRB origins in terms of their progenitors. In
this paper, we report a peculiarly long-duration gamma-ray burst, GRB 211211A,
that is associated with a kilonova in optical and near-infrared bands and is
therefore likely the result of a binary neutron star merger. The burst broadly
resembles the properties of GRB 060614 but with a much higher brightness in its
light curve and harder spectra in both the main and extended emission phases,
making it difficult to be explained as a short GRB with soft extended emission.
Such a genuinely long-duration GRB suggests that merger product is likely a
magnetar, which powers up the burst through magnetic and rotation energy for at
least $\sim 70$ seconds.

### Title: Smoothing distributions for conditional Fleming-Viot and Dawson-Watanabe diffusions
* Paper ID: 2204.12738v1
* Paper URL: [http://arxiv.org/abs/2204.12738v1](http://arxiv.org/abs/2204.12738v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: We study the distribution of the unobserved states of two measure-valued
diffusions of Fleming-Viot and Dawson-Watanabe type, conditional on
observations from the underlying populations collected at past, present and
future times. If seen as nonparametric hidden Markov models, this amounts to
finding the smoothing distributions of these processes, which we show can be
explicitly described in recursive form as finite mixtures of laws of Dirichlet
and gamma random measures respectively. We characterize the time-dependent
weights of these mixtures, accounting for potentially different time intervals
between data collection times, and fully describe the implications of assuming
a discrete or a nonatomic distribution for the underlying process that drives
mutations. In particular, we show that with a non atomic mutation offspring
distribution, the inference automatically upweights mixture components that
carry, as atoms, observed types shared at different collection times. The
predictive distributions for further samples from the population conditional on
the data are also identified and shown to be mixtures of generalized Polya
urns, conditionally on a latent variable in the Dawson-Watanabe case.

### Title: Modeling complex measurement error in microbiome experiments
* Paper ID: 2204.12733v1
* Paper URL: [http://arxiv.org/abs/2204.12733v1](http://arxiv.org/abs/2204.12733v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The relative abundances of species in a microbiome is a scientifically
important parameter to estimate given the critical role that microbiomes play
in human and environmental health. However, data from artificially constructed
microbiomes shows that measurement error may induce substantial bias in common
estimators of this quantity. To address this, we propose a semiparametric model
that accounts for common forms of measurement error in microbiome experiments.
Notably, our model allows relative abundances to lie on the boundary of the
simplex. We present a stable algorithm for computing parameter estimates,
asymptotically valid procedures for inference in this nonstandard problem, and
examples of the utility of the method. Our approach can be used to select or
compare experimental protocols, design experiments with appropriate control
data, analyze mixed-specimen samples, and remove across-sample contamination.

### Title: Randomness and Statistical Inference of Shapes via the Smooth Euler Characteristic Transform
* Paper ID: 2204.12699v1
* Paper URL: [http://arxiv.org/abs/2204.12699v1](http://arxiv.org/abs/2204.12699v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: In this paper, we provide the mathematical foundations for the randomness of
shapes and the distributions of smooth Euler characteristic transform. Based on
these foundations, we propose an approach for testing hypotheses on random
shapes. Simulation studies are provided to support our mathematical derivations
and show the performance of our proposed hypothesis testing framework. Our
discussions connect the following fields: algebraic and computational topology,
probability theory and stochastic processes, Sobolev spaces and functional
analysis, statistical inference, and medical imaging.

### Title: The Bolometric Luminosity Correction of Radio-Quiet and Radio-Loud Quasars at 1<z<2
* Paper ID: 2204.12697v1
* Paper URL: [http://arxiv.org/abs/2204.12697v1](http://arxiv.org/abs/2204.12697v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: To understand the impact of active galactic nuclei (AGN) on their host
galaxies and large scale environment it is crucial to determine their total
radiative power across all wavelengths (i.e., bolometric luminosity). In this
contribution we describe how quasar accretion disk spectral energy distribution
(SED) templates, parameterized by the black hole (BH) mass, Eddington ratio,
and spin can be used to estimate their total radiated luminosity. To estimate
the bolometric luminosity of AGN, we integrate the accretion disk SEDs from
1$\mu$m to 10keV. Our approach self-consistently covers any gaps in
observations and does not include reprocessed emission from the torus. The
accretion disk SED, and consequently the bolometric correction inferred from
it, strongly depend on the BH mass, the Eddington ratio, and spin. In
particular, the bolometric correction in the visible bands
(5100$\,\mathring{A}$ and 3000$\,\mathring{A}$) strongly depends on BH mass,
and at X-ray strongly depends on the Eddington ratio. At wavelengths closer to
the peak of the accretion disk SED the dependence becomes weaker. Additionally,
maximally-rotating (spin = 1) quasars require a higher bolometric correction
than their non-rotating (spin = 0) counterparts at all wavelengths. The SEDs
and the bolometric correction presented in this work can determine the
radiative power of any sample of radio-quiet to radio-loud Type 1 AGN with
observations in the range from 1$\mu$m to 10$\,$keV provided the observations
are corrected for extinction.

### Title: Multiresolution community analysis of international trade networks
* Paper ID: 2204.12687v1
* Paper URL: [http://arxiv.org/abs/2204.12687v1](http://arxiv.org/abs/2204.12687v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The international trade network is a complex system where multiple trade
blocs with varying sizes coexist and overlap with each other. However, the
resulting structures of community detection in trade networks are often
inconsistent and fails to capture the complex landscape of international trade.
To address these problems, we propose a multiresolution framework that
aggregates all the configuration information from a range of resolutions. This
allows us to consider trade communities of different sizes and illuminate the
underlying hierarchical structure of trade networks and its constituting
blocks. Furthermore, by measuring membership inconsistency (MeI) of each
country and conducting multiple regression analysis with various economic and
political indicators, we demonstrate that there exists a positive correlation
between the external instability of countries and their structural
inconsistency in terms of network topology.

### Title: Robust Face Anti-Spoofing with Dual Probabilistic Modeling
* Paper ID: 2204.12685v1
* Paper URL: [http://arxiv.org/abs/2204.12685v1](http://arxiv.org/abs/2204.12685v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The field of face anti-spoofing (FAS) has witnessed great progress with the
surge of deep learning. Due to its data-driven nature, existing FAS methods are
sensitive to the noise in the dataset, which will hurdle the learning process.
However, very few works consider noise modeling in FAS. In this work, we
attempt to fill this gap by automatically addressing the noise problem from
both label and data perspectives in a probabilistic manner. Specifically, we
propose a unified framework called Dual Probabilistic Modeling (DPM), with two
dedicated modules, DPM-LQ (Label Quality aware learning) and DPM-DQ (Data
Quality aware learning). Both modules are designed based on the assumption that
data and label should form coherent probabilistic distributions. DPM-LQ is able
to produce robust feature representations without overfitting to the
distribution of noisy semantic labels. DPM-DQ can eliminate data noise from
`False Reject' and `False Accept' during inference by correcting the prediction
confidence of noisy data based on its quality distribution. Both modules can be
incorporated into existing deep networks seamlessly and efficiently.
Furthermore, we propose the generalized DPM to address the noise problem in
practical usage without the need of semantic annotations. Extensive experiments
demonstrate that this probabilistic modeling can 1) significantly improve the
accuracy, and 2) make the model robust to the noise in real-world datasets.
Without bells and whistles, our proposed DPM achieves state-of-the-art
performance on multiple standard FAS benchmarks.

### Title: Adaptable Text Matching via Meta-Weight Regulator
* Paper ID: 2204.12668v1
* Paper URL: [http://arxiv.org/abs/2204.12668v1](http://arxiv.org/abs/2204.12668v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Neural text matching models have been used in a range of applications such as
question answering and natural language inference, and have yielded a good
performance. However, these neural models are of a limited adaptability,
resulting in a decline in performance when encountering test examples from a
different dataset or even a different task. The adaptability is particularly
important in the few-shot setting: in many cases, there is only a limited
amount of labeled data available for a target dataset or task, while we may
have access to a richly labeled source dataset or task. However, adapting a
model trained on the abundant source data to a few-shot target dataset or task
is challenging. To tackle this challenge, we propose a Meta-Weight Regulator
(MWR), which is a meta-learning approach that learns to assign weights to the
source examples based on their relevance to the target loss. Specifically, MWR
first trains the model on the uniformly weighted source examples, and measures
the efficacy of the model on the target examples via a loss function. By
iteratively performing a (meta) gradient descent, high-order gradients are
propagated to the source examples. These gradients are then used to update the
weights of source examples, in a way that is relevant to the target
performance. As MWR is model-agnostic, it can be applied to any backbone neural
model. Extensive experiments are conducted with various backbone text matching
models, on four widely used datasets and two tasks. The results demonstrate
that our proposed approach significantly outperforms a number of existing
adaptation methods and effectively improves the cross-dataset and cross-task
adaptability of the neural text matching models in the few-shot setting.

### Title: SCGC : Self-Supervised Contrastive Graph Clustering
* Paper ID: 2204.12656v1
* Paper URL: [http://arxiv.org/abs/2204.12656v1](http://arxiv.org/abs/2204.12656v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Graph clustering discovers groups or communities within networks. Deep
learning methods such as autoencoders (AE) extract effective clustering and
downstream representations but cannot incorporate rich structural information.
While Graph Neural Networks (GNN) have shown great success in encoding graph
structure, typical GNNs based on convolution or attention variants suffer from
over-smoothing, noise, heterophily, are computationally expensive and typically
require the complete graph being present. Instead, we propose Self-Supervised
Contrastive Graph Clustering (SCGC), which imposes graph-structure via
contrastive loss signals to learn discriminative node representations and
iteratively refined soft cluster labels. We also propose SCGC*, with a more
effective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer
structural information, and half the original model parameters. SCGC(*) is
faster with simple linear units, completely eliminate convolutions and
attention of traditional GNNs, yet efficiently incorporates structure. It is
impervious to layer depth and robust to over-smoothing, incorrect edges and
heterophily. It is scalable by batching, a limitation in many prior GNN models,
and trivially parallelizable. We obtain significant improvements over
state-of-the-art on a wide range of benchmark graph datasets, including images,
sensor data, text, and citation networks efficiently. Specifically, 20% on ARI
and 18% on NMI for DBLP; overall 55% reduction in training time and overall,
81% reduction on inference time. Our code is available at :
https://github.com/gayanku/SCGC

### Title: Inferring Late Stage Enrichment of Exoplanet Atmospheres from Observed Interstellar Comets
* Paper ID: 2204.12653v1
* Paper URL: [http://arxiv.org/abs/2204.12653v1](http://arxiv.org/abs/2204.12653v1)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The discovery of the first two interstellar comets implies that, on average,
every star contributes a substantial amount of material to the galactic
population by ejecting such bodies from the host system. Since scattering is a
chaotic process, a comparable amount of material should be injected into the
inner regions of each system that ejects comets. For comets that are
transported inwards and interact with planets, this Letter estimates the
fraction of material that is accreted or outward-scattered as a function of
planetary masses and orbital parameters. These calculations indicate that
planets with escape velocities smaller than their current day orbital
velocities will efficiently accrete comets. We estimate the accretion
efficiency for members of the current census of extrasolar planets, and find
that planetary populations including but not limited to hot and warm Jupiters,
sub-Neptunes and super-Earths can efficiently capture incoming comets. This
cometary enrichment may have important ramifications for post-formation
atmospheric composition and chemistry. As a result, future detections and
compositional measurements of interstellar comets will provide direct
measurements of material that potentially enriched a sub-population of the
extrasolar planets. Finally, we estimate the efficiency of this enrichment
mechanism for extrasolar planets that will be observed with the $\textit{James
Webb Space Telescope}$ (JWST). With JWST currently operational and these
observations imminently forthcoming, it is of critical importance to
investigate how enrichment from interstellar comet analogues may affect the
interpretations of exoplanet atmospheric compositions.

### Title: Nonbacktracking spectral clustering of nonuniform hypergraphs
* Paper ID: 2204.13586v1
* Paper URL: [http://arxiv.org/abs/2204.13586v1](http://arxiv.org/abs/2204.13586v1)
* Updated Date: 2022-04-27
* Code URL: [https://github.com/jamiehadd/hypergraphspectralclustering](https://github.com/jamiehadd/hypergraphspectralclustering)
* Summary: Spectral methods offer a tractable, global framework for clustering in graphs
via eigenvector computations on graph matrices. Hypergraph data, in which
entities interact on edges of arbitrary size, poses challenges for matrix
representations and therefore for spectral clustering. We study spectral
clustering for nonuniform hypergraphs based on the hypergraph nonbacktracking
operator. After reviewing the definition of this operator and its basic
properties, we prove a theorem of Ihara-Bass type to enable faster computation
of eigenpairs. We then propose an alternating algorithm for inference in a
hypergraph stochastic blockmodel via linearized belief-propagation, offering
proofs that both formalize and extend several previous results. We perform
experiments in real and synthetic data that underscore the benefits of
hypergraph methods over graph-based ones when interactions of different sizes
carry different information about cluster structure. Through an analysis of our
algorithm, we pose several conjectures about the limits of spectral methods and
detectability in hypergraph stochastic blockmodels writ large.

### Title: A Quantum Optical Microphone in the Audio Band
* Paper ID: 2204.12429v2
* Paper URL: [http://arxiv.org/abs/2204.12429v2](http://arxiv.org/abs/2204.12429v2)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: The ability to perform high-precision optical measurements is paramount to
science and engineering. Laser interferometry enables interaction-free sensing
with a precision ultimately limited by shot noise. Quantum optical sensors can
surpass this limit, but single- or multi-photon schemes are challenged by low
experimental sampling rates, while squeezed-light approaches require complex
optical setups and sophisticated time gating. Here, we introduce a simple
method that infers optical phase shifts through standard intensity measurements
while still maintaining the quantum advantage in the measurement precision.
Capitalising on the robustness and high sampling rates of our device, we
implement a quantum optical microphone in the audio band. Its performance is
benchmarked against a classical laser microphone in a standardised
medically-approved speech recognition test on 45 subjects. We find that
quantum-recorded words improve the speech recognition threshold by $-0.57\,
\text{dB}_{\text{SPL}}$, thus making the quantum advantage audible. Not only do
these results open the door towards applications in quantum nonlinear
interferometry, but they also show that quantum phenomena can be experienced by
humans.

### Title: Contrastive learning-based computational histopathology predict differential expression of cancer driver genes
* Paper ID: 2204.11994v2
* Paper URL: [http://arxiv.org/abs/2204.11994v2](http://arxiv.org/abs/2204.11994v2)
* Updated Date: 2022-04-27
* Code URL: [https://github.com/hoarjour/histcode](https://github.com/hoarjour/histcode)
* Summary: Digital pathological analysis is run as the main examination used for cancer
diagnosis. Recently, deep learning-driven feature extraction from pathology
images is able to detect genetic variations and tumor environment, but few
studies focus on differential gene expression in tumor cells. In this paper, we
propose a self-supervised contrastive learning framework, HistCode, to infer
differential gene expressions from whole slide images (WSIs). We leveraged
contrastive learning on large-scale unannotated WSIs to derive slide-level
histopathological feature in latent space, and then transfer it to tumor
diagnosis and prediction of differentially expressed cancer driver genes. Our
extensive experiments showed that our method outperformed other
state-of-the-art models in tumor diagnosis tasks, and also effectively
predicted differential gene expressions. Interestingly, we found the higher
fold-changed genes can be more precisely predicted. To intuitively illustrate
the ability to extract informative features from pathological images, we
spatially visualized the WSIs colored by the attentive scores of image tiles.
We found that the tumor and necrosis areas were highly consistent with the
annotations of experienced pathologists. Moreover, the spatial heatmap
generated by lymphocyte-specific gene expression patterns was also consistent
with the manually labeled WSI.

### Title: An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models
* Paper ID: 2204.11351v2
* Paper URL: [http://arxiv.org/abs/2204.11351v2](http://arxiv.org/abs/2204.11351v2)
* Updated Date: 2022-04-27
* Code URL: null
* Summary: Nowadays, the interpretation of why a machine learning (ML) model makes
certain inferences is as crucial as the accuracy of such inferences. Some ML
models like the decision tree possess inherent interpretability that can be
directly comprehended by humans. Others like artificial neural networks (ANN),
however, rely on external methods to uncover the deduction mechanism. SHapley
Additive exPlanations (SHAP) is one of such external methods, which requires a
background dataset when interpreting ANNs. Generally, a background dataset
consists of instances randomly sampled from the training dataset. However, the
sampling size and its effect on SHAP remain to be unexplored. In our empirical
study on the MIMIC-III dataset, we show that the two core explanations - SHAP
values and variable rankings fluctuate when using different background datasets
acquired from random sampling, indicating that users cannot unquestioningly
trust the one-shot interpretation from SHAP. Luckily, such fluctuation
decreases with the increase of the background dataset size. Also, we notice an
U-shape in the stability assessment of SHAP variable rankings, demonstrating
that SHAP is more reliable in ranking the most and least important variables
compared to moderately important ones. Overall, our results suggest that users
should take into account how background data affects SHAP results, with
improved SHAP stability as the background sample size increases.

