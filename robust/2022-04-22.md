### Title: Incompatibility of local measurements provide advantage in local quantum state discrimination
* Paper ID: 2204.10948v1
* Paper URL: [http://arxiv.org/abs/2204.10948v1](http://arxiv.org/abs/2204.10948v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: A pack of quantum measurements that cannot be measured simultaneously is said
to form a set of incompatible measurements. Every set of incompatible
measurements have advantage over the compatible ones in a quantum state
discrimination task where one prepares a state from an ensemble and sends it to
another party, and the latter tries to detect the state using available
measurements. We consider the local quantum state discrimination task where a
sender prepares a bipartite state and sends the subsystems to two receivers.
The receivers try to detect the sent state using locally incompatible
measurements. We analyze the ratio of the probability of successfully guessing
the state using incompatible measurements and the maximum probability of
successfully guessing the state using compatible measurements. We find that
this ratio is upper bounded by a simple function of robustnesses of
incompatibilities of the local measurements. Interestingly, corresponding to
every pair of sets of incompatible measurements, there exists at least one
local state discrimination task where this bound can be achieved. We argue that
the optimal local quantum state discrimination task does not present any
"nonlocality", where the term is used in the sense of a difference between the
ratios, of probabilities of successful detection via incompatible and
compatible measurements, in global and local state discriminations. The results
can be generalized to the regime of multipartite local quantum state
distinguishing tasks.

### Title: Certifiable Robot Design Optimization using Differentiable Programming
* Paper ID: 2204.10935v1
* Paper URL: [http://arxiv.org/abs/2204.10935v1](http://arxiv.org/abs/2204.10935v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: There is a growing need for computational tools to automatically design and
verify autonomous systems, especially complex robotic systems involving
perception, planning, control, and hardware in the autonomy stack.
Differentiable programming has recently emerged as powerful tool for modeling
and optimization. However, very few studies have been done to understand how
differentiable programming can be used for robust, certifiable end-to-end
design optimization. In this paper, we fill this gap by combining
differentiable programming for robot design optimization with a novel
statistical framework for certifying the robustness of optimized designs. Our
framework can conduct end-to-end optimization and robustness certification for
robotics systems, enabling simultaneous optimization of navigation, perception,
planning, control, and hardware subsystems.
  Using simulation and hardware experiments, we show how our tool can be used
to solve practical problems in robotics. First, we optimize sensor placements
for robot navigation (a design with 5 subsystems and 6 tunable parameters) in
under 5 minutes to achieve an 8.4x performance improvement compared to the
initial design. Second, we solve a multi-agent collaborative manipulation task
(3 subsystems and 454 parameters) in under an hour to achieve a 44% performance
improvement over the initial design. We find that differentiable programming
enables much faster (32% and 20x, respectively for each example) optimization
than approximate gradient methods. We certify the robustness of each design and
successfully deploy the optimized designs in hardware. An open-source
implementation is available at https://github.com/MIT-REALM/architect

### Title: Baxos: Backing off for Robust and Efficient Consensus
* Paper ID: 2204.10934v1
* Paper URL: [http://arxiv.org/abs/2204.10934v1](http://arxiv.org/abs/2204.10934v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Leader-based consensus algorithms are vulnerable to liveness and performance
downgrade attacks. We explore the possibility of replacing leader election in
Multi-Paxos with random exponential backoff (REB), a simpler approach that
requires minimum modifications to the two phase Synod Paxos and achieves better
resiliency under attacks. We propose Baxos, a new resilient consensus protocol
that leverages a random exponential backoff scheme as a replacement for leader
election in consensus algorithms. Our backoff scheme addresses the common
challenges of random exponential backoff such as scalability and robustness to
changing wide area latency. We extensively evaluate Baxos to illustrate its
performance and robustness against two liveness and performance downgrade
attacks using an implementation running on Amazon EC2 in a wide area network
and a combination of a micro benchmark and YCSB-A workload on Redis. Our
results show that Baxos offers more robustness to liveness and performance
downgrade attacks than leader-based consensus protocols. Baxos outperforms
Multi-Paxos and Raft up to 185% in throughput under liveness and performance
downgrade attacks under worst case contention scenarios where each replica
proposes requests concurrently while only incurring a 7% reduction on the
maximum throughput in the synchronous attack-free scenario.

### Title: One-transmitter Multiple-receiver Wireless Power Transfer System Using an Exceptional Point of Degeneracy
* Paper ID: 2204.10928v1
* Paper URL: [http://arxiv.org/abs/2204.10928v1](http://arxiv.org/abs/2204.10928v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Robust transfer efficiency against the various operating conditions in a
wireless power transfer system remains a fundamentally important challenge.
This challenge becomes even more critical when transferring power to groups of
inductively coupled receivers. We propose a method for efficient wireless power
transfer to multiple receivers exploiting the concept of exceptional points of
degeneracy (EPD). In previous studies based on PT symmetry, a receiver's
operation has been divided into two strong and weak coupling regimes, and the
power transfer efficiency is constant in the strong coupling regime when
varying the coupling factor.Here the concept of strong and weak coupling and
constant power efficiency is extended to a system of multiple receivers that do
not follow PT symmetry. We show that the important feature to have a roughly
constant power efficiency, independently of the positions of the receivers, is
the existence of an EPD that separates the weak and strong regimes. Our
proposed method demonstrates a system with less sensitivity to the coupling
change than a conventional system without EPD when the receivers and their
couplings to the transmitter are not necessarily identical.

### Title: Adaptive robust electric vehicle routing under energy consumption uncertainty
* Paper ID: 2204.10913v1
* Paper URL: [http://arxiv.org/abs/2204.10913v1](http://arxiv.org/abs/2204.10913v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Electric vehicles (EVs) have been highly favoured as a future transportation
mode in the transportation section in recent years. EVs have many advantages
compared to traditional transportation, especially the environmental aspect.
However, despite many EVs' benefits, operating EVs has limitations in their
usage. One of the significant issues is the uncertainty in their driving range.
The driving range of EVs is closely related to their energy consumption, which
is highly affected by exogenous and endogenous factors. Since those factors are
unpredictable, uncertainty in EVs' energy consumption should be considered for
efficient operation. This paper proposes an adaptive robust optimization
framework for the electric vehicle routing problem. The objective is to
minimize the worst-case energy consumption while guaranteeing that services are
delivered at the appointed time windows without battery level deficiency. We
postulate that EVs can be recharged en route, and the charging amount can be
adjusted depending on the circumstance. The proposed problem is formulated as a
two-stage adaptive robust problem. A column-and-constraint generation based
heuristic algorithm, which is cooperated with variable neighborhood search and
alternating direction algorithm, is proposed to solve the proposed model. The
computational results show the economic efficiency and robustness of the
proposed model, and that there is a tradeoff between the total required energy
and the risk of failing to satisfy all customers' demand.

### Title: Robustness-by-Construction Synthesis: Adapting to the Environment at Runtime
* Paper ID: 2204.10912v1
* Paper URL: [http://arxiv.org/abs/2204.10912v1](http://arxiv.org/abs/2204.10912v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: While most of the current synthesis algorithms only focus on
correctness-by-construction, ensuring robustness has remained a challenge.
Hence, in this paper, we address the robust-by-construction synthesis problem
by considering the specifications to be expressed by a robust version of Linear
Temporal Logic (LTL), called robust LTL (rLTL). rLTL has a many-valued
semantics to capture different degrees of satisfaction of a specification,
i.e., satisfaction is a quantitative notion.
  We argue that the current algorithms for rLTL synthesis do not compute
optimal strategies in a non-antagonistic setting. So, a natural question is
whether there is a way of satisfying the specification "better" if the
environment is indeed not antagonistic. We address this question by developing
two new notions of strategies. The first notion is that of adaptive strategies,
which, in response to the opponent's non-antagonistic moves, maximize the
degree of satisfaction. The idea is to monitor non-optimal moves of the
opponent at runtime using multiple parity automata and adaptively change the
system strategy to ensure optimality. The second notion is that of strongly
adaptive strategies, which is a further refinement of the first notion. These
strategies also maximize the opportunities for the opponent to make non-optimal
moves. We show that computing such strategies for rLTL specifications is not
harder than the standard synthesis problem, e.g., computing strategies with LTL
specifications, and takes doubly-exponential time.

### Title: Quantum Control Noise Spectroscopy with Optimal Suppression of Dephasing
* Paper ID: 2204.10894v1
* Paper URL: [http://arxiv.org/abs/2204.10894v1](http://arxiv.org/abs/2204.10894v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We extend quantum noise spectroscopy (QNS) of amplitude control noise to
settings where dephasing noise or detuning errors make significant
contributions to qubit dynamics. Previous approaches to characterize amplitude
noise are limited by their vulnerability to low-frequency dephasing noise and
static detuning errors, which can overwhelm the target control noise signal and
introduce bias into estimates of the amplitude noise spectrum. To overcome this
problem, we leverage optimal control to identify a family of amplitude control
waveforms that optimally suppress low-frequency dephasing noise and detuning
errors, while maintaining the spectral concentration in the amplitude filter
essential for spectral estimation. The waveforms found via numerical
optimization have surprisingly simple analytic forms, consisting of oscillating
sine waves obeying particular amplitude and frequency constraints. In
numerically simulated QNS experiments, these waveforms demonstrate superior
robustness, enabling accurate estimation of the amplitude noise spectrum in
regimes where existing approaches are biased by low-frequency dephasing noise
and detuning errors.

### Title: Locally Aggregated Feature Attribution on Natural Language Model Understanding
* Paper ID: 2204.10893v1
* Paper URL: [http://arxiv.org/abs/2204.10893v1](http://arxiv.org/abs/2204.10893v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: With the growing popularity of deep-learning models, model understanding
becomes more important. Much effort has been devoted to demystify deep neural
networks for better interpretability. Some feature attribution methods have
shown promising results in computer vision, especially the gradient-based
methods where effectively smoothing the gradients with reference data is key to
a robust and faithful result. However, direct application of these
gradient-based methods to NLP tasks is not trivial due to the fact that the
input consists of discrete tokens and the "reference" tokens are not explicitly
defined. In this work, we propose Locally Aggregated Feature Attribution
(LAFA), a novel gradient-based feature attribution method for NLP models.
Instead of relying on obscure reference tokens, it smooths gradients by
aggregating similar reference texts derived from language model embeddings. For
evaluation purpose, we also design experiments on different NLP tasks including
Entity Recognition and Sentiment Analysis on public datasets as well as key
feature detection on a constructed Amazon catalogue dataset. The superior
performance of the proposed method is demonstrated through experiments.

### Title: Identity Preserving Loss for Learned Image Compression
* Paper ID: 2204.10869v1
* Paper URL: [http://arxiv.org/abs/2204.10869v1](http://arxiv.org/abs/2204.10869v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Deep learning model inference on embedded devices is challenging due to the
limited availability of computation resources. A popular alternative is to
perform model inference on the cloud, which requires transmitting images from
the embedded device to the cloud. Image compression techniques are commonly
employed in such cloud-based architectures to reduce transmission latency over
low bandwidth networks. This work proposes an end-to-end image compression
framework that learns domain-specific features to achieve higher compression
ratios than standard HEVC/JPEG compression techniques while maintaining
accuracy on downstream tasks (e.g., recognition). Our framework does not
require fine-tuning of the downstream task, which allows us to drop-in any
off-the-shelf downstream task model without retraining. We choose faces as an
application domain due to the ready availability of datasets and off-the-shelf
recognition models as representative downstream tasks. We present a novel
Identity Preserving Reconstruction (IPR) loss function which achieves
Bits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression
for LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,
respectively, while maintaining parity in recognition accuracy. The superior
compression ratio is achieved as the model learns to retain the domain-specific
features (e.g., facial features) while sacrificing details in the background.
Furthermore, images reconstructed by our proposed compression model are robust
to changes in downstream model architectures. We show at-par recognition
performance on the LFW dataset with an unseen recognition model while retaining
a lower BPP value of ~38% of CRF-23 HEVC compression.

### Title: Metric Learning and Adaptive Boundary for Out-of-Domain Detection
* Paper ID: 2204.10849v1
* Paper URL: [http://arxiv.org/abs/2204.10849v1](http://arxiv.org/abs/2204.10849v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/tgargiani/adaptive-boundary](https://github.com/tgargiani/adaptive-boundary)
* Summary: Conversational agents are usually designed for closed-world environments.
Unfortunately, users can behave unexpectedly. Based on the open-world
environment, we often encounter the situation that the training and test data
are sampled from different distributions. Then, data from different
distributions are called out-of-domain (OOD). A robust conversational agent
needs to react to these OOD utterances adequately. Thus, the importance of
robust OOD detection is emphasized. Unfortunately, collecting OOD data is a
challenging task. We have designed an OOD detection algorithm independent of
OOD data that outperforms a wide range of current state-of-the-art algorithms
on publicly available datasets. Our algorithm is based on a simple but
efficient approach of combining metric learning with adaptive decision
boundary. Furthermore, compared to other algorithms, we have found that our
proposed algorithm has significantly improved OOD performance in a scenario
with a lower number of classes while preserving the accuracy for in-domain
(IND) classes.

### Title: How Sampling Impacts the Robustness of Stochastic Neural Networks
* Paper ID: 2204.10839v1
* Paper URL: [http://arxiv.org/abs/2204.10839v1](http://arxiv.org/abs/2204.10839v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Stochastic neural networks (SNNs) are random functions and predictions are
gained by averaging over multiple realizations of this random function.
Consequently, an adversarial attack is calculated based on one set of samples
and applied to the prediction defined by another set of samples. In this paper
we analyze robustness in this setting by deriving a sufficient condition for
the given prediction process to be robust against the calculated attack. This
allows us to identify the factors that lead to an increased robustness of SNNs
and helps to explain the impact of the variance and the amount of samples.
Among other things, our theoretical analysis gives insights into (i) why
increasing the amount of samples drawn for the estimation of adversarial
examples increases the attack's strength, (ii) why decreasing sample size
during inference hardly influences the robustness, and (iii) why a higher
prediction variance between realizations relates to a higher robustness. We
verify the validity of our theoretical findings by an extensive empirical
analysis.

### Title: Robust and efficient primal-dual Newton-Krylov solvers for viscous-plastic sea-ice models
* Paper ID: 2204.10822v1
* Paper URL: [http://arxiv.org/abs/2204.10822v1](http://arxiv.org/abs/2204.10822v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We present a Newton-Krylov solver for a viscous-plastic sea-ice model. This
constitutive relation is commonly used in climate models to describe the large
scale sea-ice motion. Due to the strong nonlinearity of the momentum equation,
the development of fast, robust and scalable solvers is still a substantial
challenge. We propose a novel primal-dual Newton linearization for the momentum
equation. In contrast to existing methods, it converges faster and more
robustly with respect to mesh refinement, and thus allows fully resolved
sea-ice simulations. Combined with an algebraic multigrid-preconditioned Krylov
method for the Newton linearized systems, which contain strongly varying
coefficients, the resulting solver scales well and can be used in parallel. We
present highly resolved benchmark solutions and solve problems with up to 8.4
million spatial unknowns.

### Title: A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning
* Paper ID: 2204.10815v1
* Paper URL: [http://arxiv.org/abs/2204.10815v1](http://arxiv.org/abs/2204.10815v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Subword tokenization is a commonly used input pre-processing step in most
recent NLP models. However, it limits the models' ability to leverage
end-to-end task learning. Its frequency-based vocabulary creation compromises
tokenization in low-resource languages, leading models to produce suboptimal
representations. Additionally, the dependency on a fixed vocabulary limits the
subword models' adaptability across languages and domains. In this work, we
propose a vocabulary-free neural tokenizer by distilling segmentation
information from heuristic-based subword tokenization. We pre-train our
character-based tokenizer by processing unique words from multilingual corpus,
thereby extensively increasing word diversity across languages. Unlike the
predefined and fixed vocabularies in subword methods, our tokenizer allows
end-to-end task learning, resulting in optimal task-specific tokenization. The
experimental results show that replacing the subword tokenizer with our neural
tokenizer consistently improves performance on multilingual (NLI) and
code-switching (sentiment analysis) tasks, with larger gains in low-resource
languages. Additionally, our neural tokenizer exhibits a robust performance on
downstream tasks when adversarial noise is present (typos and misspelling),
further increasing the initial improvements over statistical subword
tokenizers.

### Title: Machine Learning methods to estimate observational properties of galaxy clusters in large volume cosmological N-body simulations
* Paper ID: 2204.10751v1
* Paper URL: [http://arxiv.org/abs/2204.10751v1](http://arxiv.org/abs/2204.10751v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: In this paper we study the applicability of a set of supervised machine
learning (ML) models specifically trained to infer observed related properties
of the baryonic component (stars and gas) from a set of features of dark matter
only cluster-size halos. The training set is built from THE THREE HUNDRED
project which consists of a series of zoomed hydrodynamical simulations of
cluster-size regions extracted from the 1 Gpc volume MultiDark dark-matter only
simulation (MDPL2). We use as target variables a set of baryonic properties for
the intra cluster gas and stars derived from the hydrodynamical simulations and
correlate them with the properties of the dark matter halos from the MDPL2
N-body simulation. The different ML models are trained from this database and
subsequently used to infer the same baryonic properties for the whole range of
cluster-size halos identified in the MDPL2. We also test the robustness of the
predictions of the models against mass resolution of the dark matter halos and
conclude that their inferred baryonic properties are rather insensitive to
their DM properties which are resolved with almost an order of magnitude
smaller number of particles. We conclude that the ML models presented in this
paper can be used as an accurate and computationally efficient tool for
populating cluster-size halos with observational related baryonic properties in
large volume N-body simulations making them more valuable for comparison with
full sky galaxy cluster surveys at different wavelengths. We make the best ML
trained model publicly available.

### Title: Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines
* Paper ID: 2204.10746v1
* Paper URL: [http://arxiv.org/abs/2204.10746v1](http://arxiv.org/abs/2204.10746v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We propose an end-to-end pipeline for both building and tracking 3D facial
models from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)
video data. First, we present a method for automatic data curation and
retrieval based on a hierarchical clustering framework typical of collision
detection algorithms in traditional computer graphics pipelines. Subsequently,
we utilize synthetic turntables and leverage deepfake technology in order to
build a synthetic multi-view stereo pipeline for appearance capture that is
robust to imperfect synthetic geometry and image misalignment. The resulting
model is fit with an animation rig, which is then used to track facial
performances. Notably, our novel use of deepfake technology enables us to
perform robust tracking of in-the-wild data using differentiable renderers
despite a significant synthetic-to-real domain gap. Finally, we outline how we
train a motion capture regressor, leveraging the aforementioned techniques to
avoid the need for real-world ground truth data and/or a high-end calibrated
camera capture setup.

### Title: Embracing AWKWARD! Real-time Adjustment of Reactive Planning Using Social Norms
* Paper ID: 2204.10740v1
* Paper URL: [http://arxiv.org/abs/2204.10740v1](http://arxiv.org/abs/2204.10740v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/lulock/dota](https://github.com/lulock/dota)
* Summary: This paper presents the AWKWARD agent architecture for the development of
agents in Multi-Agent Systems. AWKWARD agents can have their plans
re-configured in real time to align with social role requirements under
changing environmental and social circumstances. The proposed hybrid
architecture makes use of Behaviour Oriented Design (BOD) to develop agents
with reactive planning and of the well-established OperA framework to provide
organisational, social, and interaction definitions in order to validate and
adjust agents' behaviours. Together, OperA and BOD can achieve real-time
adjustment of agent plans for evolving social roles, while providing the
additional benefit of transparency into the interactions that drive this
behavioural change in individual agents. We present this architecture to
motivate the bridging between traditional symbolic- and behaviour-based AI
communities, where such combined solutions can help MAS researchers in their
pursuit of building stronger, more robust intelligent agent teams. We use DOTA2
-- a game where success is heavily dependent on social interactions -- as a
medium to demonstrate a sample implementation of our proposed hybrid
architecture

### Title: Analysing Opportunity Cost of Care Work using Mixed Effects Random Forests under Aggregated Census Data
* Paper ID: 2204.10736v1
* Paper URL: [http://arxiv.org/abs/2204.10736v1](http://arxiv.org/abs/2204.10736v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Reliable estimators of the spatial distribution of socio-economic indicators
are essential for evidence-based policy-making. As sample sizes are small for
highly disaggregated domains, the accuracy of the direct estimates is reduced.
To overcome this problem small area estimation approaches are promising. In
this work we propose a small area methodology using machine learning methods.
The semi-parametric framework of mixed effects random forest combines the
advantages of random forests (robustness against outliers and implicit
model-selection) with the ability to model hierarchical dependencies. Existing
random forest-based methods require access to auxiliary information on
population-level. We present a methodology that deals with the lack of
population micro-data. Our strategy adaptively incorporates aggregated
auxiliary information through calibration-weights - based on empirical
likelihood - for the estimation of area-level means. In addition to our point
estimator, we provide a non-parametric bootstrap estimator measuring its
uncertainty. The performance of the proposed point estimator and its
uncertainty measure is studied in model-based simulations. Finally, the
proposed methodology is applied to the $2011$ Socio-Economic Panel and
aggregate census information from the same year to estimate the average
opportunity cost of care work for $96$ regional planning regions in Germany.

### Title: SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across Drone and Satellite
* Paper ID: 2204.10704v1
* Paper URL: [http://arxiv.org/abs/2204.10704v1](http://arxiv.org/abs/2204.10704v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/Reza-Zhu/SUES-200-Benchmark](https://github.com/Reza-Zhu/SUES-200-Benchmark)
* Summary: The purpose of cross-view image matching is to match images acquired from the
different platforms of the same target scene and then help positioning system
to infer the location of the target scene. With the rapid development of drone
technology, how to help Drone positioning or navigation through cross-view
matching technology has become a challenging research topic. However, the
accuracy of current cross-view matching models is still low, mainly because the
existing public datasets do not include the differences in images obtained by
drones at different heights, and the types of scenes are relatively
homogeneous, which makes the models unable to adapt to complex and changing
scenes. We propose a new cross-view dataset, SUES-200, to address these
issues.SUES-200 contains images acquired by the drone at four flight heights
and the corresponding satellite view images under the same target scene. To our
knowledge, SUES-200 is the first dataset that considers the differences
generated by aerial photography of drones at different flight heights. In
addition, we build a pipeline for efficient training testing and evaluation of
cross-view matching models. Then, we comprehensively evaluate the performance
of feature extractors with different CNN architectures on SUES-200 through an
evaluation system for cross-view matching models and propose a robust baseline
model. The experimental results show that SUES-200 can help the model learn
features with high discrimination at different heights. Evaluating indicators
of the matching system improves as the drone flight height gets higher because
the drone camera pose and the surrounding environment have less influence on
aerial photography.

### Title: Graph Gain: A Concave-Hull Based Volumetric Gain for Robotic Exploration
* Paper ID: 2204.10698v1
* Paper URL: [http://arxiv.org/abs/2204.10698v1](http://arxiv.org/abs/2204.10698v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: The existing volumetric gain for robotic exploration is calculated in the 3D
occupancy map, while the sampling-based exploration method is extended in the
reachable (free) space. The inconsistency between them makes the existing
calculation of volumetric gain inappropriate for a complete exploration of the
environment. To address this issue, we propose a concave-hull based volumetric
gain in a sampling-based exploration framework. The concave hull is constructed
based on the viewpoints generated by Rapidly-exploring Random Tree (RRT) and
the nodes that fail to expand. All space outside this concave hull is
considered unknown. The volumetric gain is calculated based on the viewpoints
configuration rather than using the occupancy map. With the new volumetric
gain, robots can avoid inefficient or even erroneous exploration behavior
caused by the inappropriateness of existing volumetric gain calculation
methods. Our exploration method is evaluated against the existing
state-of-the-art RRT-based method in a benchmark environment. In the evaluated
environment, the average running time of our method is about 38.4% of the
existing state-of-the-art method and our method is more robust.

### Title: Universum-inspired Supervised Contrastive Learning
* Paper ID: 2204.10695v1
* Paper URL: [http://arxiv.org/abs/2204.10695v1](http://arxiv.org/abs/2204.10695v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Mixup is an efficient data augmentation method which generates additional
samples through respective convex combinations of original data points and
labels. Although being theoretically dependent on data properties, Mixup is
reported to perform well as a regularizer and calibrator contributing reliable
robustness and generalization to neural network training. In this paper,
inspired by Universum Learning which uses out-of-class samples to assist the
target tasks, we investigate Mixup from a largely under-explored perspective -
the potential to generate in-domain samples that belong to none of the target
classes, that is, universum. We find that in the framework of supervised
contrastive learning, universum-style Mixup produces surprisingly high-quality
hard negatives, greatly relieving the need for a large batch size in
contrastive learning. With these findings, we propose Universum-inspired
Contrastive learning (UniCon), which incorporates Mixup strategy to generate
universum data as g-negatives and pushes them apart from anchor samples of the
target classes. Our approach not only improves Mixup with hard labels, but also
innovates a novel measure to generate universum data. With a linear classifier
on the learned representations, our method achieves 81.68% top-1 accuracy on
CIFAR-100, surpassing the state of art by a significant margin of 5% with a
much smaller batch size, typically, 256 in UniCon vs. 1024 in SupCon using
ResNet-50.

### Title: Implicit Channel Charting with Application to UAV-aided Localization
* Paper ID: 2204.10690v1
* Paper URL: [http://arxiv.org/abs/2204.10690v1](http://arxiv.org/abs/2204.10690v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Traditional localization algorithms based on features such as time difference
of arrival are impaired by non-line of sight propagation, which negatively
affects the consistency that they expect among distance estimates. Instead,
fingerprinting localization is robust to these propagation conditions but
requires the costly collection of large data sets. To alleviate these
limitations, the present paper capitalizes on the recently-proposed notion of
channel charting to learn the geometry of the space that contains the channel
state information (CSI) measurements collected by the nodes to be localized.
The proposed algorithm utilizes a deep neural network that learns distances
between pairs of nodes using their measured CSI. Unlike standard channel
charting approaches, this algorithm directly works with the physical geometry
and therefore only implicitly learns the geometry of the radio domain.
Simulation results demonstrate that the proposed algorithm outperforms its
competitors and allows accurate localization in emergency scenarios using an
unmanned aerial vehicle.

### Title: Log-based Sparse Nonnegative Matrix Factorization for Data Representation
* Paper ID: 2204.10647v1
* Paper URL: [http://arxiv.org/abs/2204.10647v1](http://arxiv.org/abs/2204.10647v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Nonnegative matrix factorization (NMF) has been widely studied in recent
years due to its effectiveness in representing nonnegative data with
parts-based representations. For NMF, a sparser solution implies better
parts-based representation.However, current NMF methods do not always generate
sparse solutions.In this paper, we propose a new NMF method with log-norm
imposed on the factor matrices to enhance the sparseness.Moreover, we propose a
novel column-wisely sparse norm, named $\ell_{2,\log}$-(pseudo) norm to enhance
the robustness of the proposed method.The $\ell_{2,\log}$-(pseudo) norm is
invariant, continuous, and differentiable.For the $\ell_{2,\log}$ regularized
shrinkage problem, we derive a closed-form solution, which can be used for
other general problems.Efficient multiplicative updating rules are developed
for the optimization, which theoretically guarantees the convergence of the
objective value sequence.Extensive experimental results confirm the
effectiveness of the proposed method, as well as the enhanced sparseness and
robustness.

### Title: A robust Bayesian bias-adjusted random effects model for consideration of uncertainty about bias terms in evidence synthesis
* Paper ID: 2204.10645v1
* Paper URL: [http://arxiv.org/abs/2204.10645v1](http://arxiv.org/abs/2204.10645v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Meta-analysis is a statistical method used in evidence synthesis for
combining, analyzing and summarizing studies that have the same target endpoint
and aims to derive a pooled quantitative estimate using fixed and random
effects models or network models. Differences among included studies depend on
variations in target populations (i.e. heterogeneity) and variations in study
quality due to study design and execution (i.e. bias). The risk of bias is
usually assessed qualitatively using critical appraisal, and quantitative bias
analysis can be used to evaluate the influence of bias on the quantity of
interest. We propose a way to consider ignorance or ambiguity in how to
quantify bias terms in a bias analysis by characterizing bias with imprecision
(as bounds on probability) and use robust Bayesian analysis to estimate the
overall effect. Robust Bayesian analysis is here seen as Bayesian updating
performed over a set of coherent probability distributions, where the set
emerges from a set of bias terms. We show how the set of bias terms can be
specified based on judgments on the relative magnitude of biases (i.e., low,
unclear and high risk of bias) in one or several domains of the Cochrane's risk
of bias table. For illustration, we apply a robust Bayesian bias-adjusted
random effects model to an already published meta-analysis on the effect of
Rituximab for rheumatoid arthritis from the Cochrane Database of Systematic
Reviews.

### Title: Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks
* Paper ID: 2204.10615v1
* Paper URL: [http://arxiv.org/abs/2204.10615v1](http://arxiv.org/abs/2204.10615v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/ruixiangcui/gqnli](https://github.com/ruixiangcui/gqnli)
* Summary: Logical approaches to representing language have developed and evaluated
computational models of quantifier words since the 19th century, but today's
NLU models still struggle to capture their semantics. We rely on Generalized
Quantifier Theory for language-independent representations of the semantics of
quantifier words, to quantify their contribution to the errors of NLU models.
We find that quantifiers are pervasive in NLU benchmarks, and their occurrence
at test time is associated with performance drops. Multilingual models also
exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse
for non-English languages. To facilitate directly-targeted probing, we present
an adversarial generalized quantifier NLI task (GQNLI) and show that
pre-trained language models have a clear lack of robustness in generalized
quantifier reasoning.

### Title: Non-Uniformly Terminating Chase: Size and Complexity
* Paper ID: 2204.10584v1
* Paper URL: [http://arxiv.org/abs/2204.10584v1](http://arxiv.org/abs/2204.10584v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: The chase procedure, originally introduced for checking implication of
database constraints, and later on used for computing data exchange solutions,
has recently become a central algorithmic tool in rule-based ontological
reasoning. In this context, a key problem is non-uniform chase termination:
does the chase of a database w.r.t. a rule-based ontology terminate? And if
this is the case, what is the size of the result of the chase? We focus on
guarded tuple-generating dependencies (TGDs), which form a robust rule-based
ontology language, and study the above central questions for the semi-oblivious
version of the chase. One of our main findings is that non-uniform
semi-oblivious chase termination for guarded TGDs is feasible in polynomial
time w.r.t. the database, and the size of the result of the chase (whenever is
finite) is linear w.r.t. the database. Towards our results concerning
non-uniform chase termination, we show that basic techniques such as
simplification and linearization, originally introduced in the context of
ontological query answering, can be safely applied to the chase termination
problem.

### Title: Simulation of the FDA Nozzle Benchmark: A Lattice Boltzmann Study
* Paper ID: 2204.10566v1
* Paper URL: [http://arxiv.org/abs/2204.10566v1](http://arxiv.org/abs/2204.10566v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Background and objective: Contrary to flows in small intracranial vessels,
many blood flow configurations such as those found in aortic vessels and
aneurysms involve larger Reynolds numbers and, therefore, transitional or
turbulent conditions. Dealing with such systems require both robust and
efficient numerical methods. Methods: We assess here the performance of a
lattice Boltzmann solver with full Hermite expansion of the equilibrium and
central Hermite moments collision operator at higher Reynolds numbers,
especially for under-resolved simulations. To that end the food and drug
administration's benchmark nozzle is considered at three different Reynolds
numbers covering all regimes: 1) laminar at a Reynolds number of 500, 2)
transitional at a Reynolds number of $3500$, and 3) low-level turbulence at a
Reynolds number of 6500. Results: The lattice Boltzmann results are compared
with previously published inter-laboratory experimental data obtained by
particle image velocimetry. Our results show good agreement with the
experimental measurements throughout the nozzle, demonstrating the good
performance of the solver even in under-resolved simulations. Conclusion: In
this manner, fast but sufficiently accurate numerical predictions can be
achieved for flow configurations of practical interest regarding medical
applications.

### Title: Making Parameterization and Constrains of Object Landmark Globally Consistent via SPD(3) Manifold and Improved Cost Functions
* Paper ID: 2204.10552v1
* Paper URL: [http://arxiv.org/abs/2204.10552v1](http://arxiv.org/abs/2204.10552v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Object-level SLAM introduces semantic meaningful and compact object landmarks
that help both indoor robot applications and outdoor autonomous driving tasks.
However, the back end of object-level SLAM suffers from singularity problems
because existing methods parameterize object landmark separately by their
scales and poses. Under that parameterization method, the same abstract object
can be represented by rotating the object coordinate frame by 90 deg and
swapping its length with width value, making the pose of the same object
landmark not globally consistent. To avoid the singularity problem, we first
introduce the symmetric positive-definite (SPD) matrix manifold as an improved
object-level landmark representation and further improve the cost functions in
the back end to make them compatible with the representation. Our method
demonstrates a faster convergence rate and more robustness in simulation
experiments. Experiments on real datasets also reveal that using the same
front-end data, our strategy improves the mapping accuracy by 22% on average.

### Title: Multi-view Information Bottleneck Without Variational Approximation
* Paper ID: 2204.10530v1
* Paper URL: [http://arxiv.org/abs/2204.10530v1](http://arxiv.org/abs/2204.10530v1)
* Updated Date: 2022-04-22
* Code URL: [https://github.com/archy666/meib](https://github.com/archy666/meib)
* Summary: By "intelligently" fusing the complementary information across different
views, multi-view learning is able to improve the performance of classification
tasks. In this work, we extend the information bottleneck principle to a
supervised multi-view learning scenario and use the recently proposed
matrix-based R{\'e}nyi's $\alpha$-order entropy functional to optimize the
resulting objective directly, without the necessity of variational
approximation or adversarial training. Empirical results in both synthetic and
real-world datasets suggest that our method enjoys improved robustness to noise
and redundant information in each view, especially given limited training
samples. Code is available at~\url{https://github.com/archy666/MEIB}.

### Title: Implicit Object Mapping With Noisy Data
* Paper ID: 2204.10516v1
* Paper URL: [http://arxiv.org/abs/2204.10516v1](http://arxiv.org/abs/2204.10516v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Modelling individual objects as Neural Radiance Fields (NeRFs) within a
robotic context can benefit many downstream tasks such as scene understanding
and object manipulation. However, real-world training data collected by a robot
deviate from the ideal in several key aspects. (i) The trajectories are
constrained and full visual coverage is not guaranteed - especially when
obstructions are present. (ii) The poses associated with the images are noisy.
(iii) The objects are not easily isolated from the background. This paper
addresses the above three points and uses the outputs of an object-based SLAM
system to bound objects in the scene with coarse primitives and - in concert
with instance masks - identify obstructions in the training images. Objects are
therefore automatically bounded, and non-relevant geometry is excluded from the
NeRF representation. The method's performance is benchmarked under ideal
conditions and tested against errors in the poses and instance masks. Our
results show that object-based NeRFs are robust to pose variations but
sensitive to the quality of the instance masks.

### Title: Imputation with verifiable identification condition for nonignorable missing outcomes
* Paper ID: 2204.10508v1
* Paper URL: [http://arxiv.org/abs/2204.10508v1](http://arxiv.org/abs/2204.10508v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Missing data often results in undesirable bias and loss of efficiency. These
results become substantial problems when the response mechanism is
nonignorable, such that the response model depends on the unobserved variable.
It is often necessary to estimate the joint distribution of the unobserved
variables and response indicators to further manage nonignorable nonresponse.
However, model misspecification and identification issues prevent robust
estimates, despite carefully estimating the target joint distribution. In this
study we model the distribution of the observed parts and derived sufficient
conditions for model identifiability, assuming a logistic distribution of the
response mechanism and a generalized linear model as the main outcome model of
interest. More importantly, the derived sufficient conditions are testable with
the observed data and do not require any instrumental variables, which have
often been assumed to guarantee model identifiability but cannot be practically
determined beforehand. To analyse missing data, we propose a new fractional
imputation method which incorporates verifiable identifiability using only the
observed data. Furthermore, we present the performance of the proposed
estimators in numerical studies and apply the proposed method to two sets of
real data, namely, Opinion Poll for the 2022 South Korean Presidential
Election, and public data collected from the US National Supported Work
Evaluation Study.

### Title: Application of Federated Learning in Building a Robust COVID-19 Chest X-ray Classification Model
* Paper ID: 2204.10505v1
* Paper URL: [http://arxiv.org/abs/2204.10505v1](http://arxiv.org/abs/2204.10505v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: While developing artificial intelligence (AI)-based algorithms to solve
problems, the amount of data plays a pivotal role - large amount of data helps
the researchers and engineers to develop robust AI algorithms. In the case of
building AI-based models for problems related to medical imaging, these data
need to be transferred from the medical institutions where they were acquired
to the organizations developing the algorithms. This movement of data involves
time-consuming formalities like complying with HIPAA, GDPR, etc.There is also a
risk of patients' private data getting leaked, compromising their
confidentiality. One solution to these problems is using the Federated Learning
framework.
  Federated Learning (FL) helps AI models to generalize better and create a
robust AI model by using data from different sources having different
distributions and data characteristics without moving all the data to a central
server. In our paper, we apply the FL framework for training a deep learning
model to solve a binary classification problem of predicting the presence or
absence of COVID-19. We took three different sources of data and trained
individual models on each source. Then we trained an FL model on the complete
data and compared all the model performances. We demonstrated that the FL model
performs better than the individual models. Moreover, the FL model performed at
par with the model trained on all the data combined at a central server. Thus
Federated Learning leads to generalized AI models without the cost of data
transfer and regulatory overhead.

### Title: Gravity aided navigation using Viterbi map matching algorithm
* Paper ID: 2204.10492v1
* Paper URL: [http://arxiv.org/abs/2204.10492v1](http://arxiv.org/abs/2204.10492v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: In GNSS-denied environments, aiding a vehicle's inertial navigation system
(INS) is crucial to reducing the accumulated navigation drift caused by sensor
errors (e.g. bias and noise). One potential solution is to use measurements of
gravity as an aiding source. The measurements are matched to a geo-referenced
map of Earth's gravity in order to estimate the vehicle's position. In this
paper, we propose a novel formulation of the map matching problem using a
hidden Markov model (HMM). Specifically, we treat the spatial cells of the map
as the hidden states of the HMM and present a Viterbi style algorithm to
estimate the most likely sequence of states, i.e. most likely sequence of
vehicle positions, that results in the sequence of observed gravity
measurements. Using a realistic gravity map, we demonstrate the accuracy of our
Viterbi map matching algorithm in a navigation scenario and illustrate its
robustness compared to existing methods.

### Title: On the Robustness of Second-Price Auctions in Prior-Independent Mechanism Design
* Paper ID: 2204.10478v1
* Paper URL: [http://arxiv.org/abs/2204.10478v1](http://arxiv.org/abs/2204.10478v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Classical Bayesian mechanism design relies on the common prior assumption,
but the common prior is often not available in practice. We study the design of
prior-independent mechanisms that relax this assumption: the seller is selling
an indivisible item to $n$ buyers such that the buyers' valuations are drawn
from a joint distribution that is unknown to both the buyers and the seller;
buyers do not need to form beliefs about competitors, and the seller assumes
the distribution is adversarially chosen from a specified class. We measure
performance through the worst-case regret, or the difference between the
expected revenue achievable with perfect knowledge of buyers' valuations and
the actual mechanism revenue.
  We study a broad set of classes of valuation distributions that capture a
wide spectrum of possible dependencies: independent and identically distributed
(i.i.d.) distributions, mixtures of i.i.d. distributions, affiliated and
exchangeable distributions, exchangeable distributions, and all joint
distributions. We derive in quasi closed form the minimax values and the
associated optimal mechanism. In particular, we show that the first three
classes admit the same minimax regret value, which is decreasing with the
number of competitors, while the last two have the same minimax regret equal to
that of the case $n = 1$. Furthermore, we show that the minimax optimal
mechanisms have a simple form across all settings: a second-price auction with
random reserve prices, which shows its robustness in prior-independent
mechanism design. En route to our results, we also develop a principled
methodology to determine the form of the optimal mechanism and worst-case
distribution via first-order conditions that should be of independent interest
in other minimax problems.

### Title: Sparse dynamical system identification with simultaneous structural parameters and initial condition estimation
* Paper ID: 2204.10472v1
* Paper URL: [http://arxiv.org/abs/2204.10472v1](http://arxiv.org/abs/2204.10472v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: Sparse Identification of Nonlinear Dynamics (SINDy) has been shown to
successfully recover governing equations from data; however, this approach
assumes the initial condition to be exactly known in advance and is sensitive
to noise. In this work we propose an integral SINDy (ISINDy) method to
simultaneously identify model structure and parameters of nonlinear ordinary
differential equations (ODEs) from noisy time-series observations. First, the
states are estimated via penalized spline smoothing and then substituted into
the integral-form numerical discretization solver, leading to a pseudo-linear
regression. The sequential threshold least squares is performed to extract the
fewest active terms from the overdetermined set of candidate features, thereby
estimating structural parameters and initial condition simultaneously and
meanwhile, making the identified dynamics parsimonious and interpretable.
Simulations detail the method's recovery accuracy and robustness to noise.
Examples include a logistic equation, Lokta-Volterra system, and Lorenz system.

### Title: Parameter-robust Braess-Sarazin-type smoothers for linear elasticity problems
* Paper ID: 2204.10462v1
* Paper URL: [http://arxiv.org/abs/2204.10462v1](http://arxiv.org/abs/2204.10462v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: In this work, we propose three Braess-Sarazin-type multigrid relaxation
schemes for solving linear elasticity problems, where the marker and cell
scheme, a finite difference method, is used for the discretization. The three
relaxation schemes are Jacobi-Braess-Sarazin, Mass-Braess-Sarazin, and
Vanka-Braess-Sarazin. A local Fourier analysis (LFA) for the block-structured
relaxation schemes is presented to study multigrid convergence behavior. From
LFA, we derive optimal LFA smoothing factor for each case. We obtain highly
efficient smoothing factors, which are independent of Lam\'{e} constants.
Vanka-Braess-Sarazin relaxation scheme leads to the most efficient one. In each
relaxation, a Schur complement system needs to be solved. Due to the fact that
direct solve is often expensive, an inexact version is developed, where we
simply use at most three weighted Jacobi iterations on the Schur complement
system. Finally, two-grid and V-cycle multigrid performances are presented to
validate our theoretical results. Our numerical results show that inexact
versions can achieve the same performance as that of exact versions and our
methods are robust to the Lam\'{e} constants.

### Title: Robust estimation and model diagnostic of insurance loss data: a weighted likelihood approach
* Paper ID: 2204.10459v1
* Paper URL: [http://arxiv.org/abs/2204.10459v1](http://arxiv.org/abs/2204.10459v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: This paper presents a score-based weighted likelihood estimator (SWLE) for
robust estimations of generalized linear model (GLM) for insurance loss data.
The SWLE exhibits a limited sensitivity to the outliers, theoretically
justifying its robustness against model contaminations. Also, with the
specially designed weight function to effectively diminish the contributions of
extreme losses to the GLM parameter estimations, most statistical quantities
can still be derived analytically, minimizing the computational burden for
parameter calibrations. Apart from robust estimations, the SWLE can also act as
a quantitative diagnostic tool to detect outliers and systematic model
misspecifications. Motivated by the coverage modifications which make insurance
losses often random censored and truncated, the SWLE is extended to accommodate
censored and truncated data. We exemplify the SWLE on three simulation studies
and two real insurance datasets. Empirical results suggest that the SWLE
produces more reliable parameter estimates than the MLE if outliers contaminate
the dataset. The SWLE diagnostic tool also successfully detects any systematic
model misspecifications with high power, accompanying some potential model
improvements.

### Title: Fixed-Time Parameter Adaptation for Safe Control Synthesis
* Paper ID: 2204.10453v1
* Paper URL: [http://arxiv.org/abs/2204.10453v1](http://arxiv.org/abs/2204.10453v1)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: We propose a fixed-time stable parameter adaptation law that guarantees that
an additive, parameter-affine disturbance appearing in the dynamics of a class
of nonlinear, control-affine systems is learned within a fixed time. We then
provide an upper bound on the parameter estimation error as an explicit
function of time, and use such knowledge to formulate a control barrier
function condition to guarantee safety of the system trajectories at all times.
We further show that our proposed parameter adaptation law is robust against
bounded perturbations to the system dynamics in that the resulting estimated
parametric disturbance converges to a known neighborhood of the true parametric
disturbance within a fixed time. To illustrate the advantages of our proposed
approach, we conduct a comparative numerical study against various controllers
from the literature. Finally, we validate our approach on a trajectory-tracking
problem subject to safety requirements using a 6 degree-of-freedom quadrotor
model in an unknown wind field.

### Title: Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction
* Paper ID: 2204.09204v2
* Paper URL: [http://arxiv.org/abs/2204.09204v2](http://arxiv.org/abs/2204.09204v2)
* Updated Date: 2022-04-22
* Code URL: null
* Summary: When applying multi-instance learning (MIL) to make predictions for bags of
instances, the prediction accuracy of an instance often depends on not only the
instance itself but also its context in the corresponding bag. From the
viewpoint of causal inference, such bag contextual prior works as a confounder
and may result in model robustness and interpretability issues. Focusing on
this problem, we propose a novel interventional multi-instance learning (IMIL)
framework to achieve deconfounded instance-level prediction. Unlike traditional
likelihood-based strategies, we design an Expectation-Maximization (EM)
algorithm based on causal intervention, providing a robust instance selection
in the training phase and suppressing the bias caused by the bag contextual
prior. Experiments on pathological image analysis demonstrate that our IMIL
method substantially reduces false positives and outperforms state-of-the-art
MIL methods.

