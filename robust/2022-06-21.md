### Title: Robust SDE-Based Variational Formulations for Solving Linear PDEs via Deep Learning
* Paper ID: 2206.10588v1
* Paper URL: [http://arxiv.org/abs/2206.10588v1](http://arxiv.org/abs/2206.10588v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: The combination of Monte Carlo methods and deep learning has recently led to
efficient algorithms for solving partial differential equations (PDEs) in high
dimensions. Related learning problems are often stated as variational
formulations based on associated stochastic differential equations (SDEs),
which allow the minimization of corresponding losses using gradient-based
optimization methods. In respective numerical implementations it is therefore
crucial to rely on adequate gradient estimators that exhibit low variance in
order to reach convergence accurately and swiftly. In this article, we
rigorously investigate corresponding numerical aspects that appear in the
context of linear Kolmogorov PDEs. In particular, we systematically compare
existing deep learning approaches and provide theoretical explanations for
their performances. Subsequently, we suggest novel methods that can be shown to
be more robust both theoretically and numerically, leading to substantial
performance improvements.

### Title: D-CIPHER: Discovery of Closed-form PDEs
* Paper ID: 2206.10586v1
* Paper URL: [http://arxiv.org/abs/2206.10586v1](http://arxiv.org/abs/2206.10586v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Closed-form differential equations, including partial differential equations
and higher-order ordinary differential equations, are one of the most important
tools used by scientists to model and better understand natural phenomena.
Discovering these equations directly from data is challenging because it
requires modeling relationships between various derivatives that are not
observed in the data (\textit{equation-data mismatch}) and it involves
searching across a huge space of possible equations. Current approaches make
strong assumptions about the form of the equation and thus fail to discover
many well-known systems. Moreover, many of them resolve the equation-data
mismatch by estimating the derivatives, which makes them inadequate for noisy
and infrequently sampled systems. To this end, we propose D-CIPHER, which is
robust to measurement artifacts and can uncover a new and very general class of
differential equations. We further design a novel optimization procedure,
CoLLie, to help D-CIPHER search through this class efficiently. Finally, we
demonstrate empirically that it can discover many well-known equations that are
beyond the capabilities of current methods.

### Title: How viable is quantum annealing for solving linear algebra problems?
* Paper ID: 2206.10576v1
* Paper URL: [http://arxiv.org/abs/2206.10576v1](http://arxiv.org/abs/2206.10576v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: With the increasing popularity of quantum computing and in particular quantum
annealing, there has been growing research to evaluate the meta-heuristic for
various problems in linear algebra: from linear least squares to matrix and
tensor factorization. At the core of this effort is to evaluate quantum
annealing for solving linear least squares and linear systems of equations. In
this work, we focus on the viability of using quantum annealing for solving
these problems. We use simulations based on the adiabatic principle to provide
new insights for previously observed phenomena with the D-wave machines, such
as quantum annealing being robust against ill-conditioned systems of equations
and scaling quite well against the number of rows in a system. We then propose
a hybrid approach which uses a quantum annealer to provide a initial guess of
the solution $x_0$, which would then be iteratively improved with classical
fixed point iteration methods.

### Title: Ensembling over Classifiers: a Bias-Variance Perspective
* Paper ID: 2206.10566v1
* Paper URL: [http://arxiv.org/abs/2206.10566v1](http://arxiv.org/abs/2206.10566v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Ensembles are a straightforward, remarkably effective method for improving
the accuracy,calibration, and robustness of models on classification tasks;
yet, the reasons that underlie their success remain an active area of research.
We build upon the extension to the bias-variance decomposition by Pfau (2013)
in order to gain crucial insights into the behavior of ensembles of
classifiers. Introducing a dual reparameterization of the bias-variance
tradeoff, we first derive generalized laws of total expectation and variance
for nonsymmetric losses typical of classification tasks. Comparing conditional
and bootstrap bias/variance estimates, we then show that conditional estimates
necessarily incur an irreducible error. Next, we show that ensembling in dual
space reduces the variance and leaves the bias unchanged, whereas standard
ensembling can arbitrarily affect the bias. Empirically, standard ensembling
reducesthe bias, leading us to hypothesize that ensembles of classifiers may
perform well in part because of this unexpected reduction.We conclude by an
empirical analysis of recent deep learning methods that ensemble over
hyperparameters, revealing that these techniques indeed favor bias reduction.
This suggests that, contrary to classical wisdom, targeting bias reduction may
be a promising direction for classifier ensembles.

### Title: (Certified!!) Adversarial Robustness for Free!
* Paper ID: 2206.10550v1
* Paper URL: [http://arxiv.org/abs/2206.10550v1](http://arxiv.org/abs/2206.10550v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In this paper we show how to achieve state-of-the-art certified adversarial
robustness to 2-norm bounded perturbations by relying exclusively on
off-the-shelf pretrained models. To do so, we instantiate the denoised
smoothing approach of Salman et al. by combining a pretrained denoising
diffusion probabilistic model and a standard high-accuracy classifier. This
allows us to certify 71% accuracy on ImageNet under adversarial perturbations
constrained to be within a 2-norm of 0.5, an improvement of 14 percentage
points over the prior certified SoTA using any approach, or an improvement of
30 percentage points over denoised smoothing. We obtain these results using
only pretrained diffusion models and image classifiers, without requiring any
fine tuning or retraining of model parameters.

### Title: Robustness against data loss with Algebraic Statistics
* Paper ID: 2206.10521v1
* Paper URL: [http://arxiv.org/abs/2206.10521v1](http://arxiv.org/abs/2206.10521v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: The paper describes an algorithm that, given an initial design
$\mathcal{F}_n$ of size $n$ and a linear model with $p$ parameters, provides a
sequence $\mathcal{F}_n \supset \ldots \supset \mathcal{F}_{n-k} \supset \ldots
\supset \mathcal{F}_p$ of nested \emph{robust} designs. The sequence is
obtained by the removal, one by one, of the runs of $\mathcal{F}_n$ till a
$p$-run \emph{saturated} design $\mathcal{F}_p$ is obtained. The potential
impact of the algorithm on real applications is high. The initial fraction
$\mathcal{F}_n$ can be of any type and the output sequence can be used to
organize the experimental activity. The experiments can start with the runs
corresponding to $\mathcal{F}_p$ and continue adding one run after the other
(from $\mathcal{F}_{n-k}$ to $\mathcal{F}_{n-k+1}$) till the initial design
$\mathcal{F}_n$ is obtained. In this way, if for some unexpected reasons the
experimental activity must be stopped before the end when only $n-k$ runs are
completed, the corresponding $\mathcal{F}_{n-k}$ has a high value of robustness
for $k \in \{1, \ldots, n-p\}$. The algorithm uses the circuit basis, a special
representation of the kernel of a matrix with integer entries. The
effectiveness of the algorithm is demonstrated through the use of simulations.

### Title: GRMHD beyond Kerr: An extension of the HARM code for thin disks to non-Kerr spacetimes
* Paper ID: 2206.10474v1
* Paper URL: [http://arxiv.org/abs/2206.10474v1](http://arxiv.org/abs/2206.10474v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Black hole based tests of general relativity have proliferated in recent
times with new and improved detectors and telescopes. Modelling of the black
hole neighborhood, where most of the radiation carrying strong-field signature
originates, is of utmost importance for robust and accurate constraints on
possible violations of general relativity. As a first step, this paper presents
the extension of general relativistic magnetohydrodynamic simulations of thin
accretion disks to parametrically deformed black holes that generalize the Kerr
solution. The extension is based on \textsc{harmpi}, a publicly available
member of the \textsc{harm} family of codes, and uses a phenomenological metric
to study parametric deviations away from Kerr. The extended model is used to
study the disk structure, stability, and radiative efficiency. We also compute
the Fe K$\alpha$ profiles in simplified scenarios and present an outlook for
the future.

### Title: Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning
* Paper ID: 2206.10442v1
* Paper URL: [http://arxiv.org/abs/2206.10442v1](http://arxiv.org/abs/2206.10442v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: We study offline meta-reinforcement learning, a practical reinforcement
learning paradigm that learns from offline data to adapt to new tasks. The
distribution of offline data is determined jointly by the behavior policy and
the task. Existing offline meta-reinforcement learning algorithms cannot
distinguish these factors, making task representations unstable to the change
of behavior policies. To address this problem, we propose a contrastive
learning framework for task representations that are robust to the distribution
mismatch of behavior policies in training and test. We design a bi-level
encoder structure, use mutual information maximization to formalize task
representation learning, derive a contrastive learning objective, and introduce
several approaches to approximate the true distribution of negative pairs.
Experiments on a variety of offline meta-reinforcement learning benchmarks
demonstrate the advantages of our method over prior methods, especially on the
generalization to out-of-distribution behavior policies. The code is available
at https://github.com/PKU-AI-Edge/CORRO.

### Title: Plug and Play Counterfactual Text Generation for Model Robustness
* Paper ID: 2206.10429v1
* Paper URL: [http://arxiv.org/abs/2206.10429v1](http://arxiv.org/abs/2206.10429v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Generating counterfactual test-cases is an important backbone for testing NLP
models and making them as robust and reliable as traditional software. In
generating the test-cases, a desired property is the ability to control the
test-case generation in a flexible manner to test for a large variety of
failure cases and to explain and repair them in a targeted manner. In this
direction, significant progress has been made in the prior works by manually
writing rules for generating controlled counterfactuals. However, this approach
requires heavy manual supervision and lacks the flexibility to easily introduce
new controls. Motivated by the impressive flexibility of the plug-and-play
approach of PPLM, we propose bringing the framework of plug-and-play to
counterfactual test case generation task. We introduce CASPer, a plug-and-play
counterfactual generation framework to generate test cases that satisfy goal
attributes on demand. Our plug-and-play model can steer the test case
generation process given any attribute model without requiring
attribute-specific training of the model. In experiments, we show that CASPer
effectively generates counterfactual text that follow the steering provided by
an attribute model while also being fluent, diverse and preserving the original
content. We also show that the generated counterfactuals from CASPer can be
used for augmenting the training data and thereby fixing and making the test
model more robust.

### Title: Neural Moving Horizon Estimation for Robust Flight Control
* Paper ID: 2206.10397v1
* Paper URL: [http://arxiv.org/abs/2206.10397v1](http://arxiv.org/abs/2206.10397v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Estimating and reacting to external disturbances is crucial for robust flight
control of quadrotors. Existing estimators typically require significant tuning
for a specific flight scenario or training with extensive real-world data to
achieve satisfactory performance. In this paper, we propose a neural moving
horizon estimator (NeuroMHE) that can automatically tune the MHE parameters
modeled by a neural network and adapt to different flight scenarios. We achieve
this by deriving the analytical gradient of the MHE estimates with respect to
the tunable parameters, enabling a seamless embedding of MHE as a layer into
the neural network for highly effective learning. Most interestingly, we show
that the gradient can be solved efficiently from a Kalman filter in a recursive
form. Moreover, we develop a model-based policy gradient algorithm to train
NeuroMHE directly from the trajectory tracking error without the need for the
ground-truth disturbance. The effectiveness of NeuroMHE is verified extensively
via both simulations and physical experiments on a quadrotor in various
challenging flights. Notably, NeuroMHE outperforms the state-of-the-art
estimator with force estimation error reductions of up to 49.4% by using only a
2.5% amount of parameters. The proposed method is general and can be applied to
robust adaptive control for other robotic systems.

### Title: MEStereo-Du2CNN: A Novel Dual Channel CNN for Learning Robust Depth Estimates from Multi-exposure Stereo Images for HDR 3D Applications
* Paper ID: 2206.10375v1
* Paper URL: [http://arxiv.org/abs/2206.10375v1](http://arxiv.org/abs/2206.10375v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Display technologies have evolved over the years. It is critical to develop
practical HDR capturing, processing, and display solutions to bring 3D
technologies to the next level. Depth estimation of multi-exposure stereo image
sequences is an essential task in the development of cost-effective 3D HDR
video content. In this paper, we develop a novel deep architecture for
multi-exposure stereo depth estimation. The proposed architecture has two novel
components. First, the stereo matching technique used in traditional stereo
depth estimation is revamped. For the stereo depth estimation component of our
architecture, a mono-to-stereo transfer learning approach is deployed. The
proposed formulation circumvents the cost volume construction requirement,
which is replaced by a ResNet based dual-encoder single-decoder CNN with
different weights for feature fusion. EfficientNet based blocks are used to
learn the disparity. Secondly, we combine disparity maps obtained from the
stereo images at different exposure levels using a robust disparity feature
fusion approach. The disparity maps obtained at different exposures are merged
using weight maps calculated for different quality measures. The final
predicted disparity map obtained is more robust and retains best features that
preserve the depth discontinuities. The proposed CNN offers flexibility to
train using standard dynamic range stereo data or with multi-exposure low
dynamic range stereo sequences. In terms of performance, the proposed model
surpasses state-of-the-art monocular and stereo depth estimation methods, both
quantitatively and qualitatively, on challenging Scene flow and differently
exposed Middlebury stereo datasets. The architecture performs exceedingly well
on complex natural scenes, demonstrating its usefulness for diverse 3D HDR
applications.

### Title: Structure and Stability of the Indian Power Transmission Network
* Paper ID: 2206.10366v1
* Paper URL: [http://arxiv.org/abs/2206.10366v1](http://arxiv.org/abs/2206.10366v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: We present the study on the Indian power transmission network using the
framework of complex network and quantify its network properties. For this, we
build the network structure underlying the Indian power grid, basically using
the two most prevalent power lines. We construct an equivalent model of
exponential network and study its stability against cascading failures. This
helps to gain insight into the relation of network topology to its stability
and desirable performance. We then discuss robustness of the Indian power grid
using link failure model and how the optimum choice of parameters can result in
a failure-resistant power grid structure.

### Title: Using the Polar Transform for Efficient Deep Learning-Based Aorta Segmentation in CTA Images
* Paper ID: 2206.10294v1
* Paper URL: [http://arxiv.org/abs/2206.10294v1](http://arxiv.org/abs/2206.10294v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Medical image segmentation often requires segmenting multiple elliptical
objects on a single image. This includes, among other tasks, segmenting vessels
such as the aorta in axial CTA slices. In this paper, we present a general
approach to improving the semantic segmentation performance of neural networks
in these tasks and validate our approach on the task of aorta segmentation. We
use a cascade of two neural networks, where one performs a rough segmentation
based on the U-Net architecture and the other performs the final segmentation
on polar image transformations of the input. Connected component analysis of
the rough segmentation is used to construct the polar transformations, and
predictions on multiple transformations of the same image are fused using
hysteresis thresholding. We show that this method improves aorta segmentation
performance without requiring complex neural network architectures. In
addition, we show that our approach improves robustness and pixel-level recall
while achieving segmentation performance in line with the state of the art.

### Title: Algorithmic Gaussianization through Sketching: Converting Data into Sub-gaussian Random Designs
* Paper ID: 2206.10291v1
* Paper URL: [http://arxiv.org/abs/2206.10291v1](http://arxiv.org/abs/2206.10291v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Algorithmic Gaussianization is a phenomenon that can arise when using
randomized sketching or sampling methods to produce smaller representations of
large datasets: For certain tasks, these sketched representations have been
observed to exhibit many robust performance characteristics that are known to
occur when a data sample comes from a sub-gaussian random design, which is a
powerful statistical model of data distributions. However, this phenomenon has
only been studied for specific tasks and metrics, or by relying on
computationally expensive methods. We address this by providing an algorithmic
framework for gaussianizing data distributions via averaging, proving that it
is possible to efficiently construct data sketches that are nearly
indistinguishable (in terms of total variation distance) from sub-gaussian
random designs. In particular, relying on a recently introduced sketching
technique called Leverage Score Sparsified (LESS) embeddings, we show that one
can construct an $n\times d$ sketch of an $N\times d$ matrix $A$, where $n\ll
N$, that is nearly indistinguishable from a sub-gaussian design, in time
$O(\text{nnz}(A)\log N + nd^2)$, where $\text{nnz}(A)$ is the number of
non-zero entries in $A$. As a consequence, strong statistical guarantees and
precise asymptotics available for the estimators produced from sub-gaussian
designs (e.g., for least squares and Lasso regression, covariance estimation,
low-rank approximation, etc.) can be straightforwardly adapted to our sketching
framework. We illustrate this with a new approximation guarantee for sketched
least squares, among other examples.

### Title: Discretization and index-robust error analysis for constrained high-index saddle dynamics on high-dimensional sphere
* Paper ID: 2206.10290v1
* Paper URL: [http://arxiv.org/abs/2206.10290v1](http://arxiv.org/abs/2206.10290v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: We develop and analyze numerical discretization to the constrained high-index
saddle dynamics, the dynamics searching for the high-index saddle points
confined on the high-dimensional unit sphere. Compared with the saddle dynamics
without constraints, the constrained high-index saddle dynamics has more
complex dynamical forms, and additional operations such as the retraction and
vector transport are required due to the constraint, which significantly
complicate the numerical scheme and the corresponding numerical analysis.
Furthermore, as the existing numerical analysis results usually depend on the
index of the saddle points implicitly, the proved numerical accuracy may be
reduced if the index is high in many applications, which indicates the lack of
robustness with respect to the index. To address these issues, we derive the
error estimates for numerical discretization of the constrained high-index
saddle dynamics on high-dimensional sphere, and then improve it by providing an
index-robust error analysis in an averaged norm by adjusting the relaxation
parameters. The developed results provide mathematical supports for the
accuracy of numerical computations.

### Title: Kardar-Parisi-Zhang growth on square domains that enlarge nonlinearly in time
* Paper ID: 2206.10282v1
* Paper URL: [http://arxiv.org/abs/2206.10282v1](http://arxiv.org/abs/2206.10282v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: We study discrete KPZ growth models deposited on square lattice substrates,
whose (average) lateral size enlarges as $L= L_0 + \omega t^{\gamma}$. Our
numerical simulations reveal that the competition between the substrate
expansion and the increase of the correlation length parallel to the substrate,
$\xi \simeq c t^{1/z}$, gives rise to a number of interesting results. For
instance, when $\gamma < 1/z$ the interface becomes fully correlated, but its
squared roughness, $W_2$, keeps increasing as $W_2 \sim t^{2\alpha \gamma}$, as
previously observed for 1D systems. A careful analysis of this scaling,
accounting for an intrinsic width on it, allows us to estimate the roughness
exponent of the 2D KPZ class as $\alpha = 0.387(1)$, which is very accurate and
robust, once it was obtained averaging the exponents for different models and
growth conditions (i.e., for various $\gamma$'s and $\omega$'s). In this
correlated regime, the height distributions (HDs) and spatial covariances are
consistent with those expected for the steady-state regime of the 2D KPZ class
for flat geometry. For $\gamma \approx 1/z$, we find a family of distributions
and covariances continuously interpolating between those for the steady-state
and the growth regime of radial KPZ interfaces, as the ratio $\omega/c$
augments. When $\gamma>1/z$ the system stays forever in the growth regime and
the HDs always converge to the same asymptotic distribution, which is the one
for the radial case. The spatial covariances, on the other hand, are
$(\gamma,\omega)$-dependent, showing a trend towards the covariance of a random
deposition in enlarging substrates as the expansion rate increases. These
results considerably generalize our understanding of the height fluctuations in
2D KPZ systems, revealing a scenario very similar to the one previously found
in the 1D case.

### Title: Core-elements for Least Squares Estimation
* Paper ID: 2206.10240v1
* Paper URL: [http://arxiv.org/abs/2206.10240v1](http://arxiv.org/abs/2206.10240v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: The coresets approach, also called subsampling or subset selection, aims to
select a subsample as a surrogate for the observed sample. Such an approach has
been used pervasively in large-scale data analysis. Existing coresets methods
construct the subsample using a subset of rows from the predictor matrix. Such
methods can be significantly inefficient when the predictor matrix is sparse or
numerically sparse. To overcome the limitation, we develop a novel element-wise
subset selection approach, called core-elements. We provide a deterministic
algorithm to construct the core-elements estimator, only requiring an
$O(\mathrm{nnz}(\mathbf{X})+rp^2)$ computational cost, where
$\mathbf{X}\in\mathbb{R}^{n\times p}$ is the predictor matrix, $r$ is the
number of elements selected from each column of $\mathbf{X}$, and
$\mathrm{nnz}(\cdot)$ denotes the number of non-zero elements. Theoretically,
we show that the proposed estimator is unbiased and approximately minimizes an
upper bound of the estimation variance. We also provide an approximation
guarantee by deriving a coresets-like finite sample bound for the proposed
estimator. To handle potential outliers in the data, we further combine
core-elements with the median-of-means procedure, resulting in an efficient and
robust estimator with theoretical consistency guarantees. Numerical studies on
various synthetic and real-world datasets demonstrate the proposed method's
superior performance compared to mainstream competitors.

### Title: Riemannian data-dependent randomized smoothing for neural networks certification
* Paper ID: 2206.10235v1
* Paper URL: [http://arxiv.org/abs/2206.10235v1](http://arxiv.org/abs/2206.10235v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Certification of neural networks is an important and challenging problem that
has been attracting the attention of the machine learning community since few
years. In this paper, we focus on randomized smoothing (RS) which is considered
as the state-of-the-art method to obtain certifiably robust neural networks. In
particular, a new data-dependent RS technique called ANCER introduced recently
can be used to certify ellipses with orthogonal axis near each input data of
the neural network. In this work, we remark that ANCER is not invariant under
rotation of input data and propose a new rotationally-invariant formulation of
it which can certify ellipses without constraints on their axis. Our approach
called Riemannian Data Dependant Randomized Smoothing (RDDRS) relies on
information geometry techniques on the manifold of covariance matrices and can
certify bigger regions than ANCER based on our experiments on the MNIST
dataset.

### Title: FEAT: Fair Coordinated Iterative Water-Filling Algorithm
* Paper ID: 2206.10211v1
* Paper URL: [http://arxiv.org/abs/2206.10211v1](http://arxiv.org/abs/2206.10211v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In this paper, we consider a perfect coordinated water-filling game, where
each user transmits solely on a given carrier. The main goal of the proposed
algorithm (which we call FEAT) is to get close to the optimal, while keeping a
decent level of fairness. The key idea within FEAT is to minimize the ratio
between the best and the worst utilities of the users. This is done by ensuring
that, at each iteration (channel assignment), a user is satisfied with this
assignment as long as he does not loose much more than other users in the
system. It has been shown that FEAT outperforms most related algorithms in many
aspects, especially in interference-limited systems. Indeed, with FEAT we can
ensure a near-optimal, fair and energy efficient solution with low
computational complexity. In terms of robustness, it turns out that the balance
between being nearly globally optimal and good from individual point of view
seems hard to sustain with a significant number of users. Also notice that, in
this regard, global optimality gets less affected than the individual one,
which offers hope that such an accurate water-filling algorithm can be designed
around competition in interference-limited systems.

### Title: Topological Inference of the Conley Index
* Paper ID: 2206.10198v1
* Paper URL: [http://arxiv.org/abs/2206.10198v1](http://arxiv.org/abs/2206.10198v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: The Conley index of an isolated invariant set is a fundamental object in the
study of dynamical systems. Here we consider smooth functions on closed
submanifolds of Euclidean space and describe a framework for inferring the
Conley index of any compact, connected isolated critical set of such a function
with high confidence from a sufficiently large finite point sample. The main
construction of this paper is a specific index pair which is local to the
critical set in question. We establish that these index pairs have positive
reach and hence admit a sampling theory for robust homology inference. This
allows us to estimate the Conley index, and as a direct consequence, we are
also able to estimate the Morse index of any critical point of a Morse function
using finitely many local evaluations.

### Title: Tyler's and Maronna's M-estimators: Non-Asymptotic Concentration Results
* Paper ID: 2206.10167v1
* Paper URL: [http://arxiv.org/abs/2206.10167v1](http://arxiv.org/abs/2206.10167v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Tyler's and Maronna's M-estimators, as well as their regularized variants,
are popular robust methods to estimate the scatter or covariance matrix of a
multivariate distribution. In this work, we study the non-asymptotic behavior
of these estimators, for data sampled from a distribution that satisfies one of
the following properties: 1) independent sub-Gaussian entries, up to a linear
transformation; 2) log-concave distributions; 3) distributions satisfying a
convex concentration property. Our main contribution is the derivation of tight
non-asymptotic concentration bounds of these M-estimators around a suitably
scaled version of the data sample covariance matrix. Prior to our work,
non-asymptotic bounds were derived only for Elliptical and Gaussian
distributions. Our proof uses a variety of tools from non asymptotic random
matrix theory and high dimensional geometry. Finally, we illustrate the utility
of our results on two examples of practical interest: sparse covariance and
sparse precision matrix estimation.

### Title: Certifiably Robust Policy Learning against Adversarial Communication in Multi-agent Systems
* Paper ID: 2206.10158v1
* Paper URL: [http://arxiv.org/abs/2206.10158v1](http://arxiv.org/abs/2206.10158v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Communication is important in many multi-agent reinforcement learning (MARL)
problems for agents to share information and make good decisions. However, when
deploying trained communicative agents in a real-world application where noise
and potential attackers exist, the safety of communication-based policies
becomes a severe issue that is underexplored. Specifically, if communication
messages are manipulated by malicious attackers, agents relying on
untrustworthy communication may take unsafe actions that lead to catastrophic
consequences. Therefore, it is crucial to ensure that agents will not be misled
by corrupted communication, while still benefiting from benign communication.
In this work, we consider an environment with $N$ agents, where the attacker
may arbitrarily change the communication from any $C<\frac{N-1}{2}$ agents to a
victim agent. For this strong threat model, we propose a certifiable defense by
constructing a message-ensemble policy that aggregates multiple randomly
ablated message sets. Theoretical analysis shows that this message-ensemble
policy can utilize benign communication while being certifiably robust to
adversarial communication, regardless of the attacking algorithm. Experiments
in multiple environments verify that our defense significantly improves the
robustness of trained policies against various types of attacks.

### Title: Propagation with Adaptive Mask then Training for Node Classification on Attributed Networks
* Paper ID: 2206.10142v1
* Paper URL: [http://arxiv.org/abs/2206.10142v1](http://arxiv.org/abs/2206.10142v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Node classification on attributed networks is a semi-supervised task that is
crucial for network analysis. By decoupling two critical operations in Graph
Convolutional Networks (GCNs), namely feature transformation and neighborhood
aggregation, some recent works of decoupled GCNs could support the information
to propagate deeper and achieve advanced performance. However, they follow the
traditional structure-aware propagation strategy of GCNs, making it hard to
capture the attribute correlation of nodes and sensitive to the structure noise
described by edges whose two endpoints belong to different categories. To
address these issues, we propose a new method called the itshape Propagation
with Adaptive Mask then Training (PAMT). The key idea is to integrate the
attribute similarity mask into the structure-aware propagation process. In this
way, PAMT could preserve the attribute correlation of adjacent nodes during the
propagation and effectively reduce the influence of structure noise. Moreover,
we develop an iterative refinement mechanism to update the similarity mask
during the training process for improving the training performance. Extensive
experiments on four real-world datasets demonstrate the superior performance
and robustness of PAMT.

### Title: Mechanism Design Approaches to Blockchain Consensus
* Paper ID: 2206.10065v1
* Paper URL: [http://arxiv.org/abs/2206.10065v1](http://arxiv.org/abs/2206.10065v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Blockchain consensus is a state whereby each node in a network agrees on the
current state of the blockchain. Existing protocols achieve consensus via a
contest or voting procedure to select one node as a dictator to propose new
blocks. However, this procedure can still lead to potential attacks that make
consensus harder to achieve or lead to coordination issues if multiple,
competing chains (i.e., forks) are created with the potential that an
untruthful fork might be selected. We explore the potential for mechanisms to
be used to achieve consensus that are triggered when there is a dispute
impeding consensus. Using the feature that nodes stake tokens in proof of stake
(POS) protocols, we construct revelation mechanisms in which the unique
(subgame perfect) equilibrium involves validating nodes propose truthful blocks
using only the information that exists amongst all nodes. We construct
operationally and computationally simple mechanisms under both Byzantine Fault
Tolerance and a Longest Chain Rule, and discuss their robustness to attacks.
Our perspective is that the use of simple mechanisms is an unexplored area of
blockchain consensus and has the potential to mitigate known trade-offs and
enhance scalability.

### Title: Robust Deep Reinforcement Learning through Bootstrapped Opportunistic Curriculum
* Paper ID: 2206.10057v1
* Paper URL: [http://arxiv.org/abs/2206.10057v1](http://arxiv.org/abs/2206.10057v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Despite considerable advances in deep reinforcement learning, it has been
shown to be highly vulnerable to adversarial perturbations to state
observations. Recent efforts that have attempted to improve adversarial
robustness of reinforcement learning can nevertheless tolerate only very small
perturbations, and remain fragile as perturbation size increases. We propose
Bootstrapped Opportunistic Adversarial Curriculum Learning (BCL), a novel
flexible adversarial curriculum learning framework for robust reinforcement
learning. Our framework combines two ideas: conservatively bootstrapping each
curriculum phase with highest quality solutions obtained from multiple runs of
the previous phase, and opportunistically skipping forward in the curriculum.
In our experiments we show that the proposed BCL framework enables dramatic
improvements in robustness of learned policies to adversarial perturbations.
The greatest improvement is for Pong, where our framework yields robustness to
perturbations of up to 25/255; in contrast, the best existing approach can only
tolerate adversarial noise up to 5/255. Our code is available at:
https://github.com/jlwu002/BCL.

