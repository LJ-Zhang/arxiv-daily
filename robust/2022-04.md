# robust

## 04-17

### Title: Learning Optimal Dynamic Treatment Regimes Using Causal Tree Methods in Medicine
* Paper ID: 2204.07124v1
* Paper URL: [http://arxiv.org/abs/2204.07124v1](http://arxiv.org/abs/2204.07124v1)
* Updated Date: 2022-04-14 17:27:08+00:00
* Code URL: null
* Summary: Dynamic treatment regimes (DTRs) are used in medicine to tailor sequential
treatment decisions to patients by considering patient heterogeneity. Common
methods for learning optimal DTRs, however, have shortcomings: they are
typically based on outcome prediction and not treatment effect estimation, or
they use linear models that are restrictive for patient data from modern
electronic health records. To address these shortcomings, we develop two novel
methods for learning optimal DTRs that effectively handle complex patient data.
We call our methods DTR-CT and DTR-CF. Our methods are based on a data-driven
estimation of heterogeneous treatment effects using causal tree methods,
specifically causal trees and causal forests, that learn non-linear
relationships, control for time-varying confounding, are doubly robust, and
explainable. To the best of our knowledge, our paper is the first that adapts
causal tree methods for learning optimal DTRs. We evaluate our proposed methods
using synthetic data and then apply them to real-world data from intensive care
units. Our methods outperform state-of-the-art baselines in terms of cumulative
regret and percentage of optimal decisions by a considerable margin. Our work
improves treatment recommendations from electronic health record and is thus of
direct relevance for personalized medicine.

### Title: Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems
* Paper ID: 2204.07135v1
* Paper URL: [http://arxiv.org/abs/2204.07135v1](http://arxiv.org/abs/2204.07135v1)
* Updated Date: 2022-04-14 17:46:14+00:00
* Code URL: null
* Summary: Skill routing is an important component in large-scale conversational
systems. In contrast to traditional rule-based skill routing, state-of-the-art
systems use a model-based approach to enable natural conversations. To provide
supervision signal required to train such models, ideas such as human
annotation, replication of a rule-based system, relabeling based on user
paraphrases, and bandit-based learning were suggested. However, these
approaches: (a) do not scale in terms of the number of skills and skill
on-boarding, (b) require a very costly expert annotation/rule-design, (c)
introduce risks in the user experience with each model update. In this paper,
we present a scalable self-learning approach to explore routing alternatives
without causing abrupt policy changes that break the user experience, learn
from the user interaction, and incrementally improve the routing via frequent
model refreshes. To enable such robust frequent model updates, we suggest a
simple and effective approach that ensures controlled policy updates for
individual domains, followed by an off-policy evaluation for making deployment
decisions without any need for lengthy A/B experimentation. We conduct various
offline and online A/B experiments on a commercial large-scale conversational
system to demonstrate the effectiveness of the proposed method in real-world
production settings.

### Title: What's in your hands? 3D Reconstruction of Generic Objects in Hands
* Paper ID: 2204.07153v1
* Paper URL: [http://arxiv.org/abs/2204.07153v1](http://arxiv.org/abs/2204.07153v1)
* Updated Date: 2022-04-14 17:59:02+00:00
* Code URL: null
* Summary: Our work aims to reconstruct hand-held objects given a single RGB image. In
contrast to prior works that typically assume known 3D templates and reduce the
problem to 3D pose estimation, our work reconstructs generic hand-held object
without knowing their 3D templates. Our key insight is that hand articulation
is highly predictive of the object shape, and we propose an approach that
conditionally reconstructs the object based on the articulation and the visual
input. Given an image depicting a hand-held object, we first use off-the-shelf
systems to estimate the underlying hand pose and then infer the object shape in
a normalized hand-centric coordinate frame. We parameterized the object by
signed distance which are inferred by an implicit network which leverages the
information from both visual feature and articulation-aware coordinates to
process a query point. We perform experiments across three datasets and show
that our method consistently outperforms baselines and is able to reconstruct a
diverse set of objects. We analyze the benefits and robustness of explicit
articulation conditioning and also show that this allows the hand pose
estimation to further improve in test-time optimization.

