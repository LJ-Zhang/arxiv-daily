### Title: Real-Time Neural Character Rendering with Pose-Guided Multiplane Images
* Paper ID: 2204.11820v1
* Paper URL: [http://arxiv.org/abs/2204.11820v1](http://arxiv.org/abs/2204.11820v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: We propose pose-guided multiplane image (MPI) synthesis which can render an
animatable character in real scenes with photorealistic quality. We use a
portable camera rig to capture the multi-view images along with the driving
signal for the moving subject. Our method generalizes the image-to-image
translation paradigm, which translates the human pose to a 3D scene
representation -- MPIs that can be rendered in free viewpoints, using the
multi-views captures as supervision. To fully cultivate the potential of MPI,
we propose depth-adaptive MPI which can be learned using variable exposure
images while being robust to inaccurate camera registration. Our method
demonstrates advantageous novel-view synthesis quality over the
state-of-the-art approaches for characters with challenging motions. Moreover,
the proposed method is generalizable to novel combinations of training poses
and can be explicitly controlled. Our method achieves such expressive and
animatable character rendering all in real time, serving as a promising
solution for practical applications.

### Title: Generalizable Neural Performer: Learning Robust Radiance Fields for Human Novel View Synthesis
* Paper ID: 2204.11798v1
* Paper URL: [http://arxiv.org/abs/2204.11798v1](http://arxiv.org/abs/2204.11798v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/generalizable-neural-performer/gnr](https://github.com/generalizable-neural-performer/gnr)
* Summary: This work targets at using a general deep learning framework to synthesize
free-viewpoint images of arbitrary human performers, only requiring a sparse
number of camera views as inputs and skirting per-case fine-tuning. The large
variation of geometry and appearance, caused by articulated body poses, shapes
and clothing types, are the key bottlenecks of this task. To overcome these
challenges, we present a simple yet powerful framework, named Generalizable
Neural Performer (GNR), that learns a generalizable and robust neural body
representation over various geometry and appearance. Specifically, we compress
the light fields for novel view human rendering as conditional implicit neural
radiance fields from both geometry and appearance aspects. We first introduce
an Implicit Geometric Body Embedding strategy to enhance the robustness based
on both parametric 3D human body model and multi-view images hints. We further
propose a Screen-Space Occlusion-Aware Appearance Blending technique to
preserve the high-quality appearance, through interpolating source view
appearance to the radiance fields with a relax but approximate geometric
guidance.
  To evaluate our method, we present our ongoing effort of constructing a
dataset with remarkable complexity and diversity. The dataset GeneBody-1.0,
includes over 360M frames of 370 subjects under multi-view cameras capturing,
performing a large variety of pose actions, along with diverse body shapes,
clothing, accessories and hairdos. Experiments on GeneBody-1.0 and ZJU-Mocap
show better robustness of our methods than recent state-of-the-art
generalizable methods among all cross-dataset, unseen subjects and unseen poses
settings. We also demonstrate the competitiveness of our model compared with
cutting-edge case-specific ones. Dataset, code and model will be made publicly
available.

### Title: Casimir preserving spectrum of two-dimensional turbulence
* Paper ID: 2204.11794v1
* Paper URL: [http://arxiv.org/abs/2204.11794v1](http://arxiv.org/abs/2204.11794v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: We present predictions of the energy spectrum of forced two-dimensional
turbulence obtained by employing a structure-preserving integrator. In
particular, we construct a finite-mode approximation of the Navier-Stokes
equations on the unit sphere, which, in the limit of vanishing viscosity,
preserves the Lie-Poisson structure. As a result, integrated powers of
vorticity are conserved in the inviscid limit. We obtain robust evidence for
the existence of the double energy cascade, including the formation of the -3
scaling of the inertial range of the direct cascade. We show that this can be
achieved at modest resolutions compared to those required by traditional
numerical methods.

### Title: Can Rationalization Improve Robustness?
* Paper ID: 2204.11790v1
* Paper URL: [http://arxiv.org/abs/2204.11790v1](http://arxiv.org/abs/2204.11790v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/princeton-nlp/rationale-robustness](https://github.com/princeton-nlp/rationale-robustness)
* Summary: A growing line of work has investigated the development of neural NLP models
that can produce rationales--subsets of input that can explain their model
predictions. In this paper, we ask whether such rationale models can also
provide robustness to adversarial attacks in addition to their interpretable
nature. Since these models need to first generate rationales ("rationalizer")
before making predictions ("predictor"), they have the potential to ignore
noise or adversarially added text by simply masking it out of the generated
rationale. To this end, we systematically generate various types of 'AddText'
attacks for both token and sentence-level rationalization tasks, and perform an
extensive empirical evaluation of state-of-the-art rationale models across five
different tasks. Our experiments reveal that the rationale models show the
promise to improve robustness, while they struggle in certain scenarios--when
the rationalizer is sensitive to positional bias or lexical choices of attack
text. Further, leveraging human rationale as supervision does not always
translate to better performance. Our study is a first step towards exploring
the interplay between interpretability and robustness in the
rationalize-then-predict framework.

### Title: Optimal Discrete Decisions when Payoffs are Partially Identified
* Paper ID: 2204.11748v1
* Paper URL: [http://arxiv.org/abs/2204.11748v1](http://arxiv.org/abs/2204.11748v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: We derive optimal statistical decision rules for discrete choice problems
when the decision maker is unable to discriminate among a set of payoff
distributions. In this problem, the decision maker must confront both model
uncertainty (about the identity of the true payoff distribution) and
statistical uncertainty (the set of payoff distributions must be estimated). We
derive "efficient-robust decision rules" which minimize maximum risk or regret
over the set of payoff distributions and which use the data to learn
efficiently about features of the set of payoff distributions germane to the
choice problem. We discuss implementation of these decision rules via the
bootstrap and Bayesian methods, for both parametric and semiparametric models.
Using a limits of experiments framework, we show that efficient-robust decision
rules are optimal and can dominate seemingly natural alternatives. We present
applications to treatment assignment using observational data and optimal
pricing in environments with rich unobserved heterogeneity.

### Title: Compression-Complexity with Ordinal Patterns for Robust Causal Inference in Irregularly-Sampled Time Series
* Paper ID: 2204.11731v1
* Paper URL: [http://arxiv.org/abs/2204.11731v1](http://arxiv.org/abs/2204.11731v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Distinguishing cause from effect is a scientific challenge resisting
solutions from mathematics, statistics, information theory and computer
science. Compression-Complexity Causality (CCC) is a recently proposed
interventional measure of causality, inspired by Wiener-Granger's idea. It
estimates causality based on change in dynamical compression-complexity (or
compressibility) of the effect variable, given the cause variable. CCC works
with minimal assumptions on given data and is robust to irregular-sampling,
missing-data and finite-length effects. However, it only works for
one-dimensional time series. We propose an ordinal pattern symbolization scheme
to encode multidimensional patterns into one-dimensional symbolic sequences,
and thus introduce the Permutation CCC (PCCC), which retains all advantages of
the original CCC and can be applied to data from multidimensional systems with
potentially hidden variables. PCCC is tested on numerical simulations and
applied to paleoclimate data characterized by irregular and uncertain sampling
and limited numbers of samples.

### Title: Five key exoplanet questions answered via the analysis of 25 hot Jupiter atmospheres in eclipse
* Paper ID: 2204.11729v1
* Paper URL: [http://arxiv.org/abs/2204.11729v1](http://arxiv.org/abs/2204.11729v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Population studies of exoplanets are key to unlocking their statistical
properties. So far the inferred properties have been mostly limited to
planetary, orbital and stellar parameters extracted from, e.g., Kepler, radial
velocity, and GAIA data. More recently an increasing number of exoplanet
atmospheres have been observed in detail from space and the ground. Generally,
however, these atmospheric studies have focused on individual planets, with the
exception of a couple of works which have detected the presence of water vapor
and clouds in populations of gaseous planets via transmission spectroscopy.
Here, using a suite of retrieval tools, we analyse spectroscopic and
photometric data of 25 hot Jupiters, obtained with the Hubble and Spitzer Space
Telescopes via the eclipse technique. By applying the tools uniformly across
the entire set of 25 planets, we extract robust trends in the thermal structure
and chemical properties of hot Jupiters not obtained in past studies. With the
recent launch of JWST and the upcoming missions Twinkle, and Ariel, population
based studies of exoplanet atmospheres, such as the one presented here, will be
a key approach to understanding planet characteristics, formation, and
evolution in our galaxy.

### Title: Tac2Pose: Tactile Object Pose Estimation from the First Touch
* Paper ID: 2204.11701v1
* Paper URL: [http://arxiv.org/abs/2204.11701v1](http://arxiv.org/abs/2204.11701v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: In this paper, we present Tac2Pose, an object-specific approach to tactile
pose estimation from the first touch for known objects. Given the object
geometry, we learn a tailored perception model in simulation that estimates a
probability distribution over possible object poses given a tactile
observation. To do so, we simulate the contact shapes that a dense set of
object poses would produce on the sensor. Then, given a new contact shape
obtained from the sensor, we match it against the pre-computed set using an
object-specific embedding learned using contrastive learning. We obtain contact
shapes from the sensor with an object-agnostic calibration step that maps RGB
tactile observations to binary contact shapes. This mapping, which can be
reused across object and sensor instances, is the only step trained with real
sensor data. This results in a perception model that localizes objects from the
first real tactile observation. Importantly, it produces pose distributions and
can incorporate additional pose constraints coming from other perception
systems, contacts, or priors.
  We provide quantitative results for 20 objects. Tac2Pose provides high
accuracy pose estimations from distinctive tactile observations while
regressing meaningful pose distributions to account for those contact shapes
that could result from different object poses. We also test Tac2Pose on object
models reconstructed from a 3D scanner, to evaluate the robustness to
uncertainty in the object model. Finally, we demonstrate the advantages of
Tac2Pose compared with three baseline methods for tactile pose estimation:
directly regressing the object pose with a neural network, matching an observed
contact to a set of possible contacts using a standard classification neural
network, and direct pixel comparison of an observed contact with a set of
possible contacts.
  Website: http://mcube.mit.edu/research/tac2pose.html

### Title: Data-driven prediction and control of extreme events in a chaotic flow
* Paper ID: 2204.11682v1
* Paper URL: [http://arxiv.org/abs/2204.11682v1](http://arxiv.org/abs/2204.11682v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: An extreme event is a sudden and violent change in the state of a nonlinear
system. In fluid dynamics, extreme events can have adverse effects on the
system's optimal design and operability, which calls for accurate methods for
their prediction and control. In this paper, we propose a data-driven
methodology for the prediction and control of extreme events in a chaotic shear
flow. The approach is based on echo state networks, which are a type of
reservoir computing that learn temporal correlations within a time-dependent
dataset. The objective is five-fold. First, we exploit ad-hoc metrics from
binary classification to analyse (i) how many of the extreme events predicted
by the network actually occur in the test set (precision), and (ii) how many
extreme events are missed by the network (recall). We apply a principled
strategy for optimal hyperparameter selection, which is key to the networks'
performance. Second, we focus on the time-accurate prediction of extreme
events. We show that echo state networks are able to predict extreme events
well beyond the predictability time, i.e., up to more than five Lyapunov times.
Third, we focus on the long-term prediction of extreme events from a
statistical point of view. By training the networks with datasets that contain
non-converged statistics, we show that the networks are able to learn and
extrapolate the flow's long-term statistics. In other words, the networks are
able to extrapolate in time from relatively short time series. Fourth, we
design a simple and effective control strategy to prevent extreme events from
occurring. The control strategy decreases the occurrence of extreme events up
to one order of magnitude with respect to the uncontrolled system. Finally, we
analyse the robustness of the results for a range of Reynolds numbers. We show
that the networks perform well across a wide range of regimes.

### Title: Universal compilation for quantum state preparation and tomography
* Paper ID: 2204.11635v1
* Paper URL: [http://arxiv.org/abs/2204.11635v1](http://arxiv.org/abs/2204.11635v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/vutuanhai237/UC-VQA](https://github.com/vutuanhai237/UC-VQA)
* Summary: Universal compilation is a training process that compiles a trainable unitary
into a target unitary and it serves vast potential applications from quantum
dynamic simulations to optimal circuits with deep-compressing, device
benchmarking, quantum error mitigation, and so on. Here, we propose a universal
compilation-based variational algorithm for the preparation and tomography of
quantum states in low-depth quantum circuits. We apply the Fubini-Study
distance to be a trainable cost function under various gradient-based
optimizers, including the quantum natural gradient approach. We evaluate the
performance of various unitary topologies and the trainability of different
optimizers for getting high efficiency. In practice, we address different
circuit ansatzes in quantum state preparation, including the linear and
graph-based ansatzes for preparing different entanglement target states such as
representative GHZ and W states. We also discuss the effect of the circuit
depth, barren plateau, readout noise in the model, and the error mitigation
solution. We next evaluate the reconstructing efficiency in quantum state
tomography via various popular circuit ansatzes and reveal the crucial role of
the circuit depth in the robust fidelity. The results are comparable with the
shadow tomography method, a similar fashion in the field. Our work expresses
the adequate capacity of the universal compilation-based variational algorithm
to maximize the efficiency in the quantum state preparation and tomography.
Further, it promises applications in quantum metrology and sensing and is
applicable in the near-term quantum computers for verification of the circuits
fidelity and various quantum computing tasks.

### Title: MLO: Multi-Object Tracking and Lidar Odometry in Dynamic Envirnoment
* Paper ID: 2204.11621v1
* Paper URL: [http://arxiv.org/abs/2204.11621v1](http://arxiv.org/abs/2204.11621v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: The SLAM system built on the static scene assumption will introduce
significant estimation errors when a large number of moving objects appear in
the field of view. Tracking and maintaining semantic objects is beneficial to
understand the scene and provide rich decision information for planning and
control modules. This paper introduces MLO, a multi-object Lidar odometry which
tracks ego-motion and movable objects with only the lidar sensor. First, it
achieves information extraction of foreground movable objects, surface road,
and static background features based on geometry and object fusion perception
module. While robustly estimating ego-motion, it accomplishes multi-object
tracking through the least-squares method fused by 3D bounding boxes and
geometric point clouds. Then, a continuous 4D semantic object map on the
timeline can be created. Our approach is evaluated qualitatively and
quantitatively under different scenarios on the public KITTI dataset. The
experiment results show that the ego localization accuracy of MLO is better
than A-LOAM system in highly dynamic, unstructured, and unknown semantic
scenes. Meanwhile, the multi-object tracking method with semantic-geometry
fusion also has apparent advantages in accuracy and tracking robustness
compared with the single method.

### Title: A Simple Structure For Building A Robust Model
* Paper ID: 2204.11596v1
* Paper URL: [http://arxiv.org/abs/2204.11596v1](http://arxiv.org/abs/2204.11596v1)
* Updated Date: 2022-04-25
* Code URL: [https://github.com/dowdyboy/simple_structure_for_robust_model](https://github.com/dowdyboy/simple_structure_for_robust_model)
* Summary: As deep learning applications, especially programs of computer vision, are
increasingly deployed in our lives, we have to think more urgently about the
security of these applications.One effective way to improve the security of
deep learning models is to perform adversarial training, which allows the model
to be compatible with samples that are deliberately created for use in
attacking the model.Based on this, we propose a simple architecture to build a
model with a certain degree of robustness, which improves the robustness of the
trained network by adding an adversarial sample detection network for
cooperative training.At the same time, we design a new data sampling strategy
that incorporates multiple existing attacks, allowing the model to adapt to
many different adversarial attacks with a single training.We conducted some
experiments to test the effectiveness of this design based on Cifar10 dataset,
and the results indicate that it has some degree of positive effect on the
robustness of the model.Our code could be found at
https://github.com/dowdyboy/simple_structure_for_robust_model.

### Title: Maximum Mean Discrepancy Distributionally Robust Nonlinear Chance-Constrained Optimization with Finite-Sample Guarantee
* Paper ID: 2204.11564v1
* Paper URL: [http://arxiv.org/abs/2204.11564v1](http://arxiv.org/abs/2204.11564v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: This paper is motivated by addressing open questions in distributionally
robust chance-constrained programs (DRCCP) using the popular Wasserstein
ambiguity sets. Specifically, the computational techniques for those programs
typically place restrictive assumptions on the constraint functions and the
size of the Wasserstein ambiguity sets is often set using costly
cross-validation (CV) procedures or conservative measure concentration bounds.
In contrast, we propose a practical DRCCP algorithm using kernel maximum mean
discrepancy (MMD) ambiguity sets, which we term MMD-DRCCP, to treat general
nonlinear constraints without using ad-hoc reformulation techniques. MMD-DRCCP
can handle general nonlinear and non-convex constraints with a proven
finite-sample constraint satisfaction guarantee of a dimension-independent
$\mathcal{O}(\frac{1}{\sqrt{N}})$ rate, achievable by a practical algorithm. We
further propose an efficient bootstrap scheme for constructing sharp MMD
ambiguity sets in practice without resorting to CV. Our algorithm is validated
numerically on a portfolio optimization problem and a tube-based
distributionally robust model predictive control problem with non-convex
constraints.

### Title: Robust inference for non-destructive one-shot device testing under step-stress model with exponential lifetimes
* Paper ID: 2204.11560v1
* Paper URL: [http://arxiv.org/abs/2204.11560v1](http://arxiv.org/abs/2204.11560v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: One-shot devices analysis involves an extreme case of interval censoring,
wherein one can only know whether the failure time is either before or after
the test time. Some kind of one-shot devices do not get destroyed when tested,
and so can continue within the experiment, providing extra information for
inference, if they did not fail before an inspection time. In addition, their
reliability can be rapidly estimated via accelerated life tests (ALTs) by
running the tests at varying and higher stress levels than working conditions.
In particular, step-stress tests allow the experimenter to increase the stress
levels at pre-fixed times gradually during the life-testing experiment. The
cumulative exposure model is commonly assumed for step-stress models, relating
the lifetime distribution of units at one stress level to the lifetime
distributions at preceding stress levels. In this paper,vwe develop robust
estimators and Z-type test statistics based on the density power divergence
(DPD) for testing linear null hypothesis for non-destructive one-shot devices
under the step-stress ALTs with exponential lifetime distribution. We study
asymptotic and robustness properties of the estimators and test statistics,
yielding point estimation and confidence intervals for different lifetime
characteristic such as reliability, distribution quantiles and mean lifetime of
the devices. A simulation study is carried out to assess the performance of the
methods of inference developed here and some real-life data sets are analyzed
finally for illustrative purpose.

### Title: Experimental demonstration of remotely creating Wigner negativity via quantum steering
* Paper ID: 2204.11552v1
* Paper URL: [http://arxiv.org/abs/2204.11552v1](http://arxiv.org/abs/2204.11552v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Non-Gaussian states with Wigner negativity are of particular interest in
quantum technology due to their potential applications in quantum computing and
quantum metrology. However, how to create such states at a remote location
remains a challenge, which is important for efficiently distributing quantum
resource between distant nodes in a network. Here, we experimentally prepare
optical non-Gaussian state with negative Wigner function at a remote node via
local non-Gaussian operation and shared Gaussian entangled state existing
quantum steering. By performing photon subtraction on one mode, Wigner
negativity is created in the remote target mode. We show that the Wigner
negativity is sensitive to loss on the target mode, but robust to loss on the
mode performing photon subtraction. This experiment confirms the connection
between the remotely created Wigner negativity and quantum steering. As an
application, we present that the generated non-Gaussian state exhibits
metrological power in quantum phase estimation.

### Title: LoL: A Comparative Regularization Loss over Query Reformulation Losses for Pseudo-Relevance Feedback
* Paper ID: 2204.11545v1
* Paper URL: [http://arxiv.org/abs/2204.11545v1](http://arxiv.org/abs/2204.11545v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Pseudo-relevance feedback (PRF) has proven to be an effective query
reformulation technique to improve retrieval accuracy. It aims to alleviate the
mismatch of linguistic expressions between a query and its potential relevant
documents. Existing PRF methods independently treat revised queries originating
from the same query but using different numbers of feedback documents,
resulting in severe query drift. Without comparing the effects of two different
revisions from the same query, a PRF model may incorrectly focus on the
additional irrelevant information increased in the more feedback, and thus
reformulate a query that is less effective than the revision using the less
feedback. Ideally, if a PRF model can distinguish between irrelevant and
relevant information in the feedback, the more feedback documents there are,
the better the revised query will be. To bridge this gap, we propose the
Loss-over-Loss (LoL) framework to compare the reformulation losses between
different revisions of the same query during training. Concretely, we revise an
original query multiple times in parallel using different amounts of feedback
and compute their reformulation losses. Then, we introduce an additional
regularization loss on these reformulation losses to penalize revisions that
use more feedback but gain larger losses. With such comparative regularization,
the PRF model is expected to learn to suppress the extra increased irrelevant
information by comparing the effects of different revised queries. Further, we
present a differentiable query reformulation method to implement this
framework. This method revises queries in the vector space and directly
optimizes the retrieval performance of query vectors, applicable for both
sparse and dense retrieval models. Empirical evaluation demonstrates the
effectiveness and robustness of our method for two typical sparse and dense
retrieval models.

### Title: VITA: A Multi-Source Vicinal Transfer Augmentation Method for Out-of-Distribution Generalization
* Paper ID: 2204.11531v1
* Paper URL: [http://arxiv.org/abs/2204.11531v1](http://arxiv.org/abs/2204.11531v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Invariance to diverse types of image corruption, such as noise, blurring, or
colour shifts, is essential to establish robust models in computer vision. Data
augmentation has been the major approach in improving the robustness against
common corruptions. However, the samples produced by popular augmentation
strategies deviate significantly from the underlying data manifold. As a
result, performance is skewed toward certain types of corruption. To address
this issue, we propose a multi-source vicinal transfer augmentation (VITA)
method for generating diverse on-manifold samples. The proposed VITA consists
of two complementary parts: tangent transfer and integration of multi-source
vicinal samples. The tangent transfer creates initial augmented samples for
improving corruption robustness. The integration employs a generative model to
characterize the underlying manifold built by vicinal samples, facilitating the
generation of on-manifold samples. Our proposed VITA significantly outperforms
the current state-of-the-art augmentation methods, demonstrated in extensive
experiments on corruption benchmarks.

### Title: SELECTOR: Selecting a Representative Benchmark Suite for Reproducible Statistical Comparison
* Paper ID: 2204.11527v1
* Paper URL: [http://arxiv.org/abs/2204.11527v1](http://arxiv.org/abs/2204.11527v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Fair algorithm evaluation is conditioned on the existence of high-quality
benchmark datasets that are non-redundant and are representative of typical
optimization scenarios. In this paper, we evaluate three heuristics for
selecting diverse problem instances which should be involved in the comparison
of optimization algorithms in order to ensure robust statistical algorithm
performance analysis. The first approach employs clustering to identify similar
groups of problem instances and subsequent sampling from each cluster to
construct new benchmarks, while the other two approaches use graph algorithms
for identifying dominating and maximal independent sets of nodes. We
demonstrate the applicability of the proposed heuristics by performing a
statistical performance analysis of five portfolios consisting of three
optimization algorithms on five of the most commonly used optimization
benchmarks. The results indicate that the statistical analyses of the
algorithms' performance, conducted on each benchmark separately, produce
conflicting outcomes, which can be used to give a false indication of the
superiority of one algorithm over another. On the other hand, when the analysis
is conducted on the problem instances selected with the proposed heuristics,
which uniformly cover the problem landscape, the statistical outcomes are
robust and consistent.

### Title: Go Wide or Go Deep: Levering Watermarking Performance with Computational Cost for Specific Images
* Paper ID: 2204.11513v1
* Paper URL: [http://arxiv.org/abs/2204.11513v1](http://arxiv.org/abs/2204.11513v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Digital watermarking has been widely studied for the protection of
intellectual property. Traditional watermarking schemes often design in a
"wider" rule, which applies one general embedding mechanism to all images. But
this will limit the scheme into a robustness-invisibility trade-off, where the
improvements of robustness can only be achieved by the increase of embedding
intensity thus causing the visual quality decay. However, a new scenario comes
out at this stage that many businesses wish to give high level protection to
specific valuable images, which requires high robustness and high visual
quality at the same time. Such scenario makes the watermarking schemes should
be designed in a "deeper" way which makes the embedding mechanism customized to
specific images. To achieve so, we break the robustness-invisibility trade-off
by introducing computation cost in, and propose a novel auto-decoder-like
image-specified watermarking framework (ISMark). Based on ISMark, the strong
robustness and high visual quality for specific images can be both achieved. In
detail, we apply an optimization procedure (OPT) to replace the traditional
embedding mechanism. Unlike existing schemes that embed watermarks using a
learned encoder, OPT regards the cover image as the optimizable parameters to
minimize the extraction error of the decoder, thus the features of each
specified image can be effectively exploited to achieve superior performance.
Extensive experiments indicate that ISMark outperforms the state-of-the-art
methods by a large margin, which improves the average bit error rate by 4.64%
(from 4.86% to 0.22%) and PSNR by 2.20dB (from 32.50dB to 34.70dB).

### Title: Brain-Computer Interfaces: Investigating the Transition from Visually Evoked to Purely Imagined Steady-State Potentials
* Paper ID: 2204.11503v1
* Paper URL: [http://arxiv.org/abs/2204.11503v1](http://arxiv.org/abs/2204.11503v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Brain-Computer Interfaces (BCIs) based on Steady State Visually Evoked
Potentials (SSVEPs) have proven effective and provide significant accuracy and
information-transfer rates. This family of strategies, however, requires
external devices that provide the frequency stimuli required by the technique.
This limits the scenarios in which they can be applied, especially when
compared to other BCI approaches. In this work, we have investigated the
possibility of obtaining frequency responses in the EEG output based on the
pure visual imagination of SSVEP-eliciting stimuli. Our results show that not
only that EEG signals present frequency-specific peaks related to the frequency
the user is focusing on, but also that promising classification accuracy can be
achieved, paving the way for a robust and reliable visual imagery BCI modality.

### Title: End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network
* Paper ID: 2204.11479v1
* Paper URL: [http://arxiv.org/abs/2204.11479v1](http://arxiv.org/abs/2204.11479v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: While efficient architectures and a plethora of augmentations for end-to-end
image classification tasks have been suggested and heavily investigated,
state-of-the-art techniques for audio classifications still rely on numerous
representations of the audio signal together with large architectures,
finetuned from large datasets. By utilizing the inherited lightweight nature of
audio and novel audio augmentations, we were able to present an efficient
end-to-end1 network with strong generalization ability. Experiments on a
variety of sound classification sets demonstrate the effectiveness and
robustness of our approach, by achieving state-of-the-art results in various
settings. Public code will be available.

### Title: Trusted Multi-View Classification with Dynamic Evidential Fusion
* Paper ID: 2204.11423v1
* Paper URL: [http://arxiv.org/abs/2204.11423v1](http://arxiv.org/abs/2204.11423v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Existing multi-view classification algorithms focus on promoting accuracy by
exploiting different views, typically integrating them into common
representations for follow-up tasks. Although effective, it is also crucial to
ensure the reliability of both the multi-view integration and the final
decision, especially for noisy, corrupted and out-of-distribution data.
Dynamically assessing the trustworthiness of each view for different samples
could provide reliable integration. This can be achieved through uncertainty
estimation. With this in mind, we propose a novel multi-view classification
algorithm, termed trusted multi-view classification (TMC), providing a new
paradigm for multi-view learning by dynamically integrating different views at
an evidence level. The proposed TMC can promote classification reliability by
considering evidence from each view. Specifically, we introduce the variational
Dirichlet to characterize the distribution of the class probabilities,
parameterized with evidence from different views and integrated with the
Dempster-Shafer theory. The unified learning framework induces accurate
uncertainty and accordingly endows the model with both reliability and
robustness against possible noise or corruption. Both theoretical and
experimental results validate the effectiveness of the proposed model in
accuracy, robustness and trustworthiness.

### Title: Riemannian Hamiltonian methods for min-max optimization on manifolds
* Paper ID: 2204.11418v1
* Paper URL: [http://arxiv.org/abs/2204.11418v1](http://arxiv.org/abs/2204.11418v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: In this paper, we study the min-max optimization problems on Riemannian
manifolds. We introduce a Riemannian Hamiltonian function, minimization of
which serves as a proxy for solving the original min-max problems. Under the
Riemannian Polyak--{\L}ojasiewicz (PL) condition on the Hamiltonian function,
its minimizer corresponds to the desired min-max saddle point. We also provide
cases where this condition is satisfied. To minimize the Hamiltonian function,
we propose Riemannian Hamiltonian methods (RHM) and present their convergence
analysis. We extend RHM to include a consensus regularization and to the
stochastic setting. We illustrate the efficacy of the proposed RHM in
applications such as subspace robust Wasserstein distance, robust training of
neural networks, and generative adversarial networks.

### Title: Single Object Tracking Research: A Survey
* Paper ID: 2204.11410v1
* Paper URL: [http://arxiv.org/abs/2204.11410v1](http://arxiv.org/abs/2204.11410v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Visual object tracking is an important task in computer vision, which has
many real-world applications, e.g., video surveillance, visual navigation.
Visual object tracking also has many challenges, e.g., object occlusion and
deformation. To solve above problems and track the target accurately and
efficiently, many tracking algorithms have emerged in recent years. This paper
presents the rationale and representative works of two most popular tracking
frameworks in past ten years, i.e., the corelation filter and Siamese network
for object tracking. Then we present some deep learning based tracking methods
categorized by different network structures. We also introduce some classical
strategies for handling the challenges in tracking problem. Further, this paper
detailedly present and compare the benchmarks and challenges for tracking, from
which we summarize the development history and development trend of visual
tracking. Focusing on the future development of object tracking, which we think
would be applied in real-world scenes before some problems to be addressed,
such as the problems in long-term tracking, low-power high-speed tracking and
attack-robust tracking. In the future, the integration of multimodal data,
e.g., the depth image, thermal image with traditional color image, will provide
more solutions for visual tracking. Moreover, tracking task will go together
with some other tasks, e.g., video object detection and segmentation.

### Title: PointInst3D: Segmenting 3D Instances by Points
* Paper ID: 2204.11402v1
* Paper URL: [http://arxiv.org/abs/2204.11402v1](http://arxiv.org/abs/2204.11402v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.

### Title: Offline Retrieval Evaluation Without Evaluation Metrics
* Paper ID: 2204.11400v1
* Paper URL: [http://arxiv.org/abs/2204.11400v1](http://arxiv.org/abs/2204.11400v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Offline evaluation of information retrieval and recommendation has
traditionally focused on distilling the quality of a ranking into a scalar
metric such as average precision or normalized discounted cumulative gain. We
can use this metric to compare the performance of multiple systems for the same
request. Although evaluation metrics provide a convenient summary of system
performance, they also collapse subtle differences across users into a single
number and can carry assumptions about user behavior and utility not supported
across retrieval scenarios. We propose recall-paired preference (RPP), a
metric-free evaluation method based on directly computing a preference between
ranked lists. RPP simulates multiple user subpopulations per query and compares
systems across these pseudo-populations. Our results across multiple search and
recommendation tasks demonstrate that RPP substantially improves discriminative
power while correlating well with existing metrics and being equally robust to
incomplete data.

### Title: A high-fidelity seismic intensity measure to assess dynamic liquefaction in tailings
* Paper ID: 2204.11393v1
* Paper URL: [http://arxiv.org/abs/2204.11393v1](http://arxiv.org/abs/2204.11393v1)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Deformation analyses of tailings dams under dynamic conditions requires using
earthquake records as input loading. Moreover, these records must represent the
local seismicity, expressed by a set of ground motion power indicators
denominated intensity measures (IM). The ability and accuracy to describe the
characteristics of a seismic record play a fundamental role in earthquake
engineering and damage assessment of geotechnical facilities. None of the
existing IMs represents a robust enough predictor of a given seismic demand
(e.g., residual displacements) as different signals may generate a wide
spectrum of results, with diverse effects that could produce from insignificant
damage to global failure depending on the structure. To overcome this
limitation, usual engineering procedures select a huge number of records and
develop a large set of numerical simulations to bound the uncertainty of the
results, which becomes a time-consuming approach. In order to perform more
accurate ground motion {selection}, this paper presents a new high-fidelity
seismic IM, which captures the spectral properties of the record for the
frequency content that the dam does not filter. This IM represents a way to
estimate beforehand a seismic demand, expressed for instance in terms of
displacements. The proposed IM is applied to a finite element model for an
upstream tailings dam cross section, using a constitutive model capable to
capture dynamic liquefaction. The obtained results show that our proposal gives
highly reliable correlations with different selected demands. Comparisons with
classical IMs are also discussed, showing that our proposal emerges as a
practical solution to a large dated discussion within our community.

### Title: Following Closely: A Robust Monocular Person Following System for Mobile Robot
* Paper ID: 2204.10540v2
* Paper URL: [http://arxiv.org/abs/2204.10540v2](http://arxiv.org/abs/2204.10540v2)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Monocular person following (MPF) is a capability that supports many useful
applications of a mobile robot. However, existing MPF solutions are not
completely satisfactory. Firstly, they often fail to track the target at a
close distance either because they are based on a visual servo or they need the
observation of the full body by the robot. Secondly, their target
Re-IDentification (Re-ID) abilities are weak in cases of target appearance
change and highly similar appearance of distracting people. To remove the
assumption of full-body observation, we propose a width-based tracking module,
which relies on the target width, which can be observed even at a close
distance. For handling issues related to appearance variation, we use a global
CNN (convolutional neural network) descriptor to represent the target and a
ridge regression model to learn a target appearance model online. We adopt a
sampling strategy for online classifier learning, in which both long-term and
short-term samples are involved. We evaluate our method in two datasets
including a public person following dataset and a custom-built one with
challenging target appearance and target distance. Our method achieves
state-of-the-art (SOTA) results on both datasets. For the benefit of the
community, we make public the dataset and the source code.

### Title: Working memory inspired hierarchical video decomposition with transformative representations
* Paper ID: 2204.10105v2
* Paper URL: [http://arxiv.org/abs/2204.10105v2](http://arxiv.org/abs/2204.10105v2)
* Updated Date: 2022-04-25
* Code URL: null
* Summary: Video decomposition is very important to extract moving foreground objects
from complex backgrounds in computer vision, machine learning, and medical
imaging, e.g., extracting moving contrast-filled vessels from the complex and
noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges
caused by dynamic backgrounds, overlapping heterogeneous environments and
complex noises still exist in video decomposition. To solve these problems,
this study is the first to introduce a flexible visual working memory model in
video decomposition tasks to provide interpretable and high-performance
hierarchical deep architecture, integrating the transformative representations
between sensory and control layers from the perspective of visual and cognitive
neuroscience. Specifically, robust PCA unrolling networks acting as a
structure-regularized sensor layer decompose XCA into sparse/low-rank
structured representations to separate moving contrast-filled vessels from
noisy and complex backgrounds. Then, patch recurrent convolutional LSTM
networks with a backprojection module embody unstructured random
representations of the control layer in working memory, recurrently projecting
spatiotemporally decomposed nonlocal patches into orthogonal subspaces for
heterogeneous vessel retrieval and interference suppression. This video
decomposition deep architecture effectively restores the heterogeneous profiles
of intensity and the geometries of moving objects against the complex
background interferences. Experiments show that the proposed method
significantly outperforms state-of-the-art methods in accurate moving
contrast-filled vessel extraction with excellent flexibility and computational
efficiency.

