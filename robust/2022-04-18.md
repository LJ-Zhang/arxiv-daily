### Title: Is the Contralateral Delay Activity (CDA) a robust neural correlate for Visual Working Memory (VWM) tasks? A reproducibility study
* Paper ID: 2204.08578v1
* Paper URL: [http://arxiv.org/abs/2204.08578v1](http://arxiv.org/abs/2204.08578v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Visual working memory (VWM) allows us to actively store, update and
manipulate visual information surrounding us. While the underlying neural
mechanisms of VWM remain unclear, contralateral delay activity (CDA), a
sustained negativity over the hemisphere contralateral to the positions of
visual items to be remembered, is often used to study VWM. To investigate if
the CDA is a robust neural correlate for VWM tasks, we reproduced eight
CDA-related studies with a publicly accessible EEG dataset. We used the raw EEG
data from these eight studies and analyzed all of them with the same basic
pipeline to extract CDA. We were able to reproduce the results from all the
studies and show that with a basic automated EEG pipeline we can extract a
clear CDA signal. We share insights from the trends observed across the studies
and raise some questions about the CDA decay and the CDA during the recall
phase, which surprisingly, none of the eight studies did address. Finally, we
also provide reproducibility recommendations based on our experience and
challenges in reproducing these studies.

### Title: Expert-Calibrated Learning for Online Optimization with Switching Costs
* Paper ID: 2204.08572v1
* Paper URL: [http://arxiv.org/abs/2204.08572v1](http://arxiv.org/abs/2204.08572v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We study online convex optimization with switching costs, a practically
important but also extremely challenging problem due to the lack of complete
offline information. By tapping into the power of machine learning (ML) based
optimizers, ML-augmented online algorithms (also referred to as expert
calibration in this paper) have been emerging as state of the art, with
provable worst-case performance guarantees. Nonetheless, by using the standard
practice of training an ML model as a standalone optimizer and plugging it into
an ML-augmented algorithm, the average cost performance can be even worse than
purely using ML predictions. In order to address the "how to learn" challenge,
we propose EC-L2O (expert-calibrated learning to optimize), which trains an
ML-based optimizer by explicitly taking into account the downstream expert
calibrator. To accomplish this, we propose a new differentiable expert
calibrator that generalizes regularized online balanced descent and offers a
provably better competitive ratio than pure ML predictions when the prediction
error is large. For training, our loss function is a weighted sum of two
different losses -- one minimizing the average ML prediction error for better
robustness, and the other one minimizing the post-calibration average cost. We
also provide theoretical analysis for EC-L2O, highlighting that expert
calibration can be even beneficial for the average cost performance and that
the high-percentile tail ratio of the cost achieved by EC-L2O to that of the
offline optimal oracle (i.e., tail cost ratio) can be bounded. Finally, we test
EC-L2O by running simulations for sustainable datacenter demand response. Our
results demonstrate that EC-L2O can empirically achieve a lower average cost as
well as a lower competitive ratio than the existing baseline algorithms.

### Title: UNBUS: Uncertainty-aware Deep Botnet Detection System in Presence of Perturbed Samples
* Paper ID: 2204.09502v1
* Paper URL: [http://arxiv.org/abs/2204.09502v1](http://arxiv.org/abs/2204.09502v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: A rising number of botnet families have been successfully detected using deep
learning architectures. While the variety of attacks increases, these
architectures should become more robust against attacks. They have been proven
to be very sensitive to small but well constructed perturbations in the input.
Botnet detection requires extremely low false-positive rates (FPR), which are
not commonly attainable in contemporary deep learning. Attackers try to
increase the FPRs by making poisoned samples. The majority of recent research
has focused on the use of model loss functions to build adversarial examples
and robust models. In this paper, two LSTM-based classification algorithms for
botnet classification with an accuracy higher than 98\% are presented. Then,
the adversarial attack is proposed, which reduces the accuracy to about30\%.
Then, by examining the methods for computing the uncertainty, the defense
method is proposed to increase the accuracy to about 70\%. By using the deep
ensemble and stochastic weight averaging quantification methods it has been
investigated the uncertainty of the accuracy in the proposed methods.

### Title: A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability
* Paper ID: 2204.08570v1
* Paper URL: [http://arxiv.org/abs/2204.08570v1](http://arxiv.org/abs/2204.08570v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Graph Neural Networks (GNNs) have made rapid developments in the recent
years. Due to their great ability in modeling graph-structured data, GNNs are
vastly used in various applications, including high-stakes scenarios such as
financial analysis, traffic predictions, and drug discovery. Despite their
great potential in benefiting humans in the real world, recent study shows that
GNNs can leak private information, are vulnerable to adversarial attacks, can
inherit and magnify societal bias from training data and lack interpretability,
which have risk of causing unintentional harm to the users and society. For
example, existing works demonstrate that attackers can fool the GNNs to give
the outcome they desire with unnoticeable perturbation on training graph. GNNs
trained on social networks may embed the discrimination in their decision
process, strengthening the undesirable societal bias. Consequently, trustworthy
GNNs in various aspects are emerging to prevent the harm from GNN models and
increase the users' trust in GNNs. In this paper, we give a comprehensive
survey of GNNs in the computational aspects of privacy, robustness, fairness,
and explainability. For each aspect, we give the taxonomy of the related
methods and formulate the general frameworks for the multiple categories of
trustworthy GNNs. We also discuss the future research directions of each aspect
and connections between these aspects to help achieve trustworthiness.

### Title: PIDGeuN: Graph Neural Network-Enabled Transient Dynamics Prediction of Networked Microgrids Through Full-Field Measurement
* Paper ID: 2204.08557v1
* Paper URL: [http://arxiv.org/abs/2204.08557v1](http://arxiv.org/abs/2204.08557v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: A Physics-Informed Dynamic Graph Neural Network (PIDGeuN) is presented to
accurately, efficiently and robustly predict the nonlinear transient dynamics
of microgrids in the presence of disturbances. The graph-based architecture of
PIDGeuN provides a natural representation of the microgrid topology. Using only
the state information that is practically measurable, PIDGeuN employs a time
delay embedding formulation to fully reproduce the system dynamics, avoiding
the dependency of conventional methods on internal dynamic states such as
controllers. Based on a judiciously designed message passing mechanism, the
PIDGeuN incorporates two physics-informed techniques to improve its prediction
performance, including a physics-data-infusion approach to determining the
inter-dependencies between buses, and a loss term to respect the known physical
law of the power system, i.e., the Kirchhoff's law, to ensure the feasibility
of the model prediction. Extensive tests show that PIDGeuN can provide accurate
and robust prediction of transient dynamics for nonlinear microgrids over a
long-term time period. Therefore, the PIDGeuN offers a potent tool for the
modeling of large scale networked microgrids (NMs), with potential applications
to predictive or preventive control in real time applications for the stable
and resilient operations of NMs.

### Title: Topological order and entanglement dynamics in the measurement-only XZZX quantum code
* Paper ID: 2204.08489v1
* Paper URL: [http://arxiv.org/abs/2204.08489v1](http://arxiv.org/abs/2204.08489v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We examine the dynamics of a $(1+1)$-dimensional measurement-only circuit
defined by the stabilizers of the [[5,1,3]] quantum error correcting code
interrupted by single-qubit Pauli measurements. The code corrects arbitrary
single-qubit errors and it stabilizes an area law entangled state with a $D_2 =
\mathbb{Z}_2 \times \mathbb{Z}_2$ symmetry protected topological (SPT) order,
as well as a symmetry breaking (SB) order from a two-fold bulk degeneracy. The
Pauli measurements break the topological order and induce a phase transition
into a trivial area law phase. Allowing more than one type of Pauli measurement
increases the measurement-induced frustration, and the SPT and SB order can be
broken either simultaneously or separately at nonzero measurement rate. This
yields a rich phase diagram and unanticipated critical behavior at the phase
transitions. Although the correlation length exponent $\nu=\tfrac43$ and the
dynamical critical exponent $z=1$ are consistent with bond percolation, the
prefactor of the logarithmic entanglement growth may take non-integer multiples
of the percolation value. Remarkably, we identify a robust transient scaling
regime for the purification dynamics of $L$ qubits. It reveals a modified
dynamical critical exponent $z^*\neq z$, which is observable up to times $t\sim
L^{z^*}$ and is reminiscent of the relaxation of critical systems into a
prethermal state.

### Title: Observed Extra Mixing Trends in Red Giants are Reproduced by the Reduced Density Ratio in Thermohaline Zones
* Paper ID: 2204.08487v1
* Paper URL: [http://arxiv.org/abs/2204.08487v1](http://arxiv.org/abs/2204.08487v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Observations show an almost ubiquitous presence of extra mixing in low-mass
upper giant branch stars. The most commonly invoked explanation for this is the
thermohaline instability. One dimensional stellar evolution models include
prescriptions for thermohaline mixing, but our ability to make direct
comparisons between models and observations has thus far been limited. Here, we
propose a new framework to facilitate direct comparison: Using carbon to
nitrogen measurements from the SDSS-IV APOGEE survey as a probe of mixing and a
fluid parameter known as the reduced density ratio from one dimensional stellar
evolution programs, we compare the observed amount of extra mixing on the upper
giant branch to predicted trends from three-dimensional fluid dynamics
simulations. By applying this method, we are able to place empirical
constraints on the efficiency of mixing across a range of masses and
metallicities. We find that the observed amount of extra mixing is strongly
correlated with the reduced density ratio and that trends between reduced
density ratio and fundamental stellar parameters are robust across choices for
modeling prescription. We show that stars with available mixing data tend to
have relatively low density ratios, which should inform the regimes selected
for future simulation efforts. Finally, we show that there is increased mixing
at low values of the reduced density ratio, which is consistent with current
hydrodynamical models of the thermohaline instability. The introduction of this
framework sets a new standard for theoretical modeling efforts, as validation
for not only the amount of extra mixing, but trends between the degree of extra
mixing and fundamental stellar parameters is now possible.

### Title: Efficient reverse engineering of one-qubit filter functions with dynamical invariants
* Paper ID: 2204.08457v1
* Paper URL: [http://arxiv.org/abs/2204.08457v1](http://arxiv.org/abs/2204.08457v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We derive an integral expression for the filter-transfer function of an
arbitrary one-qubit gate through the use of dynamical invariant theory and
Hamiltonian reverse engineering. We use this result to define a cost functional
which can be efficiently optimized to produce one-qubit control pulses that are
robust against specified frequency bands of the noise power spectral density.
We demonstrate the utility of our result by generating optimal control pulses
that are designed to suppress broadband detuning and pulse amplitude noise. We
report an order of magnitude improvement in gate fidelity in comparison with
known composite pulse sequences. More broadly, we also use the same theoretical
framework to prove the robustness of nonadiabatic geometric quantum gates under
specific error models and control constraints.

### Title: Robust, Nonparametric, Efficient Decomposition of Spectral Peaks under Distortion and Interference
* Paper ID: 2204.08411v1
* Paper URL: [http://arxiv.org/abs/2204.08411v1](http://arxiv.org/abs/2204.08411v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We propose a decomposition method for the spectral peaks in an observed
frequency spectrum, which is efficiently acquired by utilizing the Fast Fourier
Transform. In contrast to the traditional methods of waveform fitting on the
spectrum, we optimize the problem from a more robust perspective. We model the
peaks in spectrum as pseudo-symmetric functions, where the only constraint is a
nonincreasing behavior around a central frequency when the distance increases.
Our approach is more robust against arbitrary distortion, interference and
noise on the spectrum that may be caused by an observation system. The time
complexity of our method is linear, i.e., $O(N)$ per extracted spectral peak.
Moreover, the decomposed spectral peaks show a pseudo-orthogonal behavior,
where they conform to a power preserving equality.

### Title: Non-Markovian random walks characterize network robustness to nonlocal cascades
* Paper ID: 2204.08407v1
* Paper URL: [http://arxiv.org/abs/2204.08407v1](http://arxiv.org/abs/2204.08407v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Localized perturbations in a real-world network have the potential to trigger
cascade failures at the whole system level, hindering its operations and
functions. Standard approaches analytically tackling this problem are mostly
based either on static descriptions, such as percolation, or on models where
the failure evolves through first-neighbor connections, crucially failing to
capture the nonlocal behavior typical of real cascades. We introduce a
dynamical model that maps the failure propagation across the network to a
self-avoiding random walk that, at each step, has a probability to perform
nonlocal jumps toward operational systems' units. Despite the inherent
non-Markovian nature of the process, we are able to characterize the critical
behavior of the system out of equilibrium, as well as the stopping time
distribution of the cascades. Our numerical experiments on synthetic and
empirical biological and transportation networks are in excellent agreement
with theoretical expectation, demonstrating the ability of our framework to
quantify the vulnerability to nonlocal cascade failures of complex systems with
interconnected structure.

### Title: Rank Based Tests for High Dimensional White Noise
* Paper ID: 2204.08402v1
* Paper URL: [http://arxiv.org/abs/2204.08402v1](http://arxiv.org/abs/2204.08402v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The development of high-dimensional white noise test is important in both
statistical theories and applications, where the dimension of the time series
can be comparable to or exceed the length of the time series. This paper
proposes several distribution-free tests using the rank based statistics for
testing the high-dimensional white noise, which are robust to the heavy tails
and do not quire the finite-order moment assumptions for the sample
distributions. Three families of rank based tests are analyzed in this paper,
including the simple linear rank statistics, non-degenerate U-statistics and
degenerate U-statistics. The asymptotic null distributions and rate optimality
are established for each family of these tests. Among these tests, the test
based on degenerate U-statistics can also detect the non-linear and
non-monotone relationships in the autocorrelations. Moreover, this is the first
result on the asymptotic distributions of rank correlation statistics which
allowing for the cross-sectional dependence in high dimensional data.

### Title: Detecting Deepfakes with Self-Blended Images
* Paper ID: 2204.08376v1
* Paper URL: [http://arxiv.org/abs/2204.08376v1](http://arxiv.org/abs/2204.08376v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: In this paper, we present novel synthetic training data called self-blended
images (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source
and target images from single pristine images, reproducing common forgery
artifacts (e.g., blending boundaries and statistical inconsistencies between
source and target images). The key idea behind SBIs is that more general and
hardly recognizable fake samples encourage classifiers to learn generic and
robust representations without overfitting to manipulation-specific artifacts.
We compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC,
DFDCP, and FFIW datasets by following the standard cross-dataset and
cross-manipulation protocols. Extensive experiments show that our method
improves the model generalization to unknown manipulations and scenes. In
particular, on DFDC and DFDCP where existing methods suffer from the domain gap
between the training and test sets, our approach outperforms the baseline by
4.90% and 11.78% points in the cross-dataset evaluation, respectively.

### Title: Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads
* Paper ID: 2204.08364v1
* Paper URL: [http://arxiv.org/abs/2204.08364v1](http://arxiv.org/abs/2204.08364v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: In many Asian countries with unconstrained road traffic conditions, driving
violations such as not wearing helmets and triple-riding are a significant
source of fatalities involving motorcycles. Identifying and penalizing such
riders is vital in curbing road accidents and improving citizens' safety. With
this motivation, we propose an approach for detecting, tracking, and counting
motorcycle riding violations in videos taken from a vehicle-mounted dashboard
camera. We employ a curriculum learning-based object detector to better tackle
challenging scenarios such as occlusions. We introduce a novel trapezium-shaped
object boundary representation to increase robustness and tackle the
rider-motorcycle association. We also introduce an amodal regressor that
generates bounding boxes for the occluded riders. Experimental results on a
large-scale unconstrained driving dataset demonstrate the superiority of our
approach compared to existing approaches and other ablative variants.

### Title: Predictive analytics for appointment bookings
* Paper ID: 2204.08475v1
* Paper URL: [http://arxiv.org/abs/2204.08475v1](http://arxiv.org/abs/2204.08475v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: One of the service providers in the financial service sector, who provide
premium service to the customers, wanted to harness the power of data analytics
as data mining can uncover valuable insights for better decision making.
Therefore, the author aimed to use predictive analytics to discover crucial
factors that will affect the customers' showing up for their appointment and
booking the service. The first model predicts whether a customer will show up
for the meeting, while the second model indicates whether a customer will book
a premium service. Both models produce accurate results with more than a 75%
accuracy rate, thus providing a more robust model for implementation than gut
feeling and intuition. Finally, this paper offers a framework for resource
planning using the predicted demand.

### Title: Tracking monocular camera pose and deformation for SLAM inside the human body
* Paper ID: 2204.08309v1
* Paper URL: [http://arxiv.org/abs/2204.08309v1](http://arxiv.org/abs/2204.08309v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Monocular SLAM in deformable scenes will open the way to multiple medical
applications like computer-assisted navigation in endoscopy, automatic drug
delivery or autonomous robotic surgery. In this paper we propose a novel method
to simultaneously track the camera pose and the 3D scene deformation, without
any assumption about environment topology or shape. The method uses an
illumination-invariant photometric method to track image features and estimates
camera motion and deformation combining reprojection error with spatial and
temporal regularization of deformations. Our results in simulated colonoscopies
show the method's accuracy and robustness in complex scenes under increasing
levels of deformation. Our qualitative results in human colonoscopies from
Endomapper dataset show that the method is able to successfully cope with the
challenges of real endoscopies: deformations, low texture and strong
illumination changes. We also compare with previous tracking methods in simpler
scenarios from Hamlyn dataset where we obtain competitive performance, without
needing any topological assumption.

### Title: StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts
* Paper ID: 2204.08292v1
* Paper URL: [http://arxiv.org/abs/2204.08292v1](http://arxiv.org/abs/2204.08292v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.

### Title: Predictive Accuracy of a Hybrid Generalized Long Memory Model for Short Term Electricity Price Forecasting
* Paper ID: 2204.09568v1
* Paper URL: [http://arxiv.org/abs/2204.09568v1](http://arxiv.org/abs/2204.09568v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Accurate electricity price forecasting is the main management goal for market
participants since it represents the fundamental basis to maximize the profits
for market players. However, electricity is a non-storable commodity and the
electricity prices are affected by some social and natural factors that make
the price forecasting a challenging task. This study investigates the
predictive performance of a new hybrid model based on the Generalized long
memory autoregressive model (k-factor GARMA), the Gegenbauer Generalized
Autoregressive Conditional Heteroscedasticity(G-GARCH) process, Wavelet
decomposition, and Local Linear Wavelet Neural Network (LLWNN) optimized using
two different learning algorithms; the Backpropagation algorithm (BP) and the
Particle Swarm optimization algorithm (PSO). The performance of the proposed
model is evaluated using data from Nord Pool Electricity markets. Moreover, it
is compared with some other parametric and non-parametric models in order to
prove its robustness. The empirical results prove that the proposed method
performs well than other competing techniques.

### Title: Modx: Binary Level Partial Imported Third-Party Library Detection through Program Modularization and Semantic Matching
* Paper ID: 2204.08237v1
* Paper URL: [http://arxiv.org/abs/2204.08237v1](http://arxiv.org/abs/2204.08237v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: With the rapid growth of software, using third-party libraries (TPLs) has
become increasingly popular. The prosperity of the library usage has provided
the software engineers with handful of methods to facilitate and boost the
program development. Unfortunately, it also poses great challenges as it
becomes much more difficult to manage the large volume of libraries. Researches
and studies have been proposed to detect and understand the TPLs in the
software. However, most existing approaches rely on syntactic features, which
are not robust when these features are changed or deliberately hidden by the
adversarial parties. Moreover, these approaches typically model each of the
imported libraries as a whole, therefore, cannot be applied to scenarios where
the host software only partially uses the library code segments.
  To detect both fully and partially imported TPLs at the semantic level, we
propose ModX, a framework that leverages novel program modularization
techniques to decompose the program into finegrained functionality-based
modules. By extracting both syntactic and semantic features, it measures the
distance between modules to detect similar library module reuse in the program.
Experimental results show that ModX outperforms other modularization tools by
distinguishing more coherent program modules with 353% higher module quality
scores and beats other TPL detection tools with on average 17% better in
precision and 8% better in recall.

### Title: The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training
* Paper ID: 2204.08227v1
* Paper URL: [http://arxiv.org/abs/2204.08227v1](http://arxiv.org/abs/2204.08227v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The self-supervised Masked Image Modeling (MIM) schema, following
"mask-and-reconstruct" pipeline of recovering contents from masked image, has
recently captured the increasing interest in the multimedia community, owing to
the excellent ability of learning visual representation from unlabeled data.
Aiming at learning representations with high semantics abstracted, a group of
works attempts to reconstruct non-semantic pixels with large-ratio masking
strategy, which may suffer from "over-smoothing" problem, while others directly
infuse semantics into targets in off-line way requiring extra data. Different
from them, we shift the perspective to the Fourier domain which naturally has
global perspective and present a new Masked Image Modeling (MIM), termed
Geminated Gestalt Autoencoder (Ge$^2$-AE) for visual pre-training.
Specifically, we equip our model with geminated decoders in charge of
reconstructing image contents from both pixel and frequency space, where each
other serves as not only the complementation but also the reciprocal
constraints. Through this way, more robust representations can be learned in
the pre-trained encoders, of which the effectiveness is confirmed by the
juxtaposing experimental results on downstream recognition tasks. We also
conduct several quantitative and qualitative experiments to investigate the
learning behavior of our method. To our best knowledge, this is the first MIM
work to solve the visual pre-training through the lens of frequency domain.

### Title: Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge
* Paper ID: 2204.08189v1
* Paper URL: [http://arxiv.org/abs/2204.08189v1](http://arxiv.org/abs/2204.08189v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Adversarial example attack endangers the mobile edge systems such as vehicles
and drones that adopt deep neural networks for visual sensing. This paper
presents {\em Sardino}, an active and dynamic defense approach that renews the
inference ensemble at run time to develop security against the adaptive
adversary who tries to exfiltrate the ensemble and construct the corresponding
effective adversarial examples. By applying consistency check and data fusion
on the ensemble's predictions, Sardino can detect and thwart adversarial
inputs. Compared with the training-based ensemble renewal, we use HyperNet to
achieve {\em one million times} acceleration and per-frame ensemble renewal
that presents the highest level of difficulty to the prerequisite exfiltration
attacks. Moreover, the robustness of the renewed ensembles against adversarial
examples is enhanced with adversarial learning for the HyperNet. We design a
run-time planner that maximizes the ensemble size in favor of security while
maintaining the processing frame rate. Beyond adversarial examples, Sardino can
also address the issue of out-of-distribution inputs effectively. This paper
presents extensive evaluation of Sardino's performance in counteracting
adversarial examples and applies it to build a real-time car-borne traffic sign
recognition system. Live on-road tests show the built system's effectiveness in
maintaining frame rate and detecting out-of-distribution inputs due to the
false positives of a preceding YOLO-based traffic sign detector.

### Title: A Taxonomy of Error Sources in HPC I/O Machine Learning Models
* Paper ID: 2204.08180v1
* Paper URL: [http://arxiv.org/abs/2204.08180v1](http://arxiv.org/abs/2204.08180v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: I/O efficiency is crucial to productivity in scientific computing, but the
increasing complexity of the system and the applications makes it difficult for
practitioners to understand and optimize I/O behavior at scale. Data-driven
machine learning-based I/O throughput models offer a solution: they can be used
to identify bottlenecks, automate I/O tuning, or optimize job scheduling with
minimal human intervention. Unfortunately, current state-of-the-art I/O models
are not robust enough for production use and underperform after being deployed.
  We analyze multiple years of application, scheduler, and storage system logs
on two leadership-class HPC platforms to understand why I/O models underperform
in practice. We propose a taxonomy consisting of five categories of I/O
modeling errors: poor application and system modeling, inadequate dataset
coverage, I/O contention, and I/O noise. We develop litmus tests to quantify
each category, allowing researchers to narrow down failure modes, enhance I/O
throughput models, and improve future generations of HPC logging and analysis
tools.

### Title: TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval
* Paper ID: 2204.08173v1
* Paper URL: [http://arxiv.org/abs/2204.08173v1](http://arxiv.org/abs/2204.08173v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Entity retrieval--retrieving information about entity mentions in a query--is
a key step in open-domain tasks, such as question answering or fact checking.
However, state-of-the-art entity retrievers struggle to retrieve rare entities
for ambiguous mentions due to biases towards popular entities. Incorporating
knowledge graph types during training could help overcome popularity biases,
but there are several challenges: (1) existing type-based retrieval methods
require mention boundaries as input, but open-domain tasks run on unstructured
text, (2) type-based methods should not compromise overall performance, and (3)
type-based methods should be robust to noisy and missing types. In this work,
we introduce TABi, a method to jointly train bi-encoders on knowledge graph
types and unstructured text for entity retrieval for open-domain tasks. TABi
leverages a type-enforced contrastive loss to encourage entities and queries of
similar types to be close in the embedding space. TABi improves retrieval of
rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining
strong overall retrieval performance on open-domain tasks in the KILT benchmark
compared to state-of-the-art retrievers. TABi is also robust to incomplete type
systems, improving rare entity retrieval over baselines with only 5% type
coverage of the training dataset. We make our code publicly available at
https://github.com/HazyResearch/tabi.

### Title: Robust End-to-end Speaker Diarization with Generic Neural Clustering
* Paper ID: 2204.08164v1
* Paper URL: [http://arxiv.org/abs/2204.08164v1](http://arxiv.org/abs/2204.08164v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: End-to-end speaker diarization approaches have shown exceptional performance
over the traditional modular approaches. To further improve the performance of
the end-to-end speaker diarization for real speech recordings, recently works
have been proposed which integrate unsupervised clustering algorithms with the
end-to-end neural diarization models. However, these methods have a number of
drawbacks: 1) The unsupervised clustering algorithms cannot leverage the
supervision from the available datasets; 2) The K-means-based unsupervised
algorithms that are explored often suffer from the constraint violation
problem; 3) There is unavoidable mismatch between the supervised training and
the unsupervised inference. In this paper, a robust generic neural clustering
approach is proposed that can be integrated with any chunk-level predictor to
accomplish a fully supervised end-to-end speaker diarization model. Also, by
leveraging the sequence modelling ability of a recurrent neural network, the
proposed neural clustering approach can dynamically estimate the number of
speakers during inference. Experimental show that when integrating an
attractor-based chunk-level predictor, the proposed neural clustering approach
can yield better Diarization Error Rate (DER) than the constrained
K-means-based clustering approaches under the mismatched conditions.

### Title: Multi-scale Anomaly Detection for Big Time Series of Industrial Sensors
* Paper ID: 2204.08159v1
* Paper URL: [http://arxiv.org/abs/2204.08159v1](http://arxiv.org/abs/2204.08159v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Given a multivariate big time series, can we detect anomalies as soon as they
occur? Many existing works detect anomalies by learning how much a time series
deviates away from what it should be in the reconstruction framework. However,
most models have to cut the big time series into small pieces empirically since
optimization algorithms cannot afford such a long series. The question is
raised: do such cuts pollute the inherent semantic segments, like incorrect
punctuation in sentences? Therefore, we propose a reconstruction-based anomaly
detection method, MissGAN, iteratively learning to decode and encode naturally
smooth time series in coarse segments, and finding out a finer segment from
low-dimensional representations based on HMM. As a result, learning from
multi-scale segments, MissGAN can reconstruct a meaningful and robust time
series, with the help of adversarial regularization and extra conditional
states. MissGAN does not need labels or only needs labels of normal instances,
making it widely applicable. Experiments on industrial datasets of real water
network sensors show our MissGAN outperforms the baselines with scalability.
Besides, we use a case study on the CMU Motion dataset to demonstrate that our
model can well distinguish unexpected gestures from a given conditional motion.

### Title: Optimal Power Flow in Four-Wire Distribution Networks: Formulation and Benchmarking
* Paper ID: 2204.08126v1
* Paper URL: [http://arxiv.org/abs/2204.08126v1](http://arxiv.org/abs/2204.08126v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: In recent years, several applications have been proposed in the context of
distribution networks. Many of these can be formulated as an optimal power flow
problem, a mathematical optimization program which includes a model of the
steady-state physics of the electricity network. If the network loading is
balanced and the lines are transposed, the network model can be simplified to a
single-phase equivalent model. However, these assumptions do not apply to
low-voltage distribution networks, so the network model should model the
effects of phase unbalance correctly. In many parts of the world, the
low-voltage distribution network has four conductors, i.e. three phases and a
neutral. This paper develops OPF formulations for such networks, including
transformers, shunts and voltage-dependent loads, in two variable spaces, i.e.
current-voltage and power-voltage, and compares them for robustness and
scalability. A case study across 128 low-voltage networks also quantifies the
modelling error introduced by Kron reductions and its impact on the solve time.
This work highlights the advantages of formulations in current-voltage
variables over power-voltage, for four-wire networks.

### Title: Testing synchrotron models and frequency resolution in BINGO 21 cm simulated maps using GNILC
* Paper ID: 2204.08112v1
* Paper URL: [http://arxiv.org/abs/2204.08112v1](http://arxiv.org/abs/2204.08112v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: To recover the 21 cm hydrogen line, it is essential to separate the
cosmological signal from the much stronger foreground contributions at radio
frequencies. The BINGO radio telescope is designed to measure the 21 cm line
and detect BAOs using the intensity mapping technique. This work analyses the
performance of the GNILC method, combined with a power spectrum debiasing
procedure. The method was applied to a simulated BINGO mission, building upon
previous work from the collaboration. It compares two different synchrotron
emission models and different instrumental configurations, in addition to the
combination with ancillary data to optimize both the foreground removal and
recovery of the 21 cm signal across the full BINGO frequency band, as well as
to determine an optimal number of frequency bands for the signal recovery. We
have produced foreground emissions maps using the Planck Sky Model, the
cosmological Hi emission maps are generated using the FLASK package and thermal
noise maps are created according to the instrumental setup. We apply the GNILC
method to the simulated sky maps to separate the Hi plus thermal noise
contribution and, through a debiasing procedure, recover an estimate of the
noiseless 21 cm power spectrum. We found a near optimal reconstruction of the
Hi signal using a 80 bins configuration, which resulted in a power spectrum
reconstruction average error over all frequencies of 3%. Furthermore, our tests
showed that GNILC is robust against different synchrotron emission models.
Finally, adding an extra channel with CBASS foregrounds information, we reduced
the estimation error of the 21 cm signal. The optimisation of our previous
work, producing a configuration with an optimal number of channels for binning
the data, impacts greatly the decisions regarding BINGO hardware configuration
before commissioning.

