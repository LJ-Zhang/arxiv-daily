### Title: The Security War in File Systems: An Empirical Study from A Vulnerability-Centric Perspective
* Paper ID: 2204.12590v1
* Paper URL: [http://arxiv.org/abs/2204.12590v1](http://arxiv.org/abs/2204.12590v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: This paper presents a systematic study on the security of modern file
systems, following a vulnerability-centric perspective. Specifically, we
collected 377 file system vulnerabilities committed to the CVE database in the
past 20 years. We characterize them from four dimensions that include why the
vulnerabilities appear, how the vulnerabilities can be exploited, what
consequences can arise, and how the vulnerabilities are fixed. This way, we
build a deep understanding of the attack surfaces faced by file systems, the
threats imposed by the attack surfaces, and the good and bad practices in
mitigating the attacks in file systems. We envision that our study will bring
insights towards the future development of file systems, the enhancement of
file system security, and the relevant vulnerability mitigating solutions.

### Title: The Influence of the Other-Race Effect on Susceptibility to Face Morphing Attacks
* Paper ID: 2204.12591v1
* Paper URL: [http://arxiv.org/abs/2204.12591v1](http://arxiv.org/abs/2204.12591v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Facial morphs created between two identities resemble both of the faces used
to create the morph. Consequently, humans and machines are prone to mistake
morphs made from two identities for either of the faces used to create the
morph. This vulnerability has been exploited in "morph attacks" in security
scenarios. Here, we asked whether the "other-race effect" (ORE) -- the human
advantage for identifying own- vs. other-race faces -- exacerbates morph attack
susceptibility for humans. We also asked whether face-identification
performance in a deep convolutional neural network (DCNN) is affected by the
race of morphed faces. Caucasian (CA) and East-Asian (EA) participants
performed a face-identity matching task on pairs of CA and EA face images in
two conditions. In the morph condition, different-identity pairs consisted of
an image of identity "A" and a 50/50 morph between images of identity "A" and
"B". In the baseline condition, morphs of different identities never appeared.
As expected, morphs were identified mistakenly more often than original face
images. Moreover, CA participants showed an advantage for CA faces in
comparison to EA faces (a partial ORE). Of primary interest, morph
identification was substantially worse for cross-race faces than for own-race
faces. Similar to humans, the DCNN performed more accurately for original face
images than for morphed image pairs. Notably, the deep network proved
substantially more accurate than humans in both cases. The results point to the
possibility that DCNNs might be useful for improving face identification
accuracy when morphed faces are presented. They also indicate the significance
of the ORE in morph attack susceptibility in applied settings.

### Title: Wasmati: An Efficient Static Vulnerability Scanner for WebAssembly
* Paper ID: 2204.12575v1
* Paper URL: [http://arxiv.org/abs/2204.12575v1](http://arxiv.org/abs/2204.12575v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: WebAssembly is a new binary instruction format that allows targeted compiled
code written in high-level languages to be executed with near-native speed by
the browser's JavaScript engine. However, given that WebAssembly binaries can
be compiled from unsafe languages like C/C++, classical code vulnerabilities
such as buffer overflows or format strings can be transferred over from the
original programs down to the cross-compiled binaries. As a result, this
possibility of incorporating vulnerabilities in WebAssembly modules has widened
the attack surface of modern web applications. This paper presents Wasmati, a
static analysis tool for finding security vulnerabilities in WebAssembly
binaries. It is based on the generation of a code property graph (CPG), a
program representation previously adopted for detecting vulnerabilities in
various languages but hitherto unapplied to WebAssembly. We formalize the
definition of CPG for WebAssembly, introduce techniques to generate CPG for
complex WebAssembly, and present four different query specification languages
for finding vulnerabilities by traversing a program's CPG. We implemented ten
queries capturing different vulnerability types and extensively tested Wasmati
on four heterogeneous datasets. We show that Wasmati can scale the generation
of CPGs for large real-world applications and can efficiently find
vulnerabilities for all our query types. We have also tested our tool on
WebAssembly binaries collected in the wild and identified several potential
vulnerabilities, some of which we have manually confirmed to exist unless the
enclosing application properly sanitizes the interaction with such affected
binaries.

### Title: Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages
* Paper ID: 2204.12543v1
* Paper URL: [http://arxiv.org/abs/2204.12543v1](http://arxiv.org/abs/2204.12543v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Abusive language is a growing concern in many social media platforms.
Repeated exposure to abusive speech has created physiological effects on the
target users. Thus, the problem of abusive language should be addressed in all
forms for online peace and safety. While extensive research exists in abusive
speech detection, most studies focus on English. Recently, many smearing
incidents have occurred in India, which provoked diverse forms of abusive
speech in online space in various languages based on the geographic location.
Therefore it is essential to deal with such malicious content. In this paper,
to bridge the gap, we demonstrate a large-scale analysis of multilingual
abusive speech in Indic languages. We examine different interlingual transfer
mechanisms and observe the performance of various multilingual models for
abusive speech detection for eight different Indic languages. We also
experiment to show how robust these models are on adversarial attacks. Finally,
we conduct an in-depth error analysis by looking into the models' misclassified
posts across various settings. We have made our code and models public for
other researchers.

### Title: Poisoning Deep Learning based Recommender Model in Federated Learning Scenarios
* Paper ID: 2204.13594v1
* Paper URL: [http://arxiv.org/abs/2204.13594v1](http://arxiv.org/abs/2204.13594v1)
* Updated Date: 2022-04-26
* Code URL: [https://github.com/rdz98/poisonfeddlrs](https://github.com/rdz98/poisonfeddlrs)
* Summary: Various attack methods against recommender systems have been proposed in the
past years, and the security issues of recommender systems have drawn
considerable attention. Traditional attacks attempt to make target items
recommended to as many users as possible by poisoning the training data.
Benifiting from the feature of protecting users' private data, federated
recommendation can effectively defend such attacks. Therefore, quite a few
works have devoted themselves to developing federated recommender systems. For
proving current federated recommendation is still vulnerable, in this work we
probe to design attack approaches targeting deep learning based recommender
models in federated learning scenarios. Specifically, our attacks generate
poisoned gradients for manipulated malicious users to upload based on two
strategies (i.e., random approximation and hard user mining). Extensive
experiments show that our well-designed attacks can effectively poison the
target models, and the attack effectiveness sets the state-of-the-art.

### Title: Restricted Black-box Adversarial Attack Against DeepFake Face Swapping
* Paper ID: 2204.12347v1
* Paper URL: [http://arxiv.org/abs/2204.12347v1](http://arxiv.org/abs/2204.12347v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: DeepFake face swapping presents a significant threat to online security and
social media, which can replace the source face in an arbitrary photo/video
with the target face of an entirely different person. In order to prevent this
fraud, some researchers have begun to study the adversarial methods against
DeepFake or face manipulation. However, existing works focus on the white-box
setting or the black-box setting driven by abundant queries, which severely
limits the practical application of these methods. To tackle this problem, we
introduce a practical adversarial attack that does not require any queries to
the facial image forgery model. Our method is built on a substitute model
persuing for face reconstruction and then transfers adversarial examples from
the substitute model directly to inaccessible black-box DeepFake models.
Specially, we propose the Transferable Cycle Adversary Generative Adversarial
Network (TCA-GAN) to construct the adversarial perturbation for disrupting
unknown DeepFake systems. We also present a novel post-regularization module
for enhancing the transferability of generated adversarial examples. To
comprehensively measure the effectiveness of our approaches, we construct a
challenging benchmark of DeepFake adversarial attacks for future development.
Extensive experiments impressively show that the proposed adversarial attack
method makes the visual quality of DeepFake face images plummet so that they
are easier to be detected by humans and algorithms. Moreover, we demonstrate
that the proposed algorithm can be generalized to offer face image protection
against various face translation methods.

### Title: Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies
* Paper ID: 2204.12495v1
* Paper URL: [http://arxiv.org/abs/2204.12495v1](http://arxiv.org/abs/2204.12495v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Federated learning reduces the risk of information leakage, but remains
vulnerable to attacks. We investigate how several neural network design
decisions can defend against gradients inversion attacks. We show that
overlapping gradients provides numerical resistance to gradient inversion on
the highly vulnerable dense layer. Specifically, we propose to leverage
batching to maximise mixing of gradients by choosing an appropriate loss
function and drawing identical labels. We show that otherwise it is possible to
directly recover all vectors in a mini-batch without any numerical optimisation
due to the de-mixing nature of the cross entropy loss. To accurately assess
data recovery, we introduce an absolute variation distance (AVD) metric for
information leakage in images, derived from total variation. In contrast to
standard metrics, e.g. Mean Squared Error or Structural Similarity Index, AVD
offers a continuous metric for extracting information in noisy images. Finally,
our empirical results on information recovery from various inversion attacks
and training performance supports our defense strategies. These strategies are
also shown to be useful for deep convolutional neural networks such as LeNET
for image recognition. We hope that this study will help guide the development
of further strategies that achieve a trustful federation policy.

### Title: Open or not open: Are conventional radio access networks more secure and trustworthy than Open-RAN?
* Paper ID: 2204.12227v1
* Paper URL: [http://arxiv.org/abs/2204.12227v1](http://arxiv.org/abs/2204.12227v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The Open-RAN architecture is a highly promising and future-oriented
architecture. It is intended to open up the radio access network and enable
more innovation and competition in the market. This will lead to RANs for
current 5G networks, but especially for future 6G networks, to move away from
the current centralised, provider-specific 3G RAN architecture and therefore
even better meet the requirements for future RANs. However, the change in
design has also created a drastic shift in the attack surface compared to
conventional RANs. In the past, this has often led to negative headlines, which
in summary have often associated O-RAN with faulty or inadequate security. In
this paper, we analyze what components are involved in an Open-RAN deployment,
how the current state of security is to be assessed and what measures need to
be taken to ensure secure operation.

### Title: Boosting Adversarial Transferability of MLP-Mixer
* Paper ID: 2204.12204v1
* Paper URL: [http://arxiv.org/abs/2204.12204v1](http://arxiv.org/abs/2204.12204v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The security of models based on new architectures such as MLP-Mixer and ViTs
needs to be studied urgently. However, most of the current researches are
mainly aimed at the adversarial attack against ViTs, and there is still
relatively little adversarial work on MLP-mixer. We propose an adversarial
attack method against MLP-Mixer called Maxwell's demon Attack (MA). MA breaks
the channel-mixing and token-mixing mechanism of MLP-Mixer by controlling the
part input of MLP-Mixer's each Mixer layer, and disturbs MLP-Mixer to obtain
the main information of images. Our method can mask the part input of the Mixer
layer, avoid overfitting of the adversarial examples to the source model, and
improve the transferability of cross-architecture. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed MA. Our method can be easily combined with existing methods and can
improve the transferability by up to 38.0% on MLP-based ResMLP. Adversarial
examples produced by our method on MLP-Mixer are able to exceed the
transferability of adversarial examples produced using DenseNet against CNNs.
To the best of our knowledge, we are the first work to study adversarial
transferability of MLP-Mixer.

### Title: Drag reduction on a transonic airfoil
* Paper ID: 2204.12172v1
* Paper URL: [http://arxiv.org/abs/2204.12172v1](http://arxiv.org/abs/2204.12172v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Flow control for turbulent skin-friction drag reduction is applied to a
transonic airfoil to improve its aerodynamic performance. The study is based on
direct numerical simulations (with up to 1.8 billions cells) of the
compressible turbulent flow around a supercritical airfoil, at Reynolds and
Mach numbers of $Re_\infty= 3 \times 10^5$ and $M_\infty =0.7$. Control via
spanwise forcing is applied over a fraction of the suction side of the airfoil.
Besides locally reducing friction, the control modifies the shock wave and
significantly improves the aerodynamic efficiency of the airfoil by increasing
lift and decreasing drag. Hence, the airfoil can achieve the required lift at a
lower angle of attack and with a lower drag. Estimates at the aircraft level
indicate that substantial savings are possible; when control is active, its
energy cost becomes negligible thanks to the small application area. We suggest
that skin-friction drag reduction should be considered not only as a goal, but
also as a tool to improve the global aerodynamics of complex flows.

### Title: Mixed Strategies for Security Games with General Defending Requirements
* Paper ID: 2204.12158v1
* Paper URL: [http://arxiv.org/abs/2204.12158v1](http://arxiv.org/abs/2204.12158v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: The Stackelberg security game is played between a defender and an attacker,
where the defender needs to allocate a limited amount of resources to multiple
targets in order to minimize the loss due to adversarial attack by the
attacker. While allowing targets to have different values, classic settings
often assume uniform requirements to defend the targets. This enables existing
results that study mixed strategies (randomized allocation algorithms) to adopt
a compact representation of the mixed strategies.
  In this work, we initiate the study of mixed strategies for the security
games in which the targets can have different defending requirements. In
contrast to the case of uniform defending requirement, for which an optimal
mixed strategy can be computed efficiently, we show that computing the optimal
mixed strategy is NP-hard for the general defending requirements setting.
However, we show that strong upper and lower bounds for the optimal mixed
strategy defending result can be derived. We propose an efficient
close-to-optimal Patching algorithm that computes mixed strategies that use
only few pure strategies. We also study the setting when the game is played on
a network and resource sharing is enabled between neighboring targets. Our
experimental results demonstrate the effectiveness of our algorithm in several
large real-world datasets.

### Title: Source-independent quantum random number generator against detector blinding attacks
* Paper ID: 2204.12156v1
* Paper URL: [http://arxiv.org/abs/2204.12156v1](http://arxiv.org/abs/2204.12156v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Randomness, mainly in the form of random numbers, is the fundamental
prerequisite for the security of many cryptographic tasks. Quantum randomness
can be extracted even if adversaries are fully aware of the protocol and even
control the randomness source. However, an adversary can further manipulate the
randomness via detector blinding attacks, which are a hacking attack suffered
by protocols with trusted detectors. Here, by treating no-click events as valid
error events, we propose a quantum random number generation protocol that can
simultaneously address source vulnerability and ferocious detector blinding
attacks. The method can be extended to high-dimensional random number
generation. We experimentally demonstrate the ability of our protocol to
generate random numbers for two-dimensional measurement with a generation speed
of 0.515 Mbps, which is two orders of magnitude higher than that of
device-independent protocols that can address both issues of imperfect sources
and imperfect detectors.

### Title: Cyber-Physical Vulnerability Assessment of P2P Energy Exchanges in Active Distribution Networks
* Paper ID: 2204.12081v1
* Paper URL: [http://arxiv.org/abs/2204.12081v1](http://arxiv.org/abs/2204.12081v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Owing to the decreasing costs of distributed energy resources (DERs) as well
as decarbonization policies, power systems are undergoing a modernization
process. The large deployment of DERs together with internet of things (IoT)
devices provide a platform for peer-to-peer (P2P) energy trading in active
distribution networks. However, P2P energy trading with IoT devices have driven
the grid more vulnerable to cyber-physical threats. To this end, in this paper,
a resilience-oriented P2P energy exchange model is developed considering three
phase unbalanced distribution systems. In addition, various scenarios for
vulnerability assessment of P2P energy exchanges considering adverse prosumers
and consumers, who provide false information regarding the price and quantity
with the goal of maximum financial benefit and system operation disruption, are
considered. Techno-economic survivability analysis against these attacks are
investigated on a IEEE 13-node unbalanced distribution test system. Simulation
results demonstrate that adverse peers can affect the physical operation of
grid, maximize their benefits, and cause financial loss of other agents.

### Title: Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks
* Paper ID: 2204.12050v1
* Paper URL: [http://arxiv.org/abs/2204.12050v1](http://arxiv.org/abs/2204.12050v1)
* Updated Date: 2022-04-26
* Code URL: null
* Summary: Malicious intelligent algorithms greatly threaten the security of social
users' privacy by detecting and analyzing the uploaded photos to social network
platforms. The destruction to DNNs brought by the adversarial attack sparks the
potential that adversarial examples serve as a new protection mechanism for
privacy security in social networks. However, the existing adversarial example
does not have recoverability for serving as an effective protection mechanism.
To address this issue, we propose a recoverable generative adversarial network
to generate self-recoverable adversarial examples. By modeling the adversarial
attack and recovery as a united task, our method can minimize the error of the
recovered examples while maximizing the attack ability, resulting in better
recoverability of adversarial examples. To further boost the recoverability of
these examples, we exploit a dimension reducer to optimize the distribution of
adversarial perturbation. The experimental results prove that the adversarial
examples generated by the proposed method present superior recoverability,
attack ability, and robustness on different datasets and network architectures,
which ensure its effectiveness as a protection mechanism in social networks.

