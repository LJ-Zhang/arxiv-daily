### Title: The Greater The Power, The More Dangerous The Abuse: Facing Malicious Insiders in The Cloud
* Paper ID: 2206.09834v1
* Paper URL: [http://arxiv.org/abs/2206.09834v1](http://arxiv.org/abs/2206.09834v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: The financial crisis made companies around the world search for cheaper and
more efficient solutions to cover their needs in terms of computational power
and storage. Their quest came to end with the birth of Cloud Computing
infrastructures. However, along with the new promising technology, new attack
vectors were born, and one old and known threat, that of Malicious Insiders
reappeared. Insiders can use their privileged position inside the Cloud
infrastructure to accomplish or help in attacks against a Cloud infrastructure.
In this paper, we propose a practical and efficient intrusion detection system
solution for Cloud infrastructures based on Graphical Processing Unit (GPU)
acceleration. Our solution monitors the deployed virtual machines' operations
and especially those of the host Operating System, known as Dom0, correlating
the collected information to detect uncommon behavior based on the
Smith-Waterman algorithm. Our proposal makes possible the cooperation of a
variety of known hypervisors along with every known GPU acceleration unit used,
thus offering the maximum of security mechanics while at the same time
minimizing the imposed overhead in terms of Central Processing Unit (CPU)
usage.

### Title: Diversified Adversarial Attacks based on Conjugate Gradient Method
* Paper ID: 2206.09628v1
* Paper URL: [http://arxiv.org/abs/2206.09628v1](http://arxiv.org/abs/2206.09628v1)
* Updated Date: 2022-06-20
* Code URL: [https://github.com/yamamura-k/ACG](https://github.com/yamamura-k/ACG)
* Summary: Deep learning models are vulnerable to adversarial examples, and adversarial
attacks used to generate such examples have attracted considerable research
interest. Although existing methods based on the steepest descent have achieved
high attack success rates, ill-conditioned problems occasionally reduce their
performance. To address this limitation, we utilize the conjugate gradient (CG)
method, which is effective for this type of problem, and propose a novel attack
algorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG)
attack. The results of large-scale evaluation experiments conducted on the
latest robust models show that, for most models, ACG was able to find more
adversarial examples with fewer iterations than the existing SOTA algorithm
Auto-PGD (APGD). We investigated the difference in search performance between
ACG and APGD in terms of diversification and intensification, and define a
measure called Diversity Index (DI) to quantify the degree of diversity. From
the analysis of the diversity using this index, we show that the more diverse
search of the proposed method remarkably improves its attack success rate.

