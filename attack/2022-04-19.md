### Title: Identifying Near-Optimal Single-Shot Attacks on ICSs with Limited Process Knowledge
* Paper ID: 2204.09106v1
* Paper URL: [http://arxiv.org/abs/2204.09106v1](http://arxiv.org/abs/2204.09106v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Industrial Control Systems (ICSs) rely on insecure protocols and devices to
monitor and operate critical infrastructure. Prior work has demonstrated that
powerful attackers with detailed system knowledge can manipulate exchanged
sensor data to deteriorate performance of the process, even leading to full
shutdowns of plants. Identifying those attacks requires iterating over all
possible sensor values, and running detailed system simulation or analysis to
identify optimal attacks. That setup allows adversaries to identify attacks
that are most impactful when applied on the system for the first time, before
the system operators become aware of the manipulations.
  In this work, we investigate if constrained attackers without detailed system
knowledge and simulators can identify comparable attacks. In particular, the
attacker only requires abstract knowledge on general information flow in the
plant, instead of precise algorithms, operating parameters, process models, or
simulators. We propose an approach that allows single-shot attacks, i.e.,
near-optimal attacks that are reliably shutting down a system on the first try.
The approach is applied and validated on two use cases, and demonstrated to
achieve comparable results to prior work, which relied on detailed system
information and simulations.

### Title: Indiscriminate Data Poisoning Attacks on Neural Networks
* Paper ID: 2204.09092v1
* Paper URL: [http://arxiv.org/abs/2204.09092v1](http://arxiv.org/abs/2204.09092v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Data poisoning attacks, in which a malicious adversary aims to influence a
model by injecting "poisoned" data into the training process, have attracted
significant recent attention. In this work, we take a closer look at existing
poisoning attacks and connect them with old and new algorithms for solving
sequential Stackelberg games. By choosing an appropriate loss function for the
attacker and optimizing with algorithms that exploit second-order information,
we design poisoning attacks that are effective on neural networks. We present
efficient implementations that exploit modern auto-differentiation packages and
allow simultaneous and coordinated generation of tens of thousands of poisoned
points, in contrast to existing methods that generate poisoned points one by
one. We further perform extensive experiments that empirically explore the
effect of data poisoning attacks on deep neural networks.

### Title: Cascading traffic jamming in a two-dimensional Motter and Lai model
* Paper ID: 2204.09011v1
* Paper URL: [http://arxiv.org/abs/2204.09011v1](http://arxiv.org/abs/2204.09011v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We study the cascading traffic jamming on a two-dimensional random geometric
graph using the Motter and Lai model. The traffic jam is caused by a localized
attack incapacitating circular region or a line of a certain size, as well as a
dispersed attack on an equal number of randomly selected nodes. We investigate
if there is a critical size of the attack above which the network becomes
completely jammed due to cascading jamming, and how this critical size depends
on the average degree $\langle k\rangle$ of the graph, on the number of nodes
$N$ in the system, and the tolerance parameter $\alpha$ of the Motter and Lai
model.

### Title: Disappeared Command: Spoofing Attack On Automatic Speech Recognition Systems with Sound Masking
* Paper ID: 2204.08977v1
* Paper URL: [http://arxiv.org/abs/2204.08977v1](http://arxiv.org/abs/2204.08977v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The development of deep learning technology has greatly promoted the
performance improvement of automatic speech recognition (ASR) technology, which
has demonstrated an ability comparable to human hearing in many tasks. Voice
interfaces are becoming more and more widely used as input for many
applications and smart devices. However, existing research has shown that DNN
is easily disturbed by slight disturbances and makes false recognition, which
is extremely dangerous for intelligent voice applications controlled by voice.

### Title: Seculator: A Fast and Secure Neural Processing Unit
* Paper ID: 2204.08951v1
* Paper URL: [http://arxiv.org/abs/2204.08951v1](http://arxiv.org/abs/2204.08951v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Securing deep neural networks (DNNs) is a problem of significant interest
since an ML model incorporates high-quality intellectual property, features of
data sets painstakingly collated by mechanical turks, and novel methods of
training on large cluster computers. Sadly, attacks to extract model parameters
are on the rise, and thus designers are being forced to create architectures
for securing such models. State-of-the-art proposals in this field take the
deterministic memory access patterns of such networks into cognizance (albeit
partially), group a set of memory blocks into a tile, and maintain state at the
level of tiles (to reduce storage space). For providing integrity guarantees
(tamper avoidance), they don't propose any significant optimizations, and still
maintain block-level state.
  We observe that it is possible to exploit the deterministic memory access
patterns of DNNs even further, and maintain state information for only the
current tile and current layer, which may comprise a large number of tiles.
This reduces the storage space, reduces the number of memory accesses,
increases performance, and simplifies the design without sacrificing any
security guarantees. The key techniques in our proposed accelerator
architecture, Seculator, are to encode memory access patterns to create a small
HW-based tile version number generator for a given layer, and to store
layer-level MACs. We completely eliminate the need for having a MAC cache and a
tile version number store (as used in related work). We show that using
intelligently-designed mathematical operations, these structures are not
required. By reducing such overheads, we show a speedup of 16% over the closest
competing work.

### Title: Model Checking Strategic Abilities in Information-sharing Systems
* Paper ID: 2204.08896v1
* Paper URL: [http://arxiv.org/abs/2204.08896v1](http://arxiv.org/abs/2204.08896v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: We introduce a subclass of concurrent game structures (CGS) with imperfect
information in which agents are endowed with private data-sharing capabilities.
Importantly, our CGSs are such that it is still decidable to model-check these
CGSs against a relevant fragment of ATL. These systems can be thought as a
generalisation of architectures allowing information forks, in the sense that,
in the initial states of the system, we allow information forks from agents
outside a given set A to agents inside this A. For this reason, together with
the fact that the communication in our models underpins a specialised form of
broadcast, we call our formalism A-cast systems. To underline, the fragment of
ATL for which we show the model-checking problem to be decidable over A-cast is
a large and significant one; it expresses coalitions over agents in any subset
of the set A. Indeed, as we show, our systems and this ATL fragments can encode
security problems that are notoriously hard to express faithfully:
terrorist-fraud attacks in identity schemes.

### Title: Event-triggered Approximate Byzantine Consensus with Multi-hop Communication
* Paper ID: 2204.08883v1
* Paper URL: [http://arxiv.org/abs/2204.08883v1](http://arxiv.org/abs/2204.08883v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: In this paper, we consider a resilient consensus problem for the multi-agent
network where some of the agents are subject to Byzantine attacks and may
transmit erroneous state values to their neighbors. In particular, we develop
an event-triggered update rule to tackle this problem as well as reduce the
communication for each agent. Our approach is based on the mean subsequence
reduced (MSR) algorithm with agents being capable to communicate with multi-hop
neighbors. Since delays are critical in such an environment, we provide
necessary graph conditions for the proposed algorithm to perform well with
delays in the communication. We highlight that through multi-hop communication,
the network connectivity can be reduced especially in comparison with the
common onehop communication case. Lastly, we show the effectiveness of the
proposed algorithm by a numerical example.

### Title: CoFHEE: A Co-processor for Fully Homomorphic Encryption Execution
* Paper ID: 2204.08742v1
* Paper URL: [http://arxiv.org/abs/2204.08742v1](http://arxiv.org/abs/2204.08742v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: The migration of computation to the cloud has raised privacy concerns as
sensitive data becomes vulnerable to attacks since they need to be decrypted
for processing. Fully Homomorphic Encryption (FHE) mitigates this issue as it
enables meaningful computations to be performed directly on encrypted data.
Nevertheless, FHE is orders of magnitude slower than unencrypted computation,
which hinders its practicality and adoption. Therefore, improving FHE
performance is essential for its real world deployment. In this paper, we
present a year-long effort to design, implement, fabricate, and post-silicon
validate a hardware accelerator for Fully Homomorphic Encryption dubbed CoFHEE.
With a design area of $12mm^2$, CoFHEE aims to improve performance of
ciphertext multiplications, the most demanding arithmetic FHE operation, by
accelerating several primitive operations on polynomials, such as polynomial
additions and subtractions, Hadamard product, and Number Theoretic Transform.
CoFHEE supports polynomial degrees of up to $n = 2^{14}$ with a maximum
coefficient sizes of 128 bits, while it is capable of performing ciphertext
multiplications entirely on chip for $n \leq 2^{13}$. CoFHEE is fabricated in
55nm CMOS technology and achieves 250 MHz with our custom-built low-power
digital PLL design. In addition, our chip includes two communication interfaces
to the host machine: UART and SPI. This manuscript presents all steps and
design techniques in the ASIC development process, ranging from RTL design to
fabrication and validation. We evaluate our chip with performance and power
experiments and compare it against state-of-the-art software implementations
and other ASIC designs. Developed RTL files are available in an open-source
repository.

### Title: Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks
* Paper ID: 2204.08726v1
* Paper URL: [http://arxiv.org/abs/2204.08726v1](http://arxiv.org/abs/2204.08726v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Deep neural networks have become an integral part of our software
infrastructure and are being deployed in many widely-used and safety-critical
applications. However, their integration into many systems also brings with it
the vulnerability to test time attacks in the form of Universal Adversarial
Perturbations (UAPs). UAPs are a class of perturbations that when applied to
any input causes model misclassification. Although there is an ongoing effort
to defend models against these adversarial attacks, it is often difficult to
reconcile the trade-offs in model accuracy and robustness to adversarial
attacks. Jacobian regularization has been shown to improve the robustness of
models against UAPs, whilst model ensembles have been widely adopted to improve
both predictive performance and model robustness. In this work, we propose a
novel approach, Jacobian Ensembles-a combination of Jacobian regularization and
model ensembles to significantly increase the robustness against UAPs whilst
maintaining or improving model accuracy. Our results show that Jacobian
Ensembles achieves previously unseen levels of accuracy and robustness, greatly
improving over previous methods that tend to skew towards only either accuracy
or robustness.

### Title: Topology and geometry of data manifold in deep learning
* Paper ID: 2204.08624v1
* Paper URL: [http://arxiv.org/abs/2204.08624v1](http://arxiv.org/abs/2204.08624v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Despite significant advances in the field of deep learning in applications to
various fields, explaining the inner processes of deep learning models remains
an important and open question. The purpose of this article is to describe and
substantiate the geometric and topological view of the learning process of
neural networks. Our attention is focused on the internal representation of
neural networks and on the dynamics of changes in the topology and geometry of
the data manifold on different layers. We also propose a method for assessing
the generalizing ability of neural networks based on topological descriptors.
In this paper, we use the concepts of topological data analysis and intrinsic
dimension, and we present a wide range of experiments on different datasets and
different configurations of convolutional neural network architectures. In
addition, we consider the issue of the geometry of adversarial attacks in the
classification task and spoofing attacks on face recognition systems. Our work
is a contribution to the development of an important area of explainable and
interpretable AI through the example of computer vision.

### Title: Poisons that are learned faster are more effective
* Paper ID: 2204.08615v1
* Paper URL: [http://arxiv.org/abs/2204.08615v1](http://arxiv.org/abs/2204.08615v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Imperceptible poisoning attacks on entire datasets have recently been touted
as methods for protecting data privacy. However, among a number of defenses
preventing the practical use of these techniques, early-stopping stands out as
a simple, yet effective defense. To gauge poisons' vulnerability to
early-stopping, we benchmark error-minimizing, error-maximizing, and synthetic
poisons in terms of peak test accuracy over 100 epochs and make a number of
surprising observations. First, we find that poisons that reach a low training
loss faster have lower peak test accuracy. Second, we find that a current
state-of-the-art error-maximizing poison is 7 times less effective when poison
training is stopped at epoch 8. Third, we find that stronger, more transferable
adversarial attacks do not make stronger poisons. We advocate for evaluating
poisons in terms of peak test accuracy.

### Title: Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors
* Paper ID: 2204.08612v1
* Paper URL: [http://arxiv.org/abs/2204.08612v1](http://arxiv.org/abs/2204.08612v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic
media where the likeness of one person is replaced with another. There are
growing concerns that deepfakes can be maliciously used to create misleading
and harmful digital contents. As deepfakes become more common, there is a dire
need for deepfake detection technology to help spot deepfake media. Present
deepfake detection models are able to achieve outstanding accuracy (>90%).
However, most of them are limited to within-dataset scenario, where the same
dataset is used for training and testing. Most models do not generalise well
enough in cross-dataset scenario, where models are tested on unseen datasets
from another source. Furthermore, state-of-the-art deepfake detection models
rely on neural network-based classification models that are known to be
vulnerable to adversarial attacks. Motivated by the need for a robust deepfake
detection model, this study adapts metamorphic testing (MT) principles to help
identify potential factors that could influence the robustness of the examined
model, while overcoming the test oracle problem in this domain. Metamorphic
testing is specifically chosen as the testing technique as it fits our demand
to address learning-based system testing with probabilistic outcomes from
largely black-box components, based on potentially large input domains. We
performed our evaluations on MesoInception-4 and TwoStreamNet models, which are
the state-of-the-art deepfake detection models. This study identified makeup
application as an adversarial attack that could fool deepfake detectors. Our
experimental results demonstrate that both the MesoInception-4 and TwoStreamNet
models degrade in their performance by up to 30\% when the input data is
perturbed with makeup.

### Title: Context-Auditor: Context-sensitive Content Injection Mitigation
* Paper ID: 2204.08592v1
* Paper URL: [http://arxiv.org/abs/2204.08592v1](http://arxiv.org/abs/2204.08592v1)
* Updated Date: 2022-04-19
* Code URL: null
* Summary: Cross-site scripting (XSS) is the most common vulnerability class in web
applications over the last decade. Much research attention has focused on
building exploit mitigation defenses for this problem, but no technique
provides adequate protection in the face of advanced attacks. One technique
that bypasses XSS mitigations is the scriptless attack: a content injection
technique that uses (among other options) CSS and HTML injection to infiltrate
data. In studying this technique and others, we realized that the common
property among the exploitation of all content injection vulnerabilities,
including not just XSS and scriptless attacks, but also command injections and
several others, is an unintended context switch in the victim program's parsing
engine that is caused by untrusted user input.
  In this paper, we propose Context-Auditor, a novel technique that leverages
this insight to identify content injection vulnerabilities ranging from XSS to
scriptless attacks and command injections. We implemented Context-Auditor as a
general solution to content injection exploit detection problem in the form of
a flexible, stand-alone detection module. We deployed instances of
Context-Auditor as (1) a browser plugin, (2) a web proxy (3) a web server
plugin, and (4) as a wrapper around potentially-injectable system endpoints.
Because Context-Auditor targets the root cause of content injection
exploitation (and, more specifically for the purpose of our prototype, XSS
exploitation, scriptless exploitation, and command injection), our evaluation
results demonstrate that Context-Auditor can identify and block content
injection exploits that modern defenses cannot while maintaining low throughput
overhead and avoiding false positives.

