### Title: Automatic Hardware Trojan Insertion using Machine Learning
* Paper ID: 2204.08580v1
* Paper URL: [http://arxiv.org/abs/2204.08580v1](http://arxiv.org/abs/2204.08580v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Due to the current horizontal business model that promotes increasing
reliance on untrusted third-party Intellectual Properties (IPs), CAD tools, and
design facilities, hardware Trojan attacks have become a serious threat to the
semiconductor industry. Development of effective countermeasures against
hardware Trojan attacks requires: (1) fast and reliable exploration of the
viable Trojan attack space for a given design and (2) a suite of high-quality
Trojan-inserted benchmarks that meet specific standards. The latter has become
essential for the development and evaluation of design/verification solutions
to achieve quantifiable assurance against Trojan attacks. While existing static
benchmarks provide a baseline for comparing different countermeasures, they
only enumerate a limited number of handcrafted Trojans from the complete Trojan
design space. To accomplish these dual objectives, in this paper, we present
MIMIC, a novel AI-guided framework for automatic Trojan insertion, which can
create a large population of valid Trojans for a given design by mimicking the
properties of a small set of known Trojans. While there exist tools to
automatically insert Trojan instances using fixed Trojan templates, they cannot
analyze known Trojan attacks for creating new instances that accurately capture
the threat model. MIMIC works in two major steps: (1) it analyzes structural
and functional features of existing Trojan populations in a multi-dimensional
space to train machine learning models and generate a large number of "virtual
Trojans" of the given design, (2) next, it binds them into the design by
matching their functional/structural properties with suitable nets of the
internal logic structure. We have developed a complete tool flow for MIMIC,
extensively evaluated the framework by exploring several use-cases, and
quantified its effectiveness to demonstrate highly promising results.

### Title: Collusion-resistant fingerprinting of parallel content channels
* Paper ID: 2204.08575v1
* Paper URL: [http://arxiv.org/abs/2204.08575v1](http://arxiv.org/abs/2204.08575v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The fingerprinting game is analysed when the coalition size $k$ is known to
the tracer, but the colluders can distribute themselves across $L$ TV channels.
The collusion channel is introduced and the extra degrees of freedom for the
coalition are made manifest in our formulation. We introduce a payoff
functional that is analogous to the single TV channel case, and is conjectured
to be closely related to the fingerprinting capacity. For the binary alphabet
case under the marking assumption, and the restriction of access to one TV
channel per person per segment, we derive the asymptotic behavior of the payoff
functional. We find that the value of the maximin game for our payoff is
asymptotically equal to $L^2/k^2 2 \ln 2$, with optimal strategy for the tracer
being the arcsine distribution, and for the coalition being the interleaving
attack across all TV channels, as well as assigning an equal number of
colluders across the $L$ TV channels.

### Title: UNBUS: Uncertainty-aware Deep Botnet Detection System in Presence of Perturbed Samples
* Paper ID: 2204.09502v1
* Paper URL: [http://arxiv.org/abs/2204.09502v1](http://arxiv.org/abs/2204.09502v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: A rising number of botnet families have been successfully detected using deep
learning architectures. While the variety of attacks increases, these
architectures should become more robust against attacks. They have been proven
to be very sensitive to small but well constructed perturbations in the input.
Botnet detection requires extremely low false-positive rates (FPR), which are
not commonly attainable in contemporary deep learning. Attackers try to
increase the FPRs by making poisoned samples. The majority of recent research
has focused on the use of model loss functions to build adversarial examples
and robust models. In this paper, two LSTM-based classification algorithms for
botnet classification with an accuracy higher than 98\% are presented. Then,
the adversarial attack is proposed, which reduces the accuracy to about30\%.
Then, by examining the methods for computing the uncertainty, the defense
method is proposed to increase the accuracy to about 70\%. By using the deep
ensemble and stochastic weight averaging quantification methods it has been
investigated the uncertainty of the accuracy in the proposed methods.

### Title: A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability
* Paper ID: 2204.08570v1
* Paper URL: [http://arxiv.org/abs/2204.08570v1](http://arxiv.org/abs/2204.08570v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Graph Neural Networks (GNNs) have made rapid developments in the recent
years. Due to their great ability in modeling graph-structured data, GNNs are
vastly used in various applications, including high-stakes scenarios such as
financial analysis, traffic predictions, and drug discovery. Despite their
great potential in benefiting humans in the real world, recent study shows that
GNNs can leak private information, are vulnerable to adversarial attacks, can
inherit and magnify societal bias from training data and lack interpretability,
which have risk of causing unintentional harm to the users and society. For
example, existing works demonstrate that attackers can fool the GNNs to give
the outcome they desire with unnoticeable perturbation on training graph. GNNs
trained on social networks may embed the discrimination in their decision
process, strengthening the undesirable societal bias. Consequently, trustworthy
GNNs in various aspects are emerging to prevent the harm from GNN models and
increase the users' trust in GNNs. In this paper, we give a comprehensive
survey of GNNs in the computational aspects of privacy, robustness, fairness,
and explainability. For each aspect, we give the taxonomy of the related
methods and formulate the general frameworks for the multiple categories of
trustworthy GNNs. We also discuss the future research directions of each aspect
and connections between these aspects to help achieve trustworthiness.

### Title: Special Session: Towards an Agile Design Methodology for Efficient, Reliable, and Secure ML Systems
* Paper ID: 2204.09514v1
* Paper URL: [http://arxiv.org/abs/2204.09514v1](http://arxiv.org/abs/2204.09514v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: The real-world use cases of Machine Learning (ML) have exploded over the past
few years. However, the current computing infrastructure is insufficient to
support all real-world applications and scenarios. Apart from high efficiency
requirements, modern ML systems are expected to be highly reliable against
hardware failures as well as secure against adversarial and IP stealing
attacks. Privacy concerns are also becoming a first-order issue. This article
summarizes the main challenges in agile development of efficient, reliable and
secure ML systems, and then presents an outline of an agile design methodology
to generate efficient, reliable and secure ML systems based on user-defined
constraints and objectives.

### Title: Optimal Layered Defense For Site Protection
* Paper ID: 2204.08961v1
* Paper URL: [http://arxiv.org/abs/2204.08961v1](http://arxiv.org/abs/2204.08961v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We present a model for layered security with applications to the protection
of sites such as stadiums or large gathering places. We formulate the problem
as one of maximizing the capture of illegal contraband. The objective function
is indefinite and only limited information can be gained when the problem is
solved by standard convex optimization methods. In order to solve the model, we
develop a dynamic programming approach, and study its convergence properties.
Additionally, we formulate a version of the problem aimed at addressing
intelligent adversaries who can adjust their direction of attack as they
observe changes in the site security. Furthermore, we also develop a method for
the solution of the latter model. Finally, we perform computational experiments
to demonstrate the use of our methods.

### Title: Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge
* Paper ID: 2204.08189v1
* Paper URL: [http://arxiv.org/abs/2204.08189v1](http://arxiv.org/abs/2204.08189v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: Adversarial example attack endangers the mobile edge systems such as vehicles
and drones that adopt deep neural networks for visual sensing. This paper
presents {\em Sardino}, an active and dynamic defense approach that renews the
inference ensemble at run time to develop security against the adaptive
adversary who tries to exfiltrate the ensemble and construct the corresponding
effective adversarial examples. By applying consistency check and data fusion
on the ensemble's predictions, Sardino can detect and thwart adversarial
inputs. Compared with the training-based ensemble renewal, we use HyperNet to
achieve {\em one million times} acceleration and per-frame ensemble renewal
that presents the highest level of difficulty to the prerequisite exfiltration
attacks. Moreover, the robustness of the renewed ensembles against adversarial
examples is enhanced with adversarial learning for the HyperNet. We design a
run-time planner that maximizes the ensemble size in favor of security while
maintaining the processing frame rate. Beyond adversarial examples, Sardino can
also address the issue of out-of-distribution inputs effectively. This paper
presents extensive evaluation of Sardino's performance in counteracting
adversarial examples and applies it to build a real-time car-borne traffic sign
recognition system. Live on-road tests show the built system's effectiveness in
maintaining frame rate and detecting out-of-distribution inputs due to the
false positives of a preceding YOLO-based traffic sign detector.

### Title: Securing Signal-free Intersections against Strategic Jamming Attacks: A Macroscopic Approach
* Paper ID: 2204.08187v1
* Paper URL: [http://arxiv.org/abs/2204.08187v1](http://arxiv.org/abs/2204.08187v1)
* Updated Date: 2022-04-18
* Code URL: null
* Summary: We consider the security-by-design of a signal-free intersection for
connected and autonomous vehicles in the face of strategic jamming attacks. We
use a fluid model to characterize macroscopic traffic flow through the
intersection, where the saturation rate is derived from a vehicle coordination
algorithm. We model jamming attacks as sudden increase in communication latency
induced on vehicle-to-infrastructure connectivity; such latency triggers the
safety mode for vehicle coordination and thus reduces the intersection
saturation rate. A strategic attacker selects the attacking rate, while a
system operator selects key design parameters, either the saturation rate or
the recovery rate. Both players' actions induce technological costs and jointly
determine the mean travel delay. By analyzing the equilibrium of the security
game, we study the preferable level of investment in the intersection's nominal
discharging capability or recovery capability, for balance between
hardware/infrastructure cost and security-by-design.

