### Title: BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced Taint Tracking
* Paper ID: 2204.09649v1
* Paper URL: [http://arxiv.org/abs/2204.09649v1](http://arxiv.org/abs/2204.09649v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We present Blinded Memory (BliMe), a way to realize efficient and secure
outsourced computation. BliMe consists of a novel and minimal set of ISA
extensions that uses taint tracking to ensure the confidentiality of sensitive
(client) data even in the presence of server malware, run-time attacks, and
side-channel attacks. To secure outsourced computation, the BliMe extensions
can be used together with an attestable, fixed-function trusted execution
environment (TEE) and an encryption engine that provides atomic
decrypt-and-taint and encrypt-and-untaint operations. The TEE engages in an
attestation and key agreement protocol with the client. It provides the
resulting client-specific keys to the encryption engine. Clients rely on remote
attestation to ensure that their data will always be protected by BliMe's taint
tracking policy after decryption. We provide a machine-checked security proof
and an FPGA implementation (BliMe-Ibex) of BliMe's taint tracking policy. We
show that BliMe-Ibex does not reduce performance relative to the unmodified
core, and incurs only minor increases in resource consumption in terms of power
($<2\%$), LUTs ($<1\%$), and registers ($<3\%$).

### Title: Backdooring Explainable Machine Learning
* Paper ID: 2204.09498v1
* Paper URL: [http://arxiv.org/abs/2204.09498v1](http://arxiv.org/abs/2204.09498v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Explainable machine learning holds great potential for analyzing and
understanding learning-based systems. These methods can, however, be
manipulated to present unfaithful explanations, giving rise to powerful and
stealthy adversaries. In this paper, we demonstrate blinding attacks that can
fully disguise an ongoing attack against the machine learning model. Similar to
neural backdoors, we modify the model's prediction upon trigger presence but
simultaneously also fool the provided explanation. This enables an adversary to
hide the presence of the trigger or point the explanation to entirely different
portions of the input, throwing a red herring. We analyze different
manifestations of such attacks for different explanation types in the image
domain, before we resume to conduct a red-herring attack against malware
classification.

### Title: SiamHAN: IPv6 Address Correlation Attacks on TLS Encrypted Traffic via Siamese Heterogeneous Graph Attention Network
* Paper ID: 2204.09465v1
* Paper URL: [http://arxiv.org/abs/2204.09465v1](http://arxiv.org/abs/2204.09465v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/cuitianyu961030/siamhan](https://github.com/cuitianyu961030/siamhan)
* Summary: Unlike IPv4 addresses, which are typically masked by a NAT, IPv6 addresses
could easily be correlated with user activity, endangering their privacy.
Mitigations to address this privacy concern have been deployed, making existing
approaches for address-to-user correlation unreliable. This work demonstrates
that an adversary could still correlate IPv6 addresses with users accurately,
even with these protection mechanisms. To do this, we propose an IPv6 address
correlation model - SiamHAN. The model uses a Siamese Heterogeneous Graph
Attention Network to measure whether two IPv6 client addresses belong to the
same user even if the user's traffic is protected by TLS encryption. Using a
large real-world dataset, we show that, for the tasks of tracking target users
and discovering unique users, the state-of-the-art techniques could achieve
only 85% and 60% accuracy, respectively. However, SiamHAN exhibits 99% and 88%
accuracy.

### Title: Adversarial Scratches: Deployable Attacks to CNN Classifiers
* Paper ID: 2204.09397v1
* Paper URL: [http://arxiv.org/abs/2204.09397v1](http://arxiv.org/abs/2204.09397v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/loris2222/adversarialscratches](https://github.com/loris2222/adversarialscratches)
* Summary: A growing body of work has shown that deep neural networks are susceptible to
adversarial examples. These take the form of small perturbations applied to the
model's input which lead to incorrect predictions. Unfortunately, most
literature focuses on visually imperceivable perturbations to be applied to
digital images that often are, by design, impossible to be deployed to physical
targets. We present Adversarial Scratches: a novel L0 black-box attack, which
takes the form of scratches in images, and which possesses much greater
deployability than other state-of-the-art attacks. Adversarial Scratches
leverage B\'ezier Curves to reduce the dimension of the search space and
possibly constrain the attack to a specific location. We test Adversarial
Scratches in several scenarios, including a publicly available API and images
of traffic signs. Results show that, often, our attack achieves higher fooling
rate than other deployable state-of-the-art methods, while requiring
significantly fewer queries and modifying very few pixels.

### Title: You Are What You Write: Preserving Privacy in the Era of Large Language Models
* Paper ID: 2204.09391v1
* Paper URL: [http://arxiv.org/abs/2204.09391v1](http://arxiv.org/abs/2204.09391v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques.

### Title: Runtime Prevention of Deserialization Attacks
* Paper ID: 2204.09388v1
* Paper URL: [http://arxiv.org/abs/2204.09388v1](http://arxiv.org/abs/2204.09388v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Untrusted deserialization exploits, where a serialised object graph is used
to achieve denial-of-service or arbitrary code execution, have become so
prominent that they were introduced in the 2017 OWASP Top 10. In this paper, we
present a novel and lightweight approach for runtime prevention of
deserialization attacks using Markov chains. The intuition behind our work is
that the features and ordering of classes in malicious object graphs make them
distinguishable from benign ones. Preliminary results indeed show that our
approach achieves an F1-score of 0.94 on a dataset of 264 serialised payloads,
collected from an industrial Java EE application server and a repository of
deserialization exploits.

### Title: Causality-based Neural Network Repair
* Paper ID: 2204.09274v1
* Paper URL: [http://arxiv.org/abs/2204.09274v1](http://arxiv.org/abs/2204.09274v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Neural networks have had discernible achievements in a wide range of
applications. The wide-spread adoption also raises the concern of their
dependability and reliability. Similar to traditional decision-making programs,
neural networks can have defects that need to be repaired. The defects may
cause unsafe behaviors, raise security concerns or unjust societal impacts. In
this work, we address the problem of repairing a neural network for desirable
properties such as fairness and the absence of backdoor. The goal is to
construct a neural network that satisfies the property by (minimally) adjusting
the given neural network's parameters (i.e., weights). Specifically, we propose
CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural
network repair technique that 1) performs causality-based fault localization to
identify the `guilty' neurons and 2) optimizes the parameters of the identified
neurons to reduce the misbehavior. We have empirically evaluated CARE on
various tasks such as backdoor removal, neural network repair for fairness and
safety properties. Our experiment results show that CARE is able to repair all
neural networks efficiently and effectively. For fairness repair tasks, CARE
successfully improves fairness by $61.91\%$ on average. For backdoor removal
tasks, CARE reduces the attack success rate from over $98\%$ to less than
$1\%$. For safety property repair tasks, CARE reduces the property violation
rate to less than $1\%$. Results also show that thanks to the causality-based
fault localization, CARE's repair focuses on the misbehavior and preserves the
accuracy of the neural networks.

### Title: STPA-driven Multilevel Runtime Monitoring for In-time Hazard Detection
* Paper ID: 2204.08999v2
* Paper URL: [http://arxiv.org/abs/2204.08999v2](http://arxiv.org/abs/2204.08999v2)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Runtime verification or runtime monitoring equips safety-critical
cyber-physical systems to augment design assurance measures and ensure
operational safety and security. Cyber-physical systems have interaction
failures, attack surfaces, and attack vectors resulting in unanticipated
hazards and loss scenarios. These interaction failures pose challenges to
runtime verification regarding monitoring specifications and monitoring
placements for in-time detection of hazards. We develop a well-formed workflow
model that connects system theoretic process analysis, commonly referred to as
STPA, hazard causation information to lower-level runtime monitoring to detect
hazards at the operational phase. Specifically, our model follows the DepDevOps
paradigm to provide evidence and insights to runtime monitoring on what to
monitor, where to monitor, and the monitoring context. We demonstrate and
evaluate the value of multilevel monitors by injecting hazards on an autonomous
emergency braking system model.

### Title: GUARD: Graph Universal Adversarial Defense
* Paper ID: 2204.09803v1
* Paper URL: [http://arxiv.org/abs/2204.09803v1](http://arxiv.org/abs/2204.09803v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/edisonleeeee/guard](https://github.com/edisonleeeee/guard)
* Summary: Recently, graph convolutional networks (GCNs) have shown to be vulnerable to
small adversarial perturbations, which becomes a severe threat and largely
limits their applications in security-critical scenarios. To mitigate such a
threat, considerable research efforts have been devoted to increasing the
robustness of GCNs against adversarial attacks. However, current approaches for
defense are typically designed for the whole graph and consider the global
performance, posing challenges in protecting important local nodes from
stronger adversarial targeted attacks. In this work, we present a simple yet
effective method, named \textbf{\underline{G}}raph
\textbf{\underline{U}}niversal
\textbf{\underline{A}}dve\textbf{\underline{R}}sarial
\textbf{\underline{D}}efense (GUARD). Unlike previous works, GUARD protects
each individual node from attacks with a universal defensive patch, which is
generated once and can be applied to any node (node-agnostic) in a graph.
Extensive experiments on four benchmark datasets demonstrate that our method
significantly improves robustness for several established GCNs against multiple
adversarial attacks and outperforms existing adversarial defense methods by
large margins. Our code is publicly available at
https://github.com/EdisonLeeeee/GUARD.

### Title: ARLIF-IDS -- Attention augmented Real-Time Isolation Forest Intrusion Detection System
* Paper ID: 2204.09737v1
* Paper URL: [http://arxiv.org/abs/2204.09737v1](http://arxiv.org/abs/2204.09737v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Distributed Denial of Service (DDoS) attack is a malicious attempt to disrupt
the normal traffic of a targeted server, service or network by overwhelming the
target or its surrounding infrastructure with a flood of Internet traffic.
Emerging technologies such as the Internet of Things and Software Defined
Networking leverage lightweight strategies for the early detection of DDoS
attacks. Previous literature demonstrates the utility of lower number of
significant features for intrusion detection. Thus, it is essential to have a
fast and effective security identification model based on low number of
features.
  In this work, a novel Attention-based Isolation Forest Intrusion Detection
System is proposed. The model considerably reduces training time and memory
consumption of the generated model. For performance assessment, the model is
assessed over two benchmark datasets, the NSL-KDD dataset & the KDDCUP'99
dataset. Experimental results demonstrate that the proposed attention augmented
model achieves a significant reduction in execution time, by 91.78%, and an
average detection F1-Score of 0.93 on the NSL-KDD and KDDCUP'99 dataset. The
results of performance evaluation show that the proposed methodology has low
complexity and requires less processing time and computational resources,
outperforming other current IDS based on machine learning algorithms.

### Title: BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced Taint Tracking
* Paper ID: 2204.09649v1
* Paper URL: [http://arxiv.org/abs/2204.09649v1](http://arxiv.org/abs/2204.09649v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We present Blinded Memory (BliMe), a way to realize efficient and secure
outsourced computation. BliMe consists of a novel and minimal set of ISA
extensions that uses taint tracking to ensure the confidentiality of sensitive
(client) data even in the presence of server malware, run-time attacks, and
side-channel attacks. To secure outsourced computation, the BliMe extensions
can be used together with an attestable, fixed-function trusted execution
environment (TEE) and an encryption engine that provides atomic
decrypt-and-taint and encrypt-and-untaint operations. The TEE engages in an
attestation and key agreement protocol with the client. It provides the
resulting client-specific keys to the encryption engine. Clients rely on remote
attestation to ensure that their data will always be protected by BliMe's taint
tracking policy after decryption. We provide a machine-checked security proof
and an FPGA implementation (BliMe-Ibex) of BliMe's taint tracking policy. We
show that BliMe-Ibex does not reduce performance relative to the unmodified
core, and incurs only minor increases in resource consumption in terms of power
($<2\%$), LUTs ($<1\%$), and registers ($<3\%$).

### Title: Backdooring Explainable Machine Learning
* Paper ID: 2204.09498v1
* Paper URL: [http://arxiv.org/abs/2204.09498v1](http://arxiv.org/abs/2204.09498v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Explainable machine learning holds great potential for analyzing and
understanding learning-based systems. These methods can, however, be
manipulated to present unfaithful explanations, giving rise to powerful and
stealthy adversaries. In this paper, we demonstrate blinding attacks that can
fully disguise an ongoing attack against the machine learning model. Similar to
neural backdoors, we modify the model's prediction upon trigger presence but
simultaneously also fool the provided explanation. This enables an adversary to
hide the presence of the trigger or point the explanation to entirely different
portions of the input, throwing a red herring. We analyze different
manifestations of such attacks for different explanation types in the image
domain, before we resume to conduct a red-herring attack against malware
classification.

### Title: SiamHAN: IPv6 Address Correlation Attacks on TLS Encrypted Traffic via Siamese Heterogeneous Graph Attention Network
* Paper ID: 2204.09465v1
* Paper URL: [http://arxiv.org/abs/2204.09465v1](http://arxiv.org/abs/2204.09465v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/cuitianyu961030/siamhan](https://github.com/cuitianyu961030/siamhan)
* Summary: Unlike IPv4 addresses, which are typically masked by a NAT, IPv6 addresses
could easily be correlated with user activity, endangering their privacy.
Mitigations to address this privacy concern have been deployed, making existing
approaches for address-to-user correlation unreliable. This work demonstrates
that an adversary could still correlate IPv6 addresses with users accurately,
even with these protection mechanisms. To do this, we propose an IPv6 address
correlation model - SiamHAN. The model uses a Siamese Heterogeneous Graph
Attention Network to measure whether two IPv6 client addresses belong to the
same user even if the user's traffic is protected by TLS encryption. Using a
large real-world dataset, we show that, for the tasks of tracking target users
and discovering unique users, the state-of-the-art techniques could achieve
only 85% and 60% accuracy, respectively. However, SiamHAN exhibits 99% and 88%
accuracy.

### Title: Adversarial Scratches: Deployable Attacks to CNN Classifiers
* Paper ID: 2204.09397v1
* Paper URL: [http://arxiv.org/abs/2204.09397v1](http://arxiv.org/abs/2204.09397v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/loris2222/adversarialscratches](https://github.com/loris2222/adversarialscratches)
* Summary: A growing body of work has shown that deep neural networks are susceptible to
adversarial examples. These take the form of small perturbations applied to the
model's input which lead to incorrect predictions. Unfortunately, most
literature focuses on visually imperceivable perturbations to be applied to
digital images that often are, by design, impossible to be deployed to physical
targets. We present Adversarial Scratches: a novel L0 black-box attack, which
takes the form of scratches in images, and which possesses much greater
deployability than other state-of-the-art attacks. Adversarial Scratches
leverage B\'ezier Curves to reduce the dimension of the search space and
possibly constrain the attack to a specific location. We test Adversarial
Scratches in several scenarios, including a publicly available API and images
of traffic signs. Results show that, often, our attack achieves higher fooling
rate than other deployable state-of-the-art methods, while requiring
significantly fewer queries and modifying very few pixels.

### Title: You Are What You Write: Preserving Privacy in the Era of Large Language Models
* Paper ID: 2204.09391v1
* Paper URL: [http://arxiv.org/abs/2204.09391v1](http://arxiv.org/abs/2204.09391v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques.

### Title: Runtime Prevention of Deserialization Attacks
* Paper ID: 2204.09388v1
* Paper URL: [http://arxiv.org/abs/2204.09388v1](http://arxiv.org/abs/2204.09388v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Untrusted deserialization exploits, where a serialised object graph is used
to achieve denial-of-service or arbitrary code execution, have become so
prominent that they were introduced in the 2017 OWASP Top 10. In this paper, we
present a novel and lightweight approach for runtime prevention of
deserialization attacks using Markov chains. The intuition behind our work is
that the features and ordering of classes in malicious object graphs make them
distinguishable from benign ones. Preliminary results indeed show that our
approach achieves an F1-score of 0.94 on a dataset of 264 serialised payloads,
collected from an industrial Java EE application server and a repository of
deserialization exploits.

### Title: Causality-based Neural Network Repair
* Paper ID: 2204.09274v1
* Paper URL: [http://arxiv.org/abs/2204.09274v1](http://arxiv.org/abs/2204.09274v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Neural networks have had discernible achievements in a wide range of
applications. The wide-spread adoption also raises the concern of their
dependability and reliability. Similar to traditional decision-making programs,
neural networks can have defects that need to be repaired. The defects may
cause unsafe behaviors, raise security concerns or unjust societal impacts. In
this work, we address the problem of repairing a neural network for desirable
properties such as fairness and the absence of backdoor. The goal is to
construct a neural network that satisfies the property by (minimally) adjusting
the given neural network's parameters (i.e., weights). Specifically, we propose
CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural
network repair technique that 1) performs causality-based fault localization to
identify the `guilty' neurons and 2) optimizes the parameters of the identified
neurons to reduce the misbehavior. We have empirically evaluated CARE on
various tasks such as backdoor removal, neural network repair for fairness and
safety properties. Our experiment results show that CARE is able to repair all
neural networks efficiently and effectively. For fairness repair tasks, CARE
successfully improves fairness by $61.91\%$ on average. For backdoor removal
tasks, CARE reduces the attack success rate from over $98\%$ to less than
$1\%$. For safety property repair tasks, CARE reduces the property violation
rate to less than $1\%$. Results also show that thanks to the causality-based
fault localization, CARE's repair focuses on the misbehavior and preserves the
accuracy of the neural networks.

### Title: GUARD: Graph Universal Adversarial Defense
* Paper ID: 2204.09803v1
* Paper URL: [http://arxiv.org/abs/2204.09803v1](http://arxiv.org/abs/2204.09803v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/edisonleeeee/guard](https://github.com/edisonleeeee/guard)
* Summary: Recently, graph convolutional networks (GCNs) have shown to be vulnerable to
small adversarial perturbations, which becomes a severe threat and largely
limits their applications in security-critical scenarios. To mitigate such a
threat, considerable research efforts have been devoted to increasing the
robustness of GCNs against adversarial attacks. However, current approaches for
defense are typically designed for the whole graph and consider the global
performance, posing challenges in protecting important local nodes from
stronger adversarial targeted attacks. In this work, we present a simple yet
effective method, named \textbf{\underline{G}}raph
\textbf{\underline{U}}niversal
\textbf{\underline{A}}dve\textbf{\underline{R}}sarial
\textbf{\underline{D}}efense (GUARD). Unlike previous works, GUARD protects
each individual node from attacks with a universal defensive patch, which is
generated once and can be applied to any node (node-agnostic) in a graph.
Extensive experiments on four benchmark datasets demonstrate that our method
significantly improves robustness for several established GCNs against multiple
adversarial attacks and outperforms existing adversarial defense methods by
large margins. Our code is publicly available at
https://github.com/EdisonLeeeee/GUARD.

### Title: ARLIF-IDS -- Attention augmented Real-Time Isolation Forest Intrusion Detection System
* Paper ID: 2204.09737v1
* Paper URL: [http://arxiv.org/abs/2204.09737v1](http://arxiv.org/abs/2204.09737v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Distributed Denial of Service (DDoS) attack is a malicious attempt to disrupt
the normal traffic of a targeted server, service or network by overwhelming the
target or its surrounding infrastructure with a flood of Internet traffic.
Emerging technologies such as the Internet of Things and Software Defined
Networking leverage lightweight strategies for the early detection of DDoS
attacks. Previous literature demonstrates the utility of lower number of
significant features for intrusion detection. Thus, it is essential to have a
fast and effective security identification model based on low number of
features.
  In this work, a novel Attention-based Isolation Forest Intrusion Detection
System is proposed. The model considerably reduces training time and memory
consumption of the generated model. For performance assessment, the model is
assessed over two benchmark datasets, the NSL-KDD dataset & the KDDCUP'99
dataset. Experimental results demonstrate that the proposed attention augmented
model achieves a significant reduction in execution time, by 91.78%, and an
average detection F1-Score of 0.93 on the NSL-KDD and KDDCUP'99 dataset. The
results of performance evaluation show that the proposed methodology has low
complexity and requires less processing time and computational resources,
outperforming other current IDS based on machine learning algorithms.

### Title: BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced Taint Tracking
* Paper ID: 2204.09649v1
* Paper URL: [http://arxiv.org/abs/2204.09649v1](http://arxiv.org/abs/2204.09649v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: We present Blinded Memory (BliMe), a way to realize efficient and secure
outsourced computation. BliMe consists of a novel and minimal set of ISA
extensions that uses taint tracking to ensure the confidentiality of sensitive
(client) data even in the presence of server malware, run-time attacks, and
side-channel attacks. To secure outsourced computation, the BliMe extensions
can be used together with an attestable, fixed-function trusted execution
environment (TEE) and an encryption engine that provides atomic
decrypt-and-taint and encrypt-and-untaint operations. The TEE engages in an
attestation and key agreement protocol with the client. It provides the
resulting client-specific keys to the encryption engine. Clients rely on remote
attestation to ensure that their data will always be protected by BliMe's taint
tracking policy after decryption. We provide a machine-checked security proof
and an FPGA implementation (BliMe-Ibex) of BliMe's taint tracking policy. We
show that BliMe-Ibex does not reduce performance relative to the unmodified
core, and incurs only minor increases in resource consumption in terms of power
($<2\%$), LUTs ($<1\%$), and registers ($<3\%$).

### Title: Backdooring Explainable Machine Learning
* Paper ID: 2204.09498v1
* Paper URL: [http://arxiv.org/abs/2204.09498v1](http://arxiv.org/abs/2204.09498v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Explainable machine learning holds great potential for analyzing and
understanding learning-based systems. These methods can, however, be
manipulated to present unfaithful explanations, giving rise to powerful and
stealthy adversaries. In this paper, we demonstrate blinding attacks that can
fully disguise an ongoing attack against the machine learning model. Similar to
neural backdoors, we modify the model's prediction upon trigger presence but
simultaneously also fool the provided explanation. This enables an adversary to
hide the presence of the trigger or point the explanation to entirely different
portions of the input, throwing a red herring. We analyze different
manifestations of such attacks for different explanation types in the image
domain, before we resume to conduct a red-herring attack against malware
classification.

### Title: SiamHAN: IPv6 Address Correlation Attacks on TLS Encrypted Traffic via Siamese Heterogeneous Graph Attention Network
* Paper ID: 2204.09465v1
* Paper URL: [http://arxiv.org/abs/2204.09465v1](http://arxiv.org/abs/2204.09465v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/cuitianyu961030/siamhan](https://github.com/cuitianyu961030/siamhan)
* Summary: Unlike IPv4 addresses, which are typically masked by a NAT, IPv6 addresses
could easily be correlated with user activity, endangering their privacy.
Mitigations to address this privacy concern have been deployed, making existing
approaches for address-to-user correlation unreliable. This work demonstrates
that an adversary could still correlate IPv6 addresses with users accurately,
even with these protection mechanisms. To do this, we propose an IPv6 address
correlation model - SiamHAN. The model uses a Siamese Heterogeneous Graph
Attention Network to measure whether two IPv6 client addresses belong to the
same user even if the user's traffic is protected by TLS encryption. Using a
large real-world dataset, we show that, for the tasks of tracking target users
and discovering unique users, the state-of-the-art techniques could achieve
only 85% and 60% accuracy, respectively. However, SiamHAN exhibits 99% and 88%
accuracy.

### Title: Adversarial Scratches: Deployable Attacks to CNN Classifiers
* Paper ID: 2204.09397v1
* Paper URL: [http://arxiv.org/abs/2204.09397v1](http://arxiv.org/abs/2204.09397v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/loris2222/adversarialscratches](https://github.com/loris2222/adversarialscratches)
* Summary: A growing body of work has shown that deep neural networks are susceptible to
adversarial examples. These take the form of small perturbations applied to the
model's input which lead to incorrect predictions. Unfortunately, most
literature focuses on visually imperceivable perturbations to be applied to
digital images that often are, by design, impossible to be deployed to physical
targets. We present Adversarial Scratches: a novel L0 black-box attack, which
takes the form of scratches in images, and which possesses much greater
deployability than other state-of-the-art attacks. Adversarial Scratches
leverage B\'ezier Curves to reduce the dimension of the search space and
possibly constrain the attack to a specific location. We test Adversarial
Scratches in several scenarios, including a publicly available API and images
of traffic signs. Results show that, often, our attack achieves higher fooling
rate than other deployable state-of-the-art methods, while requiring
significantly fewer queries and modifying very few pixels.

### Title: You Are What You Write: Preserving Privacy in the Era of Large Language Models
* Paper ID: 2204.09391v1
* Paper URL: [http://arxiv.org/abs/2204.09391v1](http://arxiv.org/abs/2204.09391v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques.

### Title: Runtime Prevention of Deserialization Attacks
* Paper ID: 2204.09388v1
* Paper URL: [http://arxiv.org/abs/2204.09388v1](http://arxiv.org/abs/2204.09388v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Untrusted deserialization exploits, where a serialised object graph is used
to achieve denial-of-service or arbitrary code execution, have become so
prominent that they were introduced in the 2017 OWASP Top 10. In this paper, we
present a novel and lightweight approach for runtime prevention of
deserialization attacks using Markov chains. The intuition behind our work is
that the features and ordering of classes in malicious object graphs make them
distinguishable from benign ones. Preliminary results indeed show that our
approach achieves an F1-score of 0.94 on a dataset of 264 serialised payloads,
collected from an industrial Java EE application server and a repository of
deserialization exploits.

### Title: Causality-based Neural Network Repair
* Paper ID: 2204.09274v1
* Paper URL: [http://arxiv.org/abs/2204.09274v1](http://arxiv.org/abs/2204.09274v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Neural networks have had discernible achievements in a wide range of
applications. The wide-spread adoption also raises the concern of their
dependability and reliability. Similar to traditional decision-making programs,
neural networks can have defects that need to be repaired. The defects may
cause unsafe behaviors, raise security concerns or unjust societal impacts. In
this work, we address the problem of repairing a neural network for desirable
properties such as fairness and the absence of backdoor. The goal is to
construct a neural network that satisfies the property by (minimally) adjusting
the given neural network's parameters (i.e., weights). Specifically, we propose
CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural
network repair technique that 1) performs causality-based fault localization to
identify the `guilty' neurons and 2) optimizes the parameters of the identified
neurons to reduce the misbehavior. We have empirically evaluated CARE on
various tasks such as backdoor removal, neural network repair for fairness and
safety properties. Our experiment results show that CARE is able to repair all
neural networks efficiently and effectively. For fairness repair tasks, CARE
successfully improves fairness by $61.91\%$ on average. For backdoor removal
tasks, CARE reduces the attack success rate from over $98\%$ to less than
$1\%$. For safety property repair tasks, CARE reduces the property violation
rate to less than $1\%$. Results also show that thanks to the causality-based
fault localization, CARE's repair focuses on the misbehavior and preserves the
accuracy of the neural networks.

### Title: GUARD: Graph Universal Adversarial Defense
* Paper ID: 2204.09803v1
* Paper URL: [http://arxiv.org/abs/2204.09803v1](http://arxiv.org/abs/2204.09803v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/edisonleeeee/guard](https://github.com/edisonleeeee/guard)
* Summary: Recently, graph convolutional networks (GCNs) have shown to be vulnerable to
small adversarial perturbations, which becomes a severe threat and largely
limits their applications in security-critical scenarios. To mitigate such a
threat, considerable research efforts have been devoted to increasing the
robustness of GCNs against adversarial attacks. However, current approaches for
defense are typically designed for the whole graph and consider the global
performance, posing challenges in protecting important local nodes from
stronger adversarial targeted attacks. In this work, we present a simple yet
effective method, named \textbf{\underline{G}}raph
\textbf{\underline{U}}niversal
\textbf{\underline{A}}dve\textbf{\underline{R}}sarial
\textbf{\underline{D}}efense (GUARD). Unlike previous works, GUARD protects
each individual node from attacks with a universal defensive patch, which is
generated once and can be applied to any node (node-agnostic) in a graph.
Extensive experiments on four benchmark datasets demonstrate that our method
significantly improves robustness for several established GCNs against multiple
adversarial attacks and outperforms existing adversarial defense methods by
large margins. Our code is publicly available at
https://github.com/EdisonLeeeee/GUARD.

### Title: ARLIF-IDS -- Attention augmented Real-Time Isolation Forest Intrusion Detection System
* Paper ID: 2204.09737v1
* Paper URL: [http://arxiv.org/abs/2204.09737v1](http://arxiv.org/abs/2204.09737v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Distributed Denial of Service (DDoS) attack is a malicious attempt to disrupt
the normal traffic of a targeted server, service or network by overwhelming the
target or its surrounding infrastructure with a flood of Internet traffic.
Emerging technologies such as the Internet of Things and Software Defined
Networking leverage lightweight strategies for the early detection of DDoS
attacks. Previous literature demonstrates the utility of lower number of
significant features for intrusion detection. Thus, it is essential to have a
fast and effective security identification model based on low number of
features.
  In this work, a novel Attention-based Isolation Forest Intrusion Detection
System is proposed. The model considerably reduces training time and memory
consumption of the generated model. For performance assessment, the model is
assessed over two benchmark datasets, the NSL-KDD dataset & the KDDCUP'99
dataset. Experimental results demonstrate that the proposed attention augmented
model achieves a significant reduction in execution time, by 91.78%, and an
average detection F1-Score of 0.93 on the NSL-KDD and KDDCUP'99 dataset. The
results of performance evaluation show that the proposed methodology has low
complexity and requires less processing time and computational resources,
outperforming other current IDS based on machine learning algorithms.

### Title: Backdooring Explainable Machine Learning
* Paper ID: 2204.09498v1
* Paper URL: [http://arxiv.org/abs/2204.09498v1](http://arxiv.org/abs/2204.09498v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Explainable machine learning holds great potential for analyzing and
understanding learning-based systems. These methods can, however, be
manipulated to present unfaithful explanations, giving rise to powerful and
stealthy adversaries. In this paper, we demonstrate blinding attacks that can
fully disguise an ongoing attack against the machine learning model. Similar to
neural backdoors, we modify the model's prediction upon trigger presence but
simultaneously also fool the provided explanation. This enables an adversary to
hide the presence of the trigger or point the explanation to entirely different
portions of the input, throwing a red herring. We analyze different
manifestations of such attacks for different explanation types in the image
domain, before we resume to conduct a red-herring attack against malware
classification.

### Title: SiamHAN: IPv6 Address Correlation Attacks on TLS Encrypted Traffic via Siamese Heterogeneous Graph Attention Network
* Paper ID: 2204.09465v1
* Paper URL: [http://arxiv.org/abs/2204.09465v1](http://arxiv.org/abs/2204.09465v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/cuitianyu961030/siamhan](https://github.com/cuitianyu961030/siamhan)
* Summary: Unlike IPv4 addresses, which are typically masked by a NAT, IPv6 addresses
could easily be correlated with user activity, endangering their privacy.
Mitigations to address this privacy concern have been deployed, making existing
approaches for address-to-user correlation unreliable. This work demonstrates
that an adversary could still correlate IPv6 addresses with users accurately,
even with these protection mechanisms. To do this, we propose an IPv6 address
correlation model - SiamHAN. The model uses a Siamese Heterogeneous Graph
Attention Network to measure whether two IPv6 client addresses belong to the
same user even if the user's traffic is protected by TLS encryption. Using a
large real-world dataset, we show that, for the tasks of tracking target users
and discovering unique users, the state-of-the-art techniques could achieve
only 85% and 60% accuracy, respectively. However, SiamHAN exhibits 99% and 88%
accuracy.

### Title: Adversarial Scratches: Deployable Attacks to CNN Classifiers
* Paper ID: 2204.09397v1
* Paper URL: [http://arxiv.org/abs/2204.09397v1](http://arxiv.org/abs/2204.09397v1)
* Updated Date: 2022-04-20
* Code URL: [https://github.com/loris2222/adversarialscratches](https://github.com/loris2222/adversarialscratches)
* Summary: A growing body of work has shown that deep neural networks are susceptible to
adversarial examples. These take the form of small perturbations applied to the
model's input which lead to incorrect predictions. Unfortunately, most
literature focuses on visually imperceivable perturbations to be applied to
digital images that often are, by design, impossible to be deployed to physical
targets. We present Adversarial Scratches: a novel L0 black-box attack, which
takes the form of scratches in images, and which possesses much greater
deployability than other state-of-the-art attacks. Adversarial Scratches
leverage B\'ezier Curves to reduce the dimension of the search space and
possibly constrain the attack to a specific location. We test Adversarial
Scratches in several scenarios, including a publicly available API and images
of traffic signs. Results show that, often, our attack achieves higher fooling
rate than other deployable state-of-the-art methods, while requiring
significantly fewer queries and modifying very few pixels.

### Title: You Are What You Write: Preserving Privacy in the Era of Large Language Models
* Paper ID: 2204.09391v1
* Paper URL: [http://arxiv.org/abs/2204.09391v1](http://arxiv.org/abs/2204.09391v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Large scale adoption of large language models has introduced a new era of
convenient knowledge transfer for a slew of natural language processing tasks.
However, these models also run the risk of undermining user trust by exposing
unwanted information about the data subjects, which may be extracted by a
malicious party, e.g. through adversarial attacks. We present an empirical
investigation into the extent of the personal information encoded into
pre-trained representations by a range of popular models, and we show a
positive correlation between the complexity of a model, the amount of data used
in pre-training, and data leakage. In this paper, we present the first wide
coverage evaluation and comparison of some of the most popular
privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment
analysis annotated with demographic information (location, age and gender). The
results show since larger and more complex models are more prone to leaking
private information, use of privacy-preserving methods is highly desirable. We
also find that highly privacy-preserving technologies like differential privacy
(DP) can have serious model utility effects, which can be ameliorated using
hybrid or metric-DP techniques.

### Title: Runtime Prevention of Deserialization Attacks
* Paper ID: 2204.09388v1
* Paper URL: [http://arxiv.org/abs/2204.09388v1](http://arxiv.org/abs/2204.09388v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Untrusted deserialization exploits, where a serialised object graph is used
to achieve denial-of-service or arbitrary code execution, have become so
prominent that they were introduced in the 2017 OWASP Top 10. In this paper, we
present a novel and lightweight approach for runtime prevention of
deserialization attacks using Markov chains. The intuition behind our work is
that the features and ordering of classes in malicious object graphs make them
distinguishable from benign ones. Preliminary results indeed show that our
approach achieves an F1-score of 0.94 on a dataset of 264 serialised payloads,
collected from an industrial Java EE application server and a repository of
deserialization exploits.

### Title: Causality-based Neural Network Repair
* Paper ID: 2204.09274v1
* Paper URL: [http://arxiv.org/abs/2204.09274v1](http://arxiv.org/abs/2204.09274v1)
* Updated Date: 2022-04-20
* Code URL: null
* Summary: Neural networks have had discernible achievements in a wide range of
applications. The wide-spread adoption also raises the concern of their
dependability and reliability. Similar to traditional decision-making programs,
neural networks can have defects that need to be repaired. The defects may
cause unsafe behaviors, raise security concerns or unjust societal impacts. In
this work, we address the problem of repairing a neural network for desirable
properties such as fairness and the absence of backdoor. The goal is to
construct a neural network that satisfies the property by (minimally) adjusting
the given neural network's parameters (i.e., weights). Specifically, we propose
CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural
network repair technique that 1) performs causality-based fault localization to
identify the `guilty' neurons and 2) optimizes the parameters of the identified
neurons to reduce the misbehavior. We have empirically evaluated CARE on
various tasks such as backdoor removal, neural network repair for fairness and
safety properties. Our experiment results show that CARE is able to repair all
neural networks efficiently and effectively. For fairness repair tasks, CARE
successfully improves fairness by $61.91\%$ on average. For backdoor removal
tasks, CARE reduces the attack success rate from over $98\%$ to less than
$1\%$. For safety property repair tasks, CARE reduces the property violation
rate to less than $1\%$. Results also show that thanks to the causality-based
fault localization, CARE's repair focuses on the misbehavior and preserves the
accuracy of the neural networks.

