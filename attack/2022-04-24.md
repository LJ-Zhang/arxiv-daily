### Title: Improving Deep Learning Model Robustness Against Adversarial Attack by Increasing the Network Capacity
* Paper ID: 2204.11357v1
* Paper URL: [http://arxiv.org/abs/2204.11357v1](http://arxiv.org/abs/2204.11357v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: Nowadays, we are more and more reliant on Deep Learning (DL) models and thus
it is essential to safeguard the security of these systems. This paper explores
the security issues in Deep Learning and analyses, through the use of
experiments, the way forward to build more resilient models. Experiments are
conducted to identify the strengths and weaknesses of a new approach to improve
the robustness of DL models against adversarial attacks. The results show
improvements and new ideas that can be used as recommendations for researchers
and practitioners to create increasingly better DL algorithms.

### Title: Learning to Attack Powergrids with DERs
* Paper ID: 2204.11352v1
* Paper URL: [http://arxiv.org/abs/2204.11352v1](http://arxiv.org/abs/2204.11352v1)
* Updated Date: 2022-04-24
* Code URL: [https://gitlab.com/Niwen/attack-paper](https://gitlab.com/Niwen/attack-paper)
* Summary: In the past years, power grids have become a valuable target for
cyber-attacks. Especially the attacks on the Ukrainian power grid has sparked
numerous research into possible attack vectors, their extent, and possible
mitigations. However, many fail to consider realistic scenarios in which time
series are incorporated into simulations to reflect the transient behaviour of
independent generators and consumers. Moreover, very few consider the limited
sensory input of a potential attacker. In this paper, we describe a reactive
power attack based on a well-understood scenario. We show that independent
agents can learn to use the dynamics of the power grid against it and that the
attack works even in the face of other generator and consumer nodes acting
independently.

### Title: A Comprehensive Test Pattern Generation Approach Exploiting SAT Attack for Logic Locking
* Paper ID: 2204.11307v1
* Paper URL: [http://arxiv.org/abs/2204.11307v1](http://arxiv.org/abs/2204.11307v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: The need for reducing manufacturing defect escape in today's safety-critical
applications requires increased fault coverage. However, generating a test set
using commercial automatic test pattern generation (ATPG) tools that lead to
zero-defect escape is still an open problem. It is challenging to detect all
stuck-at faults to reach 100% fault coverage. In parallel, the hardware
security community has been actively involved in developing solutions for logic
locking to prevent IP piracy. Locks (e.g., XOR gates) are inserted in different
locations of the netlist so that an adversary cannot determine the secret key.
Unfortunately, the Boolean satisfiability (SAT) based attack, introduced in
[1], can break different logic locking schemes in minutes. In this paper, we
propose a novel test pattern generation approach using the powerful SAT attack
on logic locking. A stuck-at fault is modeled as a locked gate with a secret
key. Our modeling of stuck-at faults preserves the property of fault activation
and propagation. We show that the input pattern that determines the key is a
test for the stuck-at fault. We propose two different approaches for test
pattern generation. First, a single stuck-at fault is targeted, and a
corresponding locked circuit with one key bit is created. This approach
generates one test pattern per fault. Second, we consider a group of faults and
convert the circuit to its locked version with multiple key bits. The inputs
obtained from the SAT tool are the test set for detecting this group of faults.
Our approach is able to find test patterns for hard-to-detect faults that were
previously failed in commercial ATPG tools. The proposed test pattern
generation approach can efficiently detect redundant faults present in a
circuit. We demonstrate the effectiveness of the approach on ITC'99 benchmarks.
The results show that we can achieve a perfect fault coverage reaching 100%.

### Title: Dictionary Attacks on Speaker Verification
* Paper ID: 2204.11304v1
* Paper URL: [http://arxiv.org/abs/2204.11304v1](http://arxiv.org/abs/2204.11304v1)
* Updated Date: 2022-04-24
* Code URL: null
* Summary: In this paper, we propose dictionary attacks against speaker verification - a
novel attack vector that aims to match a large fraction of speaker population
by chance. We introduce a generic formulation of the attack that can be used
with various speech representations and threat models. The attacker uses
adversarial optimization to maximize raw similarity of speaker embeddings
between a seed speech sample and a proxy population. The resulting master voice
successfully matches a non-trivial fraction of people in an unknown population.
Adversarial waveforms obtained with our approach can match on average 69% of
females and 38% of males enrolled in the target system at a strict decision
threshold calibrated to yield false alarm rate of 1%. By using the attack with
a black-box voice cloning system, we obtain master voices that are effective in
the most challenging conditions and transferable between speaker encoders. We
also show that, combined with multiple attempts, this attack opens even more to
serious issues on the security of these systems.

### Title: Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation
* Paper ID: 2204.09975v2
* Paper URL: [http://arxiv.org/abs/2204.09975v2](http://arxiv.org/abs/2204.09975v2)
* Updated Date: 2022-04-24
* Code URL: [https://github.com/BililiCode/ARGD](https://github.com/BililiCode/ARGD)
* Summary: Due to the prosperity of Artificial Intelligence (AI) techniques, more and
more backdoors are designed by adversaries to attack Deep Neural Networks
(DNNs).Although the state-of-the-art method Neural Attention Distillation (NAD)
can effectively erase backdoor triggers from DNNs, it still suffers from
non-negligible Attack Success Rate (ASR) together with lowered classification
ACCuracy (ACC), since NAD focuses on backdoor defense using attention features
(i.e., attention maps) of the same order. In this paper, we introduce a novel
backdoor defense framework named Attention Relation Graph Distillation (ARGD),
which fully explores the correlation among attention features with different
orders using our proposed Attention Relation Graphs (ARGs). Based on the
alignment of ARGs between both teacher and student models during knowledge
distillation, ARGD can eradicate more backdoor triggers than NAD. Comprehensive
experimental results show that, against six latest backdoor attacks, ARGD
outperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by
up to 3.23%.

