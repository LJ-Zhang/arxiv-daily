### Title: Smart App Attack: Hacking Deep Learning Models in Android Apps
* Paper ID: 2204.11075v1
* Paper URL: [http://arxiv.org/abs/2204.11075v1](http://arxiv.org/abs/2204.11075v1)
* Updated Date: 2022-04-23
* Code URL: [https://github.com/jinxhy/smartappattack](https://github.com/jinxhy/smartappattack)
* Summary: On-device deep learning is rapidly gaining popularity in mobile applications.
Compared to offloading deep learning from smartphones to the cloud, on-device
deep learning enables offline model inference while preserving user privacy.
However, such mechanisms inevitably store models on users' smartphones and may
invite adversarial attacks as they are accessible to attackers. Due to the
characteristic of the on-device model, most existing adversarial attacks cannot
be directly applied for on-device models. In this paper, we introduce a
grey-box adversarial attack framework to hack on-device models by crafting
highly similar binary classification models based on identified transfer
learning approaches and pre-trained models from TensorFlow Hub. We evaluate the
attack effectiveness and generality in terms of four different settings
including pre-trained models, datasets, transfer learning approaches and
adversarial attack algorithms. The results demonstrate that the proposed
attacks remain effective regardless of different settings, and significantly
outperform state-of-the-art baselines. We further conduct an empirical study on
real-world deep learning mobile apps collected from Google Play. Among 53 apps
adopting transfer learning, we find that 71.7\% of them can be successfully
attacked, which includes popular ones in medicine, automation, and finance
categories with critical usage scenarios. The results call for the awareness
and actions of deep learning mobile app developers to secure the on-device
models. The code of this work is available at
https://github.com/Jinxhy/SmartAppAttack

### Title: Towards Data-Free Model Stealing in a Hard Label Setting
* Paper ID: 2204.11022v1
* Paper URL: [http://arxiv.org/abs/2204.11022v1](http://arxiv.org/abs/2204.11022v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Machine learning models deployed as a service (MLaaS) are susceptible to
model stealing attacks, where an adversary attempts to steal the model within a
restricted access framework. While existing attacks demonstrate near-perfect
clone-model performance using softmax predictions of the classification
network, most of the APIs allow access to only the top-1 labels. In this work,
we show that it is indeed possible to steal Machine Learning models by
accessing only top-1 predictions (Hard Label setting) as well, without access
to model gradients (Black-Box setting) or even the training dataset (Data-Free
setting) within a low query budget. We propose a novel GAN-based framework that
trains the student and generator in tandem to steal the model effectively while
overcoming the challenge of the hard label setting by utilizing gradients of
the clone network as a proxy to the victim's gradients. We propose to overcome
the large query costs associated with a typical Data-Free setting by utilizing
publicly available (potentially unrelated) datasets as a weak image prior. We
additionally show that even in the absence of such data, it is possible to
achieve state-of-the-art results within a low query budget using synthetically
crafted samples. We are the first to demonstrate the scalability of Model
Stealing in a restricted access setting on a 100 class dataset as well.

### Title: GFCL: A GRU-based Federated Continual Learning Framework against Adversarial Attacks in IoV
* Paper ID: 2204.11010v1
* Paper URL: [http://arxiv.org/abs/2204.11010v1](http://arxiv.org/abs/2204.11010v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: The integration of ML in 5G-based Internet of Vehicles (IoV) networks has
enabled intelligent transportation and smart traffic management. Nonetheless,
the security against adversarial attacks is also increasingly becoming a
challenging task. Specifically, Deep Reinforcement Learning (DRL) is one of the
widely used ML designs in IoV applications. The standard ML security techniques
are not effective in DRL where the algorithm learns to solve sequential
decision-making through continuous interaction with the environment, and the
environment is time-varying, dynamic, and mobile. In this paper, we propose a
Gated Recurrent Unit (GRU)-based federated continual learning (GFCL) anomaly
detection framework against adversarial attacks in IoV. The objective is to
present a lightweight and scalable framework that learns and detects the
illegitimate behavior without having a-priori training dataset consisting of
attack samples. We use GRU to predict a future data sequence to analyze and
detect illegitimate behavior from vehicles in a federated learning-based
distributed manner. We investigate the performance of our framework using
real-world vehicle mobility traces. The results demonstrate the effectiveness
of our proposed solution for different performance metrics.

### Title: STC-IDS: Spatial-Temporal Correlation Feature Analyzing based Intrusion Detection System for Intelligent Connected Vehicles
* Paper ID: 2204.10990v1
* Paper URL: [http://arxiv.org/abs/2204.10990v1](http://arxiv.org/abs/2204.10990v1)
* Updated Date: 2022-04-23
* Code URL: null
* Summary: Intrusion detection is an important defensive measure for the security of
automotive communications. Accurate frame detection models assist vehicles to
avoid malicious attacks. Uncertainty and diversity regarding attack methods
make this task challenging. However, the existing works have the limitation of
only considering local features or the weak feature mapping of multi-features.
To address these limitations, we present a novel model for automotive intrusion
detection by spatial-temporal correlation features of in-vehicle communication
traffic (STC-IDS). Specifically, the proposed model exploits an
encoding-detection architecture. In the encoder part, spatial and temporal
relations are encoded simultaneously. To strengthen the relationship between
features, the attention-based convolution network still captures spatial and
channel features to increase the receptive field, while attention-LSTM build
important relationships from previous time series or crucial bytes. The encoded
information is then passed to the detector for generating forceful
spatial-temporal attention features and enabling anomaly classification. In
particular, single-frame and multi-frame models are constructed to present
different advantages respectively. Under automatic hyper-parameter selection
based on Bayesian optimization, the model is trained to attain the best
performance. Extensive empirical studies based on a real-world vehicle attack
dataset demonstrate that STC-IDS has outperformed baseline methods and cables
fewer false-positive rates while maintaining efficiency.

