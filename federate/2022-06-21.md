### Title: sqSGD: Locally Private and Communication Efficient Federated Learning
* Paper ID: 2206.10565v1
* Paper URL: [http://arxiv.org/abs/2206.10565v1](http://arxiv.org/abs/2206.10565v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated learning (FL) is a technique that trains machine learning models
from decentralized data sources. We study FL under local notions of privacy
constraints, which provides strong protection against sensitive data
disclosures via obfuscating the data before leaving the client. We identify two
major concerns in designing practical privacy-preserving FL algorithms:
communication efficiency and high-dimensional compatibility. We then develop a
gradient-based learning algorithm called \emph{sqSGD} (selective quantized
stochastic gradient descent) that addresses both concerns. The proposed
algorithm is based on a novel privacy-preserving quantization scheme that uses
a constant number of bits per dimension per client. Then we improve the base
algorithm in three ways: first, we apply a gradient subsampling strategy that
simultaneously offers better training performance and smaller communication
costs under a fixed privacy budget. Secondly, we utilize randomized rotation as
a preprocessing step to reduce quantization error. Thirdly, an adaptive
gradient norm upper bound shrinkage strategy is adopted to improve accuracy and
stabilize training. Finally, the practicality of the proposed framework is
demonstrated on benchmark datasets. Experiment results show that sqSGD
successfully learns large models like LeNet and ResNet with local privacy
constraints. In addition, with fixed privacy and communication level, the
performance of sqSGD significantly dominates that of various baseline
algorithms.

### Title: FedHiSyn: A Hierarchical Synchronous Federated Learning Framework for Resource and Data Heterogeneity
* Paper ID: 2206.10546v1
* Paper URL: [http://arxiv.org/abs/2206.10546v1](http://arxiv.org/abs/2206.10546v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated Learning (FL) enables training a global model without sharing the
decentralized raw data stored on multiple devices to protect data privacy. Due
to the diverse capacity of the devices, FL frameworks struggle to tackle the
problems of straggler effects and outdated models. In addition, the data
heterogeneity incurs severe accuracy degradation of the global model in the FL
training process. To address aforementioned issues, we propose a hierarchical
synchronous FL framework, i.e., FedHiSyn. FedHiSyn first clusters all available
devices into a small number of categories based on their computing capacity.
After a certain interval of local training, the models trained in different
categories are simultaneously uploaded to a central server. Within a single
category, the devices communicate the local updated model weights to each other
based on a ring topology. As the efficiency of training in the ring topology
prefers devices with homogeneous resources, the classification based on the
computing capacity mitigates the impact of straggler effects. Besides, the
combination of the synchronous update of multiple categories and the device
communication within a single category help address the data heterogeneity
issue while achieving high accuracy. We evaluate the proposed framework based
on MNIST, EMNIST, CIFAR10 and CIFAR100 datasets and diverse heterogeneous
settings of devices. Experimental results show that FedHiSyn outperforms six
baseline methods, e.g., FedAvg, SCAFFOLD, and FedAT, in terms of training
accuracy and efficiency.

### Title: A nation-wide experiment: fuel tax cuts and almost free public transport for three months in Germany -- Report 2 First wave results
* Paper ID: 2206.10510v1
* Paper URL: [http://arxiv.org/abs/2206.10510v1](http://arxiv.org/abs/2206.10510v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In spring 2022, the German federal government agreed on a set of measures
that aim at reducing households' financial burden resulting from a recent price
increase, especially in energy and mobility. These measures include among
others, a nation-wide public transport ticket for 9 EUR per month and a fuel
tax cut that reduces fuel prices by more than 15%. In transportation research
this is an almost unprecedented behavioral experiment. It allows to study not
only behavioral responses in mode choice and induced demand but also to assess
the effectiveness of transport policy instruments. We observe this natural
experiment with a three-wave survey and an app-based travel diary on a sample
of hundreds of participants as well as an analysis of traffic counts. In this
second report, we update the information on study participation, provide first
insights on the smartphone app usage as well as insights on the first wave
results, particularly on the 9 EUR-ticket purchase intention.

### Title: Optimal homotopy reconstruction results Ã  la Niyogi, Smale, and Weinberger
* Paper ID: 2206.10485v1
* Paper URL: [http://arxiv.org/abs/2206.10485v1](http://arxiv.org/abs/2206.10485v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In this article we show that the proof of the homotopy reconstruction result
by Niyogi, Smale, and Weinberger can be streamlined considerably using
Federer's work on the reach and several geometric observations. While Niyogi,
Smale, and Weinberger restricted themselves to C2 manifolds with positive
reach, our proof extends to sets S of positive reach.
  The sample we consider does not have to lie directly on the set of positive
reach. Instead, we assume that the two one-sided Hausdorff distances (delta and
epsilon) -- between the sample P to the set S, are bounded. We provide explicit
bounds in terms of epsilon and delta, that guarantee that there exists a
parameter r such that the union of balls of radii r centered on the points of
the sample P deformation retracts to S. We provide even better bounds for the
manifold case. In both cases, our bounds improve considerably on the
state-of-the-art in almost all settings. In fact the bounds are optimal.

### Title: WrapperFL: A Model Agnostic Plug-in for Industrial Federated Learning
* Paper ID: 2206.10407v1
* Paper URL: [http://arxiv.org/abs/2206.10407v1](http://arxiv.org/abs/2206.10407v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Federated learning, as a privacy-preserving collaborative machine learning
paradigm, has been gaining more and more attention in the industry. With the
huge rise in demand, there have been many federated learning platforms that
allow federated participants to set up and build a federated model from
scratch. However, exiting platforms are highly intrusive, complicated, and hard
to integrate with built machine learning models. For many real-world businesses
that already have mature serving models, existing federated learning platforms
have high entry barriers and development costs. This paper presents a simple
yet practical federated learning plug-in inspired by ensemble learning, dubbed
WrapperFL, allowing participants to build/join a federated system with existing
models at minimal costs. The WrapperFL works in a plug-and-play way by simply
attaching to the input and output interfaces of an existing model, without the
need of re-development, significantly reducing the overhead of manpower and
resources. We verify our proposed method on diverse tasks under heterogeneous
data distributions and heterogeneous models. The experimental results
demonstrate that WrapperFL can be successfully applied to a wide range of
applications under practical settings and improves the local model with
federated learning at a low cost.

### Title: An Energy and Carbon Footprint Analysis of Distributed and Federated Learning
* Paper ID: 2206.10380v1
* Paper URL: [http://arxiv.org/abs/2206.10380v1](http://arxiv.org/abs/2206.10380v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Classical and centralized Artificial Intelligence (AI) methods require moving
data from producers (sensors, machines) to energy hungry data centers, raising
environmental concerns due to computational and communication resource demands,
while violating privacy. Emerging alternatives to mitigate such high energy
costs propose to efficiently distribute, or federate, the learning tasks across
devices, which are typically low-power. This paper proposes a novel framework
for the analysis of energy and carbon footprints in distributed and federated
learning (FL). The proposed framework quantifies both the energy footprints and
the carbon equivalent emissions for vanilla FL methods and consensus-based
fully decentralized approaches. We discuss optimal bounds and operational
points that support green FL designs and underpin their sustainability
assessment. Two case studies from emerging 5G industry verticals are analyzed:
these quantify the environmental footprints of continual and reinforcement
learning setups, where the training process is repeated periodically for
continuous improvements. For all cases, sustainability of distributed learning
relies on the fulfillment of specific requirements on communication efficiency
and learner population size. Energy and test accuracy should be also traded off
considering the model and the data footprints for the targeted industrial
applications.

### Title: Personalized Subgraph Federated Learning
* Paper ID: 2206.10206v1
* Paper URL: [http://arxiv.org/abs/2206.10206v1](http://arxiv.org/abs/2206.10206v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: In real-world scenarios, subgraphs of a larger global graph may be
distributed across multiple devices or institutions, and only locally
accessible due to privacy restrictions, although there may be links between
them. Recently proposed subgraph Federated Learning (FL) methods deal with
those missing links across private local subgraphs while distributively
training Graph Neural Networks (GNNs) on them. However, they have overlooked
the inevitable heterogeneity among subgraphs, caused by subgraphs comprising
different parts of a global graph. For example, a subgraph may belong to one of
the communities within the larger global graph. A naive subgraph FL in such a
case will collapse incompatible knowledge from local GNN models trained on
heterogeneous graph distributions. To overcome such a limitation, we introduce
a new subgraph FL problem, personalized subgraph FL, which focuses on the joint
improvement of the interrelated local GNN models rather than learning a single
global GNN model, and propose a novel framework, FEDerated Personalized
sUBgraph learning (FED-PUB), to tackle it. A crucial challenge in personalized
subgraph FL is that the server does not know which subgraph each client has.
FED-PUB thus utilizes functional embeddings of the local GNNs using random
graphs as inputs to compute similarities between them, and use them to perform
weighted averaging for server-side aggregation. Further, it learns a
personalized sparse mask at each client to select and update only the
subgraph-relevant subset of the aggregated parameters. We validate FED-PUB for
its subgraph FL performance on six datasets, considering both non-overlapping
and overlapping subgraphs, on which ours largely outperforms relevant
baselines.

### Title: A General Theory for Federated Optimization with Asynchronous and Heterogeneous Clients Updates
* Paper ID: 2206.10189v1
* Paper URL: [http://arxiv.org/abs/2206.10189v1](http://arxiv.org/abs/2206.10189v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: We propose a novel framework to study asynchronous federated learning
optimization with delays in gradient updates. Our theoretical framework extends
the standard FedAvg aggregation scheme by introducing stochastic aggregation
weights to represent the variability of the clients update time, due for
example to heterogeneous hardware capabilities. Our formalism applies to the
general federated setting where clients have heterogeneous datasets and perform
at least one step of stochastic gradient descent (SGD). We demonstrate
convergence for such a scheme and provide sufficient conditions for the related
minimum to be the optimum of the federated problem. We show that our general
framework applies to existing optimization schemes including centralized
learning, FedAvg, asynchronous FedAvg, and FedBuff. The theory here provided
allows drawing meaningful guidelines for designing a federated learning
experiment in heterogeneous conditions. In particular, we develop in this work
FedFix, a novel extension of FedAvg enabling efficient asynchronous federated
training while preserving the convergence stability of synchronous aggregation.
We empirically demonstrate our theory on a series of experiments showing that
asynchronous FedAvg leads to fast convergence at the expense of stability, and
we finally demonstrate the improvements of FedFix over synchronous and
asynchronous FedAvg.

### Title: Federated Reinforcement Learning: Linear Speedup Under Markovian Sampling
* Paper ID: 2206.10185v1
* Paper URL: [http://arxiv.org/abs/2206.10185v1](http://arxiv.org/abs/2206.10185v1)
* Updated Date: 2022-06-21
* Code URL: null
* Summary: Since reinforcement learning algorithms are notoriously data-intensive, the
task of sampling observations from the environment is usually split across
multiple agents. However, transferring these observations from the agents to a
central location can be prohibitively expensive in terms of the communication
cost, and it can also compromise the privacy of each agent's local behavior
policy. In this paper, we consider a federated reinforcement learning framework
where multiple agents collaboratively learn a global model, without sharing
their individual data and policies. Each agent maintains a local copy of the
model and updates it using locally sampled data. Although having N agents
enables the sampling of N times more data, it is not clear if it leads to
proportional convergence speedup. We propose federated versions of on-policy
TD, off-policy TD and Q-learning, and analyze their convergence. For all these
algorithms, to the best of our knowledge, we are the first to consider
Markovian noise and multiple local updates, and prove a linear convergence
speedup with respect to the number of agents. To obtain these results, we show
that federated TD and Q-learning are special cases of a general framework for
federated stochastic approximation with Markovian noise, and we leverage this
framework to provide a unified convergence analysis that applies to all the
algorithms.

