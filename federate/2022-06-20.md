### Title: Decentralized Distributed Learning with Privacy-Preserving Data Synthesis
* Paper ID: 2206.10048v1
* Paper URL: [http://arxiv.org/abs/2206.10048v1](http://arxiv.org/abs/2206.10048v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In the medical field, multi-center collaborations are often sought to yield
more generalizable findings by leveraging the heterogeneity of patient and
clinical data. However, recent privacy regulations hinder the possibility to
share data, and consequently, to come up with machine learning-based solutions
that support diagnosis and prognosis. Federated learning (FL) aims at
sidestepping this limitation by bringing AI-based solutions to data owners and
only sharing local AI models, or parts thereof, that need then to be
aggregated. However, most of the existing federated learning solutions are
still at their infancy and show several shortcomings, from the lack of a
reliable and effective aggregation scheme able to retain the knowledge learned
locally to weak privacy preservation as real data may be reconstructed from
model updates. Furthermore, the majority of these approaches, especially those
dealing with medical data, relies on a centralized distributed learning
strategy that poses robustness, scalability and trust issues. In this paper we
present a decentralized distributed method that, exploiting concepts from
experience replay and generative adversarial research, effectively integrates
features from local nodes, providing models able to generalize across multiple
datasets while maintaining privacy. The proposed approach is tested on two
tasks - tuberculosis and melanoma classification - using multiple datasets in
order to simulate realistic non-i.i.d. data scenarios. Results show that our
approach achieves performance comparable to both standard (non-federated)
learning and federated methods in their centralized (thus, more favourable)
formulation.

### Title: QuAFL: Federated Averaging Can Be Both Asynchronous and Communication-Efficient
* Paper ID: 2206.10032v1
* Paper URL: [http://arxiv.org/abs/2206.10032v1](http://arxiv.org/abs/2206.10032v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Federated Learning (FL) is an emerging paradigm to enable the large-scale
distributed training of machine learning models, while still providing privacy
guarantees.
  In this work, we jointly address two of the main practical challenges when
scaling federated optimization to large node counts: the need for tight
synchronization between the central authority and individual computing nodes,
and the large communication cost of transmissions between the central server
and clients.
  Specifically, we present a new variant of the classic federated averaging
(FedAvg) algorithm, which supports both asynchronous communication and
communication compression. We provide a new analysis technique showing that, in
spite of these system relaxations, our algorithm essentially matches the best
known bounds for FedAvg, under reasonable parameter settings.
  On the experimental side, we show that our algorithm ensures fast practical
convergence for standard federated tasks.

### Title: Mitigating Data Heterogeneity in Federated Learning with Data Augmentation
* Paper ID: 2206.09979v1
* Paper URL: [http://arxiv.org/abs/2206.09979v1](http://arxiv.org/abs/2206.09979v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Federated Learning (FL) is a prominent framework that enables training a
centralized model while securing user privacy by fusing local, decentralized
models. In this setting, one major obstacle is data heterogeneity, i.e., each
client having non-identically and independently distributed (non-IID) data.
This is analogous to the context of Domain Generalization (DG), where each
client can be treated as a different domain. However, while many approaches in
DG tackle data heterogeneity from the algorithmic perspective, recent evidence
suggests that data augmentation can induce equal or greater performance.
Motivated by this connection, we present federated versions of popular DG
algorithms, and show that by applying appropriate data augmentation, we can
mitigate data heterogeneity in the federated setting, and obtain higher
accuracy on unseen clients. Equipped with data augmentation, we can achieve
state-of-the-art performance using even the most basic Federated Averaging
algorithm, with much sparser communication.

### Title: SoteriaFL: A Unified Framework for Private Federated Learning with Communication Compression
* Paper ID: 2206.09888v1
* Paper URL: [http://arxiv.org/abs/2206.09888v1](http://arxiv.org/abs/2206.09888v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: To enable large-scale machine learning in bandwidth-hungry environments such
as wireless networks, significant progress has been made recently in designing
communication-efficient federated learning algorithms with the aid of
communication compression. On the other end, privacy-preserving, especially at
the client level, is another important desideratum that has not been addressed
simultaneously in the presence of advanced communication compression techniques
yet. In this paper, we propose a unified framework that enhances the
communication efficiency of private federated learning with communication
compression. Exploiting both general compression operators and local
differential privacy, we first examine a simple algorithm that applies
compression directly to differentially-private stochastic gradient descent, and
identify its limitations. We then propose a unified framework SoteriaFL for
private federated learning, which accommodates a general family of local
gradient estimators including popular stochastic variance-reduced gradient
methods and the state-of-the-art shifted compression scheme. We provide a
comprehensive characterization of its performance trade-offs in terms of
privacy, utility, and communication complexity, where SoteraFL is shown to
achieve better communication complexity without sacrificing privacy nor utility
than other private federated learning algorithms without communication
compression.

### Title: Performance-Oriented Design for Intelligent Reflecting Surface Assisted Federated Learning
* Paper ID: 2206.09578v1
* Paper URL: [http://arxiv.org/abs/2206.09578v1](http://arxiv.org/abs/2206.09578v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: To efficiently exploit the massive raw data that is pervading generated at
mobile edge networks, federated learning (FL) has emerged as a promising
distributed learning technique that was regarded as a substitute for
centralized learning operations. By collaboratively training a shared learning
model at edge devices, the raw data transmission and storage are bypassed via
the local computed parameters/gradients exchange in FL. Hence, FL can overcome
high communication latency and privacy issues. While the high dimensionality in
iterative updates (millions of parameters/gradients may be included in the
model training) still conflicts with the scarcity of communication resources.
Over-the-air computation (AirComp) has come into the spotlight recently which
profitably leverages the inherent superposition property of wireless channels
to perform efficient model aggeration. However, the model aggregation accuracy
is still severely damaged by the unfavorable wireless propagation channels. In
this paper, we harness the intelligent reflecting surface (IRS) to program the
wireless channel, thus acquiring a satisfying learning performance.
Specifically, a performance-oriented design scheme that directly minimizes the
optimality gap of the loss function is proposed to accelerate the convergence
of AirComp based FL. Firstly, we analyze the convergence behavior of the FL
procedure. Then, both offline and online design approaches are proposed based
on the obtained optimality gap. We adopt the block coordinate descent (BCD)
method to tackle the highly-intractable problem. Simulation results demonstrate
that such a performance-oriented design strategy can achieve higher test
accuracy than the conventional isolated mean square error (MSE) minimization
approach in FL.

### Title: FedSSO: A Federated Server-Side Second-Order Optimization Algorithm
* Paper ID: 2206.09576v1
* Paper URL: [http://arxiv.org/abs/2206.09576v1](http://arxiv.org/abs/2206.09576v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: In this work, we propose FedSSO, a server-side second-order optimization
method for federated learning (FL). In contrast to previous works in this
direction, we employ a server-side approximation for the Quasi-Newton method
without requiring any training data from the clients. In this way, we not only
shift the computation burden from clients to server, but also eliminate the
additional communication for second-order updates between clients and server
entirely. We provide theoretical guarantee for convergence of our novel method,
and empirically demonstrate our fast convergence and communication savings in
both convex and non-convex settings.

### Title: Shuffle Gaussian Mechanism for Differential Privacy
* Paper ID: 2206.09569v1
* Paper URL: [http://arxiv.org/abs/2206.09569v1](http://arxiv.org/abs/2206.09569v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: We study Gaussian mechanism in the shuffle model of differential privacy
(DP). Particularly, we characterize the mechanism's R\'enyi differential
privacy (RDP), showing that it is of the form: $$ \epsilon(\lambda) \leq
\frac{1}{\lambda-1}\log\left(\frac{e^{-\lambda/2\sigma^2}}{n^\lambda}\sum_{\substack{k_1+\dotsc+k_n=\lambda;\\k_1,\dotsc,k_n\geq
0}}\binom{\lambda!}{k_1,\dotsc,k_n}e^{\sum_{i=1}^nk_i^2/2\sigma^2}\right) $$ We
further prove that the RDP is strictly upper-bounded by the Gaussian RDP
without shuffling. The shuffle Gaussian RDP is advantageous in composing
multiple DP mechanisms, where we demonstrate its improvement over the
state-of-the-art approximate DP composition theorems in privacy guarantees of
the shuffle model. Moreover, we extend our study to the subsampled shuffle
mechanism and the recently proposed shuffled check-in mechanism, which are
protocols geared towards distributed/federated learning. Finally, an empirical
study of these mechanisms is given to demonstrate the efficacy of employing
shuffle Gaussian mechanism under the distributed learning framework to
guarantee rigorous user privacy.

### Title: Robust One Round Federated Learning with Predictive Space Bayesian Inference
* Paper ID: 2206.09526v1
* Paper URL: [http://arxiv.org/abs/2206.09526v1](http://arxiv.org/abs/2206.09526v1)
* Updated Date: 2022-06-20
* Code URL: null
* Summary: Making predictions robust is an important challenge. A separate challenge in
federated learning (FL) is to reduce the number of communication rounds,
particularly since doing so reduces performance in heterogeneous data settings.
To tackle both issues, we take a Bayesian perspective on the problem of
learning a global model. We show how the global predictive posterior can be
approximated using client predictive posteriors. This is unlike other works
which aggregate the local model space posteriors into the global model space
posterior, and are susceptible to high approximation errors due to the
posterior's high dimensional multimodal nature. In contrast, our method
performs the aggregation on the predictive posteriors, which are typically
easier to approximate owing to the low-dimensionality of the output space. We
present an algorithm based on this idea, which performs MCMC sampling at each
client to obtain an estimate of the local posterior, and then aggregates these
in one round to obtain a global ensemble model. Through empirical evaluation on
several classification and regression tasks, we show that despite using one
round of communication, the method is competitive with other FL techniques, and
outperforms them on heterogeneous settings. The code is publicly available at
https://github.com/hasanmohsin/FedPredSpace_1Round.

