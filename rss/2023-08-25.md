## diffusion
### Title: Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation. (arXiv:2308.12350v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12350](http://arxiv.org/abs/2308.12350)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12350] Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2308.12350) #diffusion`
* Summary: <p>Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.
</p>

### Title: Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12453](http://arxiv.org/abs/2308.12453)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12453] Augmenting medical image classifiers with synthetic data from latent diffusion models](http://arxiv.org/abs/2308.12453) #diffusion`
* Summary: <p>While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
</p>

### Title: Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion. (arXiv:2308.12469v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12469](http://arxiv.org/abs/2308.12469)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12469] Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion](http://arxiv.org/abs/2308.12469) #diffusion`
* Summary: <p>Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU.
</p>

### Title: DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition. (arXiv:2308.12501v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12501](http://arxiv.org/abs/2308.12501)
* Code URL: [https://github.com/shiyin-lc/dd-gcn](https://github.com/shiyin-lc/dd-gcn)
* Copy Paste: `<input type="checkbox">[[2308.12501] DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition](http://arxiv.org/abs/2308.12501) #diffusion`
* Summary: <p>Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.
</p>

### Title: APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12605](http://arxiv.org/abs/2308.12605)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12605] APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency](http://arxiv.org/abs/2308.12605) #diffusion`
* Summary: <p>Diffusion models have exhibited promising progress in video generation.
However, they often struggle to retain consistent details within local regions
across frames. One underlying cause is that traditional diffusion models
approximate Gaussian noise distribution by utilizing predictive noise, without
fully accounting for the impact of inherent information within the input
itself. Additionally, these models emphasize the distinction between
predictions and references, neglecting information intrinsic to the videos. To
address this limitation, inspired by the self-attention mechanism, we propose a
novel text-to-video (T2V) generation network structure based on diffusion
models, dubbed Additional Perturbation for Latent noise with Adversarial
training (APLA). Our approach only necessitates a single video as input and
builds upon pre-trained stable diffusion networks. Notably, we introduce an
additional compact network, known as the Video Generation Transformer (VGT).
This auxiliary component is designed to extract perturbations from the inherent
information contained within the input, thereby refining inconsistent pixels
during temporal predictions. We leverage a hybrid architecture of transformers
and convolutions to compensate for temporal intricacies, enhancing consistency
between different frames within the video. Experiments demonstrate a noticeable
improvement in the consistency of the generated videos both qualitatively and
quantitatively.
</p>

### Title: Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12964](http://arxiv.org/abs/2308.12964)
* Code URL: [https://github.com/naver-ai/densediffusion](https://github.com/naver-ai/densediffusion)
* Copy Paste: `<input type="checkbox">[[2308.12964] Dense Text-to-Image Generation with Attention Modulation](http://arxiv.org/abs/2308.12964) #diffusion`
* Summary: <p>Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</p>

## self-supervised
### Title: Self-Supervised Learning for Endoscopic Video Analysis. (arXiv:2308.12394v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12394](http://arxiv.org/abs/2308.12394)
* Code URL: [https://github.com/royhirsch/endossl](https://github.com/royhirsch/endossl)
* Copy Paste: `<input type="checkbox">[[2308.12394] Self-Supervised Learning for Endoscopic Video Analysis](http://arxiv.org/abs/2308.12394) #self-supervised`
* Summary: <p>Self-supervised learning (SSL) has led to important breakthroughs in computer
vision by allowing learning from large amounts of unlabeled data. As such, it
might have a pivotal role to play in biomedicine where annotating data requires
a highly specialized expertise. Yet, there are many healthcare domains for
which SSL has not been extensively explored. One such domain is endoscopy,
minimally invasive procedures which are commonly used to detect and treat
infections, chronic inflammatory diseases or cancer. In this work, we study the
use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for
endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit
the power of SSL, we create sizable unlabeled endoscopic video datasets for
training MSNs. These strong image representations serve as a foundation for
secondary training with limited annotated datasets, resulting in
state-of-the-art performance in endoscopic benchmarks like surgical phase
recognition during laparoscopy and colonoscopic polyp characterization.
Additionally, we achieve a 50% reduction in annotated data size without
sacrificing performance. Thus, our work provides evidence that SSL can
dramatically reduce the need of annotated data in endoscopy.
</p>

### Title: MOFO: MOtion FOcused Self-Supervision for Video Understanding. (arXiv:2308.12447v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12447](http://arxiv.org/abs/2308.12447)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12447] MOFO: MOtion FOcused Self-Supervision for Video Understanding](http://arxiv.org/abs/2308.12447) #self-supervised`
* Summary: <p>Self-supervised learning (SSL) techniques have recently produced outstanding
results in learning visual representations from unlabeled videos. Despite the
importance of motion in supervised learning techniques for action recognition,
SSL methods often do not explicitly consider motion information in videos. To
address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for
focusing representation learning on the motion area of a video, for action
recognition. MOFO automatically detects motion areas in videos and uses these
to guide the self-supervision task. We use a masked autoencoder which randomly
masks out a high proportion of the input sequence; we force a specified
percentage of the inside of the motion area to be masked and the remainder from
outside. We further incorporate motion information into the finetuning step to
emphasise motion in the downstream task. We demonstrate that our motion-focused
innovations can significantly boost the performance of the currently leading
SSL method (VideoMAE) for action recognition. Our method improves the recent
self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%,
+1.3% accuracy on Epic-Kitchens verb, noun and action classification,
respectively, and +4.7% accuracy on Something-Something V2 action
classification. Our proposed approach significantly improves the performance of
the current SSL method for action recognition, indicating the importance of
explicitly encoding motion in SSL.
</p>

### Title: Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects. (arXiv:2308.12590v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12590](http://arxiv.org/abs/2308.12590)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12590] Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects](http://arxiv.org/abs/2308.12590) #self-supervised`
* Summary: <p>Learning 3D shape representation with dense correspondence for deformable
objects is a fundamental problem in computer vision. Existing approaches often
need additional annotations of specific semantic domain, e.g., skeleton poses
for human bodies or animals, which require extra annotation effort and suffer
from error accumulation, and they are limited to specific domain. In this
paper, we propose a novel self-supervised approach to learn neural implicit
shape representation for deformable objects, which can represent shapes with a
template shape and dense correspondence in 3D. Our method does not require the
priors of skeleton and skinning weight, and only requires a collection of
shapes represented in signed distance fields. To handle the large deformation,
we constrain the learned template shape in the same latent space with the
training shapes, design a new formulation of local rigid constraint that
enforces rigid transformation in local region and addresses local reflection
issue, and present a new hierarchical rigid constraint to reduce the ambiguity
due to the joint learning of template shape and correspondences. Extensive
experiments show that our model can represent shapes with large deformations.
We also show that our shape representation can support two typical
applications, such as texture transfer and shape editing, with competitive
performance. The code and models are available at
https://iscas3dv.github.io/deformshape
</p>

## foundation model
### Title: Overcoming General Knowledge Loss with Selective Parameter Finetuning. (arXiv:2308.12462v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12462](http://arxiv.org/abs/2308.12462)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12462] Overcoming General Knowledge Loss with Selective Parameter Finetuning](http://arxiv.org/abs/2308.12462) #foundation model`
* Summary: <p>Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in updating foundation models to accommodate novel
information while retaining their original ability. In this paper, we present a
novel approach to achieving continual model updates by effecting localized
modifications to a small subset of parameters. Guided by insights gleaned from
prior analyses of foundational models, we first localize a specific layer for
model refinement and then introduce an importance scoring mechanism designed to
update only the most crucial weights. Our method is exhaustively evaluated on
foundational vision-language models, measuring its efficacy in both learning
new information and preserving pre-established knowledge across a diverse
spectrum of continual learning tasks, including Aircraft, Birdsnap CIFAR-100,
CUB, Cars, and GTSRB. The results show that our method improves the existing
continual learning methods by 0.5\% - 10\% on average, and reduces the loss of
pre-trained knowledge from around 5\% to 0.97\%. Comprehensive ablation studies
substantiate our method design, shedding light on the contributions of each
component to controllably learning new knowledge and mitigating the forgetting
of pre-trained knowledge.
</p>

### Title: Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.12950](http://arxiv.org/abs/2308.12950)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12950] Code Llama: Open Foundation Models for Code](http://arxiv.org/abs/2308.12950) #foundation model`
* Summary: <p>We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.
</p>

### Title: FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12305](http://arxiv.org/abs/2308.12305)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12305] FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning](http://arxiv.org/abs/2308.12305) #foundation model`
* Summary: <p>Recently, foundation models have exhibited remarkable advancements in
multi-modal learning. These models, equipped with millions (or billions) of
parameters, typically require a substantial amount of data for finetuning.
However, collecting and centralizing training data from diverse sectors becomes
challenging due to distinct privacy regulations. Federated Learning (FL)
emerges as a promising solution, enabling multiple clients to collaboratively
train neural networks without centralizing their local data. To alleviate
client computation burdens and communication overheads, previous works have
adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a
small fraction of the model parameters are optimized and communicated during
federated communications. Nevertheless, most previous works have focused on a
single modality and neglected one common phenomenon, i.e., the presence of data
heterogeneity across the clients. Therefore, in this work, we propose a
finetuning framework tailored to heterogeneous multi-modal FL, called Federated
Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a
Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the
client local updates and applying Mutual Knowledge Distillation (MKD) for an
efficient knowledge transfer. FedDAT is the first approach that enables an
efficient distributed finetuning of foundation models for a variety of
heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we
conduct extensive experiments on four multi-modality FL benchmarks with
different types of data heterogeneity, where FedDAT substantially outperforms
the existing centralized PEFT methods adapted for FL.
</p>

## generative
### Title: Continual Zero-Shot Learning through Semantically Guided Generative Random Walks. (arXiv:2308.12366v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12366](http://arxiv.org/abs/2308.12366)
* Code URL: [https://github.com/wx-zhang/igczsl](https://github.com/wx-zhang/igczsl)
* Copy Paste: `<input type="checkbox">[[2308.12366] Continual Zero-Shot Learning through Semantically Guided Generative Random Walks](http://arxiv.org/abs/2308.12366) #generative`
* Summary: <p>Learning novel concepts, remembering previous knowledge, and adapting it to
future tasks occur simultaneously throughout a human's lifetime. To model such
comprehensive abilities, continual zero-shot learning (CZSL) has recently been
introduced. However, most existing methods overused unseen semantic information
that may not be continually accessible in realistic settings. In this paper, we
address the challenge of continual zero-shot learning where unseen information
is not provided during training, by leveraging generative modeling. The heart
of the generative-based methods is to learn quality representations from seen
classes to improve the generative understanding of the unseen visual space.
Motivated by this, we introduce generalization-bound tools and provide the
first theoretical explanation for the benefits of generative modeling to CZSL
tasks. Guided by the theoretical analysis, we then propose our learning
algorithm that employs a novel semantically guided Generative Random Walk (GRW)
loss. The GRW loss augments the training by continually encouraging the model
to generate realistic and characterized samples to represent the unseen space.
Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN
datasets, surpassing existing CZSL methods by 3-7\%. The code has been made
available here \url{https://github.com/wx-zhang/IGCZSL}
</p>

### Title: FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features. (arXiv:2308.12380v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12380](http://arxiv.org/abs/2308.12380)
* Code URL: [https://github.com/ihp-lab/fg-net](https://github.com/ihp-lab/fg-net)
* Copy Paste: `<input type="checkbox">[[2308.12380] FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features](http://arxiv.org/abs/2308.12380) #generative`
* Summary: <p>Automatic detection of facial Action Units (AUs) allows for objective facial
expression analysis. Due to the high cost of AU labeling and the limited size
of existing benchmarks, previous AU detection methods tend to overfit the
dataset, resulting in a significant performance loss when evaluated across
corpora. To address this problem, we propose FG-Net for generalizable facial
action unit detection. Specifically, FG-Net extracts feature maps from a
StyleGAN2 model pre-trained on a large and diverse face image dataset. Then,
these features are used to detect AUs with a Pyramid CNN Interpreter, making
the training efficient and capturing essential local features. The proposed
FG-Net achieves a strong generalization ability for heatmap-based AU detection
thanks to the generalizable and semantic-rich features extracted from the
pre-trained generative model. Extensive experiments are conducted to evaluate
within- and cross-corpus AU detection with the widely-used DISFA and BP4D
datasets. Compared with the state-of-the-art, the proposed method achieves
superior cross-domain performance while maintaining competitive within-domain
performance. In addition, FG-Net is data-efficient and achieves competitive
performance even when trained on 1000 samples. Our code will be released at
\url{https://github.com/ihp-lab/FG-Net}
</p>

### Title: MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models. (arXiv:2308.12963v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12963](http://arxiv.org/abs/2308.12963)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12963] MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models](http://arxiv.org/abs/2308.12963) #generative`
* Summary: <p>Despite tremendous advancements in bird's-eye view (BEV) perception, existing
models fall short in generating realistic and coherent semantic map layouts,
and they fail to account for uncertainties arising from partial sensor
information (such as occlusion or limited coverage). In this work, we introduce
MapPrior, a novel BEV perception framework that combines a traditional
discriminative BEV perception model with a learned generative model for
semantic map layouts. Our MapPrior delivers predictions with better accuracy,
realism, and uncertainty awareness. We evaluate our model on the large-scale
nuScenes benchmark. At the time of submission, MapPrior outperforms the
strongest competing method, with significantly improved MMD and ECE scores in
camera- and LiDAR-based BEV perception.
</p>

### Title: Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.12833](http://arxiv.org/abs/2308.12833)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12833] Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities](http://arxiv.org/abs/2308.12833) #generative`
* Summary: <p>Spurred by the recent rapid increase in the development and distribution of
large language models (LLMs) across industry and academia, much recent work has
drawn attention to safety- and security-related threats and vulnerabilities of
LLMs, including in the context of potentially criminal activities.
Specifically, it has been shown that LLMs can be misused for fraud,
impersonation, and the generation of malware; while other authors have
considered the more general problem of AI alignment. It is important that
developers and practitioners alike are aware of security-related problems with
such models. In this paper, we provide an overview of existing - predominantly
scientific - efforts on identifying and mitigating threats and vulnerabilities
arising from LLMs. We present a taxonomy describing the relationship between
threats caused by the generative capabilities of LLMs, prevention measures
intended to address such threats, and vulnerabilities arising from imperfect
prevention measures. With our work, we hope to raise awareness of the
limitations of LLMs in light of such security concerns, among both experienced
developers and novel users of such technologies.
</p>

### Title: Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.12890](http://arxiv.org/abs/2308.12890)
* Code URL: [https://github.com/oniani/llms-vote](https://github.com/oniani/llms-vote)
* Copy Paste: `<input type="checkbox">[[2308.12890] Large Language Models Vote: Prompting for Rare Disease Identification](http://arxiv.org/abs/2308.12890) #generative`
* Summary: <p>The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases, affecting a small fraction
of the population, inherently require FSL techniques due to limited data
availability, though manual data collection and annotation is costly and
time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a
flexible prompting approach for improving the performance of LLM queries in FSL
settings. MVP works by prompting numerous LLMs to perform the same tasks and
then conducting a majority vote on the resulting outputs. This method achieves
improved results to any one model in the ensemble on one-shot rare disease
identification and classification tasks. We also release a novel rare disease
dataset for FSL, available to those who agreed to the MIMIC-IV Data Use
Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple
times, substantially increasing the time needed for manual annotation, and to
address this, we assess the feasibility of using JSON for automating generative
LLM evaluation.
</p>

### Title: PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12454](http://arxiv.org/abs/2308.12454)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12454] PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning](http://arxiv.org/abs/2308.12454) #generative`
* Summary: <p>Recent advances of generative learning models are accompanied by the growing
interest in federated learning (FL) based on generative adversarial network
(GAN) models. In the context of FL, GAN can capture the underlying client data
structure, and regenerate samples resembling the original data distribution
without compromising the private raw data. Although most existing GAN-based FL
works focus on training a global model, Personalized FL (PFL) sometimes can be
more effective in view of client data heterogeneity in terms of distinct data
sample distributions, feature spaces, and labels. To cope with client
heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation
strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in
different scenarios. More specially, we first learn the similarity among
clients and then develop an weighted collaborative data aggregation. The
empirical results through the rigorous experimentation on several well-known
datasets demonstrate the effectiveness of PFL-GAN.
</p>

## anomaly
### Title: REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12577](http://arxiv.org/abs/2308.12577)
* Code URL: [https://github.com/shuailyu/reb](https://github.com/shuailyu/reb)
* Copy Paste: `<input type="checkbox">[[2308.12577] REB: Reducing Biases in Representation for Industrial Anomaly Detection](http://arxiv.org/abs/2308.12577) #anomaly`
* Summary: <p>Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct
industrial anomaly detection in two stages: obtain feature representations with
a pre-trained CNN model and perform distance measures for defect detection.
However, the features are not fully exploited as they ignore domain bias and
the difference of local density in feature space, which limits the detection
performance. In this paper, we propose Reducing Biases (REB) in representation
by considering the domain bias of the pre-trained model and building a
self-supervised learning task for better domain adaption with a defect
generation strategy (DefectMaker) imitating the natural defects. Additionally,
we propose a local density KNN (LDKNN) to reduce the local density bias and
obtain effective anomaly detection. We achieve a promising result of 99.5\%
AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on
the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC
to the state-of-the-art result. All results are obtained with smaller backbone
networks such as Vgg11 and Resnet18, which indicates the effectiveness and
efficiency of REB for practical industrial applications.
</p>

### Title: Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12563](http://arxiv.org/abs/2308.12563)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12563] Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals](http://arxiv.org/abs/2308.12563) #anomaly`
* Summary: <p>Mainstream unsupervised anomaly detection algorithms often excel in academic
datasets, yet their real-world performance is restricted due to the controlled
experimental conditions involving clean training data. Addressing the challenge
of training with noise, a prevalent issue in practical anomaly detection, is
frequently overlooked. In a pioneering endeavor, this study delves into the
realm of label-level noise within sensory time-series anomaly detection (TSAD).
This paper presents a novel and practical end-to-end unsupervised TSAD when the
training data are contaminated with anomalies. The introduced approach, called
TSAD-C, is devoid of access to abnormality labels during the training phase.
TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities
(aka noise) present in the training data, a Variable Dependency Modeling module
to capture both long-term intra- and inter-variable dependencies within the
decontaminated data that can be considered as a surrogate of the pure normal
data, and an Anomaly Scoring module to detect anomalies. Our extensive
experiments conducted on three widely used physiological datasets conclusively
demonstrate that our approach surpasses existing methodologies, thus
establishing a new state-of-the-art performance in the field.
</p>

### Title: Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12612](http://arxiv.org/abs/2308.12612)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12612] Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection](http://arxiv.org/abs/2308.12612) #anomaly`
* Summary: <p>The rapid growth of deep learning (DL) has spurred interest in enhancing
log-based anomaly detection. This approach aims to extract meaning from log
events (log message templates) and develop advanced DL models for anomaly
detection. However, these DL methods face challenges like heavy reliance on
training data, labels, and computational resources due to model complexity. In
contrast, traditional machine learning and data mining techniques are less
data-dependent and more efficient but less effective than DL. To make log-based
anomaly detection more practical, the goal is to enhance traditional techniques
to match DL's effectiveness. Previous research in a different domain (linking
questions on Stack Overflow) suggests that optimized traditional techniques can
rival state-of-the-art DL methods. Drawing inspiration from this concept, we
conducted an empirical study. We optimized the unsupervised PCA (Principal
Component Analysis), a traditional technique, by incorporating lightweight
semantic-based log representation. This addresses the issue of unseen log
events in training data, enhancing log representation. Our study compared seven
log-based anomaly detection methods, including four DL-based, two traditional,
and the optimized PCA technique, using public and industrial datasets. Results
indicate that the optimized unsupervised PCA technique achieves similar
effectiveness to advanced supervised/semi-supervised DL methods while being
more stable with limited training data and resource-efficient. This
demonstrates the adaptability and strength of traditional techniques through
small yet impactful adaptations.
</p>

### Title: Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12925](http://arxiv.org/abs/2308.12925)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12925] Low-count Time Series Anomaly Detection](http://arxiv.org/abs/2308.12925) #anomaly`
* Summary: <p>Low-count time series describe sparse or intermittent events, which are
prevalent in large-scale online platforms that capture and monitor diverse data
types. Several distinct challenges surface when modelling low-count time
series, particularly low signal-to-noise ratios (when anomaly signatures are
provably undetectable), and non-uniform performance (when average metrics are
not representative of local behaviour). The time series anomaly detection
community currently lacks explicit tooling and processes to model and reliably
detect anomalies in these settings. We address this gap by introducing a novel
generative procedure for creating benchmark datasets comprising of low-count
time series with anomalous segments. Via a mixture of theoretical and empirical
analysis, our work explains how widely-used algorithms struggle with the
distribution overlap between normal and anomalous segments. In order to
mitigate this shortcoming, we then leverage our findings to demonstrate how
anomaly score smoothing consistently improves performance. The practical
utility of our analysis and recommendation is validated on a real-world dataset
containing sales data for retail stores.
</p>

## in-context
### Title: Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.12519](http://arxiv.org/abs/2308.12519)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12519] Large Language Model as Autonomous Decision Maker](http://arxiv.org/abs/2308.12519) #in-context`
* Summary: <p>While large language models (LLMs) exhibit impressive language understanding
and in-context learning abilities, their decision-making ability still heavily
relies on the guidance of task-specific expert knowledge when solving
real-world tasks. To unleash the potential of LLMs as autonomous decision
makers, this paper presents an approach JuDec to endow LLMs with the
self-judgment ability, enabling LLMs to achieve autonomous judgment and
exploration for decision making. Specifically, in JuDec, Elo-based
Self-Judgment Mechanism is designed to assign Elo scores to decision steps to
judge their values and utilities via pairwise comparisons between two solutions
and then guide the decision-searching process toward the optimal solution
accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's
superiority over baselines, achieving over 10% improvement in Pass Rate on
diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT
API calls), highlighting its effectiveness and efficiency.
</p>

## memory
### Title: With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12383](http://arxiv.org/abs/2308.12383)
* Code URL: [https://github.com/aimagelab/pma-net](https://github.com/aimagelab/pma-net)
* Copy Paste: `<input type="checkbox">[[2308.12383] With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning](http://arxiv.org/abs/2308.12383) #memory`
* Summary: <p>Image captioning, like many tasks involving vision and language, currently
relies on Transformer-based architectures for extracting the semantics in an
image and translating it into linguistically coherent descriptions. Although
successful, the attention operator only considers a weighted summation of
projections of the current input sample, therefore ignoring the relevant
semantic information which can come from the joint observation of other
samples. In this paper, we devise a network which can perform attention over
activations obtained while processing other training samples, through a
prototypical memory model. Our memory models the distribution of past keys and
values through the definition of prototype vectors which are both
discriminative and compact. Experimentally, we assess the performance of the
proposed model on the COCO dataset, in comparison with carefully designed
baselines and state-of-the-art approaches, and by investigating the role of
each of the proposed components. We demonstrate that our proposal can increase
the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when
training in cross-entropy only and when fine-tuning with self-critical sequence
training. Source code and trained models are available at:
https://github.com/aimagelab/PMA-Net.
</p>

### Title: MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices. (arXiv:2308.12494v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12494](http://arxiv.org/abs/2308.12494)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12494] MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices](http://arxiv.org/abs/2308.12494) #memory`
* Summary: <p>Image restoration aims to restore high-quality images from degraded
counterparts and has seen significant advancements through deep learning
techniques. The technique has been widely applied to mobile devices for tasks
such as mobile photography. Given the resource limitations on mobile devices,
such as memory constraints and runtime requirements, the efficiency of models
during deployment becomes paramount. Nevertheless, most previous works have
primarily concentrated on analyzing the efficiency of single modules and
improving them individually. This paper examines the efficiency across
different layers. We propose a roadmap that can be applied to further
accelerate image restoration models prior to deployment while simultaneously
increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity
Index). The roadmap first increases the model capacity by adding more
parameters to partial convolutions on FLOPs non-sensitive layers. Then, it
applies partial depthwise convolution coupled with decoupling
upsampling/downsampling layers to accelerate the model speed. Extensive
experiments demonstrate that our approach decreases runtime by up to 13% and
reduces the number of parameters by up to 23%, while increasing PSNR and SSIM
on several image restoration datasets. Source Code of our method is available
at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</p>

### Title: HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation. (arXiv:2308.12608v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12608](http://arxiv.org/abs/2308.12608)
* Code URL: [https://github.com/pipixin321/hr-pro](https://github.com/pipixin321/hr-pro)
* Copy Paste: `<input type="checkbox">[[2308.12608] HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation](http://arxiv.org/abs/2308.12608) #memory`
* Summary: <p>Point-supervised Temporal Action Localization (PSTAL) is an emerging research
direction for label-efficient learning. However, current methods mainly focus
on optimizing the network either at the snippet-level or the instance-level,
neglecting the inherent reliability of point annotations at both levels. In
this paper, we propose a Hierarchical Reliability Propagation (HR-Pro)
framework, which consists of two reliability-aware stages: Snippet-level
Discrimination Learning and Instance-level Completeness Learning, both stages
explore the efficient propagation of high-confidence cues in point annotations.
For snippet-level learning, we introduce an online-updated memory to store
reliable snippet prototypes for each class. We then employ a Reliability-aware
Attention Block to capture both intra-video and inter-video dependencies of
snippets, resulting in more discriminative and robust snippet representation.
For instance-level learning, we propose a point-based proposal generation
approach as a means of connecting snippets and instances, which produces
high-confidence proposals for further optimization at the instance level.
Through multi-level reliability-aware learning, we obtain more reliable
confidence scores and more accurate temporal boundaries of predicted proposals.
Our HR-Pro achieves state-of-the-art performance on multiple challenging
benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably,
our HR-Pro largely surpasses all previous point-supervised methods, and even
outperforms several competitive fully supervised methods. Code will be
available at https://github.com/pipixin321/HR-Pro.
</p>

### Title: Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization. (arXiv:2308.12609v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12609](http://arxiv.org/abs/2308.12609)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12609] Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization](http://arxiv.org/abs/2308.12609) #memory`
* Summary: <p>Weakly supervised temporal action localization (WSTAL) aims to localize
actions in untrimmed videos using video-level labels. Despite recent advances,
existing approaches mainly follow a localization-by-classification pipeline,
generally processing each segment individually, thereby exploiting only limited
contextual information. As a result, the model will lack a comprehensive
understanding (e.g. appearance and temporal structure) of various action
patterns, leading to ambiguity in classification learning and temporal
localization. Our work addresses this from a novel perspective, by exploring
and exploiting the cross-video contextual knowledge within the dataset to
recover the dataset-level semantic structure of action instances via weak
labels only, thereby indirectly improving the holistic understanding of
fine-grained action patterns and alleviating the aforementioned ambiguities.
Specifically, an end-to-end framework is proposed, including a Robust
Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge
Summarization and Aggregation (GKSA) module. First, the RMGCL module explores
the contrast and consistency of cross-video action features, assisting in
learning more structured and compact embedding space, thus reducing ambiguity
in classification learning. Further, the GKSA module is used to efficiently
summarize and propagate the cross-video representative action knowledge in a
learnable manner to promote holistic action patterns understanding, which in
turn allows the generation of high-confidence pseudo-labels for self-learning,
thus alleviating ambiguity in temporal localization. Extensive experiments on
THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method
outperforms the state-of-the-art methods, and can be easily plugged into other
WSTAL methods.
</p>

### Title: Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference. (arXiv:2308.12789v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12789](http://arxiv.org/abs/2308.12789)
* Code URL: [https://github.com/uva-dsa/runtime_robscene_seg_2context](https://github.com/uva-dsa/runtime_robscene_seg_2context)
* Copy Paste: `<input type="checkbox">[[2308.12789] Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference](http://arxiv.org/abs/2308.12789) #memory`
* Summary: <p>Surgical context inference has recently garnered significant attention in
robot-assisted surgery as it can facilitate workflow analysis, skill
assessment, and error detection. However, runtime context inference is
challenging since it requires timely and accurate detection of the interactions
among the tools and objects in the surgical scene based on the segmentation of
video data. On the other hand, existing state-of-the-art video segmentation
methods are often biased against infrequent classes and fail to provide
temporal consistency for segmented masks. This can negatively impact the
context inference and accurate detection of critical states. In this study, we
propose a solution to these challenges using a Space Time Correspondence
Network (STCN). STCN is a memory network that performs binary segmentation and
minimizes the effects of class imbalance. The use of a memory bank in STCN
allows for the utilization of past image and segmentation information, thereby
ensuring consistency of the masks. Our experiments using the publicly available
JIGSAWS dataset demonstrate that STCN achieves superior segmentation
performance for objects that are difficult to segment, such as needle and
thread, and improves context inference compared to the state-of-the-art. We
also demonstrate that segmentation and context inference can be performed at
runtime without compromising performance.
</p>

### Title: Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance. (arXiv:2308.12845v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12845](http://arxiv.org/abs/2308.12845)
* Code URL: [https://github.com/xwaiyy123/object-navigation](https://github.com/xwaiyy123/object-navigation)
* Copy Paste: `<input type="checkbox">[[2308.12845] Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance](http://arxiv.org/abs/2308.12845) #memory`
* Summary: <p>Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.
</p>

### Title: SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection. (arXiv:2308.12863v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12863](http://arxiv.org/abs/2308.12863)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12863] SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection](http://arxiv.org/abs/2308.12863) #memory`
* Summary: <p>Multi-modal fusion is increasingly being used for autonomous driving tasks,
as images from different modalities provide unique information for feature
extraction. However, the existing two-stream networks are only fused at a
specific network layer, which requires a lot of manual attempts to set up. As
the CNN goes deeper, the two modal features become more and more advanced and
abstract, and the fusion occurs at the feature level with a large gap, which
can easily hurt the performance. In this study, we propose a novel fusion
architecture called skip-cross networks (SkipcrossNets), which combines
adaptively LiDAR point clouds and camera images without being bound to a
certain fusion epoch. Specifically, skip-cross connects each layer to each
layer in a feed-forward manner, and for each layer, the feature maps of all
previous layers are used as input and its own feature maps are used as input to
all subsequent layers for the other modality, enhancing feature propagation and
multi-modal features fusion. This strategy facilitates selection of the most
similar feature layers from two data pipelines, providing a complementary
effect for sparse point cloud features during fusion processes. The network is
also divided into several blocks to reduce the complexity of feature fusion and
the number of model parameters. The advantages of skip-cross fusion were
demonstrated through application to the KITTI and A2D2 datasets, achieving a
MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model
parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could
be viable for mobile terminals and embedded devices.
</p>

### Title: Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12896](http://arxiv.org/abs/2308.12896)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12896] Beyond Document Page Classification: Design, Datasets, and Challenges](http://arxiv.org/abs/2308.12896) #memory`
* Summary: <p>This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
</p>

### Title: ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12800](http://arxiv.org/abs/2308.12800)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12800] ICU Mortality Prediction Using Long Short-Term Memory Networks](http://arxiv.org/abs/2308.12800) #memory`
* Summary: <p>Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in
complex temporal data regarding patient physiology, which presents an upscale
context for clinical data analysis. In the other hand, identifying the
time-series patterns within these data may provide a high aptitude to predict
clinical events. Hence, we investigate, during this work, the implementation of
an automatic data-driven system, which analyzes large amounts of multivariate
temporal data derived from Electronic Health Records (EHRs), and extracts
high-level information so as to predict in-hospital mortality and Length of
Stay (LOS) early. Practically, we investigate the applicability of LSTM network
by reducing the time-frame to 6-hour so as to enhance clinical tasks. The
experimental results highlight the efficiency of LSTM model with rigorous
multivariate time-series measurements for building real-world prediction
engines.
</p>

### Title: Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.12874](http://arxiv.org/abs/2308.12874)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12874] Easy attention: A simple self-attention mechanism for Transformers](http://arxiv.org/abs/2308.12874) #memory`
* Summary: <p>To improve the robustness of transformer neural networks used for
temporal-dynamics prediction of chaotic systems, we propose a novel attention
mechanism called easy attention. Due to the fact that self attention only makes
usage of the inner product of queries and keys, it is demonstrated that the
keys, queries and softmax are not necessary for obtaining the attention score
required to capture long-term dependencies in temporal sequences. Through
implementing singular-value decomposition (SVD) on the softmax attention score,
we further observe that the self attention compresses contribution from both
queries and keys in the spanned space of the attention score. Therefore, our
proposed easy-attention method directly treats the attention scores as
learnable parameters. This approach produces excellent results when
reconstructing and predicting the temporal dynamics of chaotic systems
exhibiting more robustness and less complexity than the self attention or the
widely-used long short-term memory (LSTM) network. Our results show great
potential for applications in more complex high-dimensional dynamical systems.
</p>

## few-shot
### Title: PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning. (arXiv:2308.12757v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12757](http://arxiv.org/abs/2308.12757)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.12757] PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning](http://arxiv.org/abs/2308.12757) #few-shot`
* Summary: <p>In this work, we address the task of few-shot part segmentation, which aims
to segment the different parts of an unseen object using very few labeled
examples. It is found that leveraging the textual space of a powerful
pre-trained image-language model (such as CLIP) can be beneficial in learning
visual features. Therefore, we develop a novel method termed PartSeg for
few-shot part segmentation based on multimodal learning. Specifically, we
design a part-aware prompt learning method to generate part-specific prompts
that enable the CLIP model to better understand the concept of ``part'' and
fully utilize its textual space. Furthermore, since the concept of the same
part under different object categories is general, we establish relationships
between these parts during the prompt learning process. We conduct extensive
experiments on the PartImageNet and Pascal$\_$Part datasets, and the
experimental results demonstrated that our proposed method achieves
state-of-the-art performance.
</p>

### Title: Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks. (arXiv:2308.12961v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.12961](http://arxiv.org/abs/2308.12961)
* Code URL: [https://github.com/yangyangyang127/tfs3d](https://github.com/yangyangyang127/tfs3d)
* Copy Paste: `<input type="checkbox">[[2308.12961] Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks](http://arxiv.org/abs/2308.12961) #few-shot`
* Summary: <p>To reduce the reliance on large-scale datasets, recent works in 3D
segmentation resort to few-shot learning. Current 3D few-shot semantic
segmentation methods first pre-train the models on `seen' classes, and then
evaluate their generalization performance on `unseen' classes. However, the
prior pre-training stage not only introduces excessive time overhead, but also
incurs a significant domain gap on `unseen' classes. To tackle these issues, we
propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and
a further training-based variant, TFS3D-T. Without any learnable parameters,
TFS3D extracts dense representations by trigonometric positional encodings, and
achieves comparable performance to previous training-based methods. Due to the
elimination of pre-training, TFS3D can alleviate the domain gap issue and save
a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to
train a lightweight query-support transferring attention (QUEST), which
enhances the interaction between the few-shot query and support data.
Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by
+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the
training time by -90%, indicating superior effectiveness and efficiency.
</p>

