## diffusion
### Title: SatDM: Synthesizing Realistic Satellite Image with Semantic Layout Conditioning using Diffusion Models. (arXiv:2309.16812v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16812](http://arxiv.org/abs/2309.16812)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16812] SatDM: Synthesizing Realistic Satellite Image with Semantic Layout Conditioning using Diffusion Models](http://arxiv.org/abs/2309.16812) #diffusion`
* Summary: <p>Deep learning models in the Earth Observation domain heavily rely on the
availability of large-scale accurately labeled satellite imagery. However,
obtaining and labeling satellite imagery is a resource-intensive endeavor.
While generative models offer a promising solution to address data scarcity,
their potential remains underexplored. Recently, Denoising Diffusion
Probabilistic Models (DDPMs) have demonstrated significant promise in
synthesizing realistic images from semantic layouts. In this paper, a
conditional DDPM model capable of taking a semantic map and generating
high-quality, diverse, and correspondingly accurate satellite images is
implemented. Additionally, a comprehensive illustration of the optimization
dynamics is provided. The proposed methodology integrates cutting-edge
techniques such as variance learning, classifier-free guidance, and improved
noise scheduling. The denoising network architecture is further complemented by
the incorporation of adaptive normalization and self-attention mechanisms,
enhancing the model's capabilities. The effectiveness of our proposed model is
validated using a meticulously labeled dataset introduced within the context of
this study. Validation encompasses both algorithmic methods such as Frechet
Inception Distance (FID) and Intersection over Union (IoU), as well as a human
opinion study. Our findings indicate that the generated samples exhibit minimal
deviation from real ones, opening doors for practical applications such as data
augmentation. We look forward to further explorations of DDPMs in a wider
variety of settings and data modalities. An open-source reference
implementation of the algorithm and a link to the benchmarked dataset are
provided at https://github.com/obaghirli/syn10-diffusion.
</p>

### Title: Stochastic Digital Twin for Copy Detection Patterns. (arXiv:2309.16866v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16866](http://arxiv.org/abs/2309.16866)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16866] Stochastic Digital Twin for Copy Detection Patterns](http://arxiv.org/abs/2309.16866) #diffusion`
* Summary: <p>Copy detection patterns (CDP) present an efficient technique for product
protection against counterfeiting. However, the complexity of studying CDP
production variability often results in time-consuming and costly procedures,
limiting CDP scalability. Recent advancements in computer modelling, notably
the concept of a "digital twin" for printing-imaging channels, allow for
enhanced scalability and the optimization of authentication systems. Yet, the
development of an accurate digital twin is far from trivial.
</p>
<p>This paper extends previous research which modelled a printing-imaging
channel using a machine learning-based digital twin for CDP. This model, built
upon an information-theoretic framework known as "Turbo", demonstrated superior
performance over traditional generative models such as CycleGAN and pix2pix.
However, the emerging field of Denoising Diffusion Probabilistic Models (DDPM)
presents a potential advancement in generative models due to its ability to
stochastically model the inherent randomness of the printing-imaging process,
and its impressive performance in image-to-image translation tasks.
</p>
<p>This study aims at comparing the capabilities of the Turbo framework and DDPM
on the same CDP datasets, with the goal of establishing the real-world benefits
of DDPM models for digital twin applications in CDP security. Furthermore, the
paper seeks to evaluate the generative potential of the studied models in the
context of mobile phone data acquisition. Despite the increased complexity of
DDPM methods when compared to traditional approaches, our study highlights
their advantages and explores their potential for future applications.
</p>

### Title: Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16948](http://arxiv.org/abs/2309.16948)
* Code URL: [https://github.com/alexzhou907/DDBM](https://github.com/alexzhou907/DDBM)
* Copy Paste: `<input type="checkbox">[[2309.16948] Denoising Diffusion Bridge Models](http://arxiv.org/abs/2309.16948) #diffusion`
* Summary: <p>Diffusion models are powerful generative models that map noise to data using
stochastic processes. However, for many applications such as image editing, the
model input comes from a distribution that is not random noise. As such,
diffusion models must rely on cumbersome methods like guidance or projected
sampling to incorporate this information in the generative process. In our
work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural
alternative to this paradigm based on diffusion bridges, a family of processes
that interpolate between two paired distributions given as endpoints. Our
method learns the score of the diffusion bridge from data and maps from one
endpoint distribution to the other by solving a (stochastic) differential
equation based on the learned score. Our method naturally unifies several
classes of generative models, such as score-based diffusion models and
OT-Flow-Matching, allowing us to adapt existing design and architectural
choices to our more general problem. Empirically, we apply DDBMs to challenging
image datasets in both pixel and latent space. On standard image translation
problems, DDBMs achieve significant improvement over baseline methods, and,
when we reduce the problem to image generation by setting the source
distribution to random noise, DDBMs achieve comparable FID scores to
state-of-the-art methods despite being built for a more general task.
</p>

### Title: DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation. (arXiv:2309.17074v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17074](http://arxiv.org/abs/2309.17074)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17074] DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation](http://arxiv.org/abs/2309.17074) #diffusion`
* Summary: <p>Diffusion models achieve great success in generating diverse and
high-fidelity images. The performance improvements come with low generation
speed per image, which hinders the application diffusion models in real-time
scenarios. While some certain predictions benefit from the full computation of
the model in each sample iteration, not every iteration requires the same
amount of computation, potentially leading to computation waste. In this work,
we propose DeeDiff, an early exiting framework that adaptively allocates
computation resources in each sampling step to improve the generation
efficiency of diffusion models. Specifically, we introduce a timestep-aware
uncertainty estimation module (UEM) for diffusion models which is attached to
each intermediate layer to estimate the prediction uncertainty of each layer.
The uncertainty is regarded as the signal to decide if the inference
terminates. Moreover, we propose uncertainty-aware layer-wise loss to fill the
performance gap between full models and early-exited models. With such loss
strategy, our model is able to obtain comparable results as full-layer models.
Extensive experiments of class-conditional, unconditional, and text-guided
generation on several datasets show that our method achieves state-of-the-art
performance and efficiency trade-off compared with existing early exiting
methods on diffusion models. More importantly, our method even brings extra
benefits to baseline models and obtains better performance on CIFAR-10 and
Celeb-A datasets. Full code and model are released for reproduction.
</p>

### Title: Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17166](http://arxiv.org/abs/2309.17166)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17166] Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation](http://arxiv.org/abs/2309.17166) #diffusion`
* Summary: <p>The kidney biopsy is the gold standard for the diagnosis of kidney diseases.
Lesion scores made by expert renal pathologists are semi-quantitative and
suffer from high inter-observer variability. Automatically obtaining statistics
per segmented anatomical object, therefore, can bring significant benefits in
reducing labor and this inter-observer variability. Instance segmentation for a
biopsy, however, has been a challenging problem due to (a) the on average large
number (around 300 to 1000) of densely touching anatomical structures, (b) with
multiple classes (at least 3) and (c) in different sizes and shapes. The
currently used instance segmentation models cannot simultaneously deal with
these challenges in an efficient yet generic manner. In this paper, we propose
the first anchor-free instance segmentation model that combines diffusion
models, transformer modules, and RCNNs (regional convolution neural networks).
Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can
efficiently recognize more than 500 objects with 3 common anatomical object
classes in renal biopsies, i.e., glomeruli, tubuli, and arteries. Our data set
consisted of 303 patches extracted from 148 Jones' silver-stained renal whole
slide images (WSIs), where 249 patches were used for training and 54 patches
for evaluation. In addition, without adjustment or retraining, the model can
directly transfer its domain to generate decent instance segmentation results
from PAS-stained WSIs. Importantly, it outperforms other baseline models and
reaches an AP 51.7% in detection as the new state-of-the-art.
</p>

### Title: Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors. (arXiv:2309.17261v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17261](http://arxiv.org/abs/2309.17261)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17261] Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors](http://arxiv.org/abs/2309.17261) #diffusion`
* Summary: <p>Reconstructing 3D objects from a single image guided by pretrained diffusion
models has demonstrated promising outcomes. However, due to utilizing the
case-agnostic rigid strategy, their generalization ability to arbitrary cases
and the 3D consistency of reconstruction are still poor. In this work, we
propose Consistent123, a case-aware two-stage method for highly consistent 3D
asset reconstruction from one image with both 2D and 3D diffusion priors. In
the first stage, Consistent123 utilizes only 3D structural priors for
sufficient geometry exploitation, with a CLIP-based case-aware adaptive
detection mechanism embedded within this process. In the second stage, 2D
texture priors are introduced and progressively take on a dominant guiding
role, delicately sculpting the details of the 3D model. Consistent123 aligns
more closely with the evolving trends in guidance requirements, adaptively
providing adequate 3D geometric initialization and suitable 2D texture
refinement for different objects. Consistent123 can obtain highly 3D-consistent
reconstruction and exhibits strong generalization ability across various
objects. Qualitative and quantitative experiments show that our method
significantly outperforms state-of-the-art image-to-3D methods. See
https://Consistent123.github.io for a more comprehensive exploration of our
generated 3D assets.
</p>

### Title: Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.16952](http://arxiv.org/abs/2309.16952)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16952] Leveraging Optimization for Adaptive Attacks on Image Watermarks](http://arxiv.org/abs/2309.16952) #diffusion`
* Summary: <p>Untrustworthy users can misuse image generators to synthesize high-quality
deepfakes and engage in online spam or disinformation campaigns. Watermarking
deters misuse by marking generated content with a hidden message, enabling its
detection using a secret watermarking key. A core security property of
watermarking is robustness, which states that an attacker can only evade
detection by substantially degrading image quality. Assessing robustness
requires designing an adaptive attack for the specific watermarking algorithm.
A challenge when evaluating watermarking algorithms and their (adaptive)
attacks is to determine whether an adaptive attack is optimal, i.e., it is the
best possible attack. We solve this problem by defining an objective function
and then approach adaptive attacks as an optimization problem. The core idea of
our adaptive attacks is to replicate secret watermarking keys locally by
creating surrogate keys that are differentiable and can be used to optimize the
attack's parameters. We demonstrate for Stable Diffusion models that such an
attacker can break all five surveyed watermarking methods at negligible
degradation in image quality. These findings emphasize the need for more
rigorous robustness testing against adaptive, learnable attackers.
</p>

### Title: Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16750](http://arxiv.org/abs/2309.16750)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16750] Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories](http://arxiv.org/abs/2309.16750) #diffusion`
* Summary: <p>Diffusion Models (DMs) have recently set state-of-the-art on many generation
benchmarks. However, there are myriad ways to describe them mathematically,
which makes it difficult to develop a simple understanding of how they work. In
this survey, we provide a concise overview of DMs from the perspective of
dynamical systems and Ordinary Differential Equations (ODEs) which exposes a
mathematical connection to the highly related yet often overlooked class of
energy-based models, called Associative Memories (AMs). Energy-based AMs are a
theoretical framework that behave much like denoising DMs, but they enable us
to directly compute a Lyapunov energy function on which we can perform gradient
descent to denoise data. We then summarize the 40 year history of energy-based
AMs, beginning with the original Hopfield Network, and discuss new research
directions for AMs and DMs that are revealed by characterizing the extent of
their similarities and differences
</p>

### Title: Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16984](http://arxiv.org/abs/2309.16984)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16984] Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning](http://arxiv.org/abs/2309.16984) #diffusion`
* Summary: <p>Score-based generative models like the diffusion model have been testified to
be effective in modeling multi-modal data from image generation to
reinforcement learning (RL). However, the inference process of diffusion model
can be slow, which hinders its usage in RL with iterative sampling. We propose
to apply the consistency model as an efficient yet expressive policy
representation, namely consistency policy, with an actor-critic style algorithm
for three typical RL settings: offline, offline-to-online and online. For
offline RL, we demonstrate the expressiveness of generative models as policies
from multi-modal data. For offline-to-online RL, the consistency policy is
shown to be more computational efficient than diffusion policy, with a
comparable performance. For online RL, the consistency policy demonstrates
significant speedup and even higher average performances than the diffusion
policy.
</p>

### Title: Sheaf Hypergraph Networks. (arXiv:2309.17116v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17116](http://arxiv.org/abs/2309.17116)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17116] Sheaf Hypergraph Networks](http://arxiv.org/abs/2309.17116) #diffusion`
* Summary: <p>Higher-order relations are widespread in nature, with numerous phenomena
involving complex interactions that extend beyond simple pairwise connections.
As a result, advancements in higher-order processing can accelerate the growth
of various fields requiring structured data. Current approaches typically
represent these interactions using hypergraphs. We enhance this representation
by introducing cellular sheaves for hypergraphs, a mathematical construction
that adds extra structure to the conventional hypergraph while maintaining
their local, higherorder connectivity. Drawing inspiration from existing
Laplacians in the literature, we develop two unique formulations of sheaf
hypergraph Laplacians: linear and non-linear. Our theoretical analysis
demonstrates that incorporating sheaves into the hypergraph Laplacian provides
a more expressive inductive bias than standard hypergraph diffusion, creating a
powerful instrument for effectively modelling complex data structures. We
employ these sheaf hypergraph Laplacians to design two categories of models:
Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks.
These models generalize classical Hypergraph Networks often found in the
literature. Through extensive experimentation, we show that this generalization
significantly improves performance, achieving top results on multiple benchmark
datasets for hypergraph node classification.
</p>

### Title: ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17196](http://arxiv.org/abs/2309.17196)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17196] ResBit: Residual Bit Vector for Categorical Values](http://arxiv.org/abs/2309.17196) #diffusion`
* Summary: <p>The one-hot vector has long been widely used in machine learning as a simple
and generic method for representing discrete data. However, this method
increases the number of dimensions linearly with the categorical data to be
represented, which is problematic from the viewpoint of spatial computational
complexity in deep learning, which requires a large amount of data. Recently,
Analog Bits, a method for representing discrete data as a sequence of bits, was
proposed on the basis of the high expressiveness of diffusion models. However,
since the number of category types to be represented in a generation task is
not necessarily at a power of two, there is a discrepancy between the range
that Analog Bits can represent and the range represented as category data. If
such a value is generated, the problem is that the original category value
cannot be restored. To address this issue, we propose Residual Bit Vector
(ResBit), which is a hierarchical bit representation. Although it is a
general-purpose representation method, in this paper, we treat it as numerical
data and show that it can be used as an extension of Analog Bits using Table
Residual Bit Diffusion (TRBD), which is incorporated into TabDDPM, a tabular
data generation method. We experimentally confirmed that TRBD can generate
diverse and high-quality data from small-scale table data to table data
containing diverse category values faster than TabDDPM. Furthermore, we show
that ResBit can also serve as an alternative to the one-hot vector by utilizing
ResBit for conditioning in GANs and as a label expression in image
classification.
</p>

### Title: Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation. (arXiv:2309.17296v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17296](http://arxiv.org/abs/2309.17296)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17296] Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation](http://arxiv.org/abs/2309.17296) #diffusion`
* Summary: <p>Deep generative diffusion models are a promising avenue for de novo 3D
molecular design in material science and drug discovery. However, their utility
is still constrained by suboptimal performance with large molecular structures
and limited training data. Addressing this gap, we explore the design space of
E(3) equivariant diffusion models, focusing on previously blank spots. Our
extensive comparative analysis evaluates the interplay between continuous and
discrete state spaces. Out of this investigation, we introduce the EQGAT-diff
model, which consistently surpasses the performance of established models on
the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff
takes continuous atomic positions while chemical elements and bond types are
categorical and employ a time-dependent loss weighting that significantly
increases training convergence and the quality of generated samples. To further
strengthen the applicability of diffusion models to limited training data, we
examine the transferability of EQGAT-diff trained on the large PubChem3D
dataset with implicit hydrogens to target distributions with explicit
hydrogens. Fine-tuning EQGAT-diff for a couple of iterations further pushes
state-of-the-art performance across datasets. We envision that our findings
will find applications in structure-based drug design, where the accuracy of
generative models for small datasets of complex molecules is critical.
</p>

## self-supervised
### Title: PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label. (arXiv:2309.16936v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16936](http://arxiv.org/abs/2309.16936)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16936] PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label](http://arxiv.org/abs/2309.16936) #self-supervised`
* Summary: <p>Understanding point clouds captured from the real-world is challenging due to
shifts in data distribution caused by varying object scales, sensor angles, and
self-occlusion. Prior works have addressed this issue by combining recent
learning principles such as self-supervised learning, self-training, and
adversarial training, which leads to significant computational overhead.Toward
succinct yet powerful domain adaptation for point clouds, we revisit the unique
challenges of point cloud data under domain shift scenarios and discover the
importance of the global geometry of source data and trends of target
pseudo-labels biased to the source label distribution. Motivated by our
observations, we propose an adapter-guided domain adaptation method,
PC-Adapter, that preserves the global shape information of the source domain
using an attention-based adapter, while learning the local characteristics of
the target domain via another adapter equipped with graph convolution.
Additionally, we propose a novel pseudo-labeling strategy resilient to the
classifier bias by adjusting confidence scores using their class-wise
confidence distributions to consider relative confidences. Our method
demonstrates superiority over baselines on various domain shift settings in
benchmark datasets - PointDA, GraspNetPC, and PointSegDA.
</p>

### Title: Information Flow in Self-Supervised Learning. (arXiv:2309.17281v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17281](http://arxiv.org/abs/2309.17281)
* Code URL: [https://github.com/yifanzhang-pro/m-mae](https://github.com/yifanzhang-pro/m-mae)
* Copy Paste: `<input type="checkbox">[[2309.17281] Information Flow in Self-Supervised Learning](http://arxiv.org/abs/2309.17281) #self-supervised`
* Summary: <p>In this paper, we provide a comprehensive toolbox for understanding and
enhancing self-supervised learning (SSL) methods through the lens of matrix
information theory. Specifically, by leveraging the principles of matrix mutual
information and joint entropy, we offer a unified analysis for both contrastive
and feature decorrelation based methods. Furthermore, we propose the matrix
variational masked auto-encoder (M-MAE) method, grounded in matrix information
theory, as an enhancement to masked image modeling. The empirical evaluations
underscore the effectiveness of M-MAE compared with the state-of-the-art
methods, including a 3.9% improvement in linear probing ViT-Base, and a 1%
improvement in fine-tuning ViT-Large, both on ImageNet.
</p>

### Title: SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition. (arXiv:2309.16937v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.16937](http://arxiv.org/abs/2309.16937)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16937] SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition](http://arxiv.org/abs/2309.16937) #self-supervised`
* Summary: <p>Multilingual automatic speech recognition (ASR) systems have garnered
attention for their potential to extend language coverage globally. While
self-supervised learning (SSL) has demonstrated its effectiveness in
multilingual ASR, it is worth noting that the various layers' representations
of SSL potentially contain distinct information that has not been fully
leveraged. In this study, we propose a novel method that leverages
self-supervised hierarchical representations (SSHR) to fine-tune multilingual
ASR. We first analyze the different layers of the SSL model for
language-related and content-related information, uncovering layers that show a
stronger correlation. Then, we extract a language-related frame from correlated
middle layers and guide specific content extraction through self-attention
mechanisms. Additionally, we steer the model toward acquiring more
content-related information in the final layers using our proposed Cross-CTC.
We evaluate SSHR on two multilingual datasets, Common Voice and ML-SUPERB, and
the experimental results demonstrate that our method achieves state-of-the-art
performance to the best of our knowledge.
</p>

### Title: Scaling Experiments in Self-Supervised Cross-Table Representation Learning. (arXiv:2309.17339v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17339](http://arxiv.org/abs/2309.17339)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17339] Scaling Experiments in Self-Supervised Cross-Table Representation Learning](http://arxiv.org/abs/2309.17339) #self-supervised`
* Summary: <p>To analyze the scaling potential of deep tabular representation learning
models, we introduce a novel Transformer-based architecture specifically
tailored to tabular data and cross-table representation learning by utilizing
table-specific tokenizers and a shared Transformer backbone. Our training
approach encompasses both single-table and cross-table models, trained via
missing value imputation through a self-supervised masked cell recovery
objective. To understand the scaling behavior of our method, we train models of
varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These
models are trained on a carefully curated pretraining dataset, consisting of
135M training tokens sourced from 76 diverse datasets. We assess the scaling of
our architecture in both single-table and cross-table pretraining setups by
evaluating the pretrained models using linear probing on a curated set of
benchmark datasets and comparing the results with conventional baselines.
</p>

## foundation model
### Title: nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance. (arXiv:2309.16967v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16967](http://arxiv.org/abs/2309.16967)
* Code URL: [https://github.com/kent0n-li/medical-image-segmentation](https://github.com/kent0n-li/medical-image-segmentation)
* Copy Paste: `<input type="checkbox">[[2309.16967] nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance](http://arxiv.org/abs/2309.16967) #foundation model`
* Summary: <p>The recent developments of foundation models in computer vision, especially
the Segment Anything Model (SAM), allow scalable and domain-agnostic image
segmentation to serve as a general-purpose segmentation tool. In parallel, the
field of medical image segmentation has benefited significantly from
specialized neural networks like the nnUNet, which is trained on
domain-specific datasets and can automatically configure the network to tailor
to specific segmentation challenges. To combine the advantages of foundation
models and domain-specific models, we present nnSAM, which synergistically
integrates the SAM model with the nnUNet model to achieve more accurate and
robust medical image segmentation. The nnSAM model leverages the powerful and
robust feature extraction capabilities of SAM, while harnessing the automatic
configuration capabilities of nnUNet to promote dataset-tailored learning. Our
comprehensive evaluation of nnSAM model on different sizes of training samples
shows that it allows few-shot learning, which is highly relevant for medical
image segmentation where high-quality, annotated data can be scarce and costly
to obtain. By melding the strengths of both its predecessors, nnSAM positions
itself as a potential new benchmark in medical image segmentation, offering a
tool that combines broad applicability with specialized efficiency. The code is
available at https://github.com/Kent0n-Li/Medical-Image-Segmentation.
</p>

### Title: A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17264] A Foundation Model for General Moving Object Segmentation in Medical Images](http://arxiv.org/abs/2309.17264) #foundation model`
* Summary: <p>Medical image segmentation aims to delineate the anatomical or pathological
structures of interest, playing a crucial role in clinical diagnosis. A
substantial amount of high-quality annotated data is crucial for constructing
high-precision deep segmentation models. However, medical annotation is highly
cumbersome and time-consuming, especially for medical videos or 3D volumes, due
to the huge labeling space and poor inter-frame consistency. Recently, a
fundamental task named Moving Object Segmentation (MOS) has made significant
advancements in natural images. Its objective is to delineate moving objects
from the background within image sequences, requiring only minimal annotations.
In this paper, we propose the first foundation model, named iMOS, for MOS in
medical images. Extensive experiments on a large multi-modal medical dataset
validate the effectiveness of the proposed iMOS. Specifically, with the
annotation of only a small number of images in the sequence, iMOS can achieve
satisfactory tracking and segmentation performance of moving objects throughout
the entire sequence in bi-directions. We hope that the proposed iMOS can help
accelerate the annotation speed of experts, and boost the development of
medical foundation models.
</p>

### Title: Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17007](http://arxiv.org/abs/2309.17007)
* Code URL: [https://github.com/peterhan91/fm_adv](https://github.com/peterhan91/fm_adv)
* Copy Paste: `<input type="checkbox">[[2309.17007] Medical Foundation Models are Susceptible to Targeted Misinformation Attacks](http://arxiv.org/abs/2309.17007) #foundation model`
* Summary: <p>Large language models (LLMs) have broad medical knowledge and can reason
about medical information across many domains, holding promising potential for
diverse medical applications in the near future. In this study, we demonstrate
a concerning vulnerability of LLMs in medicine. Through targeted manipulation
of just 1.1% of the model's weights, we can deliberately inject an incorrect
biomedical fact. The erroneous information is then propagated in the model's
output, whilst its performance on other biomedical tasks remains intact. We
validate our findings in a set of 1,038 incorrect biomedical facts. This
peculiar susceptibility raises serious security and trustworthiness concerns
for the application of LLMs in healthcare settings. It accentuates the need for
robust protective measures, thorough verification mechanisms, and stringent
management of access to these models, ensuring their reliable and safe use in
medical practice.
</p>

## generative
### Title: Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16779](http://arxiv.org/abs/2309.16779)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16779] Intriguing properties of generative classifiers](http://arxiv.org/abs/2309.16779) #generative`
* Summary: <p>What is the best paradigm to recognize objects -- discriminative inference
(fast but potentially prone to shortcut learning) or using a generative model
(slow but potentially more robust)? We build on recent advances in generative
modeling that turn text-to-image models into classifiers. This allows us to
study their behavior and to compare them against discriminative models and
human psychophysical data. We report four intriguing emergent properties of
generative classifiers: they show a record-breaking human-like shape bias (99%
for Imagen), near human-level out-of-distribution accuracy, state-of-the-art
alignment with human classification errors, and they understand certain
perceptual illusions. Our results indicate that while the current dominant
paradigm for modeling human object recognition is discriminative inference,
zero-shot generative models approximate human object recognition data
surprisingly well.
</p>

### Title: Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process. (arXiv:2309.17031v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17031](http://arxiv.org/abs/2309.17031)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17031] Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process](http://arxiv.org/abs/2309.17031) #generative`
* Summary: <p>Understanding the temporal dynamics of Earth's surface is a mission of
multi-temporal remote sensing image analysis, significantly promoted by deep
vision models with its fuel -- labeled multi-temporal images. However,
collecting, preprocessing, and annotating multi-temporal remote sensing images
at scale is non-trivial since it is expensive and knowledge-intensive. In this
paper, we present a scalable multi-temporal remote sensing change data
generator via generative modeling, which is cheap and automatic, alleviating
these problems. Our main idea is to simulate a stochastic change process over
time. We consider the stochastic change process as a probabilistic semantic
state transition, namely generative probabilistic change model (GPCM), which
decouples the complex simulation problem into two more trackable sub-problems,
\ie, change event simulation and semantic change synthesis. To solve these two
problems, we present the change generator (Changen), a GAN-based GPCM, enabling
controllable object change data generation, including customizable object
property, and change event. The extensive experiments suggest that our Changen
has superior generation capability, and the change detectors with Changen
pre-training exhibit excellent transferability to real-world change datasets.
</p>

### Title: GAIA-1: A Generative World Model for Autonomous Driving. (arXiv:2309.17080v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17080](http://arxiv.org/abs/2309.17080)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17080] GAIA-1: A Generative World Model for Autonomous Driving](http://arxiv.org/abs/2309.17080) #generative`
* Summary: <p>Autonomous driving promises transformative improvements to transportation,
but building systems capable of safely navigating the unstructured complexity
of real-world scenarios remains challenging. A critical problem lies in
effectively predicting the various potential outcomes that may emerge in
response to the vehicle's actions as the world evolves.
</p>
<p>To address this challenge, we introduce GAIA-1 ('Generative AI for
Autonomy'), a generative world model that leverages video, text, and action
inputs to generate realistic driving scenarios while offering fine-grained
control over ego-vehicle behavior and scene features. Our approach casts world
modeling as an unsupervised sequence modeling problem by mapping the inputs to
discrete tokens, and predicting the next token in the sequence. Emerging
properties from our model include learning high-level structures and scene
dynamics, contextual awareness, generalization, and understanding of geometry.
The power of GAIA-1's learned representation that captures expectations of
future events, combined with its ability to generate realistic samples,
provides new possibilities for innovation in the field of autonomy, enabling
enhanced and accelerated training of autonomous driving technology.
</p>

### Title: Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining. (arXiv:2309.17123v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17123](http://arxiv.org/abs/2309.17123)
* Code URL: [https://github.com/peterhan91/diffchest](https://github.com/peterhan91/diffchest)
* Copy Paste: `<input type="checkbox">[[2309.17123] Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining](http://arxiv.org/abs/2309.17123) #generative`
* Summary: <p>Detecting misleading patterns in automated diagnostic assistance systems,
such as those powered by Artificial Intelligence, is critical to ensuring their
reliability, particularly in healthcare. Current techniques for evaluating deep
learning models cannot visualize confounding factors at a diagnostic level.
Here, we propose a self-conditioned diffusion model termed DiffChest and train
it on a dataset of 515,704 chest radiographs from 194,956 patients from
multiple healthcare centers in the United States and Europe. DiffChest explains
classifications on a patient-specific level and visualizes the confounding
factors that may mislead the model. We found high inter-reader agreement when
evaluating DiffChest's capability to identify treatment-related confounders,
with Fleiss' Kappa values of 0.8 or higher across most imaging findings.
Confounders were accurately captured with 11.1% to 100% prevalence rates.
Furthermore, our pretraining process optimized the model to capture the most
relevant information from the input radiographs. DiffChest achieved excellent
diagnostic accuracy when diagnosing 11 chest conditions, such as pleural
effusion and cardiac insufficiency, and at least sufficient diagnostic accuracy
for the remaining conditions. Our findings highlight the potential of
pretraining based on diffusion models in medical image classification,
specifically in providing insights into confounding factors and model
robustness.
</p>

### Title: TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields. (arXiv:2309.17175v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17175](http://arxiv.org/abs/2309.17175)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17175] TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields](http://arxiv.org/abs/2309.17175) #generative`
* Summary: <p>Recent works learn 3D representation explicitly under text-3D guidance.
However, limited text-3D data restricts the vocabulary scale and text control
of generations. Generators may easily fall into a stereotype concept for
certain text prompts, thus losing open-vocabulary generation ability. To tackle
this issue, we introduce a conditional 3D generative model, namely TextField3D.
Specifically, rather than using the text prompts as input directly, we suggest
to inject dynamic noise into the latent space of given text prompts, i.e.,
Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the
appropriate range of textual latent space that is expanded by NTFs. To this
end, an NTFGen module is proposed to model general text latent code in noisy
fields. Meanwhile, an NTFBind module is proposed to align view-invariant image
latent code to noisy fields, further supporting image-conditional 3D
generation. To guide the conditional generation in both geometry and texture,
multi-modal discrimination is constructed with a text-3D discriminator and a
text-2.5D discriminator. Compared to previous methods, TextField3D includes
three merits: 1) large vocabulary, 2) text consistency, and 3) low latency.
Extensive experiments demonstrate that our method achieves a potential
open-vocabulary 3D generation capability.
</p>

### Title: An evaluation of GPT models for phenotype concept recognition. (arXiv:2309.17169v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.17169](http://arxiv.org/abs/2309.17169)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17169] An evaluation of GPT models for phenotype concept recognition](http://arxiv.org/abs/2309.17169) #generative`
* Summary: <p>Objective: Clinical deep phenotyping plays a critical role in both the
diagnosis of patients with rare disorders as well as in building care
coordination plans. The process relies on modelling and curating patient
profiles using ontology concepts, usually from the Human Phenotype Ontology.
Machine learning methods have been widely adopted to support this phenotype
concept recognition task. With the significant shift in the use of large
language models (LLMs) for most NLP tasks, herewithin, we examine the
performance of the latest Generative Pre-trained Transformer (GPT) models
underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The
experimental setup of the study included seven prompts of various levels of
specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold
standard for phenotype recognition. Results: Our results show that, currently,
these models have not yet achieved state of the art performance. The best run,
using few-shots learning, achieved 0.41 F1 score, compared to a 0.62 F1 score
achieved by the current best in class tool. Conclusion: The non-deterministic
nature of the outcomes and the lack of concordance between different runs using
the same prompt and input makes the use of these LLMs in clinical settings
problematic.
</p>

### Title: ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16918](http://arxiv.org/abs/2309.16918)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16918] ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks](http://arxiv.org/abs/2309.16918) #generative`
* Summary: <p>Graph neural networks (GNNs) have proven their efficacy in a variety of
real-world applications, but their underlying mechanisms remain a mystery. To
address this challenge and enable reliable decision-making, many GNN explainers
have been proposed in recent years. However, these methods often encounter
limitations, including their dependence on specific instances, lack of
generalizability to unseen graphs, producing potentially invalid explanations,
and yielding inadequate fidelity. To overcome these limitations, we, in this
paper, introduce the Auxiliary Classifier Generative Adversarial Network
(ACGAN) into the field of GNN explanation and propose a new GNN explainer
dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce
explanations for the original input graphs while incorporating a discriminator
to oversee the generation process, ensuring explanation fidelity and improving
accuracy. Experimental evaluations conducted on both synthetic and real-world
graph datasets demonstrate the superiority of our proposed method compared to
other existing GNN explainers.
</p>

## anomaly
### Title: Algorithmic Recourse for Anomaly Detection in Multivariate Time Series. (arXiv:2309.16896v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16896](http://arxiv.org/abs/2309.16896)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16896] Algorithmic Recourse for Anomaly Detection in Multivariate Time Series](http://arxiv.org/abs/2309.16896) #anomaly`
* Summary: <p>Anomaly detection in multivariate time series has received extensive study
due to the wide spectrum of applications. An anomaly in multivariate time
series usually indicates a critical event, such as a system fault or an
external attack. Therefore, besides being effective in anomaly detection,
recommending anomaly mitigation actions is also important in practice yet
under-investigated. In this work, we focus on algorithmic recourse in time
series anomaly detection, which is to recommend fixing actions on abnormal time
series with a minimum cost so that domain experts can understand how to fix the
abnormal behavior. To this end, we propose an algorithmic recourse framework,
called RecAD, which can recommend recourse actions to flip the abnormal time
steps. Experiments on two synthetic and one real-world datasets show the
effectiveness of our framework.
</p>

## in-context
### Title: Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.17012](http://arxiv.org/abs/2309.17012)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17012] Benchmarking Cognitive Biases in Large Language Models as Evaluators](http://arxiv.org/abs/2309.17012) #in-context`
* Summary: <p>Large Language Models (LLMs) have recently been shown to be effective as
automatic evaluators with simple prompting and in-context learning. In this
work, we assemble 15 LLMs of four different size ranges and evaluate their
output responses by preference ranking from the other LLMs as evaluators, such
as System Star is better than System Square. We then evaluate the quality of
ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators
(CoBBLEr), a benchmark to measure six different cognitive biases in LLM
evaluation outputs, such as the Egocentric bias where a model prefers to rank
its own outputs highly in evaluation. We find that LLMs are biased text quality
evaluators, exhibiting strong indications on our bias benchmark (average of 40%
of comparisons across all models) within each of their evaluations that
question their robustness as evaluators. Furthermore, we examine the
correlation between human and machine preferences and calculate the average
Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine
preferences are misaligned with humans. According to our findings, LLMs may
still be unable to be utilized for automatic annotation aligned with human
preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.
</p>

### Title: SCALE: Synergized Collaboration of Asymmetric Language Translation Engines. (arXiv:2309.17061v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.17061](http://arxiv.org/abs/2309.17061)
* Code URL: [https://github.com/hannibal046/scale](https://github.com/hannibal046/scale)
* Copy Paste: `<input type="checkbox">[[2309.17061] SCALE: Synergized Collaboration of Asymmetric Language Translation Engines](http://arxiv.org/abs/2309.17061) #in-context`
* Summary: <p>In this paper, we introduce SCALE, a collaborative framework that connects
compact Specialized Translation Models (STMs) and general-purpose Large
Language Models (LLMs) as one unified translation engine. By introducing
translation from STM into the triplet in-context demonstrations, SCALE unlocks
refinement and pivoting ability of LLM, thus mitigating language bias of LLM
and parallel data bias of STM, enhancing LLM speciality without sacrificing
generality, and facilitating continual learning without expensive LLM
fine-tuning. Our comprehensive experiments show that SCALE significantly
outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in
challenging low-resource settings. Moreover, in Xhosa to English translation,
SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM
and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when
equipped with a compact model consisting of merely 600M parameters. SCALE could
also effectively exploit the existing language bias of LLMs by using an
English-centric STM as a pivot for translation between any language pairs,
outperforming few-shot GPT-4 by an average of 6 COMET points across eight
translation directions. Furthermore we provide an in-depth analysis of SCALE's
robustness, translation characteristics, and latency costs, providing solid
foundation for future studies exploring the potential synergy between LLMs and
more specialized, task-specific models.
</p>

### Title: Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.17249](http://arxiv.org/abs/2309.17249)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17249] Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering](http://arxiv.org/abs/2309.17249) #in-context`
* Summary: <p>Prompting and in-context learning (ICL) have become efficient learning
paradigms for large language models (LLMs). However, LLMs suffer from prompt
brittleness and various bias factors in the prompt, including but not limited
to the formatting, the choice verbalizers, and the ICL examples. To address
this problem that results in unexpected performance degradation, calibration
methods have been developed to mitigate the effects of these biases while
recovering LLM performance. In this work, we first conduct a systematic
analysis of the existing calibration methods, where we both provide a unified
view and reveal the failure cases. Inspired by these analyses, we propose Batch
Calibration (BC), a simple yet intuitive method that controls the contextual
bias from the batched input, unifies various prior approaches, and effectively
addresses the aforementioned issues. BC is zero-shot, inference-only, and
incurs negligible additional costs. In the few-shot setup, we further extend BC
to allow it to learn the contextual bias from labeled data. We validate the
effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate
state-of-the-art performance over previous calibration baselines across more
than 10 natural language understanding and image classification tasks.
</p>

## memory
### Title: ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens. (arXiv:2309.16738v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16738](http://arxiv.org/abs/2309.16738)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16738] ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens](http://arxiv.org/abs/2309.16738) #memory`
* Summary: <p>Learning a versatile language-image model is computationally prohibitive
under a limited computing budget. This paper delves into the efficient
language-image pre-training, an area that has received relatively little
attention despite its importance in reducing computational cost and footprint.
To that end, we propose a vision token pruning and merging method, ie ELIP, to
remove less influential tokens based on the supervision of language outputs.
Our method is designed with several strengths, such as being
computation-efficient, memory-efficient, and trainable-parameter-free, and is
distinguished from previous vision-only token pruning approaches by its
alignment with task objectives. We implement this method in a progressively
pruning manner using several sequential blocks. To evaluate its generalization
performance, we apply ELIP to three commonly used language-image pre-training
models and utilize public image-caption pairs with 4M images for pre-training.
Our experiments demonstrate that with the removal of ~30$\%$ vision tokens
across 12 ViT layers, ELIP maintains significantly comparable performance with
baselines ($\sim$0.32 accuracy drop on average) over various downstream tasks
including cross-modal retrieval, VQA, image captioning, etc. In addition, the
spared GPU resources by our ELIP allow us to scale up with larger batch sizes,
thereby accelerating model pre-training and even sometimes enhancing downstream
model performance. Our code will be released at
https://github.com/guoyang9/ELIP.
</p>

### Title: Ultra-low-power Image Classification on Neuromorphic Hardware. (arXiv:2309.16795v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16795](http://arxiv.org/abs/2309.16795)
* Code URL: [https://github.com/biphasic/quartz-on-loihi](https://github.com/biphasic/quartz-on-loihi)
* Copy Paste: `<input type="checkbox">[[2309.16795] Ultra-low-power Image Classification on Neuromorphic Hardware](http://arxiv.org/abs/2309.16795) #memory`
* Summary: <p>Spiking neural networks (SNNs) promise ultra-low-power applications by
exploiting temporal and spatial sparsity. The number of binary activations,
called spikes, is proportional to the power consumed when executed on
neuromorphic hardware. Training such SNNs using backpropagation through time
for vision tasks that rely mainly on spatial features is computationally
costly. Training a stateless artificial neural network (ANN) to then convert
the weights to an SNN is a straightforward alternative when it comes to image
recognition datasets. Most conversion methods rely on rate coding in the SNN to
represent ANN activation, which uses enormous amounts of spikes and, therefore,
energy to encode information. Recently, temporal conversion methods have shown
promising results requiring significantly fewer spikes per neuron, but
sometimes complex neuron models. We propose a temporal ANN-to-SNN conversion
method, which we call Quartz, that is based on the time to first spike (TTFS).
Quartz achieves high classification accuracy and can be easily implemented on
neuromorphic hardware while using the least amount of synaptic operations and
memory accesses. It incurs a cost of two additional synapses per neuron
compared to previous temporal conversion methods, which are readily available
on neuromorphic hardware. We benchmark Quartz on MNIST, CIFAR10, and ImageNet
in simulation to show the benefits of our method and follow up with an
implementation on Loihi, a neuromorphic chip by Intel. We provide evidence that
temporal coding has advantages in terms of power consumption, throughput, and
latency for similar classification accuracy. Our code and models are publicly
available.
</p>

### Title: Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16849](http://arxiv.org/abs/2309.16849)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16849] Space-Time Attention with Shifted Non-Local Search](http://arxiv.org/abs/2309.16849) #memory`
* Summary: <p>Efficiently computing attention maps for videos is challenging due to the
motion of objects between frames. While a standard non-local search is
high-quality for a window surrounding each query point, the window's small size
cannot accommodate motion. Methods for long-range motion use an auxiliary
network to predict the most similar key coordinates as offsets from each query
location. However, accurately predicting this flow field of offsets remains
challenging, even for large-scale networks. Small spatial inaccuracies
significantly impact the attention module's quality. This paper proposes a
search strategy that combines the quality of a non-local search with the range
of predicted offsets. The method, named Shifted Non-Local Search, executes a
small grid search surrounding the predicted offsets to correct small spatial
errors. Our method's in-place computation consumes 10 times less memory and is
over 3 times faster than previous work. Experimentally, correcting the small
spatial errors improves the video frame alignment quality by over 3 dB PSNR.
Our search upgrades existing space-time attention modules, which improves video
denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We
integrate our space-time attention module into a UNet-like architecture to
achieve state-of-the-art results on video denoising.
</p>

### Title: ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16916](http://arxiv.org/abs/2309.16916)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16916] ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values](http://arxiv.org/abs/2309.16916) #memory`
* Summary: <p>Understanding why a neural network model makes certain decisions can be as
important as the inference performance. Various methods have been proposed to
help practitioners explain the prediction of a neural network model, of which
Shapley values are most popular. SHAP package is a leading implementation of
Shapley values to explain neural networks implemented in TensorFlow or PyTorch
but lacks cross-platform support, one-shot deployment and is highly
inefficient. To address these problems, we present the ONNXExplainer, which is
a generic framework to explain neural networks using Shapley values in the ONNX
ecosystem. In ONNXExplainer, we develop its own automatic differentiation and
optimization approach, which not only enables One-Shot Deployment of neural
networks inference and explanations, but also significantly improves the
efficiency to compute explanation with less memory consumption. For fair
comparison purposes, we also implement the same optimization in TensorFlow and
PyTorch and measure its performance against the current state of the art
open-source counterpart, SHAP. Extensive benchmarks demonstrate that the
proposed optimization approach improves the explanation latency of VGG19,
ResNet50, DenseNet201, and EfficientNetB0 by as much as 500%.
</p>

### Title: Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling. (arXiv:2309.17105v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17105](http://arxiv.org/abs/2309.17105)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17105] Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling](http://arxiv.org/abs/2309.17105) #memory`
* Summary: <p>Action Quality Assessment (AQA) is a task that tries to answer how well an
action is carried out. While remarkable progress has been achieved, existing
works on AQA assume that all the training data are visible for training in one
time, but do not enable continual learning on assessing new technical actions.
In this work, we address such a Continual Learning problem in AQA
(Continual-AQA), which urges a unified model to learn AQA tasks sequentially
without forgetting. Our idea for modeling Continual-AQA is to sequentially
learn a task-consistent score-discriminative feature distribution, in which the
latent features express a strong correlation with the score labels regardless
of the task or action types. From this perspective, we aim to mitigate the
forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of
new and previous data into a score-discriminative distribution, a novel
Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data
from previous tasks with limited memory size. Secondly, an Action
General-Specific Graph is developed to learn and decouple the action-general
and action-specific knowledge so that the task-consistent score-discriminative
features can be better extracted across various tasks. Extensive experiments
are conducted to evaluate the contributions of proposed components. The
comparisons with the existing continual learning methods additionally verify
the effectiveness and versatility of our approach.
</p>

### Title: Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings. (arXiv:2309.17361v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.17361](http://arxiv.org/abs/2309.17361)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17361] Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings](http://arxiv.org/abs/2309.17361) #memory`
* Summary: <p>The massive interest in deep neural networks (DNNs) for both computer vision
and natural language processing has been sparked by the growth in computational
power. However, this led to an increase in the memory footprint, to a point
where it can be challenging to simply load a model on commodity devices such as
mobile phones. To address this limitation, quantization is a favored solution
as it maps high precision tensors to a low precision, memory efficient format.
In terms of memory footprint reduction, its most effective variants are based
on codebooks. These methods, however, suffer from two limitations. First, they
either define a single codebook for each tensor, or use a memory-expensive
mapping to multiple codebooks. Second, gradient descent optimization of the
mapping favors jumps toward extreme values, hence not defining a proximal
search. In this work, we propose to address these two limitations. First, we
initially group similarly distributed neurons and leverage the re-ordered
structure to either apply different scale factors to the different groups, or
map weights that fall in these groups to several codebooks, without any mapping
overhead. Second, stemming from this initialization, we propose a joint
learning of the codebook and weight mappings that bears similarities with
recent gradient-based post-training quantization techniques. Third, drawing
estimation from straight-through estimation techniques, we introduce a novel
gradient update definition to enable a proximal search of the codebooks and
their mappings. The proposed jointly learnable codebooks and mappings (JLCM)
method allows a very efficient approximation of any DNN: as such, a Llama 7B
can be compressed down to 2Go and loaded on 5-year-old smartphones.
</p>

### Title: GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch. (arXiv:2309.16809v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16809](http://arxiv.org/abs/2309.16809)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16809] GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch](http://arxiv.org/abs/2309.16809) #memory`
* Summary: <p>The online Gradient Balancing (GraB) algorithm greedily choosing the examples
ordering by solving the herding problem using per-sample gradients is proved to
be the theoretically optimal solution that guarantees to outperform Random
Reshuffling. However, there is currently no efficient implementation of GraB
for the community to easily use it.
</p>
<p>This work presents an efficient Python library, $\textit{GraB-sampler}$, that
allows the community to easily use GraB algorithms and proposes 5 variants of
the GraB algorithm. The best performance result of the GraB-sampler reproduces
the training loss and test accuracy results while only in the cost of 8.7%
training time overhead and 0.85% peak GPU memory usage overhead.
</p>

### Title: Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling. (arXiv:2309.16882v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.16882](http://arxiv.org/abs/2309.16882)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16882] Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling](http://arxiv.org/abs/2309.16882) #memory`
* Summary: <p>Time series modeling, a crucial area in science, often encounters challenges
when training Machine Learning (ML) models like Recurrent Neural Networks
(RNNs) using the conventional mini-batch training strategy that assumes
independent and identically distributed (IID) samples and initializes RNNs with
zero hidden states. The IID assumption ignores temporal dependencies among
samples, resulting in poor performance. This paper proposes the Message
Propagation Through Time (MPTT) algorithm to effectively incorporate long
temporal dependencies while preserving faster training times relative to the
stateful solutions. MPTT utilizes two memory modules to asynchronously manage
initial hidden states for RNNs, fostering seamless information exchange between
samples and allowing diverse mini-batches throughout epochs. MPTT further
implements three policies to filter outdated and preserve essential information
in the hidden states to generate informative initial hidden states for RNNs,
facilitating robust training. Experimental results demonstrate that MPTT
outperforms seven strategies on four climate datasets with varying levels of
temporal dependencies.
</p>

### Title: Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17207](http://arxiv.org/abs/2309.17207)
* Code URL: [https://github.com/marcometer/endless-memory-gym](https://github.com/marcometer/endless-memory-gym)
* Copy Paste: `<input type="checkbox">[[2309.17207] Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes](http://arxiv.org/abs/2309.17207) #memory`
* Summary: <p>Memory Gym introduces a unique benchmark designed to test Deep Reinforcement
Learning agents, specifically comparing Gated Recurrent Unit (GRU) against
Transformer-XL (TrXL), on their ability to memorize long sequences, withstand
noise, and generalize. It features partially observable 2D environments with
discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights.
These originally finite environments are extrapolated to novel endless tasks
that act as an automatic curriculum, drawing inspiration from the car game ``I
packed my bag". These endless tasks are not only beneficial for evaluating
efficiency but also intriguingly valuable for assessing the effectiveness of
approaches in memory-based agents. Given the scarcity of publicly available
memory baselines, we contribute an implementation driven by TrXL and Proximal
Policy Optimization. This implementation leverages TrXL as episodic memory
using a sliding window approach. In our experiments on the finite environments,
TrXL demonstrates superior sample efficiency in Mystery Path and outperforms in
Mortar Mayhem. However, GRU is more efficient on Searing Spotlights. Most
notably, in all endless tasks, GRU makes a remarkable resurgence, consistently
outperforming TrXL by significant margins.
</p>

### Title: Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.17357](http://arxiv.org/abs/2309.17357)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17357] Module-wise Training of Neural Networks via the Minimizing Movement Scheme](http://arxiv.org/abs/2309.17357) #memory`
* Summary: <p>Greedy layer-wise or module-wise training of neural networks is compelling in
constrained and on-device settings where memory is limited, as it circumvents a
number of problems of end-to-end back-propagation. However, it suffers from a
stagnation problem, whereby early layers overfit and deeper layers stop
increasing the test accuracy after a certain depth. We propose to solve this
issue by introducing a module-wise regularization inspired by the minimizing
movement scheme for gradient flows in distribution space. We call the method
TRGL for Transport Regularized Greedy Learning and study it theoretically,
proving that it leads to greedy modules that are regular and that progressively
solve the task. Experimentally, we show improved accuracy of module-wise
training of various architectures such as ResNets, Transformers and VGG, when
our regularization is added, superior to that of other module-wise training
methods and often to end-to-end training, with as much as 60% less memory
usage.
</p>

## few-shot
### Title: Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis. (arXiv:2309.16859v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.16859](http://arxiv.org/abs/2309.16859)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.16859] Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis](http://arxiv.org/abs/2309.16859) #few-shot`
* Summary: <p>NeRFs have enabled highly realistic synthesis of human faces including
complex appearance and reflectance effects of hair and skin. These methods
typically require a large number of multi-view input images, making the process
hardware intensive and cumbersome, limiting applicability to unconstrained
settings. We propose a novel volumetric human face prior that enables the
synthesis of ultra high-resolution novel views of subjects that are not part of
the prior's training distribution. This prior model consists of an
identity-conditioned NeRF, trained on a dataset of low-resolution multi-view
images of diverse humans with known camera calibration. A simple sparse
landmark-based 3D alignment of the training dataset allows our model to learn a
smooth latent space of geometry and appearance despite a limited number of
training identities. A high-quality volumetric representation of a novel
subject can be obtained by model fitting to 2 or 3 camera views of arbitrary
resolution. Importantly, our method requires as few as two views of casually
captured images as input at inference time.
</p>

### Title: Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions. (arXiv:2309.17313v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.17313](http://arxiv.org/abs/2309.17313)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.17313] Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions](http://arxiv.org/abs/2309.17313) #few-shot`
* Summary: <p>Recent works considering professional legal-linguistic style (PLLS) texts
have shown promising results on the charge prediction task. However,
unprofessional users also show an increasing demand on such a prediction
service. There is a clear domain discrepancy between PLLS texts and non-PLLS
texts expressed by those laypersons, which degrades the current SOTA models'
performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data
for most charge classes. This paper proposes a novel few-shot domain adaptation
(FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP).
Compared with existing FSDA works, which solely perform instance-level
alignment without considering the negative impact of text style information
existing in latent features, DLCCP (1) disentangles the content and style
representations for better domain-invariant legal content learning with
carefully designed optimization goals for content and style spaces and, (2)
employs the constitutive elements knowledge of charges to extract and align
element-level and instance-level content representations simultaneously. We
contribute the first publicly available non-PLLS dataset named NCCP for
developing layperson-friendly charge prediction models. Experiments on NCCP
show the superiority of our methods over competitive baselines.
</p>

