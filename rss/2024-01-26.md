## diffusion
### Title: Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All. (arXiv:2401.13795v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13795](http://arxiv.org/abs/2401.13795)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13795] Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All](http://arxiv.org/abs/2401.13795) #diffusion`
* Summary: <p>As online shopping is growing, the ability for buyers to virtually visualize
products in their settings-a phenomenon we define as "Virtual Try-All"-has
become crucial. Recent diffusion models inherently contain a world model,
rendering them suitable for this task within an inpainting context. However,
traditional image-conditioned diffusion models often fail to capture the
fine-grained details of products. In contrast, personalization-driven models
such as DreamPaint are good at preserving the item's details but they are not
optimized for real-time applications. We present "Diffuse to Choose," a novel
diffusion-based image-conditioned inpainting model that efficiently balances
fast inference with the retention of high-fidelity details in a given reference
item while ensuring accurate semantic manipulations in the given scene content.
Our approach is based on incorporating fine-grained features from the reference
image directly into the latent feature maps of the main diffusion model,
alongside with a perceptual loss to further preserve the reference item's
details. We conduct extensive testing on both in-house and publicly available
datasets, and show that Diffuse to Choose is superior to existing zero-shot
diffusion inpainting methods as well as few-shot diffusion personalization
algorithms like DreamPaint.
</p>

### Title: StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models. (arXiv:2401.13942v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13942](http://arxiv.org/abs/2401.13942)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13942] StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models](http://arxiv.org/abs/2401.13942) #diffusion`
* Summary: <p>The ability to fine-tune generative models for text-to-image generation tasks
is crucial, particularly facing the complexity involved in accurately
interpreting and visualizing textual inputs. While LoRA is efficient for
language model adaptation, it often falls short in text-to-image tasks due to
the intricate demands of image generation, such as accommodating a broad
spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a
specialized fine-tuning approach tailored for text-to-image models. StyleInject
comprises multiple parallel low-rank parameter matrices, maintaining the
diversity of visual features. It dynamically adapts to varying styles by
adjusting the variance of visual features based on the characteristics of the
input signal. This approach significantly minimizes the impact on the original
model's text-image alignment capabilities while adeptly adapting to various
styles in transfer learning. StyleInject proves particularly effective in
learning from and enhancing a range of advanced, community-fine-tuned
generative models. Our comprehensive experiments, including both small-sample
and large-scale data fine-tuning as well as base model distillation, show that
StyleInject surpasses traditional LoRA in both text-image semantic consistency
and human preference evaluation, all while ensuring greater parameter
efficiency.
</p>

### Title: BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13974](http://arxiv.org/abs/2401.13974)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13974] BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models](http://arxiv.org/abs/2401.13974) #diffusion`
* Summary: <p>Recent text-to-image generation models have demonstrated incredible success
in generating images that faithfully follow input prompts. However, the
requirement of using words to describe a desired concept provides limited
control over the appearance of the generated concepts. In this work, we address
this shortcoming by proposing an approach to enable personalization
capabilities in existing text-to-image diffusion models. We propose a novel
architecture (BootPIG) that allows a user to provide reference images of an
object in order to guide the appearance of a concept in the generated images.
</p>
<p>The proposed BootPIG architecture makes minimal modifications to a pretrained
text-to-image diffusion model and utilizes a separate UNet model to steer the
generations toward the desired appearance. We introduce a training procedure
that allows us to bootstrap personalization capabilities in the BootPIG
architecture using data generated from pretrained text-to-image models, LLM
chat agents, and image segmentation models. In contrast to existing methods
that require several days of pretraining, the BootPIG architecture can be
trained in approximately 1 hour. Experiments on the DreamBooth dataset
demonstrate that BootPIG outperforms existing zero-shot methods while being
comparable with test-time finetuning approaches. Through a user study, we
validate the preference for BootPIG generations over existing methods both in
maintaining fidelity to the reference object's appearance and aligning with
textual prompts.
</p>

### Title: Diffusion-based Data Augmentation for Object Counting Problems. (arXiv:2401.13992v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13992](http://arxiv.org/abs/2401.13992)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13992] Diffusion-based Data Augmentation for Object Counting Problems](http://arxiv.org/abs/2401.13992) #diffusion`
* Summary: <p>Crowd counting is an important problem in computer vision due to its wide
range of applications in image understanding. Currently, this problem is
typically addressed using deep learning approaches, such as Convolutional
Neural Networks (CNNs) and Transformers. However, deep networks are data-driven
and are prone to overfitting, especially when the available labeled crowd
dataset is limited. To overcome this limitation, we have designed a pipeline
that utilizes a diffusion model to generate extensive training data. We are the
first to generate images conditioned on a location dot map (a binary dot map
that specifies the location of human heads) with a diffusion model. We are also
the first to use these diverse synthetic data to augment the crowd counting
models. Our proposed smoothed density map input for ControlNet significantly
improves ControlNet's performance in generating crowds in the correct
locations. Also, Our proposed counting loss for the diffusion model effectively
minimizes the discrepancies between the location dot map and the crowd images
generated. Additionally, our innovative guidance sampling further directs the
diffusion process toward regions where the generated crowd images align most
accurately with the location dot map. Collectively, we have enhanced
ControlNet's ability to generate specified objects from a location dot map,
which can be used for data augmentation in various counting problems. Moreover,
our framework is versatile and can be easily adapted to all kinds of counting
problems. Extensive experiments demonstrate that our framework improves the
counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS
datasets, showcasing its effectiveness.
</p>

### Title: CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14066](http://arxiv.org/abs/2401.14066)
* Code URL: [https://github.com/haha-lisa/creativesynth](https://github.com/haha-lisa/creativesynth)
* Copy Paste: `<input type="checkbox">[[2401.14066] CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion](http://arxiv.org/abs/2401.14066) #diffusion`
* Summary: <p>Large-scale text-to-image generative models have made impressive strides,
showcasing their ability to synthesize a vast array of high-quality images.
However, adapting these models for artistic image editing presents two
significant challenges. Firstly, users struggle to craft textual prompts that
meticulously detail visual elements of the input image. Secondly, prevalent
models, when effecting modifications in specific zones, frequently disrupt the
overall artistic style, complicating the attainment of cohesive and
aesthetically unified artworks. To surmount these obstacles, we build the
innovative unified framework CreativeSynth, which is based on a diffusion model
with the ability to coordinate multimodal inputs and multitask in the field of
artistic image generation. By integrating multimodal features with customized
attention mechanisms, CreativeSynth facilitates the importation of real-world
semantic content into the domain of art through inversion and real-time style
transfer. This allows for the precise manipulation of image style and content
while maintaining the integrity of the original model parameters. Rigorous
qualitative and quantitative evaluations underscore that CreativeSynth excels
in enhancing artistic images' fidelity and preserves their innate aesthetic
essence. By bridging the gap between generative models and artistic finesse,
CreativeSynth becomes a custom digital palette.
</p>

### Title: Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models. (arXiv:2401.14111v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14111](http://arxiv.org/abs/2401.14111)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14111] Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models](http://arxiv.org/abs/2401.14111) #diffusion`
* Summary: <p>Advancements in generative models have sparked significant interest in
generating images while adhering to specific structural guidelines. Scene graph
to image generation is one such task of generating images which are consistent
with the given scene graph. However, the complexity of visual scenes poses a
challenge in accurately aligning objects based on specified relations within
the scene graph. Existing methods approach this task by first predicting a
scene layout and generating images from these layouts using adversarial
training. In this work, we introduce a novel approach to generate images from
scene graphs which eliminates the need of predicting intermediate layouts. We
leverage pre-trained text-to-image diffusion models and CLIP guidance to
translate graph knowledge into images. Towards this, we first pre-train our
graph encoder to align graph features with CLIP features of corresponding
images using a GAN based training. Further, we fuse the graph features with
CLIP embedding of object labels present in the given scene graph to create a
graph consistent CLIP guided conditioning signal. In the conditioning input,
object embeddings provide coarse structure of the image and graph features
provide structural alignment based on relationships among objects. Finally, we
fine tune a pre-trained diffusion model with the graph consistent conditioning
signal with reconstruction and CLIP alignment loss. Elaborate experiments
reveal that our method outperforms existing methods on standard benchmarks of
COCO-stuff and Visual Genome dataset.
</p>

### Title: Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks. (arXiv:2401.14159v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14159](http://arxiv.org/abs/2401.14159)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14159] Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](http://arxiv.org/abs/2401.14159) #diffusion`
* Summary: <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object
detector to combine with the segment anything model (SAM). This integration
enables the detection and segmentation of any regions based on arbitrary text
inputs and opens a door to connecting various vision models. As shown in Fig.1,
a wide range of vision tasks can be achieved by using the versatile Grounded
SAM pipeline. For example, an automatic annotation pipeline based solely on
input images can be realized by incorporating models such as BLIP and Recognize
Anything. Additionally, incorporating Stable-Diffusion allows for controllable
image editing, while the integration of OSX facilitates promptable 3D human
motion analysis. Grounded SAM also shows superior performance on
open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in
the wild) zero-shot benchmark with the combination of Grounding DINO-Base and
SAM-Huge models.
</p>

### Title: Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14257](http://arxiv.org/abs/2401.14257)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14257] Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation](http://arxiv.org/abs/2401.14257) #diffusion`
* Summary: <p>Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
</p>

### Title: UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14379](http://arxiv.org/abs/2401.14379)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14379] UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models](http://arxiv.org/abs/2401.14379) #diffusion`
* Summary: <p>In contemporary design practices, the integration of computer vision and
generative artificial intelligence (genAI) represents a transformative shift
towards more interactive and inclusive processes. These technologies offer new
dimensions of image analysis and generation, which are particularly relevant in
the context of urban landscape reconstruction. This paper presents a novel
workflow encapsulated within a prototype application, designed to leverage the
synergies between advanced image segmentation and diffusion models for a
comprehensive approach to urban design. Our methodology encompasses the
OneFormer model for detailed image segmentation and the Stable Diffusion XL
(SDXL) diffusion model, implemented through ControlNet, for generating images
from textual descriptions. Validation results indicated a high degree of
performance by the prototype application, showcasing significant accuracy in
both object detection and text-to-image generation. This was evidenced by
superior Intersection over Union (IoU) and CLIP scores across iterative
evaluations for various categories of urban landscape features. Preliminary
testing included utilising UrbanGenAI as an educational tool enhancing the
learning experience in design pedagogy, and as a participatory instrument
facilitating community-driven urban planning. Early results suggested that
UrbanGenAI not only advances the technical frontiers of urban landscape
reconstruction but also provides significant pedagogical and participatory
planning benefits. The ongoing development of UrbanGenAI aims to further
validate its effectiveness across broader contexts and integrate additional
features such as real-time feedback mechanisms and 3D modelling capabilities.
Keywords: generative AI; panoptic image segmentation; diffusion models; urban
landscape design; design pedagogy; co-design
</p>

### Title: pix2gestalt: Amodal Segmentation by Synthesizing Wholes. (arXiv:2401.14398v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14398](http://arxiv.org/abs/2401.14398)
* Code URL: [https://github.com/cvlab-columbia/pix2gestalt](https://github.com/cvlab-columbia/pix2gestalt)
* Copy Paste: `<input type="checkbox">[[2401.14398] pix2gestalt: Amodal Segmentation by Synthesizing Wholes](http://arxiv.org/abs/2401.14398) #diffusion`
* Summary: <p>We introduce pix2gestalt, a framework for zero-shot amodal segmentation,
which learns to estimate the shape and appearance of whole objects that are
only partially visible behind occlusions. By capitalizing on large-scale
diffusion models and transferring their representations to this task, we learn
a conditional diffusion model for reconstructing whole objects in challenging
zero-shot cases, including examples that break natural and physical priors,
such as art. As training data, we use a synthetically curated dataset
containing occluded objects paired with their whole counterparts. Experiments
show that our approach outperforms supervised baselines on established
benchmarks. Our model can furthermore be used to significantly improve the
performance of existing object recognition and 3D reconstruction methods in the
presence of occlusions.
</p>

### Title: Deconstructing Denoising Diffusion Models for Self-Supervised Learning. (arXiv:2401.14404v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14404](http://arxiv.org/abs/2401.14404)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14404] Deconstructing Denoising Diffusion Models for Self-Supervised Learning](http://arxiv.org/abs/2401.14404) #diffusion`
* Summary: <p>In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
</p>

### Title: Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13858](http://arxiv.org/abs/2401.13858)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13858] Inverse Molecular Design with Multi-Conditional Diffusion Guidance](http://arxiv.org/abs/2401.13858) #diffusion`
* Summary: <p>Inverse molecular design with diffusion models holds great potential for
advancements in material and drug discovery. Despite success in unconditional
molecule generation, integrating multiple properties such as synthetic score
and gas permeability as condition constraints into diffusion models remains
unexplored. We introduce multi-conditional diffusion guidance. The proposed
Transformer-based denoising model has a condition encoder that learns the
representations of numerical and categorical conditions. The denoising model,
consisting of a structure encoder-decoder, is trained for denoising under the
representation of conditions. The diffusion process becomes graph-dependent to
accurately estimate graph-related noise in molecules, unlike the previous
models that focus solely on the marginal distributions of atoms or bonds. We
extensively validate our model for multi-conditional polymer and small molecule
generation. Results demonstrate our superiority across metrics from
distribution learning to condition control for molecular properties. An inverse
polymer design task for gas separation with feedback from domain experts
further demonstrates its practical utility.
</p>

### Title: Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs. (arXiv:2401.14381v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14381](http://arxiv.org/abs/2401.14381)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14381] Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs](http://arxiv.org/abs/2401.14381) #diffusion`
* Summary: <p>We propose two graph neural network layers for graphs with features in a
Riemannian manifold. First, based on a manifold-valued graph diffusion
equation, we construct a diffusion layer that can be applied to an arbitrary
number of nodes and graph connectivity patterns. Second, we model a tangent
multilayer perceptron by transferring ideas from the vector neuron framework to
our general setting. Both layers are equivariant with respect to node
permutations and isometries of the feature manifold. These properties have been
shown to lead to a beneficial inductive bias in many deep learning tasks.
Numerical examples on synthetic data as well as on triangle meshes of the right
hippocampus to classify Alzheimer's disease demonstrate the very good
performance of our layers.
</p>

## self-supervised
### Title: Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention. (arXiv:2401.13937v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13937](http://arxiv.org/abs/2401.13937)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13937] Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention](http://arxiv.org/abs/2401.13937) #self-supervised`
* Summary: <p>Video object segmentation is a fundamental research problem in computer
vision. Recent techniques have often applied attention mechanism to object
representation learning from video sequences. However, due to temporal changes
in the video data, attention maps may not well align with the objects of
interest across video frames, causing accumulated errors in long-term video
processing. In addition, existing techniques have utilised complex
architectures, requiring highly computational complexity and hence limiting the
ability to integrate video object segmentation into low-powered devices. To
address these issues, we propose a new method for self-supervised video object
segmentation based on distillation learning of deformable attention.
Specifically, we devise a lightweight architecture for video object
segmentation that is effectively adapted to temporal changes. This is enabled
by deformable attention mechanism, where the keys and values capturing the
memory of a video sequence in the attention module have flexible locations
updated across frames. The learnt object representations are thus adaptive to
both the spatial and temporal dimensions. We train the proposed architecture in
a self-supervised fashion through a new knowledge distillation paradigm where
deformable attention maps are integrated into the distillation loss. We
qualitatively and quantitatively evaluate our method and compare it with
existing methods on benchmark datasets including DAVIS 2016/2017 and
YouTube-VOS 2018/2019. Experimental results verify the superiority of our
method via its achieved state-of-the-art performance and optimal memory usage.
</p>

### Title: Learning to Manipulate Artistic Images. (arXiv:2401.13976v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13976](http://arxiv.org/abs/2401.13976)
* Code URL: [https://github.com/snailforce/sim-net](https://github.com/snailforce/sim-net)
* Copy Paste: `<input type="checkbox">[[2401.13976] Learning to Manipulate Artistic Images](http://arxiv.org/abs/2401.13976) #self-supervised`
* Summary: <p>Recent advancement in computer vision has significantly lowered the barriers
to artistic creation. Exemplar-based image translation methods have attracted
much attention due to flexibility and controllability. However, these methods
hold assumptions regarding semantics or require semantic information as the
input, while accurate semantics is not easy to obtain in artistic images.
Besides, these methods suffer from cross-domain artifacts due to training data
prior and generate imprecise structure due to feature compression in the
spatial domain. In this paper, we propose an arbitrary Style Image Manipulation
Network (SIM-Net), which leverages semantic-free information as guidance and a
region transportation strategy in a self-supervised manner for image
generation. Our method balances computational efficiency and high resolution to
a certain extent. Moreover, our method facilitates zero-shot style image
manipulation. Both qualitative and quantitative experiments demonstrate the
superiority of our method over state-of-the-art methods.Code is available at
https://github.com/SnailForce/SIM-Net.
</p>

## foundation model
### Title: A Survey of Deep Learning and Foundation Models for Time Series Forecasting. (arXiv:2401.13912v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13912](http://arxiv.org/abs/2401.13912)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13912] A Survey of Deep Learning and Foundation Models for Time Series Forecasting](http://arxiv.org/abs/2401.13912) #foundation model`
* Summary: <p>Deep Learning has been successfully applied to many application domains, yet
its advantages have been slow to emerge for time series forecasting. For
example, in the well-known Makridakis (M) Competitions, hybrids of traditional
statistical or machine learning techniques have only recently become the top
performers. With the recent architectural advances in deep learning being
applied to time series forecasting (e.g., encoder-decoders with attention,
transformers, and graph neural networks), deep learning has begun to show
significant advantages. Still, in the area of pandemic prediction, there remain
challenges for deep learning models: the time series is not long enough for
effective training, unawareness of accumulated scientific knowledge, and
interpretability of the model. To this end, the development of foundation
models (large deep learning models with extensive pre-training) allows models
to understand patterns and acquire knowledge that can be applied to new related
problems before extensive training data becomes available. Furthermore, there
is a vast amount of knowledge available that deep learning models can tap into,
including Knowledge Graphs and Large Language Models fine-tuned with scientific
domain knowledge. There is ongoing research examining how to utilize or inject
such knowledge into deep learning models. In this survey, several
state-of-the-art modeling techniques are reviewed, and suggestions for further
work are provided.
</p>

## generative
### Title: Inference Attacks Against Face Recognition Model without Classification Layers. (arXiv:2401.13719v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13719](http://arxiv.org/abs/2401.13719)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13719] Inference Attacks Against Face Recognition Model without Classification Layers](http://arxiv.org/abs/2401.13719) #generative`
* Summary: <p>Face recognition (FR) has been applied to nearly every aspect of daily life,
but it is always accompanied by the underlying risk of leaking private
information. At present, almost all attack models against FR rely heavily on
the presence of a classification layer. However, in practice, the FR model can
obtain complex features of the input via the model backbone, and then compare
it with the target for inference, which does not explicitly involve the outputs
of the classification layer adopting logit or other losses. In this work, we
advocate a novel inference attack composed of two stages for practical FR
models without a classification layer. The first stage is the membership
inference attack. Specifically, We analyze the distances between the
intermediate features and batch normalization (BN) parameters. The results
indicate that this distance is a critical metric for membership inference. We
thus design a simple but effective attack model that can determine whether a
face image is from the training dataset or not. The second stage is the model
inversion attack, where sensitive private data is reconstructed using a
pre-trained generative adversarial network (GAN) guided by the attack model in
the first stage. To the best of our knowledge, the proposed attack model is the
very first in the literature developed for FR models without a classification
layer. We illustrate the application of the proposed attack model in the
establishment of privacy-preserving FR techniques.
</p>

### Title: Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning. (arXiv:2401.13865v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13865](http://arxiv.org/abs/2401.13865)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13865] Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning](http://arxiv.org/abs/2401.13865) #generative`
* Summary: <p>Recently, appearance-based gaze estimation has been attracting attention in
computer vision, and remarkable improvements have been achieved using various
deep learning techniques. Despite such progress, most methods aim to infer gaze
vectors from images directly, which causes overfitting to person-specific
appearance factors. In this paper, we address these challenges and propose a
novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE),
which trains a network to generalize the appearance of subjects. We design a
Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face
identity classifier and a proposed adversarial loss. The proposed loss
generalizes face appearance factors so that the identity classifier inferences
a uniform probability distribution. In addition, the Fgen-Net is trained by a
learning mechanism that optimizes the network by reselecting a subset of
subjects at every training step to avoid overfitting. Our experimental results
verify the robustness of the method in that it yields state-of-the-art
performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets,
respectively. Furthermore, we demonstrate the positive generalization effect by
conducting further experiments using face images involving different styles
generated from the generative model.
</p>

### Title: Expression-aware video inpainting for HMD removal in XR applications. (arXiv:2401.14136v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14136](http://arxiv.org/abs/2401.14136)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14136] Expression-aware video inpainting for HMD removal in XR applications](http://arxiv.org/abs/2401.14136) #generative`
* Summary: <p>Head-mounted displays (HMDs) serve as indispensable devices for observing
extended reality (XR) environments and virtual content. However, HMDs present
an obstacle to external recording techniques as they block the upper face of
the user. This limitation significantly affects social XR applications,
specifically teleconferencing, where facial features and eye gaze information
play a vital role in creating an immersive user experience. In this study, we
propose a new network for expression-aware video inpainting for HMD removal
(EVI-HRnet) based on generative adversarial networks (GANs). Our model
effectively fills in missing information with regard to facial landmarks and a
single occlusion-free reference image of the user. The framework and its
components ensure the preservation of the user's identity across frames using
the reference frame. To further improve the level of realism of the inpainted
output, we introduce a novel facial expression recognition (FER) loss function
for emotion preservation. Our results demonstrate the remarkable capability of
the proposed framework to remove HMDs from facial videos while maintaining the
subject's facial expression and identity. Moreover, the outputs exhibit
temporal consistency along the inpainted frames. This lightweight framework
presents a practical approach for HMD occlusion removal, with the potential to
enhance various collaborative XR applications without the need for additional
hardware.
</p>

### Title: AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.14232](http://arxiv.org/abs/2401.14232)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14232] AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles](http://arxiv.org/abs/2401.14232) #generative`
* Summary: <p>This study developed a generative adversarial network (GAN)-based defense
method for traffic sign classification in an autonomous vehicle (AV), referred
to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i)
assuming zero knowledge of adversarial attack models and samples and (ii)
providing consistently high traffic sign classification performance under
various adversarial attack types. The AR-GAN classification system consists of
a generator that denoises an image by reconstruction, and a classifier that
classifies the reconstructed image. The authors have tested the AR-GAN under
no-attack and under various adversarial attacks, such as Fast Gradient Sign
Method (FGSM), DeepFool, Carlini and Wagner (C&amp;W), and Projected Gradient
Descent (PGD). The authors considered two forms of these attacks, i.e., (i)
black-box attacks (assuming the attackers possess no prior knowledge of the
classifier), and (ii) white-box attacks (assuming the attackers possess full
knowledge of the classifier). The classification performance of the AR-GAN was
compared with several benchmark adversarial defense methods. The results showed
that both the AR-GAN and the benchmark defense methods are resilient against
black-box attacks and could achieve similar classification performance to that
of the unperturbed images. However, for all the white-box attacks considered in
this study, the AR-GAN method outperformed the benchmark defense methods. In
addition, the AR-GAN was able to maintain its high classification performance
under varied white-box adversarial perturbation magnitudes, whereas the
performance of the other defense methods dropped abruptly at increased
perturbation magnitudes.
</p>

### Title: Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14019](http://arxiv.org/abs/2401.14019)
* Code URL: [https://github.com/ibm/unitxt](https://github.com/ibm/unitxt)
* Copy Paste: `<input type="checkbox">[[2401.14019] Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI](http://arxiv.org/abs/2401.14019) #generative`
* Summary: <p>In the dynamic landscape of generative NLP, traditional text processing
pipelines limit research flexibility and reproducibility, as they are tailored
to specific dataset, task, and model combinations. The escalating complexity,
involving system prompts, model-specific formats, instructions, and more, calls
for a shift to a structured, modular, and customizable solution. Addressing
this need, we present Unitxt, an innovative library for customizable textual
data preparation and evaluation tailored to generative language models. Unitxt
natively integrates with common libraries like HuggingFace and LM-eval-harness
and deconstructs processing flows into modular components, enabling easy
customization and sharing between practitioners. These components encompass
model-specific formats, task prompts, and many other comprehensive dataset
processing definitions. The Unitxt-Catalog centralizes these components,
fostering collaboration and exploration in modern textual data workflows.
Beyond being a tool, Unitxt is a community-driven platform, empowering users to
build, share, and advance their pipelines collaboratively. Join the Unitxt
community at https://github.com/IBM/unitxt!
</p>

### Title: Ta'keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14067](http://arxiv.org/abs/2401.14067)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14067] Ta'keed: The First Generative Fact-Checking System for Arabic Claims](http://arxiv.org/abs/2401.14067) #generative`
* Summary: <p>This paper introduces Ta'keed, an explainable Arabic automatic fact-checking
system. While existing research often focuses on classifying claims as "True"
or "False," there is a limited exploration of generating explanations for claim
credibility, particularly in Arabic. Ta'keed addresses this gap by assessing
claim truthfulness based on retrieved snippets, utilizing two main components:
information retrieval and LLM-based claim verification. We compiled the
ArFactEx, a testing gold-labelled dataset with manually justified references,
to evaluate the system. The initial model achieved a promising F1 score of 0.72
in the classification task. Meanwhile, the system's generated explanations are
compared with gold-standard explanations syntactically and semantically. The
study recommends evaluating using semantic similarities, resulting in an
average cosine similarity score of 0.76. Additionally, we explored the impact
of varying snippet quantities on claim classification accuracy, revealing a
potential correlation, with the model using the top seven hits outperforming
others with an F1 score of 0.77.
</p>

### Title: CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14109](http://arxiv.org/abs/2401.14109)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14109] CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks](http://arxiv.org/abs/2401.14109) #generative`
* Summary: <p>Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly
in generative Artificial Intelligence (AI), but their immense size poses
significant challenges, such as huge training and inference costs, substantial
energy demands, and limitations for on-site deployment. Traditional compression
methods such as pruning, distillation, and low-rank approximation focus on
reducing the effective number of neurons in the network, while quantization
focuses on reducing the numerical precision of individual weights to reduce the
model size while keeping the number of neurons fixed. While these compression
methods have been relatively successful in practice, there's no compelling
reason to believe that truncating the number of neurons is an optimal strategy.
In this context, this paper introduces CompactifAI, an innovative LLM
compression approach using quantum-inspired Tensor Networks that focuses on the
model's correlation space instead, allowing for a more controlled, refined and
interpretable model compression. Our method is versatile and can be implemented
with - or on top of - other compression techniques. As a benchmark, we
demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model
to only $30\%$ of its original size while recovering over $90\%$ of the
original accuracy after a brief distributed retraining.
</p>

### Title: Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13716](http://arxiv.org/abs/2401.13716)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13716] Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare](http://arxiv.org/abs/2401.13716) #generative`
* Summary: <p>Ensuring safe adoption of AI tools in healthcare hinges on access to
sufficient data for training, testing and validation. In response to privacy
concerns and regulatory requirements, using synthetic data has been suggested.
Synthetic data is created by training a generator on real data to produce a
dataset with similar statistical properties. Competing metrics with differing
taxonomies for quality evaluation have been suggested, resulting in a complex
landscape. Optimising quality entails balancing considerations that make the
data fit for use, yet relevant dimensions are left out of existing frameworks.
We performed a comprehensive literature review on the use of quality evaluation
metrics on SD within the scope of tabular healthcare data and SD made using
deep generative methods. Based on this and the collective team experiences, we
developed a conceptual framework for quality assurance. The applicability was
benchmarked against a practical case from the Dutch National Cancer Registry.
We present a conceptual framework for quality assurance of SD for AI
applications in healthcare that aligns diverging taxonomies, expands on common
quality dimensions to include the dimensions of Fairness and Carbon footprint,
and proposes stages necessary to support real-life applications. Building trust
in synthetic data by increasing transparency and reducing the safety risk will
accelerate the development and uptake of trustworthy AI tools for the benefit
of patients. Despite the growing emphasis on algorithmic fairness and carbon
footprint, these metrics were scarce in the literature review. The overwhelming
focus was on statistical similarity using distance metrics while sequential
logic detection was scarce. A consensus-backed framework that includes all
relevant quality dimensions can provide assurance for safe and responsible
real-life applications of SD.
</p>

## anomaly
### Title: Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection. (arXiv:2401.13872v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13872](http://arxiv.org/abs/2401.13872)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13872] Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection](http://arxiv.org/abs/2401.13872) #anomaly`
* Summary: <p>With the rapid advancement in cyber-physical systems, the increasing number
of sensors has significantly complicated manual monitoring of system states.
Consequently, graph-based time-series anomaly detection methods have gained
attention due to their ability to explicitly represent relationships between
sensors. However, these methods often apply a uniform source node
representation across all connected target nodes, even when updating different
target node representations. Moreover, the graph attention mechanism, commonly
used to infer unknown graph structures, could constrain the diversity of source
node representations. In this paper, we introduce the Edge Conditional
Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge
conditional node update module, dynamically transforms source node
representations based on connected edges to represent target nodes aptly. We
validate performance on three real-world datasets: SWaT, WADI, and PSM. Our
model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively,
compared to best F1 baseline models.
</p>

### Title: Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14155](http://arxiv.org/abs/2401.14155)
* Code URL: [https://github.com/blacksingular/wsdm_gdn](https://github.com/blacksingular/wsdm_gdn)
* Copy Paste: `<input type="checkbox">[[2401.14155] Alleviating Structural Distribution Shift in Graph Anomaly Detection](http://arxiv.org/abs/2401.14155) #anomaly`
* Summary: <p>Graph anomaly detection (GAD) is a challenging binary classification problem
due to its different structural distribution between anomalies and normal nodes
-- abnormal nodes are a minority, therefore holding high heterophily and low
homophily compared to normal nodes. Furthermore, due to various time factors
and the annotation preferences of human experts, the heterophily and homophily
can change across training and testing data, which is called structural
distribution shift (SDS) in this paper. The mainstream methods are built on
graph neural networks (GNNs), benefiting the classification of normals from
aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and
suffering from poor generalization.
</p>
<p>This work solves the problem from a feature view. We observe that the degree
of SDS varies between anomalies and normal nodes. Hence to address the issue,
the key lies in resisting high heterophily for anomalies meanwhile benefiting
the learning of normals from homophily. We tease out the anomaly features on
which we constrain to mitigate the effect of heterophilous neighbors and make
them invariant. We term our proposed framework as Graph Decomposition Network
(GDN). Extensive experiments are conducted on two benchmark datasets, and the
proposed framework achieves a remarkable performance boost in GAD, especially
in an SDS environment where anomalies have largely different structural
distribution across training and testing environments. Codes are open-sourced
in https://github.com/blacksingular/wsdm_GDN.
</p>

## in-context
### Title: Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4. (arXiv:2401.13810v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.13810](http://arxiv.org/abs/2401.13810)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13810] Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4](http://arxiv.org/abs/2401.13810) #in-context`
* Summary: <p>Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis
process for cloud services, requiring on-call engineers to identify the primary
issues and implement corrective actions to prevent future recurrences.
Improving the incident RCA process is vital for minimizing service downtime,
customer impact and manual toil. Recent advances in artificial intelligence
have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which
have proven effective in tackling various AIOps problems, ranging from code
authoring to incident management. Nonetheless, the GPT-4 model's immense size
presents challenges when trying to fine-tune it on user data because of the
significant GPU resource demand and the necessity for continuous model
fine-tuning with the emergence of new data. To address the high cost of
fine-tuning LLM, we propose an in-context learning approach for automated root
causing, which eliminates the need for fine-tuning. We conduct extensive study
over 100,000 production incidents, comparing several large language models
using multiple metrics. The results reveal that our in-context learning
approach outperforms the previous fine-tuned large language models such as
GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\%
improvement over the zero-shot model. Moreover, human evaluation involving
actual incident owners demonstrates its superiority over the fine-tuned model,
achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in
readability. The impressive results demonstrate the viability of utilizing a
vanilla GPT model for the RCA task, thereby avoiding the high computational and
maintenance costs associated with a fine-tuned model.
</p>

### Title: ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14003](http://arxiv.org/abs/2401.14003)
* Code URL: [https://github.com/hkust-knowcomp/constraintchecker](https://github.com/hkust-knowcomp/constraintchecker)
* Copy Paste: `<input type="checkbox">[[2401.14003] ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases](http://arxiv.org/abs/2401.14003) #in-context`
* Summary: <p>Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has
been explored as a way to acquire new commonsense knowledge based on reference
knowledge in the original CSKBs and external prior knowledge. Despite the
advancement of Large Language Models (LLM) and prompt engineering techniques in
various reasoning tasks, they still struggle to deal with CSKB reasoning. One
of the problems is that it is hard for them to acquire explicit relational
constraints in CSKBs from only in-context exemplars, due to a lack of symbolic
reasoning capabilities (Bengio et al., 2021). To this end, we proposed
**ConstraintChecker**, a plugin over prompting techniques to provide and check
explicit constraints. When considering a new knowledge instance,
ConstraintChecker employs a rule-based module to produce a list of constraints,
then it uses a zero-shot learning module to check whether this knowledge
instance satisfies all constraints. The acquired constraint-checking result is
then aggregated with the output of the main prompting technique to produce the
final output. Experimental results on CSKB Reasoning benchmarks demonstrate the
effectiveness of our method by bringing consistent improvements over all
prompting methods. Codes and data are available at
\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.
</p>

### Title: (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14040](http://arxiv.org/abs/2401.14040)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14040] (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection](http://arxiv.org/abs/2401.14040) #in-context`
* Summary: <p>In the universe of Natural Language Processing, Transformer-based language
models like BERT and (Chat)GPT have emerged as lexical superheroes with great
power to solve open research problems. In this paper, we specifically focus on
the temporal problem of semantic change, and evaluate their ability to solve
two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and
HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf
technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a
family of models that currently stand as the state-of-the-art for modeling
semantic change. Our experiments represent the first attempt to assess the use
of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT
performs significantly worse than the foundational GPT version. Furthermore,
our results demonstrate that (Chat)GPT achieves slightly lower performance than
BERT in detecting long-term changes but performs significantly worse in
detecting short-term changes.
</p>

## memory
### Title: Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers. (arXiv:2401.13714v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.13714](http://arxiv.org/abs/2401.13714)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13714] Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers](http://arxiv.org/abs/2401.13714) #memory`
* Summary: <p>Deploying neural networks on microcontroller units (MCUs) presents
substantial challenges due to their constrained computation and memory
resources. Previous researches have explored patch-based inference as a
strategy to conserve memory without sacrificing model accuracy. However, this
technique suffers from severe redundant computation overhead, leading to a
substantial increase in execution latency. A feasible solution to address this
issue is mixed-precision quantization, but it faces the challenges of accuracy
degradation and a time-consuming search time. In this paper, we propose
QuantMCU, a novel patch-based inference method that utilizes value-driven
mixed-precision quantization to reduce redundant computation. We first utilize
value-driven patch classification (VDPC) to maintain the model accuracy. VDPC
classifies patches into two classes based on whether they contain outlier
values. For patches containing outlier values, we apply 8-bit quantization to
the feature maps on the dataflow branches that follow. In addition, for patches
without outlier values, we utilize value-driven quantization search (VDQS) on
the feature maps of their following dataflow branches to reduce search time.
Specifically, VDQS introduces a novel quantization search metric that takes
into account both computation and accuracy, and it employs entropy as an
accuracy representation to avoid additional training. VDQS also adopts an
iterative approach to determine the bitwidth of each feature map to further
accelerate the search process. Experimental results on real-world MCU devices
show that QuantMCU can reduce computation by 2.2x on average while maintaining
comparable model accuracy compared to the state-of-the-art patch-based
inference methods.
</p>

### Title: A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.13887](http://arxiv.org/abs/2401.13887)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13887] A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification](http://arxiv.org/abs/2401.13887) #memory`
* Summary: <p>Although supervised machine learning is popular for information extraction
from clinical notes, creating large annotated datasets requires extensive
domain expertise and is time-consuming. Meanwhile, large language models (LLMs)
have demonstrated promising transfer learning capability. In this study, we
explored whether recent LLMs can reduce the need for large-scale data
annotations. We curated a manually-labeled dataset of 769 breast cancer
pathology reports, labeled with 13 categories, to compare zero-shot
classification capability of the GPT-4 model and the GPT-3.5 model with
supervised classification performance of three model architectures: random
forests classifier, long short-term memory networks with attention (LSTM-Att),
and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either
significantly better than or as well as the best supervised model, the LSTM-Att
model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance
between labels, the differences were more prominent. Frequent sources of GPT-4
errors included inferences from multiple samples and complex task design. On
complex tasks where large annotated datasets cannot be easily collected, LLMs
can reduce the burden of large-scale data labeling. However, if the use of LLMs
is prohibitive, the use of simpler supervised models with large annotated
datasets can provide comparable results. LLMs demonstrated the potential to
speed up the execution of clinical NLP studies by reducing the need for
curating large annotated datasets. This may result in an increase in the
utilization of NLP-based variables and outcomes in observational clinical
studies.
</p>

### Title: Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14215](http://arxiv.org/abs/2401.14215)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14215] Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement](http://arxiv.org/abs/2401.14215) #memory`
* Summary: <p>Memorizing and utilizing speakers' personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
</p>

### Title: Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13827](http://arxiv.org/abs/2401.13827)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.13827] Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models](http://arxiv.org/abs/2401.13827) #memory`
* Summary: <p>The age of information (AoI) is used to measure the freshness of the data. In
IoT networks, the traditional resource management schemes rely on a message
exchange between the devices and the base station (BS) before communication
which causes high AoI, high energy consumption, and low reliability. Unmanned
aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the
AoI, energy-saving, and throughput improvement. In this paper, we present a
novel learning-based framework that estimates the traffic arrival of IoT
devices based on Markovian events. The learning proceeds to optimize the
trajectory of multiple UAVs and their scheduling policy. First, the BS predicts
the future traffic of the devices. We compare two traffic predictors: the
forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we
propose a deep reinforcement learning (DRL) approach to optimize the optimal
policy of each UAV. Finally, we manipulate the optimum reward function for the
proposed DRL approach. Simulation results show that the proposed algorithm
outperforms the random-walk (RW) baseline model regarding the AoI, scheduling
accuracy, and transmission power.
</p>

### Title: Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13968](http://arxiv.org/abs/2401.13968)
* Code URL: [https://github.com/anwarmaxsum/mantra](https://github.com/anwarmaxsum/mantra)
* Copy Paste: `<input type="checkbox">[[2401.13968] Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks](http://arxiv.org/abs/2401.13968) #memory`
* Summary: <p>A reliable long-term time-series forecaster is highly demanded in practice
but comes across many challenges such as low computational and memory
footprints as well as robustness against dynamic learning environments. This
paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic
long-term time-series forecasting tasks. MANTRA relies on the concept of fast
and slow learners where a collection of fast learners learns different aspects
of data distributions while adapting quickly to changes. A slow learner tailors
suitable representations to fast learners. Fast adaptations to dynamic
environments are achieved using the universal representation transformer layers
producing task-adapted representations with a small number of parameters. Our
experiments using four datasets with different prediction lengths demonstrate
the advantage of our approach with at least $3\%$ improvements over the
baseline algorithms for both multivariate and univariate settings. Source codes
of MANTRA are publicly available in
\url{https://github.com/anwarmaxsum/MANTRA}.
</p>

### Title: FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14112](http://arxiv.org/abs/2401.14112)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14112] FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](http://arxiv.org/abs/2401.14112) #memory`
* Summary: <p>Six-bit quantization (FP6) can effectively reduce the size of large language
models (LLMs) and preserve the model quality consistently across varied
applications. However, existing systems do not provide Tensor Core support for
FP6 quantization and struggle to achieve practical performance improvements
during LLM inference. It is challenging to support FP6 quantization on GPUs due
to (1) unfriendly memory access of model weights with irregular bit-width and
(2) high runtime overhead of weight de-quantization. To address these problems,
we propose TC-FPx, the first full-stack GPU kernel design scheme with unified
Tensor Core support of float-point weights for various quantization bit-width.
We integrate TC-FPx kernel into an existing inference system, providing new
end-to-end support (called FP6-LLM) for quantized LLM inference, where better
trade-offs between inference cost and model quality are achieved. Experiments
show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,
achieving 1.69x-2.65x higher normalized inference throughput than the FP16
baseline. The source code will be publicly available soon.
</p>

### Title: MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning. (arXiv:2401.14199v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14199](http://arxiv.org/abs/2401.14199)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14199] MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning](http://arxiv.org/abs/2401.14199) #memory`
* Summary: <p>In this study, we explore the synergy of deep learning and financial market
applications, focusing on pair trading. This market-neutral strategy is
integral to quantitative finance and is apt for advanced deep-learning
techniques. A pivotal challenge in pair trading is discerning temporal
correlations among entities, necessitating the integration of diverse data
modalities. Addressing this, we introduce a novel framework, Multi-modal
Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and
discrete features into a temporal graph and employs a memory-based temporal
graph neural network. This approach reframes temporal correlation
identification as a temporal graph link prediction task, which has shown
empirical success. Our experiments on real-world datasets confirm the superior
performance of MTRGL, emphasizing its promise in refining automated pair
trading strategies.
</p>

### Title: ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models. (arXiv:2401.14351v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14351](http://arxiv.org/abs/2401.14351)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14351] ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models](http://arxiv.org/abs/2401.14351) #memory`
* Summary: <p>This paper presents ServerlessLLM, a locality-enhanced serverless inference
system for Large Language Models (LLMs). ServerlessLLM exploits the substantial
capacity and bandwidth of storage and memory devices available on GPU servers,
thereby reducing costly remote checkpoint downloads and achieving efficient
checkpoint loading. ServerlessLLM achieves this through three main
contributions: (i) fast LLM checkpoint loading via a novel loading-optimized
checkpoint format design, coupled with an efficient multi-tier checkpoint
loading system; (ii) locality-driven LLM inference with live migration, which
allows ServerlessLLM to effectively achieve locality-driven server allocation
while preserving the low latency of ongoing LLM inference; and (iii)
locality-aware server allocation, enabling ServerlessLLM to evaluate the status
of each server in a cluster and effectively schedule model startup time to
capitalize on local checkpoint placement. Our comprehensive experiments, which
include microbenchmarks and real-world traces, show that ServerlessLLM
surpasses state-of-the-art systems by 10 - 200X in latency performance when
running various LLM inference workloads.
</p>

## few-shot
### Title: BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.14166](http://arxiv.org/abs/2401.14166)
* Code URL: [https://github.com/ff2127/bayesprompt](https://github.com/ff2127/bayesprompt)
* Copy Paste: `<input type="checkbox">[[2401.14166] BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction](http://arxiv.org/abs/2401.14166) #few-shot`
* Summary: <p>As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
</p>

### Title: Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.13987](http://arxiv.org/abs/2401.13987)
* Code URL: [https://github.com/naeem-paeedeh/adapter](https://github.com/naeem-paeedeh/adapter)
* Copy Paste: `<input type="checkbox">[[2401.13987] Cross-Domain Few-Shot Learning via Adaptive Transformer Networks](http://arxiv.org/abs/2401.13987) #few-shot`
* Summary: <p>Most few-shot learning works rely on the same domain assumption between the
base and the target tasks, hindering their practical applications. This paper
proposes an adaptive transformer network (ADAPTER), a simple but effective
solution for cross-domain few-shot learning where there exist large domain
shifts between the base task and the target task. ADAPTER is built upon the
idea of bidirectional cross-attention to learn transferable features between
the two domains. The proposed architecture is trained with DINO to produce
diverse, and less biased features to avoid the supervision collapse problem.
Furthermore, the label smoothing approach is proposed to improve the
consistency and reliability of the predictions by also considering the
predicted labels of the close samples in the embedding space. The performance
of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it
outperforms prior arts with significant margins.
</p>

### Title: Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement. (arXiv:2401.14107v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.14107](http://arxiv.org/abs/2401.14107)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.14107] Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement](http://arxiv.org/abs/2401.14107) #few-shot`
* Summary: <p>Wearable technologies enable continuous monitoring of various health metrics,
such as physical activity, heart rate, sleep, and stress levels. A key
challenge with wearable data is obtaining quality labels. Unlike modalities
like video where the videos themselves can be effectively used to label objects
or events, wearable data do not contain obvious cues about the physical
manifestation of the users and usually require rich metadata. As a result,
label noise can become an increasingly thorny issue when labeling such data. In
this paper, we propose a novel solution to address noisy label learning,
entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially
learns a seed model using weak labels. Next, it fine-tunes the seed model using
a handful of expert corrections. Finally, it achieves better generalizability
and robustness by merging the seed and fine-tuned models via weighted parameter
averaging. We evaluate our approach on four challenging tasks and datasets, and
compare it against eight competitive baselines designed to deal with noisy
labels. We show that FHLR achieves significantly better performance when
learning from noisy labels and achieves state-of-the-art by a large margin,
with up to 19% accuracy improvement under symmetric and asymmetric noise.
Notably, we find that FHLR is particularly robust to increased label noise,
unlike prior works that suffer from severe performance degradation. Our work
not only achieves better generalization in high-stakes health sensing
benchmarks but also sheds light on how noise affects commonly-used models.
</p>

