## diffusion
### Title: IPR-NeRF: Ownership Verification meets Neural Radiance Field. (arXiv:2401.09495v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09495](http://arxiv.org/abs/2401.09495)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09495] IPR-NeRF: Ownership Verification meets Neural Radiance Field](http://arxiv.org/abs/2401.09495) #diffusion`
* Summary: <p>Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.
</p>

### Title: Image Translation as Diffusion Visual Programmers. (arXiv:2401.09742v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09742](http://arxiv.org/abs/2401.09742)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09742] Image Translation as Diffusion Visual Programmers](http://arxiv.org/abs/2401.09742) #diffusion`
* Summary: <p>We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic
image translation framework. Our proposed DVP seamlessly embeds a
condition-flexible diffusion model within the GPT architecture, orchestrating a
coherent sequence of visual programs (i.e., computer vision models) for various
pro-symbolic steps, which span RoI identification, style transfer, and position
manipulation, facilitating transparent and controllable image translation
processes. Extensive experiments demonstrate DVP's remarkable performance,
surpassing concurrent arts. This success can be attributed to several key
features of DVP: First, DVP achieves condition-flexible translation via
instance normalization, enabling the model to eliminate sensitivity caused by
the manual guidance and optimally focus on textual descriptions for
high-quality content generation. Second, the framework enhances in-context
reasoning by deciphering intricate high-dimensional concepts in feature spaces
into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]),
allowing for localized, context-free editing while maintaining overall
coherence. Last but not least, DVP improves systemic controllability and
explainability by offering explicit symbolic representations at each
programming stage, empowering users to intuitively interpret and modify
results. Our research marks a substantial step towards harmonizing artificial
image translation processes with cognitive intelligence, promising broader
applications.
</p>

### Title: Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing. (arXiv:2401.09794v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09794](http://arxiv.org/abs/2401.09794)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09794] Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing](http://arxiv.org/abs/2401.09794) #diffusion`
* Summary: <p>In the field of image editing, Null-text Inversion (NTI) enables fine-grained
editing while preserving the structure of the original image by optimizing null
embeddings during the DDIM sampling process. However, the NTI process is
time-consuming, taking more than two minutes per image. To address this, we
introduce an innovative method that maintains the principles of the NTI while
accelerating the image editing process. We propose the WaveOpt-Estimator, which
determines the text optimization endpoint based on frequency characteristics.
Utilizing wavelet transform analysis to identify the image's frequency
characteristics, we can limit text optimization to specific timesteps during
the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI)
concept, a target prompt representing the original image serves as the initial
text value for optimization. This approach maintains performance comparable to
NTI while reducing the average editing time by over 80% compared to the NTI
method. Our method presents a promising approach for efficient, high-quality
image editing based on diffusion models.
</p>

### Title: Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework. (arXiv:2401.09836v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09836](http://arxiv.org/abs/2401.09836)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09836] Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework](http://arxiv.org/abs/2401.09836) #diffusion`
* Summary: <p>Monocular 3D human pose estimation poses significant challenges due to the
inherent depth ambiguities that arise during the reprojection process from 2D
to 3D. Conventional approaches that rely on estimating an over-fit projection
matrix struggle to effectively address these challenges and often result in
noisy outputs. Recent advancements in diffusion models have shown promise in
incorporating structural priors to address reprojection ambiguities. However,
there is still ample room for improvement as these methods often overlook the
exploration of correlation between the 2D and 3D joint-level features. In this
study, we propose a novel cross-channel embedding framework that aims to fully
explore the correlation between joint-level features of 3D coordinates and
their 2D projections. In addition, we introduce a context guidance mechanism to
facilitate the propagation of joint graph attention across latent channels
during the iterative diffusion process. To evaluate the effectiveness of our
proposed method, we conduct experiments on two benchmark datasets, namely
Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement
in terms of reconstruction accuracy compared to state-of-the-art methods. The
code for our method will be made available online for further reference.
</p>

### Title: BlenDA: Domain Adaptive Object Detection through diffusion-based blending. (arXiv:2401.09921v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09921](http://arxiv.org/abs/2401.09921)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09921] BlenDA: Domain Adaptive Object Detection through diffusion-based blending](http://arxiv.org/abs/2401.09921) #diffusion`
* Summary: <p>Unsupervised domain adaptation (UDA) aims to transfer a model learned using
labeled data from the source domain to unlabeled data in the target domain. To
address the large domain gap issue between the source and target domains, we
propose a novel regularization method for domain adaptive object detection,
BlenDA, by generating the pseudo samples of the intermediate domains and their
corresponding soft domain labels for adaptation training. The intermediate
samples are generated by dynamically blending the source images with their
corresponding translated images using an off-the-shelf pre-trained
text-to-image diffusion model which takes the text label of the target domain
as input and has demonstrated superior image-to-image translation quality.
Based on experimental results from two adaptation benchmarks, our proposed
approach can significantly enhance the performance of the state-of-the-art
domain adaptive object detector, Adversarial Query Transformer (AQT).
Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an
impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous
state-of-the-art by 1.5%. It is worth noting that our proposed method is also
applicable to various paradigms of domain adaptive object detection. The code
is available at:https://github.com/aiiu-lab/BlenDA
</p>

### Title: CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects. (arXiv:2401.09962v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09962](http://arxiv.org/abs/2401.09962)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09962] CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects](http://arxiv.org/abs/2401.09962) #diffusion`
* Summary: <p>Customized text-to-video generation aims to generate high-quality videos
guided by text prompts and subject references. Current approaches designed for
single subjects suffer from tackling multiple subjects, which is a more
challenging and practical scenario. In this work, we aim to promote
multi-subject guided text-to-video customization. We propose CustomVideo, a
novel framework that can generate identity-preserving videos with the guidance
of multiple subjects. To be specific, firstly, we encourage the co-occurrence
of multiple subjects via composing them in a single image. Further, upon a
basic text-to-video diffusion model, we design a simple yet effective attention
control strategy to disentangle different subjects in the latent space of
diffusion model. Moreover, to help the model focus on the specific object area,
we segment the object from given reference images and provide a corresponding
object mask for attention learning. Also, we collect a multi-subject
text-to-video generation dataset as a comprehensive benchmark, with 69
individual subjects and 57 meaningful pairs. Extensive qualitative,
quantitative, and user study results demonstrate the superiority of our method,
compared with the previous state-of-the-art approaches.
</p>

### Title: DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10061](http://arxiv.org/abs/2401.10061)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10061] DiffusionGPT: LLM-Driven Text-to-Image Generation System](http://arxiv.org/abs/2401.10061) #diffusion`
* Summary: <p>Diffusion models have opened up new avenues for the field of image
generation, resulting in the proliferation of high-quality models shared on
open-source platforms. However, a major challenge persists in current
text-to-image systems are often unable to handle diverse inputs, or are limited
to single model results. Current unified attempts often fall into two
orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate
expert model to output. To combine the best of both worlds, we propose
DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified
generation system capable of seamlessly accommodating various types of prompts
and integrating domain-expert models. DiffusionGPT constructs domain-specific
Trees for various generative models based on prior knowledge. When provided
with an input, the LLM parses the prompt and employs the Trees-of-Thought to
guide the selection of an appropriate model, thereby relaxing input constraints
and ensuring exceptional performance across diverse domains. Moreover, we
introduce Advantage Databases, where the Tree-of-Thought is enriched with human
feedback, aligning the model selection process with human preferences. Through
extensive experiments and comparisons, we demonstrate the effectiveness of
DiffusionGPT, showcasing its potential for pushing the boundaries of image
synthesis in diverse domains.
</p>

### Title: A locally statistical active contour model for SAR image segmentation can be solved by denoising algorithms. (arXiv:2401.10083v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10083](http://arxiv.org/abs/2401.10083)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10083] A locally statistical active contour model for SAR image segmentation can be solved by denoising algorithms](http://arxiv.org/abs/2401.10083) #diffusion`
* Summary: <p>In this paper, we propose a novel locally statistical variational active
contour model based on I-divergence-TV denoising model, which hybrides geodesic
active contour (GAC) model with active contours without edges (ACWE) model, and
can be used to segment images corrupted by multiplicative gamma noise. By
adding a diffusion term into the level set evolution (LSE) equation of the
proposed model, we construct a reaction-diffusion (RD) equation, which can
gradually regularize the level set function (LSF) to be piecewise constant in
each segment domain and gain the stable solution. We further transform the
proposed model into classic ROF model by adding a proximity term. Inspired by a
fast denoising algorithm proposed by Jia-Zhao recently, we propose two fast
fixed point algorithms to solve SAR image segmentation question. Experimental
results for real SAR images show that the proposed image segmentation model can
efficiently stop the contours at weak or blurred edges, and can automatically
detect the exterior and interior boundaries of images with multiplicative gamma
noise. The proposed FPRD1/FPRD2 models are about 1/2 (or less than) of the time
required for the SBRD model based on the Split Bregman technique.
</p>

### Title: Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation. (arXiv:2401.10150v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10150](http://arxiv.org/abs/2401.10150)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10150] Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation](http://arxiv.org/abs/2401.10150) #diffusion`
* Summary: <p>Recent large-scale pre-trained diffusion models have demonstrated a powerful
generative ability to produce high-quality videos from detailed text
descriptions. However, exerting control over the motion of objects in videos
generated by any video diffusion model is a challenging problem. In this paper,
we propose a novel zero-shot moving object trajectory control framework,
Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video
diffusion model.To this end, an initial noise prior module is designed to
provide a position-based prior to improve the stability of the appearance of
the moving object and the accuracy of position. In addition, based on the
attention map of the U-net, spatial constraints are directly applied to the
denoising process of diffusion models, which further ensures the positional and
spatial consistency of moving objects during the inference. Furthermore,
temporal consistency is guaranteed with a proposed shift temporal attention
mechanism. Our method can be flexibly applied to various state-of-the-art video
diffusion models without any training process. Extensive experiments
demonstrate our proposed method can control the motion trajectories of objects
and generate high-quality videos.
</p>

### Title: Towards Language-Driven Video Inpainting via Multimodal Large Language Models. (arXiv:2401.10226v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10226](http://arxiv.org/abs/2401.10226)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10226] Towards Language-Driven Video Inpainting via Multimodal Large Language Models](http://arxiv.org/abs/2401.10226) #diffusion`
* Summary: <p>We introduce a new task -- language-driven video inpainting, which uses
natural language instructions to guide the inpainting process. This approach
overcomes the limitations of traditional video inpainting methods that depend
on manually labeled binary masks, a process often tedious and labor-intensive.
We present the Remove Objects from Videos by Instructions (ROVI) dataset,
containing 5,650 videos and 9,091 inpainting results, to support training and
evaluation for this task. We also propose a novel diffusion-based
language-driven video inpainting framework, the first end-to-end baseline for
this task, integrating Multimodal Large Language Models to understand and
execute complex language-based inpainting requests effectively. Our
comprehensive results showcase the dataset's versatility and the model's
effectiveness in various language-instructed inpainting scenarios. We will make
datasets, code, and models publicly available.
</p>

### Title: A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10227](http://arxiv.org/abs/2401.10227)
* Code URL: [https://github.com/segments-ai/latent-diffusion-segmentation](https://github.com/segments-ai/latent-diffusion-segmentation)
* Copy Paste: `<input type="checkbox">[[2401.10227] A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting](http://arxiv.org/abs/2401.10227) #diffusion`
* Summary: <p>Panoptic and instance segmentation networks are often trained with
specialized object detection modules, complex loss functions, and ad-hoc
post-processing steps to handle the permutation-invariance of the instance
masks. This work builds upon Stable Diffusion and proposes a latent diffusion
approach for panoptic segmentation, resulting in a simple architecture which
omits these complexities. Our training process consists of two steps: (1)
training a shallow autoencoder to project the segmentation masks to latent
space; (2) training a diffusion model to allow image-conditioned sampling in
latent space. The use of a generative model unlocks the exploration of mask
completion or inpainting, which has applications in interactive segmentation.
The experimental validation yields promising results for both panoptic
segmentation and mask inpainting. While not setting a new state-of-the-art, our
model's simplicity, generality, and mask completion capability are desirable
properties.
</p>

## self-supervised
## foundation model
### Title: Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2401.09883v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09883](http://arxiv.org/abs/2401.09883)
* Code URL: [https://github.com/cvi-szu/qa-clims](https://github.com/cvi-szu/qa-clims)
* Copy Paste: `<input type="checkbox">[[2401.09883] Question-Answer Cross Language Image Matching for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2401.09883) #foundation model`
* Summary: <p>Class Activation Map (CAM) has emerged as a popular tool for weakly
supervised semantic segmentation (WSSS), allowing the localization of object
regions in an image using only image-level labels. However, existing CAM
methods suffer from under-activation of target object regions and
false-activation of background regions due to the fact that a lack of detailed
supervision can hinder the model's ability to understand the image as a whole.
In this paper, we propose a novel Question-Answer Cross-Language-Image Matching
framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model
to maximize the text-based understanding of images and guide the generation of
activation maps. First, a series of carefully designed questions are posed to
the VQA (Visual Question Answering) model with Question-Answer Prompt
Engineering (QAPE) to generate a corpus of both foreground target objects and
backgrounds that are adaptive to query images. We then employ contrastive
learning in a Region Image Text Contrastive (RITC) network to compare the
obtained foreground and background regions with the generated corpus. Our
approach exploits the rich textual information from the open vocabulary as
additional supervision, enabling the model to generate high-quality CAMs with a
more complete object region and reduce false-activation of background regions.
We conduct extensive analysis to validate the proposed method and show that our
approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO
datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS
</p>

### Title: VMamba: Visual State Space Model. (arXiv:2401.10166v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10166](http://arxiv.org/abs/2401.10166)
* Code URL: [https://github.com/mzeromiko/vmamba](https://github.com/mzeromiko/vmamba)
* Copy Paste: `<input type="checkbox">[[2401.10166] VMamba: Visual State Space Model](http://arxiv.org/abs/2401.10166) #foundation model`
* Summary: <p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as
the two most popular foundation models for visual representation learning.
While CNNs exhibit remarkable scalability with linear complexity w.r.t. image
resolution, ViTs surpass them in fitting capabilities despite contending with
quadratic complexity. A closer inspection reveals that ViTs achieve superior
visual modeling performance through the incorporation of global receptive
fields and dynamic weights. This observation motivates us to propose a novel
architecture that inherits these components while enhancing computational
efficiency. To this end, we draw inspiration from the recently introduced state
space model and propose the Visual State Space Model (VMamba), which achieves
linear complexity without sacrificing global receptive fields. To address the
encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM)
to traverse the spatial domain and convert any non-causal visual image into
order patch sequences. Extensive experimental results substantiate that VMamba
not only demonstrates promising capabilities across various visual perception
tasks, but also exhibits more pronounced advantages over established benchmarks
as the image resolution increases. Source code has been available at
https://github.com/MzeroMiko/VMamba.
</p>

### Title: AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10220](http://arxiv.org/abs/2401.10220)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10220] AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data](http://arxiv.org/abs/2401.10220) #foundation model`
* Summary: <p>Foundation models encode rich representations that can be adapted to a
desired task by fine-tuning on task-specific data. However, fine-tuning a model
on one particular data distribution often compromises the model's original
performance on other distributions. Current methods for robust fine-tuning
utilize hand-crafted regularization techniques to constrain the fine-tuning
process towards the base foundation model. Yet, it is hard to precisely specify
what characteristics of the foundation model to retain during fine-tuning, as
this depends on how the pre-training, fine-tuning, and evaluation data
distributions relate to each other. We propose AutoFT, a data-driven approach
for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning
hyperparameters to maximize performance on a small out-of-distribution (OOD)
validation set. To guide fine-tuning in a granular way, AutoFT searches a
highly expressive hyperparameter space that includes weight coefficients for
many different losses, in addition to learning rate and weight decay values. We
evaluate AutoFT on nine natural distribution shifts which include domain shifts
and subpopulation shifts. Our experiments show that AutoFT significantly
improves generalization to new OOD data, outperforming existing robust
fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance
on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous
best methods by $6.0\%$ and $1.5\%$, respectively.
</p>

### Title: Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10222](http://arxiv.org/abs/2401.10222)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10222] Supervised Fine-tuning in turn Improves Visual Foundation Models](http://arxiv.org/abs/2401.10222) #foundation model`
* Summary: <p>Image-text training like CLIP has dominated the pretraining of vision
foundation models in recent years. Subsequent efforts have been made to
introduce region-level visual learning into CLIP's pretraining but face
scalability challenges due to the lack of large-scale region-level datasets.
Drawing inspiration from supervised fine-tuning (SFT) in natural language
processing such as instruction tuning, we explore the potential of fine-grained
SFT in enhancing the generation of vision foundation models after their
pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash
the fine-grained knowledge of vision foundation models. In ViSFT, the vision
foundation model is enhanced by performing visual joint learning on some
in-domain tasks and then tested on out-of-domain benchmarks. With updating
using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over
4.4B parameters shows improvements across various out-of-domain benchmarks
including vision and vision-linguistic scenarios.
</p>

### Title: RAP-SAM: Towards Real-Time All-Purpose Segment Anything. (arXiv:2401.10228v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10228](http://arxiv.org/abs/2401.10228)
* Code URL: [https://github.com/xushilin1/rap-sam](https://github.com/xushilin1/rap-sam)
* Copy Paste: `<input type="checkbox">[[2401.10228] RAP-SAM: Towards Real-Time All-Purpose Segment Anything](http://arxiv.org/abs/2401.10228) #foundation model`
* Summary: <p>Advanced by transformer architecture, vision foundation models (VFMs) achieve
remarkable progress in performance and generalization ability. Segment Anything
Model (SAM) is one remarkable model that can achieve generalized segmentation.
However, most VFMs cannot run in realtime, which makes it difficult to transfer
them into several products. On the other hand, current real-time segmentation
mainly has one purpose, such as semantic segmentation on the driving scene. We
argue that diverse outputs are needed for real applications. Thus, this work
explores a new real-time segmentation setting, named all-purpose segmentation
in real-time, to transfer VFMs in real-time deployment. It contains three
different tasks, including interactive segmentation, panoptic segmentation, and
video segmentation. We aim to use one model to achieve the above tasks in
real-time. We first benchmark several strong baselines. Then, we present
Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an
efficient decoupled decoder to perform prompt-driven decoding. Moreover, we
further explore different training strategies and tuning methods to boost
co-training performance further. Our code and model are available at
https://github.com/xushilin1/RAP-SAM/.
</p>

## generative
### Title: Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09596](http://arxiv.org/abs/2401.09596)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09596] Efficient generative adversarial networks using linear additive-attention Transformers](http://arxiv.org/abs/2401.09596) #generative`
* Summary: <p>Although the capacity of deep generative models for image generation, such as
Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has
dramatically improved in recent years, much of their success can be attributed
to computationally expensive architectures. This has limited their adoption and
use to research laboratories and companies with large resources, while
significantly raising the carbon footprint for training, fine-tuning, and
inference. In this work, we present LadaGAN, an efficient generative
adversarial network that is built upon a novel Transformer block named
Ladaformer. The main component of this block is a linear additive-attention
mechanism that computes a single attention vector per head instead of the
quadratic dot-product attention. We employ Ladaformer in both the generator and
discriminator, which reduces the computational complexity and overcomes the
training instabilities often associated with Transformer GANs. LadaGAN
consistently outperforms existing convolutional and Transformer GANs on
benchmark datasets at different resolutions while being significantly more
efficient. Moreover, LadaGAN shows competitive performance compared to
state-of-the-art multi-step generative models (e.g. DMs) using orders of
magnitude less computational resources.
</p>

### Title: HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09716](http://arxiv.org/abs/2401.09716)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09716] HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization](http://arxiv.org/abs/2401.09716) #generative`
* Summary: <p>Domain Generalization (DG) endeavors to create machine learning models that
excel in unseen scenarios by learning invariant features. In DG, the prevalent
practice of constraining models to a fixed structure or uniform
parameterization to encapsulate invariant features can inadvertently blend
specific aspects. Such an approach struggles with nuanced differentiation of
inter-domain variations and may exhibit bias towards certain domains, hindering
the precise learning of domain-invariant features. Recognizing this, we
introduce a novel method designed to supplement the model with domain-level and
task-specific characteristics. This approach aims to guide the model in more
effectively separating invariant features from specific characteristics,
thereby boosting the generalization. Building on the emerging trend of visual
prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical
\textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This
represents a significant advancement in the field, setting itself apart with a
unique generative approach to prompts, alongside an explicit model structure
and specialized loss functions. Differing from traditional visual prompts that
are often shared across entire datasets, HCVP utilizes a hierarchical prompt
generation network enhanced by prompt contrastive learning. These generative
prompts are instance-dependent, catering to the unique characteristics inherent
to different domains and tasks. Additionally, we devise a prompt modulation
network that serves as a bridge, effectively incorporating the generated visual
prompts into the vision transformer backbone. Experiments conducted on five DG
datasets demonstrate the effectiveness of HCVP, outperforming both established
DG algorithms and adaptation protocols.
</p>

### Title: CLIP Model for Images to Textual Prompts Based on Top-k Neighbors. (arXiv:2401.09763v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09763](http://arxiv.org/abs/2401.09763)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09763] CLIP Model for Images to Textual Prompts Based on Top-k Neighbors](http://arxiv.org/abs/2401.09763) #generative`
* Summary: <p>Text-to-image synthesis, a subfield of multimodal generation, has gained
significant attention in recent years. We propose a cost-effective approach for
image-to-prompt generation that leverages generative models to generate textual
prompts without the need for large amounts of annotated data. We divide our
method into two stages: online stage and offline stage. We use a combination of
the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system
consists of two main parts: an offline task and an online task. Our method owns
the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011
higher than Clip, Clip + KNN(top 10) respectively.
</p>

### Title: MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10208](http://arxiv.org/abs/2401.10208)
* Code URL: [https://github.com/opengvlab/mm-interleaved](https://github.com/opengvlab/mm-interleaved)
* Copy Paste: `<input type="checkbox">[[2401.10208] MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer](http://arxiv.org/abs/2401.10208) #generative`
* Summary: <p>Developing generative models for interleaved image-text data has both
research and practical value. It requires models to understand the interleaved
sequences and subsequently generate images and text. However, existing attempts
are limited by the issue that the fixed number of visual tokens cannot
efficiently capture image details, which is particularly problematic in the
multi-image scenarios. To address this, this paper presents MM-Interleaved, an
end-to-end generative model for interleaved image-text data. It introduces a
multi-scale and multi-image feature synchronizer module, allowing direct access
to fine-grained image features in the previous context during the generation
process. MM-Interleaved is end-to-end pre-trained on both paired and
interleaved image-text corpora. It is further enhanced through a supervised
fine-tuning phase, wherein the model improves its ability to follow complex
multi-modal instructions. Experiments demonstrate the versatility of
MM-Interleaved in recognizing visual details following multi-modal instructions
and generating consistent images following both textual and visual conditions.
Code and models are available at
\url{https://github.com/OpenGVLab/MM-Interleaved}.
</p>

### Title: ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions. (arXiv:2401.10232v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10232](http://arxiv.org/abs/2401.10232)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10232] ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions](http://arxiv.org/abs/2401.10232) #generative`
* Summary: <p>To enable machines to learn how humans interact with the physical world in
our daily activities, it is crucial to provide rich data that encompasses the
3D motion of humans as well as the motion of objects in a learnable 3D
representation. Ideally, this data should be collected in a natural setup,
capturing the authentic dynamic 3D signals during human-object interactions. To
address this challenge, we introduce the ParaHome system, designed to capture
and parameterize dynamic 3D movements of humans and objects within a common
home environment. Our system consists of a multi-view setup with 70
synchronized RGB cameras, as well as wearable motion capture devices equipped
with an IMU-based body suit and hand motion capture gloves. By leveraging the
ParaHome system, we collect a novel large-scale dataset of human-object
interaction. Notably, our dataset offers key advancement over existing datasets
in three main aspects: (1) capturing 3D body and dexterous hand manipulation
motion alongside 3D object movement within a contextual home environment during
natural activities; (2) encompassing human interaction with multiple objects in
various episodic scenarios with corresponding descriptions in texts; (3)
including articulated objects with multiple parts expressed with parameterized
articulations. Building upon our dataset, we introduce new research tasks aimed
at building a generative model for learning and synthesizing human-object
interactions in a real-world room setting.
</p>

### Title: Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.10016](http://arxiv.org/abs/2401.10016)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10016] Gender Bias in Machine Translation and The Era of Large Language Models](http://arxiv.org/abs/2401.10016) #generative`
* Summary: <p>This chapter examines the role of Machine Translation in perpetuating gender
bias, highlighting the challenges posed by cross-linguistic settings and
statistical dependencies. A comprehensive overview of relevant existing work
related to gender bias in both conventional Neural Machine Translation
approaches and Generative Pretrained Transformer models employed as Machine
Translation systems is provided. Through an experiment using ChatGPT (based on
GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's
current capacity to address gender bias. The findings emphasize the ongoing
need for advancements in mitigating bias in Machine Translation systems and
underscore the importance of fostering fairness and inclusivity in language
technologies.
</p>

### Title: Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2401.09479](http://arxiv.org/abs/2401.09479)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09479] Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning](http://arxiv.org/abs/2401.09479) #generative`
* Summary: <p>The risk of hardware Trojans being inserted at various stages of chip
production has increased in a zero-trust fabless era. To counter this, various
machine learning solutions have been developed for the detection of hardware
Trojans. While most of the focus has been on either a statistical or deep
learning approach, the limited number of Trojan-infected benchmarks affects the
detection accuracy and restricts the possibility of detecting zero-day Trojans.
To close the gap, we first employ generative adversarial networks to amplify
our data in two alternative representation modalities, a graph and a tabular,
ensuring that the dataset is distributed in a representative manner. Further,
we propose a multimodal deep learning approach to detect hardware Trojans and
evaluate the results from both early fusion and late fusion strategies. We also
estimate the uncertainty quantification metrics of each prediction for
risk-aware decision-making. The outcomes not only confirms the efficacy of our
proposed hardware Trojan detection method but also opens a new door for future
studies employing multimodality and uncertainty quantification to address other
hardware security challenges.
</p>

### Title: Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning. (arXiv:2401.09517v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09517](http://arxiv.org/abs/2401.09517)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09517] Dimensional Neuroimaging Endophenotypes: Neurobiological Representations of Disease Heterogeneity Through Machine Learning](http://arxiv.org/abs/2401.09517) #generative`
* Summary: <p>Machine learning has been increasingly used to obtain individualized
neuroimaging signatures for disease diagnosis, prognosis, and response to
treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it
has contributed to a better understanding of disease heterogeneity by
identifying disease subtypes that present significant differences in various
brain phenotypic measures. In this review, we first present a systematic
literature overview of studies using machine learning and multimodal MRI to
unravel disease heterogeneity in various neuropsychiatric and neurodegenerative
disorders, including Alzheimer disease, schizophrenia, major depressive
disorder, autism spectrum disorder, multiple sclerosis, as well as their
potential in transdiagnostic settings. Subsequently, we summarize relevant
machine learning methodologies and discuss an emerging paradigm which we call
dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological
heterogeneity of neuropsychiatric and neurodegenerative disorders into a low
dimensional yet informative, quantitative brain phenotypic representation,
serving as a robust intermediate phenotype (i.e., endophenotype) largely
reflecting underlying genetics and etiology. Finally, we discuss the potential
clinical implications of the current findings and envision future research
avenues.
</p>

### Title: GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting. (arXiv:2401.09881v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09881](http://arxiv.org/abs/2401.09881)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09881] GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme Precipitation Nowcasting](http://arxiv.org/abs/2401.09881) #generative`
* Summary: <p>In recent years, data-driven modeling approaches have gained considerable
traction in various meteorological applications, particularly in the realm of
weather forecasting. However, these approaches often encounter challenges when
dealing with extreme weather conditions. In light of this, we propose
GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of
two methodologies aimed at enhancing the performance of deep learning models
for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built
upon the successful SmaAt-UNet architecture as generator. This network
incorporates precipitation masks (binarized precipitation maps) as an
additional data source, leveraging valuable information for improved
predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented
discriminator inspired by the well-established Pix2Pix architecture.
Furthermore, we assess the performance of GA-SmaAt-GNet using real-life
precipitation dataset from the Netherlands. Our experimental results reveal a
notable improvement in both overall performance and for extreme precipitation
events. Furthermore, we conduct uncertainty analysis on the proposed
GA-SmaAt-GNet model as well as on the precipitation dataset, providing
additional insights into the predictive capabilities of the model. Finally, we
offer further insights into the predictions of our proposed model using
Grad-CAM. This visual explanation technique generates activation heatmaps,
illustrating areas of the input that are more activated for various parts of
the network.
</p>

## anomaly
### Title: CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09443](http://arxiv.org/abs/2401.09443)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09443] CRD: Collaborative Representation Distance for Practical Anomaly Detection](http://arxiv.org/abs/2401.09443) #anomaly`
* Summary: <p>Visual defect detection plays an important role in intelligent industry.
Patch based methods consider visual images as a collection of image patches
according to positions, which have stronger discriminative ability for small
defects in products, e.g. scratches on pills. However, the nearest neighbor
search for the query image and the stored patches will occupy $O(n)$ complexity
in terms of time and space requirements, posing strict challenges for
deployment in edge environments. In this paper, we propose an alternative
approach to the distance calculation of image patches via collaborative
representation models. Starting from the nearest neighbor distance with $L_0$
constraint, we relax the constraint to $L_2$ constraint and solve the distance
quickly in close-formed without actually accessing the original stored
collection of image patches. Furthermore, we point out that the main
computational burden of this close-formed solution can be pre-computed by
high-performance server before deployment. Consequently, the distance
calculation on edge devices only requires a simple matrix multiplication, which
is extremely lightweight and GPU-friendly. Performance on real industrial
scenarios demonstrates that compared to the existing state-of-the-art methods,
this distance achieves several hundred times improvement in computational
efficiency with slight performance drop, while greatly reducing memory
overhead.
</p>

### Title: PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09489](http://arxiv.org/abs/2401.09489)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09489] PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies](http://arxiv.org/abs/2401.09489) #anomaly`
* Summary: <p>In recent years there has been significant progress in time series anomaly
detection. However, after detecting an (perhaps tentative) anomaly, can we
explain it? Such explanations would be useful to triage anomalies. For example,
in an oil refinery, should we respond to an anomaly by dispatching a hydraulic
engineer, or an intern to replace the battery on a sensor? There have been some
parallel efforts to explain anomalies, however many proposed techniques produce
explanations that are indirect, and often seem more complex than the anomaly
they seek to explain. Our review of the literature/checklists/user-manuals used
by frontline practitioners in various domains reveals an interesting
near-universal commonality. Most practitioners discuss, explain and report
anomalies in the following format: The anomaly would be like normal data A, if
not for the corruption B. The reader will appreciate that is a type of
counterfactual explanation. In this work we introduce a domain agnostic
counterfactual explanation technique to produce explanations for time series
anomalies. As we will show, our method can produce both visual and text-based
explanations that are objectively correct, intuitive and in many circumstances,
directly actionable.
</p>

### Title: PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09793](http://arxiv.org/abs/2401.09793)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09793] PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection](http://arxiv.org/abs/2401.09793) #anomaly`
* Summary: <p>Anomaly detection stands as a crucial aspect of time series analysis, aiming
to identify abnormal events in time series samples. The central challenge of
this task lies in effectively learning the representations of normal and
abnormal patterns in a label-lacking scenario. Previous research mostly relied
on reconstruction-based approaches, restricting the representational abilities
of the models. In addition, most of the current deep learning-based methods are
not lightweight enough, which prompts us to design a more efficient framework
for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale
patch-based MLP-Mixer architecture that leverages contrastive learning for
representational extraction and anomaly detection. Specifically, PatchAD is
composed of four distinct MLP Mixers, exclusively utilizing the MLP
architecture for high efficiency and lightweight architecture. Additionally, we
also innovatively crafted a dual project constraint module to mitigate
potential model degradation. Comprehensive experiments demonstrate that PatchAD
achieves state-of-the-art results across multiple real-world multivariate time
series datasets. Our code is publicly
available.\footnote{\url{https://github.com/EmorZz1G/PatchAD}}
</p>

## in-context
### Title: Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.10186](http://arxiv.org/abs/2401.10186)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10186] Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation](http://arxiv.org/abs/2401.10186) #in-context`
* Summary: <p>We investigate to which extent open large language models (LLMs) can generate
coherent and relevant text from structured data. To prevent bias from
benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc
benchmark for five data-to-text (D2T) generation tasks, consisting of
structured data records in standard formats gathered from public APIs. We
leverage reference-free evaluation metrics and LLMs' in-context learning
capabilities, allowing us to test the models with no human-written references.
Our evaluation focuses on annotating semantic accuracy errors on token-level,
combining human annotators and a metric based on GPT-4. Our systematic
examination of the models' behavior across domains and tasks suggests that
state-of-the-art open LLMs with 7B parameters can generate fluent and coherent
text from various standard data formats in zero-shot settings. However, we also
show that semantic accuracy of the outputs remains a major issue: on our
benchmark, 80% of outputs of open LLMs contain a semantic error according to
human annotators (91% according to GPT-4). Our code, data, and model outputs
are available at https://d2t-llm.github.io.
</p>

## memory
### Title: MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection. (arXiv:2401.09923v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09923](http://arxiv.org/abs/2401.09923)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09923] MAMBA: Multi-level Aggregation via Memory Bank for Video Object Detection](http://arxiv.org/abs/2401.09923) #memory`
* Summary: <p>State-of-the-art video object detection methods maintain a memory structure,
either a sliding window or a memory queue, to enhance the current frame using
attention mechanisms. However, we argue that these memory structures are not
efficient or sufficient because of two implied operations: (1) concatenating
all features in memory for enhancement, leading to a heavy computational cost;
(2) frame-wise memory updating, preventing the memory from capturing more
temporal information. In this paper, we propose a multi-level aggregation
architecture via memory bank called MAMBA. Specifically, our memory bank
employs two novel operations to eliminate the disadvantages of existing
methods: (1) light-weight key-set construction which can significantly reduce
the computational cost; (2) fine-grained feature-wise updating strategy which
enables our method to utilize knowledge from the whole video. To better enhance
features from complementary levels, i.e., feature maps and proposals, we
further propose a generalized enhancement operation (GEO) to aggregate
multi-level features in a unified manner. We conduct extensive evaluations on
the challenging ImageNetVID dataset. Compared with existing state-of-the-art
methods, our method achieves superior performance in terms of both speed and
accuracy. More remarkably, MAMBA achieves mAP of 83.7/84.6% at 12.6/9.1 FPS
with ResNet-101. Code is available at
https://github.com/guanxiongsun/video_feature_enhancement.
</p>

### Title: CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification. (arXiv:2401.10011v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10011](http://arxiv.org/abs/2401.10011)
* Code URL: [https://github.com/codegallery24/cpcl](https://github.com/codegallery24/cpcl)
* Copy Paste: `<input type="checkbox">[[2401.10011] CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification](http://arxiv.org/abs/2401.10011) #memory`
* Summary: <p>Weakly supervised text-based person re-identification (TPRe-ID) seeks to
retrieve images of a target person using textual descriptions, without relying
on identity annotations and is more challenging and practical. The primary
challenge is the intra-class differences, encompassing intra-modal feature
variations and cross-modal semantic gaps. Prior works have focused on
instance-level samples and ignored prototypical features of each person which
are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical
Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP
model to weakly supervised TPRe-ID for the first time, mapping visual and
textual instances into a shared latent space. Subsequently, the proposed
Prototypical Multi-modal Memory (PMM) module captures associations between
heterogeneous modalities of image-text pairs belonging to the same person
through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping
fashion. Moreover, the Outlier Pseudo Label Mining (OPLM) module further
distinguishes valuable outlier samples from each modality, enhancing the
creation of more reliable clusters by mining implicit relationships between
image-text pairs. Experimental results demonstrate that our proposed CPCL
attains state-of-the-art performance on all three public datasets, with a
significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on
CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is
available at https://github.com/codeGallery24/CPCL.
</p>

### Title: Model Compression Techniques in Biometrics Applications: A Survey. (arXiv:2401.10139v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.10139](http://arxiv.org/abs/2401.10139)
* Code URL: [https://github.com/eduardacaldeira/compression_bias_survey](https://github.com/eduardacaldeira/compression_bias_survey)
* Copy Paste: `<input type="checkbox">[[2401.10139] Model Compression Techniques in Biometrics Applications: A Survey](http://arxiv.org/abs/2401.10139) #memory`
* Summary: <p>The development of deep learning algorithms has extensively empowered
humanity's task automatization capacity. However, the huge improvement in the
performance of these models is highly correlated with their increasing level of
complexity, limiting their usefulness in human-oriented applications, which are
usually deployed in resource-constrained devices. This led to the development
of compression techniques that drastically reduce the computational and memory
costs of deep learning models without significant performance degradation. This
paper aims to systematize the current literature on this topic by presenting a
comprehensive survey of model compression techniques in biometrics
applications, namely quantization, knowledge distillation and pruning. We
conduct a critical analysis of the comparative value of these techniques,
focusing on their advantages and disadvantages and presenting suggestions for
future work directions that can potentially improve the current methods.
Additionally, we discuss and analyze the link between model bias and model
compression, highlighting the need to direct compression research toward model
fairness in future works.
</p>

### Title: LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09486](http://arxiv.org/abs/2401.09486)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09486] LoMA: Lossless Compressed Memory Attention](http://arxiv.org/abs/2401.09486) #memory`
* Summary: <p>The ability to handle long texts is one of the most important capabilities of
Large Language Models (LLMs), but as the text length increases, the consumption
of resources also increases dramatically. At present, reducing resource
consumption by compressing the KV cache is a common approach. Although there
are many existing compression methods, they share a common drawback: the
compression is not lossless. That is, information is inevitably lost during the
compression process. If the compression rate is high, the probability of losing
important information increases dramatically. We propose a new method, Lossless
Compressed Memory Attention (LoMA), which allows for lossless compression of
information into special memory token KV pairs according to a set compression
ratio. Our experiments have achieved remarkable results, demonstrating that
LoMA can be efficiently trained and has very effective performance.
</p>

### Title: HGAttack: Transferable Heterogeneous Graph Adversarial Attack. (arXiv:2401.09945v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09945](http://arxiv.org/abs/2401.09945)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09945] HGAttack: Transferable Heterogeneous Graph Adversarial Attack](http://arxiv.org/abs/2401.09945) #memory`
* Summary: <p>Heterogeneous Graph Neural Networks (HGNNs) are increasingly recognized for
their performance in areas like the web and e-commerce, where resilience
against adversarial attacks is crucial. However, existing adversarial attack
methods, which are primarily designed for homogeneous graphs, fall short when
applied to HGNNs due to their limited ability to address the structural and
semantic complexity of HGNNs. This paper introduces HGAttack, the first
dedicated gray box evasion attack method for heterogeneous graphs. We design a
novel surrogate model to closely resemble the behaviors of the target HGNN and
utilize gradient-based methods for perturbation generation. Specifically, the
proposed surrogate model effectively leverages heterogeneous information by
extracting meta-path induced subgraphs and applying GNNs to learn node
embeddings with distinct semantics from each subgraph. This approach improves
the transferability of generated attacks on the target HGNN and significantly
reduces memory costs. For perturbation generation, we introduce a
semantics-aware mechanism that leverages subgraph gradient information to
autonomously identify vulnerable edges across a wide range of relations within
a constrained perturbation budget. We validate HGAttack's efficacy with
comprehensive experiments on three datasets, providing empirical analyses of
its generated perturbations. Outperforming baseline methods, HGAttack
demonstrated significant efficacy in diminishing the performance of target HGNN
models, affirming the effectiveness of our approach in evaluating the
robustness of HGNNs against adversarial attacks.
</p>

### Title: Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis. (arXiv:2401.09587v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09587](http://arxiv.org/abs/2401.09587)
* Code URL: [https://github.com/mingruiliu-ml-lab/bilevel-optimization-under-unbounded-smoothness](https://github.com/mingruiliu-ml-lab/bilevel-optimization-under-unbounded-smoothness)
* Copy Paste: `<input type="checkbox">[[2401.09587] Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis](http://arxiv.org/abs/2401.09587) #memory`
* Summary: <p>Bilevel optimization is an important formulation for many machine learning
problems. Current bilevel optimization algorithms assume that the gradient of
the upper-level function is Lipschitz. However, recent studies reveal that
certain neural networks such as recurrent neural networks (RNNs) and
long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness,
rendering conventional bilevel optimization algorithms unsuitable. In this
paper, we design a new bilevel optimization algorithm, namely BO-REP, to
address this challenge. This algorithm updates the upper-level variable using
normalized momentum and incorporates two novel techniques for updating the
lower-level variable: \textit{initialization refinement} and \textit{periodic
updates}. Specifically, once the upper-level variable is initialized, a
subroutine is invoked to obtain a refined estimate of the corresponding optimal
lower-level variable, and the lower-level variable is updated only after every
specific period instead of each iteration. When the upper-level problem is
nonconvex and unbounded smooth, and the lower-level problem is strongly convex,
we prove that our algorithm requires $\widetilde{\mathcal{O}}(1/\epsilon^4)$
iterations to find an $\epsilon$-stationary point in the stochastic setting,
where each iteration involves calling a stochastic gradient or Hessian-vector
product oracle. Notably, this result matches the state-of-the-art complexity
results under the bounded smoothness setting and without mean-squared
smoothness of the stochastic gradient, up to logarithmic factors. Our proof
relies on novel technical lemmas for the periodically updated lower-level
variable, which are of independent interest. Our experiments on
hyper-representation learning, hyperparameter optimization, and data
hyper-cleaning for text classification tasks demonstrate the effectiveness of
our proposed algorithm.
</p>

### Title: Enabling On-device Continual Learning with Binary Neural Networks. (arXiv:2401.09916v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09916](http://arxiv.org/abs/2401.09916)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09916] Enabling On-device Continual Learning with Binary Neural Networks](http://arxiv.org/abs/2401.09916) #memory`
* Summary: <p>On-device learning remains a formidable challenge, especially when dealing
with resource-constrained devices that have limited computational capabilities.
This challenge is primarily rooted in two key issues: first, the memory
available on embedded devices is typically insufficient to accommodate the
memory-intensive back-propagation algorithm, which often relies on
floating-point precision. Second, the development of learning algorithms on
models with extreme quantization levels, such as Binary Neural Networks (BNNs),
is critical due to the drastic reduction in bit representation. In this study,
we propose a solution that combines recent advancements in the field of
Continual Learning (CL) and Binary Neural Networks to enable on-device training
while maintaining competitive performance. Specifically, our approach leverages
binary latent replay (LR) activations and a novel quantization scheme that
significantly reduces the number of bits required for gradient computation. The
experimental validation demonstrates a significant accuracy improvement in
combination with a noticeable reduction in memory requirement, confirming the
suitability of our approach in expanding the practical applications of deep
learning in real-world scenarios.
</p>

## few-shot
### Title: Boosting Few-Shot Semantic Segmentation Via Segment Anything Model. (arXiv:2401.09826v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09826](http://arxiv.org/abs/2401.09826)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09826] Boosting Few-Shot Semantic Segmentation Via Segment Anything Model](http://arxiv.org/abs/2401.09826) #few-shot`
* Summary: <p>In semantic segmentation, accurate prediction masks are crucial for
downstream tasks such as medical image analysis and image editing. Due to the
lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in
predicting masks with precise contours. Recently, we have noticed that the
large foundation model segment anything model (SAM) performs well in processing
detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by
addressing the issue of inaccurate contour. The FSS-SAM is training-free. It
works as a post-processing tool for any FSS methods and can improve the
accuracy of predicted masks. Specifically, we use predicted masks from FSS
methods to generate prompts and then use SAM to predict new masks. To avoid
predicting wrong masks with SAM, we propose a prediction result selection (PRS)
algorithm. The algorithm can remarkably decrease wrong predictions. Experiment
results on public datasets show that our method is superior to base FSS methods
in both quantitative and qualitative aspects.
</p>

### Title: Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention. (arXiv:2401.09866v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09866](http://arxiv.org/abs/2401.09866)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09866] Boosting Few-Shot Segmentation via Instance-Aware Data Augmentation and Local Consensus Guided Cross Attention](http://arxiv.org/abs/2401.09866) #few-shot`
* Summary: <p>Few-shot segmentation aims to train a segmentation model that can fast adapt
to a novel task for which only a few annotated images are provided. Most recent
models have adopted a prototype-based paradigm for few-shot inference. These
approaches may have limited generalization capacity beyond the standard 1- or
5-shot settings. In this paper, we closely examine and reevaluate the
fine-tuning based learning scheme that fine-tunes the classification layer of a
deep segmentation network pre-trained on diverse base classes. To improve the
generalizability of the classification layer optimized with sparsely annotated
samples, we introduce an instance-aware data augmentation (IDA) strategy that
augments the support images based on the relative sizes of the target objects.
The proposed IDA effectively increases the support set's diversity and promotes
the distribution consistency between support and query images. On the other
hand, the large visual difference between query and support images may hinder
knowledge transfer and cripple the segmentation performance. To cope with this
challenge, we introduce the local consensus guided cross attention (LCCA) to
align the query feature with support features based on their dense correlation,
further improving the model's generalizability to the query image. The
significant performance improvements on the standard few-shot segmentation
benchmarks PASCAL-$5^i$ and COCO-$20^i$ verify the efficacy of our proposed
method.
</p>

### Title: Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09555](http://arxiv.org/abs/2401.09555)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09555] Improving Classification Performance With Human Feedback: Label a few, we label the rest](http://arxiv.org/abs/2401.09555) #few-shot`
* Summary: <p>In the realm of artificial intelligence, where a vast majority of data is
unstructured, obtaining substantial amounts of labeled data to train supervised
machine learning models poses a significant challenge. To address this, we
delve into few-shot and active learning, where are goal is to improve AI models
with human feedback on a few labeled examples. This paper focuses on
understanding how a continuous feedback loop can refine models, thereby
enhancing their accuracy, recall, and precision through incremental human
input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and
SetFit, we aim to analyze the efficacy of using a limited number of labeled
examples to substantially improve model accuracy. We benchmark this approach on
the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to
prove that with just a few labeled examples, we are able to surpass the
accuracy of zero shot large language models to provide enhanced text
classification performance. We demonstrate that rather than needing to manually
label millions of rows of data, we just need to label a few and the model can
effectively predict the rest.
</p>

### Title: Leveraging Biases in Large Language Models: "bias-kNN'' for Effective Few-Shot Learning. (arXiv:2401.09783v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.09783](http://arxiv.org/abs/2401.09783)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09783] Leveraging Biases in Large Language Models: "bias-kNN'' for Effective Few-Shot Learning](http://arxiv.org/abs/2401.09783) #few-shot`
* Summary: <p>Large Language Models (LLMs) have shown significant promise in various
applications, including zero-shot and few-shot learning. However, their
performance can be hampered by inherent biases. Instead of traditionally sought
methods that aim to minimize or correct these biases, this study introduces a
novel methodology named ``bias-kNN''. This approach capitalizes on the biased
outputs, harnessing them as primary features for kNN and supplementing with
gold labels. Our comprehensive evaluations, spanning diverse domain text
classification datasets and different GPT-2 model sizes, indicate the
adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach
not only outperforms conventional in-context learning in few-shot scenarios but
also demonstrates robustness across a spectrum of samples, templates and
verbalizers. This study, therefore, presents a unique perspective on harnessing
biases, transforming them into assets for enhanced model performance.
</p>

### Title: Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.10134](http://arxiv.org/abs/2401.10134)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.10134] Spatial-Temporal Large Language Model for Traffic Prediction](http://arxiv.org/abs/2401.10134) #few-shot`
* Summary: <p>Traffic prediction, a critical component for intelligent transportation
systems, endeavors to foresee future traffic at specific locations using
historical data. Although existing traffic prediction models often emphasize
developing complex neural network structures, their accuracy has not seen
improvements accordingly. Recently, Large Language Models (LLMs) have shown
outstanding capabilities in time series analysis. Differing from existing
models, LLMs progress mainly through parameter expansion and extensive
pre-training while maintaining their fundamental structures. In this paper, we
propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic
prediction. Specifically, ST-LLM redefines the timesteps at each location as
tokens and incorporates a spatial-temporal embedding module to learn the
spatial location and global temporal representations of tokens. Then these
representations are fused to provide each token with unified spatial and
temporal information. Furthermore, we propose a novel partially frozen
attention strategy of the LLM, which is designed to capture spatial-temporal
dependencies for traffic prediction. Comprehensive experiments on real traffic
datasets offer evidence that ST-LLM outperforms state-of-the-art models.
Notably, the ST-LLM also exhibits robust performance in both few-shot and
zero-shot prediction scenarios.
</p>

### Title: Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)
* Code URL: [https://github.com/EagleW/Chem-FINESE](https://github.com/EagleW/Chem-FINESE)
* Copy Paste: `<input type="checkbox">[[2401.10189] Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction](http://arxiv.org/abs/2401.10189) #few-shot`
* Summary: <p>Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
</p>

