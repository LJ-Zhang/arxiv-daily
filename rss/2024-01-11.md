## diffusion
### Title: Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton. (arXiv:2401.04921v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.04921](http://arxiv.org/abs/2401.04921)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04921] Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton](http://arxiv.org/abs/2401.04921) #diffusion`
* Summary: <p>Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed to
enhance pose accuracy by generating multiple hypotheses. However, most of the
hypotheses generated deviate substantially from the true pose. Compared to
deterministic models, the excessive uncertainty in probabilistic models leads
to weaker performance in single-hypothesis prediction. To address these two
challenges, we propose a diffusion-based refinement framework called DRPose,
which refines the output of deterministic models by reverse diffusion and
achieves more suitable multi-hypothesis prediction for the current pose
benchmark by multi-step refinement with multiple noises. To this end, we
propose a Scalable Graph Convolution Transformer (SGCT) and a Pose Refinement
Module (PRM) for denoising and refining. Extensive experiments on Human3.6M and
MPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-art
performance on both single and multi-hypothesis 3DHPE. Code is available at
https://github.com/KHB1698/DRPose.
</p>

### Title: SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image. (arXiv:2401.05093v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05093](http://arxiv.org/abs/2401.05093)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05093] SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image](http://arxiv.org/abs/2401.05093) #diffusion`
* Summary: <p>With recent advancements in aerospace technology, the volume of unlabeled
remote sensing image (RSI) data has increased dramatically. Effectively
leveraging this data through self-supervised learning (SSL) is vital in the
field of remote sensing. However, current methodologies, particularly
contrastive learning (CL), a leading SSL method, encounter specific challenges
in this domain. Firstly, CL often mistakenly identifies geographically adjacent
samples with similar semantic content as negative pairs, leading to confusion
during model training. Secondly, as an instance-level discriminative task, it
tends to neglect the essential fine-grained features and complex details
inherent in unstructured RSIs. To overcome these obstacles, we introduce
SwiMDiff, a novel self-supervised pre-training framework designed for RSIs.
SwiMDiff employs a scene-wide matching approach that effectively recalibrates
labels to recognize data from the same scene as false negatives. This
adjustment makes CL more applicable to the nuances of remote sensing.
Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through
the implementation of pixel-level diffusion constraints, we enhance the
encoder's ability to capture both the global semantic information and the
fine-grained features of the images more comprehensively. Our proposed
framework significantly enriches the information available for downstream tasks
in remote sensing. Demonstrating exceptional performance in change detection
and land-cover classification tasks, SwiMDiff proves its substantial utility
and value in the field of remote sensing.
</p>

### Title: CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model. (arXiv:2401.05153v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05153](http://arxiv.org/abs/2401.05153)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05153] CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model](http://arxiv.org/abs/2401.05153) #diffusion`
* Summary: <p>Fusion of a panchromatic (PAN) image and corresponding multispectral (MS)
image is also known as pansharpening, which aims to combine abundant spatial
details of PAN and spectral information of MS. Due to the absence of
high-resolution MS images, available deep-learning-based methods usually follow
the paradigm of training at reduced resolution and testing at both reduced and
full resolution. When taking original MS and PAN images as inputs, they always
obtain sub-optimal results due to the scale variation. In this paper, we
propose to explore the self-supervised representation of pansharpening by
designing a cross-predictive diffusion model, named CrossDiff. It has two-stage
training. In the first stage, we introduce a cross-predictive pretext task to
pre-train the UNet structure based on conditional DDPM, while in the second
stage, the encoders of the UNets are frozen to directly extract spatial and
spectral features from PAN and MS, and only the fusion head is trained to adapt
for pansharpening task. Extensive experiments show the effectiveness and
superiority of the proposed model compared with state-of-the-art supervised and
unsupervised methods. Besides, the cross-sensor experiments also verify the
generalization ability of proposed self-supervised representation learners for
other satellite's datasets. We will release our code for reproducibility.
</p>

### Title: Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN. (arXiv:2401.05159v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05159](http://arxiv.org/abs/2401.05159)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05159] Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN](http://arxiv.org/abs/2401.05159) #diffusion`
* Summary: <p>This study explores the utilization of Dermatoscopic synthetic data generated
through stable diffusion models as a strategy for enhancing the robustness of
machine learning model training. Synthetic data generation plays a pivotal role
in mitigating challenges associated with limited labeled datasets, thereby
facilitating more effective model training. In this context, we aim to
incorporate enhanced data transformation techniques by extending the recent
success of few-shot learning and a small amount of data representation in
text-to-image latent diffusion models. The optimally tuned model is further
used for rendering high-quality skin lesion synthetic data with diverse and
realistic characteristics, providing a valuable supplement and diversity to the
existing training data. We investigate the impact of incorporating newly
generated synthetic data into the training pipeline of state-of-art machine
learning models, assessing its effectiveness in enhancing model performance and
generalization to unseen real-world data. Our experimental results demonstrate
the efficacy of the synthetic data generated through stable diffusion models
helps in improving the robustness and adaptability of end-to-end CNN and vision
transformer models on two different real-world skin lesion datasets.
</p>

### Title: PIXART-{\delta}: Fast and Controllable Image Generation with Latent Consistency Models. (arXiv:2401.05252v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05252](http://arxiv.org/abs/2401.05252)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05252] PIXART-{\delta}: Fast and Controllable Image Generation with Latent Consistency Models](http://arxiv.org/abs/2401.05252) #diffusion`
* Summary: <p>This technical report introduces PIXART-{\delta}, a text-to-image synthesis
framework that integrates the Latent Consistency Model (LCM) and ControlNet
into the advanced PIXART-{\alpha} model. PIXART-{\alpha} is recognized for its
ability to generate high-quality images of 1024px resolution through a
remarkably efficient training process. The integration of LCM in
PIXART-{\delta} significantly accelerates the inference speed, enabling the
production of high-quality images in just 2-4 steps. Notably, PIXART-{\delta}
achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,
marking a 7x improvement over the PIXART-{\alpha}. Additionally,
PIXART-{\delta} is designed to be efficiently trainable on 32GB V100 GPUs
within a single day. With its 8-bit inference capability (von Platen et al.,
2023), PIXART-{\delta} can synthesize 1024px images within 8GB GPU memory
constraints, greatly enhancing its usability and accessibility. Furthermore,
incorporating a ControlNet-like module enables fine-grained control over
text-to-image diffusion models. We introduce a novel ControlNet-Transformer
architecture, specifically tailored for Transformers, achieving explicit
controllability alongside high-quality image generation. As a state-of-the-art,
open-source image generation model, PIXART-{\delta} offers a promising
alternative to the Stable Diffusion family of models, contributing
significantly to text-to-image synthesis.
</p>

### Title: Score Distillation Sampling with Learned Manifold Corrective. (arXiv:2401.05293v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05293](http://arxiv.org/abs/2401.05293)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05293] Score Distillation Sampling with Learned Manifold Corrective](http://arxiv.org/abs/2401.05293) #diffusion`
* Summary: <p>Score Distillation Sampling (SDS) is a recent but already widely popular
method that relies on an image diffusion model to control optimization problems
using text prompts. In this paper, we conduct an in-depth analysis of the SDS
loss function, identify an inherent problem with its formulation, and propose a
surprisingly easy but effective fix. Specifically, we decompose the loss into
different factors and isolate the component responsible for noisy gradients. In
the original formulation, high text guidance is used to account for the noise,
leading to unwanted side effects. Instead, we train a shallow network mimicking
the timestep-dependent denoising deficiency of the image diffusion model in
order to effectively factor it out. We demonstrate the versatility and the
effectiveness of our novel loss formulation through several qualitative and
quantitative experiments, including optimization-based image synthesis and
editing, zero-shot image translation network training, and text-to-3D
synthesis.
</p>

## self-supervised
### Title: Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data. (arXiv:2401.05014v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05014](http://arxiv.org/abs/2401.05014)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05014] Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data](http://arxiv.org/abs/2401.05014) #self-supervised`
* Summary: <p>Source-free cross-modal knowledge transfer is a crucial yet challenging task,
which aims to transfer knowledge from one source modality (e.g., RGB) to the
target modality (e.g., depth or infrared) with no access to the task-relevant
(TR) source data due to memory and privacy concerns. A recent attempt leverages
the paired task-irrelevant (TI) data and directly matches the features from
them to eliminate the modality gap. However, it ignores a pivotal clue that the
paired TI data could be utilized to effectively estimate the source data
distribution and better facilitate knowledge transfer to the target modality.
To this end, we propose a novel yet concise framework to unlock the potential
of paired TI data for enhancing source-free cross-modal knowledge transfer. Our
work is buttressed by two key technical components. Firstly, to better estimate
the source data distribution, we introduce a Task-irrelevant data-Guided
Modality Bridging (TGMB) module. It translates the target modality data (e.g.,
infrared) into the source-like RGB images based on paired TI data and the
guidance of the available source model to alleviate two key gaps: 1)
inter-modality gap between the paired TI data; 2) intra-modality gap between TI
and TR target data. We then propose a Task-irrelevant data-Guided Knowledge
Transfer (TGKT) module that transfers knowledge from the source model to the
target model by leveraging the paired TI data. Notably, due to the
unavailability of labels for the TR target data and its less reliable
prediction from the source model, our TGKT model incorporates a self-supervised
pseudo-labeling approach to enable the target model to learn from its
predictions. Extensive experiments show that our method achieves
state-of-the-art performance on three datasets (RGB-to-depth and
RGB-to-infrared).
</p>

### Title: Toward distortion-aware change detection in realistic scenarios. (arXiv:2401.05157v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05157](http://arxiv.org/abs/2401.05157)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05157] Toward distortion-aware change detection in realistic scenarios](http://arxiv.org/abs/2401.05157) #self-supervised`
* Summary: <p>In the conventional change detection (CD) pipeline, two manually registered
and labeled remote sensing datasets serve as the input of the model for
training and prediction. However, in realistic scenarios, data from different
periods or sensors could fail to be aligned as a result of various coordinate
systems. Geometric distortion caused by coordinate shifting remains a thorny
issue for CD algorithms. In this paper, we propose a reusable self-supervised
framework for bitemporal geometric distortion in CD tasks. The whole framework
is composed of Pretext Representation Pre-training, Bitemporal Image Alignment,
and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the
key components of the framework can be reused for assistance in the bitemporal
image alignment, while simultaneously enhancing the performance of the CD
decoder. Experimental results in 2 large-scale realistic scenarios demonstrate
that our proposed method can alleviate the bitemporal geometric distortion in
CD tasks.
</p>

### Title: HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting. (arXiv:2401.05012v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.05012](http://arxiv.org/abs/2401.05012)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05012] HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting](http://arxiv.org/abs/2401.05012) #self-supervised`
* Summary: <p>Time series forecasting is crucial and challenging in the real world. The
recent surge in interest regarding time series foundation models, which cater
to a diverse array of downstream tasks, is noteworthy. However, existing
methods often overlook the multi-scale nature of time series, an aspect crucial
for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical
multi-scale masked time series modeling method designed for long-term
forecasting. Specifically, it comprises four integral components: (1)
hierarchical multi-scale transformer (HMT) to capture temporal information at
different scales; (2) decoupled encoder-decoder (DED) forces the encoder to
focus on feature extraction, while the decoder to focus on pretext tasks; (3)
multi-scale masked reconstruction (MMR) provides multi-stage supervision
signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to
capture dependencies between different scales for forecasting. Collectively,
these components enhance multi-scale feature extraction capabilities in masked
time series modeling and contribute to improved prediction accuracy. We conduct
extensive experiments on 7 mainstream datasets to prove that HiMTM has obvious
advantages over contemporary self-supervised and end-to-end learning methods.
The effectiveness of HiMTM is further showcased by its application in the
industry of natural gas demand forecasting.
</p>

## foundation model
## generative
### Title: Content-Conditioned Generation of Stylized Free hand Sketches. (arXiv:2401.04739v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.04739](http://arxiv.org/abs/2401.04739)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04739] Content-Conditioned Generation of Stylized Free hand Sketches](http://arxiv.org/abs/2401.04739) #generative`
* Summary: <p>In recent years, the recognition of free-hand sketches has remained a popular
task. However, in some special fields such as the military field, free-hand
sketches are difficult to sample on a large scale. Common data augmentation and
image generation techniques are difficult to produce images with various
free-hand sketching styles. Therefore, the recognition and segmentation tasks
in related fields are limited. In this paper, we propose a novel adversarial
generative network that can accurately generate realistic free-hand sketches
with various styles. We explore the performance of the model, including using
styles randomly sampled from a prior normal distribution to generate images
with various free-hand sketching styles, disentangling the painters' styles
from known free-hand sketches to generate images with specific styles, and
generating images of unknown classes that are not in the training set. We
further demonstrate with qualitative and quantitative evaluations our
advantages in visual quality, content accuracy, and style imitation on
SketchIME.
</p>

### Title: AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction. (arXiv:2401.05018v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05018](http://arxiv.org/abs/2401.05018)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05018] AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction](http://arxiv.org/abs/2401.05018) #generative`
* Summary: <p>To achieve seamless collaboration between robots and humans in a shared
environment, accurately predicting future human movements is essential. Human
motion prediction has traditionally been approached as a sequence prediction
problem, leveraging historical human motion data to estimate future poses.
Beginning with vanilla recurrent networks, the research community has
investigated a variety of methods for learning human motion dynamics,
encompassing graph-based and generative approaches. Despite these efforts,
achieving accurate long-term predictions continues to be a significant
challenge. In this regard, we present the Adversarial Motion Transformer
(AdvMT), a novel model that integrates a transformer-based motion encoder and a
temporal continuity discriminator. This combination effectively captures
spatial and temporal dependencies simultaneously within frames. With
adversarial training, our method effectively reduces the unwanted artifacts in
predictions, thereby ensuring the learning of more realistic and fluid human
motions. The evaluation results indicate that AdvMT greatly enhances the
accuracy of long-term predictions while also delivering robust short-term
predictions
</p>

### Title: Application of Deep Learning in Blind Motion Deblurring: Current Status and Future Prospects. (arXiv:2401.05055v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05055](http://arxiv.org/abs/2401.05055)
* Code URL: [https://github.com/visionverse/blind-motion-deblurring-survey](https://github.com/visionverse/blind-motion-deblurring-survey)
* Copy Paste: `<input type="checkbox">[[2401.05055] Application of Deep Learning in Blind Motion Deblurring: Current Status and Future Prospects](http://arxiv.org/abs/2401.05055) #generative`
* Summary: <p>Motion deblurring is one of the fundamental problems of computer vision and
has received continuous attention. The variability in blur, both within and
across images, imposes limitations on non-blind deblurring techniques that rely
on estimating the blur kernel. As a response, blind motion deblurring has
emerged, aiming to restore clear and detailed images without prior knowledge of
the blur type, fueled by the advancements in deep learning methodologies.
Despite strides in this field, a comprehensive synthesis of recent progress in
deep learning-based blind motion deblurring is notably absent. This paper fills
that gap by providing an exhaustive overview of the role of deep learning in
blind motion deblurring, encompassing datasets, evaluation metrics, and methods
developed over the last six years. Specifically, we first introduce the types
of motion blur and the fundamental principles of deblurring. Next, we outline
the shortcomings of traditional non-blind deblurring algorithms, emphasizing
the advantages of employing deep learning techniques for deblurring tasks.
Following this, we categorize and summarize existing blind motion deblurring
methods based on different backbone networks, including convolutional neural
networks, generative adversarial networks, recurrent neural networks, and
Transformer networks. Subsequently, we elaborate not only on the fundamental
principles of these different categories but also provide a comprehensive
summary and comparison of their advantages and limitations. Qualitative and
quantitative experimental results conducted on four widely used datasets
further compare the performance of SOTA methods. Finally, an analysis of
present challenges and future pathways. All collected models, benchmark
datasets, source code links, and codes for evaluation have been made publicly
available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey
</p>

### Title: MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05163](http://arxiv.org/abs/2401.05163)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05163] MISS: A Generative Pretraining and Finetuning Approach for Med-VQA](http://arxiv.org/abs/2401.05163) #generative`
* Summary: <p>Medical visual question answering (VQA) is a challenging multimodal task,
where Vision-Language Pre-training (VLP) models can effectively improve the
generalization performance. However, most methods in the medical field treat
VQA as an answer classification task which is difficult to transfer to
practical application scenarios. Additionally, due to the privacy of medical
images and the expensive annotation process, large-scale medical image-text
pairs datasets for pretraining are severely lacking. In this paper, we propose
a large-scale MultI-task Self-Supervised learning based framework (MISS) for
medical VQA tasks. Unlike existing methods, we treat medical VQA as a
generative task. We unify the text encoder and multimodal encoder and align
image-text features through multi-task learning. Furthermore, we propose a
Transfer-and-Caption method that extends the feature space of single-modal
image datasets using large language models (LLMs), enabling those traditional
medical vision field task data to be applied to VLP. Experiments show that our
method achieves excellent results with fewer multimodal datasets and
demonstrates the advantages of generative VQA models. The code and model
weights will be released upon the paper's acceptance.
</p>

### Title: InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05335](http://arxiv.org/abs/2401.05335)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05335] InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes](http://arxiv.org/abs/2401.05335) #generative`
* Summary: <p>We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.
</p>

### Title: BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation. (arXiv:2401.05125v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.05125](http://arxiv.org/abs/2401.05125)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05125] BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation](http://arxiv.org/abs/2401.05125) #generative`
* Summary: <p>Biomedical entity linking (BEL) is the task of grounding entity mentions to a
knowledge base (KB). A popular approach to the task are name-based methods,
i.e. those identifying the most appropriate name in the KB for a given mention,
either via dense retrieval or autoregressive modeling. However, as these
methods directly return KB names, they cannot cope with homonyms, i.e.
different KB entities sharing the exact same name. This significantly affects
their performance, especially for KBs where homonyms account for a large amount
of entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD
(Biomedical Entity Linking with Homonym Disambiguation), a new name-based
method that copes with this challenge. Specifically, BELHD builds upon the
BioSyn (Sung et al.,2020) model introducing two crucial extensions. First, it
performs a preprocessing of the KB in which it expands homonyms with an
automatically chosen disambiguating string, thus enforcing unique linking
decisions. Second, we introduce candidate sharing, a novel strategy to select
candidates for contrastive learning that enhances the overall training signal.
Experiments with 10 corpora and five entity types show that BELHD improves upon
state-of-the-art approaches, achieving the best results in 6 out 10 corpora
with an average improvement of 4.55pp recall@1. Furthermore, the KB
preprocessing is orthogonal to the core prediction model and thus can also
improve other methods, which we exemplify for GenBioEL (Yuan et al, 2022), a
generative name-based BEL approach. Code is available at: link added upon
publication.
</p>

### Title: A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.04856](http://arxiv.org/abs/2401.04856)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04856] A Good Score Does not Lead to A Good Generative Model](http://arxiv.org/abs/2401.04856) #generative`
* Summary: <p>Score-based Generative Models (SGMs) is one leading method in generative
modeling, renowned for their ability to generate high-quality samples from
complex, high-dimensional data distributions. The method enjoys empirical
success and is supported by rigorous theoretical convergence properties. In
particular, it has been shown that SGMs can generate samples from a
distribution that is close to the ground-truth if the underlying score function
is learned well, suggesting the success of SGM as a generative model. We
provide a counter-example in this paper. Through the sample complexity
argument, we provide one specific setting where the score function is learned
well. Yet, SGMs in this setting can only output samples that are Gaussian
blurring of training data points, mimicking the effects of kernel density
estimation. The finding resonates a series of recent finding that reveal that
SGMs can demonstrate strong memorization effect and fail to generate.
</p>

### Title: Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection. (arXiv:2401.04933v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.04933](http://arxiv.org/abs/2401.04933)
* Code URL: [https://github.com/XavierXiao/Likelihood-Regret](https://github.com/XavierXiao/Likelihood-Regret)
* Copy Paste: `<input type="checkbox">[[2401.04933] Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection](http://arxiv.org/abs/2401.04933) #generative`
* Summary: <p>While likelihood is attractive in theory, its estimates by deep generative
models (DGMs) are often broken in practice, and perform poorly for out of
distribution (OOD) Detection. Various recent works started to consider
alternative scores and achieved better performances. However, such recipes do
not come with provable guarantees, nor is it clear that their choices extract
sufficient information.
</p>
<p>We attempt to change this by conducting a case study on variational
autoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,
generalizing the likelihood principle. This narrows the search for informative
summary statistics down to the minimal sufficient statistics of VAEs'
conditional likelihoods. Second, introducing new theoretic tools such as nearly
essential support, essential distance and co-Lipschitzness, we obtain
non-asymptotic provable OOD detection guarantees for certain distillation of
the minimal sufficient statistics. The corresponding LPath algorithm
demonstrates SOTA performances, even using simple and small VAEs with poor
likelihood estimates. To our best knowledge, this is the first provable
unsupervised OOD method that delivers excellent empirical results, better than
any other VAEs based techniques. We use the same model as
\cite{xiao2020likelihood}, open sourced from:
https://github.com/XavierXiao/Likelihood-Regret
</p>

## anomaly
### Title: Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics. (arXiv:2401.04942v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.04942](http://arxiv.org/abs/2401.04942)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04942] Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics](http://arxiv.org/abs/2401.04942) #anomaly`
* Summary: <p>In the past several years, road anomaly segmentation is actively explored in
the academia and drawing growing attention in the industry. The rationale
behind is straightforward: if the autonomous car can brake before hitting an
anomalous object, safety is promoted. However, this rationale naturally calls
for a temporally informed setting while existing methods and benchmarks are
designed in an unrealistic frame-wise manner. To bridge this gap, we contribute
the first video anomaly segmentation dataset for autonomous driving. Since
placing various anomalous objects on busy roads and annotating them in every
frame are dangerous and expensive, we resort to synthetic data. To improve the
relevance of this synthetic dataset to real-world applications, we train a
generative adversarial network conditioned on rendering G-buffers for
photorealism enhancement. Our dataset consists of 120,000 high-resolution
frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial
benchmarking, we provide baselines using latest supervised and unsupervised
road anomaly segmentation methods. Apart from conventional ones, we focus on
two new metrics: temporal consistency and latencyaware streaming accuracy. We
believe the latter is valuable as it measures whether an anomaly segmentation
algorithm can truly prevent a car from crashing in a temporally informed
setting.
</p>

### Title: LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection. (arXiv:2401.04749v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.04749](http://arxiv.org/abs/2401.04749)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04749] LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection](http://arxiv.org/abs/2401.04749) #anomaly`
* Summary: <p>Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios. However, previous deep models merely focused on
extracting the semantics of log sequences in the same domain, leading to poor
generalization on multi-domain logs. To alleviate this issue, we propose a
unified Transformer-based framework for Log anomaly detection (LogFormer) to
improve the generalization ability across different domains, where we establish
a two-stage process including the pre-training and adapter-based tuning stage.
Specifically, our model is first pre-trained on the source domain to obtain
shared semantic knowledge of log data. Then, we transfer such knowledge to the
target domain via shared parameters. Besides, the Log-Attention module is
proposed to supplement the information ignored by the log-paring. The proposed
method is evaluated on three public and one real-world datasets. Experimental
results on multiple benchmarks demonstrate the effectiveness of our LogFormer
with fewer trainable parameters and lower training costs.
</p>

## in-context
### Title: Leveraging Print Debugging to Improve Code Generation in Large Language Models. (arXiv:2401.05319v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.05319](http://arxiv.org/abs/2401.05319)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05319] Leveraging Print Debugging to Improve Code Generation in Large Language Models](http://arxiv.org/abs/2401.05319) #in-context`
* Summary: <p>Large language models (LLMs) have made significant progress in code
generation tasks, but their performance in tackling programming problems with
complex data structures and algorithms remains suboptimal. To address this
issue, we propose an in-context learning approach that guides LLMs to debug by
using a "print debugging" method, which involves inserting print statements to
trace and analysing logs for fixing the bug. We collect a Leetcode problem
dataset and evaluate our method using the Leetcode online judging system.
Experiments with GPT-4 demonstrate the effectiveness of our approach,
outperforming rubber duck debugging in easy and medium-level Leetcode problems
by 1.5% and 17.9%.
</p>

## memory
### Title: EmMixformer: Mix transformer for eye movement recognition. (arXiv:2401.04956v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.04956](http://arxiv.org/abs/2401.04956)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04956] EmMixformer: Mix transformer for eye movement recognition](http://arxiv.org/abs/2401.04956) #memory`
* Summary: <p>Eye movement (EM) is a new highly secure biometric behavioral modality that
has received increasing attention in recent years. Although deep neural
networks, such as convolutional neural network (CNN), have recently achieved
promising performance, current solutions fail to capture local and global
temporal dependencies within eye movement data. To overcome this problem, we
propose in this paper a mixed transformer termed EmMixformer to extract time
and frequency domain information for eye movement recognition. To this end, we
propose a mixed block consisting of three modules, transformer, attention Long
short-term memory (attention LSTM), and Fourier transformer. We are the first
to attempt leveraging transformer to learn long temporal dependencies within
eye movement. Second, we incorporate the attention mechanism into LSTM to
propose attention LSTM with the aim to learn short temporal dependencies.
Third, we perform self attention in the frequency domain to learn global
features. As the three modules provide complementary feature representations in
terms of local and global dependencies, the proposed EmMixformer is capable of
improving recognition accuracy. The experimental results on our eye movement
dataset and two public eye movement datasets show that the proposed EmMixformer
outperforms the state of the art by achieving the lowest verification error.
</p>

### Title: Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing. (arXiv:2401.04881v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.04881](http://arxiv.org/abs/2401.04881)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04881] Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing](http://arxiv.org/abs/2401.04881) #memory`
* Summary: <p>As LLMs have become capable of processing more complex types of inputs,
researchers have recently studied how to efficiently and affordably process
possibly arbitrarily long sequences. One effective approach is to use a FIFO
memory to store keys and values of an attention sublayer from past chunks to
allow subsequent queries to attend. However, this approach requires a large
memory and/or takes into the consideration the specific LM architecture.
Moreover, due to the causal nature between the key-values in prior context and
the queries at present, this approach cannot be extended to bidirectional
attention such as in an encoder-decoder or PrefixLM decoder-only architecture.
In this paper, we propose to use eviction policies, such as LRA and LFA, to
reduce the memory size and adapt to various architectures, and we also propose
the Attendre layer, a wait-to-attend mechanism by retrieving the key-value
memory (K/V memory) with evicted queries in the query memory (Q memory). As a
first step, we evaluate this method in the context length extension setup using
the TriviaQA reading comprehension task, and show the effectiveness of the
approach.
</p>

### Title: REACT: Autonomous Intrusion Response System for Intelligent Vehicles. (arXiv:2401.04792v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2401.04792](http://arxiv.org/abs/2401.04792)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.04792] REACT: Autonomous Intrusion Response System for Intelligent Vehicles](http://arxiv.org/abs/2401.04792) #memory`
* Summary: <p>Autonomous and connected vehicles are rapidly evolving, integrating numerous
technologies and software. This progress, however, has made them appealing
targets for cybersecurity attacks. As the risk of cyber threats escalates with
this advancement, the focus is shifting from solely preventing these attacks to
also mitigating their impact. Current solutions rely on vehicle security
operation centers, where attack information is analyzed before deciding on a
response strategy. However, this process can be time-consuming and faces
scalability challenges, along with other issues stemming from vehicle
connectivity. This paper proposes a dynamic intrusion response system
integrated within the vehicle. This system enables the vehicle to respond to a
variety of incidents almost instantly, thereby reducing the need for
interaction with the vehicle security operation center. The system offers a
comprehensive list of potential responses, a methodology for response
evaluation, and various response selection methods. The proposed solution was
implemented on an embedded platform. Two distinct cyberattack use cases served
as the basis for evaluating the system. The evaluation highlights the system's
adaptability, its ability to respond swiftly, its minimal memory footprint, and
its capacity for dynamic system parameter adjustments. The proposed solution
underscores the necessity and feasibility of incorporating dynamic response
mechanisms in smart vehicles. This is a crucial factor in ensuring the safety
and resilience of future smart mobility.
</p>

### Title: ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries. (arXiv:2401.05251v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.05251](http://arxiv.org/abs/2401.05251)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05251] ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries](http://arxiv.org/abs/2401.05251) #memory`
* Summary: <p>Robust and performant controllers are essential for industrial applications.
However, deriving controller parameters for complex and nonlinear systems is
challenging and time-consuming. To facilitate automatic controller
parametrization, this work presents a novel approach using deep reinforcement
learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the
control of parameter-variant systems, a class of systems with complex behavior
which depends on the operating conditions. For this system class,
gain-scheduling control structures are widely used in applications across
industries due to well-known design principles. Facilitating the expensive
controller parametrization task regarding these control structures, we deploy
an DRL agent. Based on control system observations, the agent autonomously
decides how to adapt the controller parameters. We make the adaptation process
more efficient by introducing BSGs to map the controller parameters which may
depend on numerous operating conditions. To preprocess time-series data and
extract a fixed-length feature vector, we use a long short-term memory (LSTM)
neural networks. Furthermore, this work contributes actor regularizations that
are relevant to real-world environments which differ from training.
Accordingly, we apply dropout layer normalization to the actor and critic
networks of the truncated quantile critic (TQC) algorithm. To show our
approach's working principle and effectiveness, we train and evaluate the DRL
agent on the parametrization task of an industrial control structure with
parameter lookup tables.
</p>

## few-shot
### Title: Less is More : A Closer Look at Multi-Modal Few-Shot Learning. (arXiv:2401.05010v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05010](http://arxiv.org/abs/2401.05010)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05010] Less is More : A Closer Look at Multi-Modal Few-Shot Learning](http://arxiv.org/abs/2401.05010) #few-shot`
* Summary: <p>Few-shot Learning aims to learn and distinguish new categories with a very
limited number of available images, presenting a significant challenge in the
realm of deep learning. Recent researchers have sought to leverage the
additional textual or linguistic information of these rare categories with a
pre-trained language model to facilitate learning, thus partially alleviating
the problem of insufficient supervision signals. However, the full potential of
the textual information and pre-trained language model have been underestimated
in the few-shot learning till now, resulting in limited performance
enhancements. To address this, we propose a simple but effective framework for
few-shot learning tasks, specifically designed to exploit the textual
information and language model. In more detail, we explicitly exploit the
zero-shot capability of the pre-trained language model with the learnable
prompt. And we just add the visual feature with the textual feature for
inference directly without the intricate designed fusion modules in previous
works. Additionally, we apply the self-ensemble and distillation to further
enhance these components. Our extensive experiments conducted across four
widely used few-shot datasets demonstrate that our simple framework achieves
impressive results. Particularly noteworthy is its outstanding performance in
the 1-shot learning task, surpassing state-of-the-art methods by an average of
3.0\% in classification accuracy. \footnote{We will make the source codes of
the proposed framework publicly available upon acceptance. }.
</p>

### Title: URHand: Universal Relightable Hands. (arXiv:2401.05334v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.05334](http://arxiv.org/abs/2401.05334)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.05334] URHand: Universal Relightable Hands](http://arxiv.org/abs/2401.05334) #few-shot`
* Summary: <p>Existing photorealistic relightable hand models require extensive
identity-specific observations in different views, poses, and illuminations,
and face challenges in generalizing to natural illuminations and novel
identities. To bridge this gap, we present URHand, the first universal
relightable hand model that generalizes across viewpoints, poses,
illuminations, and identities. Our model allows few-shot personalization using
images captured with a mobile phone, and is ready to be photorealistically
rendered under novel illuminations. To simplify the personalization process
while retaining photorealism, we build a powerful universal relightable prior
based on neural relighting from multi-view images of hands captured in a light
stage with hundreds of identities. The key challenge is scaling the
cross-identity training while maintaining personalized fidelity and sharp
details without compromising generalization under natural illuminations. To
this end, we propose a spatially varying linear lighting model as the neural
renderer that takes physics-inspired shading as input feature. By removing
non-linear activations and bias, our specifically designed lighting model
explicitly keeps the linearity of light transport. This enables single-stage
training from light-stage data while generalizing to real-time rendering under
arbitrary continuous illuminations across diverse identities. In addition, we
introduce the joint learning of a physically based model and our neural
relighting model, which further improves fidelity and generalization. Extensive
experiments show that our approach achieves superior performance over existing
methods in terms of both quality and generalizability. We also demonstrate
quick personalization of URHand from a short phone scan of an unseen identity.
</p>

