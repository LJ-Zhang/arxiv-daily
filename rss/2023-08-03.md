## diffusion
### Title: The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.00755](http://arxiv.org/abs/2308.00755)
* Code URL: [https://github.com/preethiseshadri518/bias-amplification-paradox](https://github.com/preethiseshadri518/bias-amplification-paradox)
* Copy Paste: `<input type="checkbox">[[2308.00755] The Bias Amplification Paradox in Text-to-Image Generation](http://arxiv.org/abs/2308.00755) #diffusion`
* Summary: <p>Bias amplification is a phenomenon in which models increase imbalances
present in the training data. In this paper, we study bias amplification in the
text-to-image domain using Stable Diffusion by comparing gender ratios in
training vs. generated images. We find that the model appears to amplify
gender-occupation biases found in the training data (LAION). However, we
discover that amplification can largely be attributed to discrepancies between
training captions and model prompts. For example, an inherent difference is
that captions from the training data often contain explicit gender information
while the prompts we use do not, which leads to a distribution shift and
consequently impacts bias measures. Once we account for various distributional
differences between texts used for training and generation, we observe that
amplification decreases considerably. Our findings illustrate the challenges of
comparing biases in models and the data they are trained on, and highlight
confounding factors that contribute to bias amplification.
</p>

### Title: Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.00994](http://arxiv.org/abs/2308.00994)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.00994] Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective](http://arxiv.org/abs/2308.00994) #diffusion`
* Summary: <p>We live in a vast ocean of data, and deep neural networks are no exception to
this. However, this data exhibits an inherent phenomenon of imbalance. This
imbalance poses a risk of deep neural networks producing biased predictions,
leading to potentially severe ethical and social consequences. To address these
challenges, we believe that the use of generative models is a promising
approach for comprehending tasks, given the remarkable advancements
demonstrated by recent diffusion models in generating high-quality images. In
this work, we propose a simple yet effective baseline, SYNAuG, that utilizes
synthetic data as a preliminary step before employing task-specific algorithms
to address data imbalance problems. This straightforward approach yields
impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT,
UTKFace, and Waterbird, surpassing the performance of existing task-specific
methods. While we do not claim that our approach serves as a complete solution
to the problem of data imbalance, we argue that supplementing the existing data
with synthetic data proves to be an effective and crucial preliminary step in
addressing data imbalance concerns.
</p>

### Title: DiffusePast: Diffusion-based Generative Replay for Class Incremental Semantic Segmentation. (arXiv:2308.01127v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.01127](http://arxiv.org/abs/2308.01127)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01127] DiffusePast: Diffusion-based Generative Replay for Class Incremental Semantic Segmentation](http://arxiv.org/abs/2308.01127) #diffusion`
* Summary: <p>The Class Incremental Semantic Segmentation (CISS) extends the traditional
segmentation task by incrementally learning newly added classes. Previous work
has introduced generative replay, which involves replaying old class samples
generated from a pre-trained GAN, to address the issues of catastrophic
forgetting and privacy concerns. However, the generated images lack semantic
precision and exhibit out-of-distribution characteristics, resulting in
inaccurate masks that further degrade the segmentation performance. To tackle
these challenges, we propose DiffusePast, a novel framework featuring a
diffusion-based generative replay module that generates semantically accurate
images with more reliable masks guided by different instructions (e.g., text
prompts or edge maps). Specifically, DiffusePast introduces a dual-generator
paradigm, which focuses on generating old class images that align with the
distribution of downstream datasets while preserving the structure and layout
of the original images, enabling more precise masks. To adapt to the novel
visual concepts of newly added classes continuously, we incorporate class-wise
token embedding when updating the dual-generator. Moreover, we assign adequate
pseudo-labels of old classes to the background pixels in the new step images,
further mitigating the forgetting of previously learned knowledge. Through
comprehensive experiments, our method demonstrates competitive performance
across mainstream benchmarks, striking a better balance between the performance
of old and novel classes.
</p>

### Title: Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation. (arXiv:2308.01147v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.01147](http://arxiv.org/abs/2308.01147)
* Code URL: [https://github.com/zgj77/fsacdm](https://github.com/zgj77/fsacdm)
* Copy Paste: `<input type="checkbox">[[2308.01147] Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation](http://arxiv.org/abs/2308.01147) #diffusion`
* Summary: <p>The recently rising markup-to-image generation poses greater challenges as
compared to natural image generation, due to its low tolerance for errors as
well as the complex sequence and context correlations between markup and
rendered image. This paper proposes a novel model named "Contrast-augmented
Diffusion Model with Fine-grained Sequence Alignment" (FSA-CDM), which
introduces contrastive positive/negative samples into the diffusion model to
boost performance for markup-to-image generation. Technically, we design a
fine-grained cross-modal alignment module to well explore the sequence
similarity between the two modalities for learning robust feature
representations. To improve the generalization ability, we propose a
contrast-augmented diffusion model to explicitly explore positive and negative
samples by maximizing a novel contrastive variational objective, which is
mathematically inferred to provide a tighter bound for the model's
optimization. Moreover, the context-aware cross attention module is developed
to capture the contextual information within markup language during the
denoising process, yielding better noise prediction results. Extensive
experiments are conducted on four benchmark datasets from different domains,
and the experimental results demonstrate the effectiveness of the proposed
components in FSA-CDM, significantly exceeding state-of-the-art performance by
about 2%-12% DTW improvements. The code will be released at
https://github.com/zgj77/FSACDM.
</p>

### Title: Patched Denoising Diffusion Models For High-Resolution Image Synthesis. (arXiv:2308.01316v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.01316](http://arxiv.org/abs/2308.01316)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01316] Patched Denoising Diffusion Models For High-Resolution Image Synthesis](http://arxiv.org/abs/2308.01316) #diffusion`
* Summary: <p>We propose an effective denoising diffusion model for generating
high-resolution images (e.g., 1024$\times$512), trained on small-size image
patches (e.g., 64$\times$64). We name our algorithm Patch-DM, in which a new
feature collage strategy is designed to avoid the boundary artifact when
synthesizing large-size images. Feature collage systematically crops and
combines partial features of the neighboring patches to predict the features of
a shifted image patch, allowing the seamless generation of the entire image due
to the overlap in the patch feature space. Patch-DM produces high-quality image
synthesis results on our newly collected dataset of nature images
(1024$\times$512), as well as on standard benchmarks of smaller sizes
(256$\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ. We compare our
method with previous patch-based generation methods and achieve
state-of-the-art FID scores on all four datasets. Further, Patch-DM also
reduces memory complexity compared to the classic diffusion models.
</p>

## self-supervised
### Title: DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.01140](http://arxiv.org/abs/2308.01140)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01140] DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning](http://arxiv.org/abs/2308.01140) #self-supervised`
* Summary: <p>In contemporary self-supervised contrastive algorithms like SimCLR, MoCo,
etc., the task of balancing attraction between two semantically similar samples
and repulsion between two samples from different classes is primarily affected
by the presence of hard negative samples. While the InfoNCE loss has been shown
to impose penalties based on hardness, the temperature hyper-parameter is the
key to regulating the penalties and the trade-off between uniformity and
tolerance. In this work, we focus our attention to improve the performance of
InfoNCE loss in SSL by studying the effect of temperature hyper-parameter
values. We propose a cosine similarity-dependent temperature scaling function
to effectively optimize the distribution of the samples in the feature space.
We further analyze the uniformity and tolerance metrics to investigate the
optimal regions in the cosine similarity space for better optimization.
Additionally, we offer a comprehensive examination of the behavior of local and
global structures in the feature space throughout the pre-training phase, as
the temperature varies. Experimental evidence shows that the proposed framework
outperforms or is at par with the contrastive loss-based SSL algorithms. We
believe our work (DySTreSS) on temperature scaling in SSL provides a foundation
for future research in contrastive learning.
</p>

### Title: Revisiting DETR Pre-training for Object Detection. (arXiv:2308.01300v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.01300](http://arxiv.org/abs/2308.01300)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01300] Revisiting DETR Pre-training for Object Detection](http://arxiv.org/abs/2308.01300) #self-supervised`
* Summary: <p>Motivated by that DETR-based approaches have established new records on COCO
detection and segmentation benchmarks, many recent endeavors show increasing
interest in how to further improve DETR-based approaches by pre-training the
Transformer in a self-supervised manner while keeping the backbone frozen. Some
studies already claimed significant improvements in accuracy. In this paper, we
take a closer look at their experimental methodology and check if their
approaches are still effective on the very recent state-of-the-art such as
$\mathcal{H}$-Deformable-DETR. We conduct thorough experiments on COCO object
detection tasks to study the influence of the choice of pre-training datasets,
localization, and classification target generation schemes. Unfortunately, we
find the previous representative self-supervised approach such as DETReg, fails
to boost the performance of the strong DETR-based approaches on full data
regimes. We further analyze the reasons and find that simply combining a more
accurate box predictor and Objects$365$ benchmark can significantly improve the
results in follow-up experiments. We demonstrate the effectiveness of our
approach by achieving strong object detection results of AP=$59.3\%$ on COCO
val set, which surpasses $\mathcal{H}$-Deformable-DETR + Swin-L by +$1.4\%$.
Last, we generate a series of synthetic pre-training datasets by combining the
very recent image-to-text captioning models (LLaVA) and text-to-image
generative models (SDXL). Notably, pre-training on these synthetic datasets
leads to notable improvements in object detection performance. Looking ahead,
we anticipate substantial advantages through the future expansion of the
synthetic pre-training dataset.
</p>

### Title: SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis. (arXiv:2308.01018v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.01018](http://arxiv.org/abs/2308.01018)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01018] SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis](http://arxiv.org/abs/2308.01018) #self-supervised`
* Summary: <p>While FastSpeech2 aims to integrate aspects of speech such as pitch, energy,
and duration as conditional inputs, it still leaves scope for richer
representations. As a part of this work, we leverage representations from
various Self-Supervised Learning (SSL) models to enhance the quality of the
synthesized speech. In particular, we pass the FastSpeech2 encoder's
length-regulated outputs through a series of encoder layers with the objective
of reconstructing the SSL representations. In the SALTTS-parallel
implementation, the representations from this second encoder are used for an
auxiliary reconstruction loss with the SSL features. The SALTTS-cascade
implementation, however, passes these representations through the decoder in
addition to having the reconstruction loss. The richness of speech
characteristics from the SSL features reflects in the output speech quality,
with the objective and subjective evaluation measures of the proposed approach
outperforming the baseline FastSpeech2.
</p>

### Title: A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC. (arXiv:2308.01271v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.01271](http://arxiv.org/abs/2308.01271)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01271] A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC](http://arxiv.org/abs/2308.01271) #self-supervised`
* Summary: <p>In this paper we present a practical Bayesian self-supervised learning method
with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this
framework, we place a prior over the parameters of a self-supervised learning
model and use cSGHMC to approximate the high dimensional and multimodal
posterior distribution over the embeddings. By exploring an expressive
posterior over the embeddings, Bayesian self-supervised learning produces
interpretable and diverse representations. Marginalizing over these
representations yields a significant gain in performance, calibration and
out-of-distribution detection on a variety of downstream classification tasks.
We provide experimental results on multiple classification tasks on four
challenging datasets. Moreover, we demonstrate the effectiveness of the
proposed method in out-of-distribution detection using the SVHN and CIFAR-10
datasets.
</p>

## foundation model
## generative
### Title: ForensicsForest Family: A Series of Multi-scale Hierarchical Cascade Forests for Detecting GAN-generated Faces. (arXiv:2308.00964v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.00964](http://arxiv.org/abs/2308.00964)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.00964] ForensicsForest Family: A Series of Multi-scale Hierarchical Cascade Forests for Detecting GAN-generated Faces](http://arxiv.org/abs/2308.00964) #generative`
* Summary: <p>The prominent progress in generative models has significantly improved the
reality of generated faces, bringing serious concerns to society. Since recent
GAN-generated faces are in high realism, the forgery traces have become more
imperceptible, increasing the forensics challenge. To combat GAN-generated
faces, many countermeasures based on Convolutional Neural Networks (CNNs) have
been spawned due to their strong learning ability. In this paper, we rethink
this problem and explore a new approach based on forest models instead of CNNs.
Specifically, we describe a simple and effective forest-based method set called
{\em ForensicsForest Family} to detect GAN-generate faces. The proposed
ForensicsForest family is composed of three variants, which are {\em
ForensicsForest}, {\em Hybrid ForensicsForest} and {\em Divide-and-Conquer
ForensicsForest} respectively. ForenscisForest is a newly proposed Multi-scale
Hierarchical Cascade Forest, which takes semantic, frequency and biology
features as input, hierarchically cascades different levels of features for
authenticity prediction, and then employs a multi-scale ensemble scheme that
can comprehensively consider different levels of information to improve the
performance further. Based on ForensicsForest, we develop Hybrid
ForensicsForest, an extended version that integrates the CNN layers into
models, to further refine the effectiveness of augmented features. Moreover, to
reduce the memory cost in training, we propose Divide-and-Conquer
ForensicsForest, which can construct a forest model using only a portion of
training samplings. In the training stage, we train several candidate forest
models using the subsets of training samples. Then a ForensicsForest is
assembled by picking the suitable components from these candidate forest
models...
</p>

### Title: Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.01184](http://arxiv.org/abs/2308.01184)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01184] Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior](http://arxiv.org/abs/2308.01184) #generative`
* Summary: <p>The learning with noisy labels has been addressed with both discriminative
and generative models. Although discriminative models have dominated the field
due to their simpler modeling and more efficient computational training
processes, generative models offer a more effective means of disentangling
clean and noisy labels and improving the estimation of the label transition
matrix. However, generative approaches maximize the joint likelihood of noisy
labels and data using a complex formulation that only indirectly optimizes the
model of interest associating data and clean labels. Additionally, these
approaches rely on generative models that are challenging to train and tend to
use uninformative clean label priors. In this paper, we propose a new
generative noisy-label learning approach that addresses these three issues.
First, we propose a new model optimisation that directly associates data and
clean labels. Second, the generative model is implicitly estimated using a
discriminative model, eliminating the inefficient training of a generative
model. Third, we propose a new informative label prior inspired by partial
label learning as supervision signal for noisy label learning. Extensive
experiments on several noisy-label benchmarks demonstrate that our generative
model provides state-of-the-art results while maintaining a similar
computational complexity as discriminative models.
</p>

### Title: Feature-aware conditional GAN for category text generation. (arXiv:2308.00939v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.00939](http://arxiv.org/abs/2308.00939)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.00939] Feature-aware conditional GAN for category text generation](http://arxiv.org/abs/2308.00939) #generative`
* Summary: <p>Category text generation receives considerable attentions since it is
beneficial for various natural language processing tasks. Recently, the
generative adversarial network (GAN) has attained promising performance in text
generation, attributed to its adversarial training process. However, there are
several issues in text GANs, including discreteness, training instability, mode
collapse, lack of diversity and controllability etc. To address these issues,
this paper proposes a novel GAN framework, the feature-aware conditional GAN
(FA-GAN), for controllable category text generation. In FA-GAN, the generator
has a sequence-to-sequence structure for improving sentence diversity, which
consists of three encoders including a special feature-aware encoder and a
category-aware encoder, and one relational-memory-core-based decoder with the
Gumbel SoftMax activation function. The discriminator has an additional
category classification head. To generate sentences with specified categories,
the multi-class classification loss is supplemented in the adversarial
training. Comprehensive experiments have been conducted, and the results show
that FA-GAN consistently outperforms 10 state-of-the-art text generation
approaches on 6 text classification datasets. The case study demonstrates that
the synthetic sentences generated by FA-GAN can match the required categories
and are aware of the features of conditioned sentences, with good readability,
fluency, and text authenticity.
</p>

### Title: Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.01138](http://arxiv.org/abs/2308.01138)
* Code URL: [https://github.com/magnomic/cnst](https://github.com/magnomic/cnst)
* Copy Paste: `<input type="checkbox">[[2308.01138] Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases](http://arxiv.org/abs/2308.01138) #generative`
* Summary: <p>Spectrum analysis systems in online water quality testing are designed to
detect types and concentrations of pollutants and enable regulatory agencies to
respond promptly to pollution incidents. However, spectral data-based testing
devices suffer from complex noise patterns when deployed in non-laboratory
environments. To make the analysis model applicable to more environments, we
propose a noise patterns transferring model, which takes the spectrum of
standard water samples in different environments as cases and learns the
differences in their noise patterns, thus enabling noise patterns to transfer
to unknown samples. Unfortunately, the inevitable sample-level baseline noise
makes the model unable to obtain the paired data that only differ in
dataset-level environmental noise. To address the problem, we generate a
sample-to-sample case-base to exclude the interference of sample-level noise on
dataset-level noise learning, enhancing the system's learning performance.
Experiments on spectral data with different background noises demonstrate the
good noise-transferring ability of the proposed method against baseline systems
ranging from wavelet denoising, deep neural networks, and generative models.
From this research, we posit that our method can enhance the performance of DL
models by generating high-quality cases. The source code is made publicly
available online at https://github.com/Magnomic/CNST.
</p>

## anomaly
### Title: IIDS: Design of Intelligent Intrusion Detection System for Internet-of-Things Applications. (arXiv:2308.00943v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2308.00943](http://arxiv.org/abs/2308.00943)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.00943] IIDS: Design of Intelligent Intrusion Detection System for Internet-of-Things Applications](http://arxiv.org/abs/2308.00943) #anomaly`
* Summary: <p>With rapid technological growth, security attacks are drastically increasing.
In many crucial Internet-of-Things (IoT) applications such as healthcare and
defense, the early detection of security attacks plays a significant role in
protecting huge resources. An intrusion detection system is used to address
this problem. The signature-based approaches fail to detect zero-day attacks.
So anomaly-based detection particularly AI tools, are becoming popular. In
addition, the imbalanced dataset leads to biased results. In Machine Learning
(ML) models, F1 score is an important metric to measure the accuracy of
class-level correct predictions. The model may fail to detect the target
samples if the F1 is considerably low. It will lead to unrecoverable
consequences in sensitive applications such as healthcare and defense. So, any
improvement in the F1 score has significant impact on the resource protection.
In this paper, we present a framework for ML-based intrusion detection system
for an imbalanced dataset. In this study, the most recent dataset, namely
CICIoT2023 is considered. The random forest (RF) algorithm is used in the
proposed framework. The proposed approach improves 3.72%, 3.75% and 4.69% in
precision, recall and F1 score, respectively, with the existing method.
Additionally, for unsaturated classes (i.e., classes with F1 score &lt; 0.99), F1
score improved significantly by 7.9%. As a result, the proposed approach is
more suitable for IoT security applications for efficient detection of
intrusion and is useful in further studies.
</p>

### Title: Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.01011](http://arxiv.org/abs/2308.01011)
* Code URL: [https://github.com/agustdd/floss](https://github.com/agustdd/floss)
* Copy Paste: `<input type="checkbox">[[2308.01011] Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach](http://arxiv.org/abs/2308.01011) #anomaly`
* Summary: <p>Time series analysis is a fundamental task in various application domains,
and deep learning approaches have demonstrated remarkable performance in this
area. However, many real-world time series data exhibit significant periodic or
quasi-periodic dynamics that are often not adequately captured by existing deep
learning-based solutions. This results in an incomplete representation of the
underlying dynamic behaviors of interest. To address this gap, we propose an
unsupervised method called Floss that automatically regularizes learned
representations in the frequency domain. The Floss method first automatically
detects major periodicities from the time series. It then employs periodic
shift and spectral density similarity measures to learn meaningful
representations with periodic consistency. In addition, Floss can be easily
incorporated into both supervised, semi-supervised, and unsupervised learning
frameworks. We conduct extensive experiments on common time series
classification, forecasting, and anomaly detection tasks to demonstrate the
effectiveness of Floss. We incorporate Floss into several representative deep
learning solutions to justify our design choices and demonstrate that it is
capable of automatically discovering periodic dynamics and improving
state-of-the-art deep learning models.
</p>

### Title: Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach. (arXiv:2308.01063v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.01063](http://arxiv.org/abs/2308.01063)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.01063] Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach](http://arxiv.org/abs/2308.01063) #anomaly`
* Summary: <p>Graph anomaly detection (GAD) has achieved success and has been widely
applied in various domains, such as fraud detection, cybersecurity, finance
security, and biochemistry. However, existing graph anomaly detection
algorithms focus on distinguishing individual entities (nodes or graphs) and
overlook the possibility of anomalous groups within the graph. To address this
limitation, this paper introduces a novel unsupervised framework for a new task
called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework
first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that
belong to potential anomaly groups by capturing long-range inconsistencies.
Subsequently, group sampling is employed to sample candidate groups, which are
then fed into the proposed Topology Pattern-based Graph Contrastive Learning
(TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to
generate embeddings for each candidate group and thus distinct anomaly groups.
The experimental results on both real-world and synthetic datasets demonstrate
that the proposed framework shows superior performance in identifying and
localizing anomaly groups, highlighting it as a promising solution for Gr-GAD.
Datasets and codes of the proposed framework are at the github repository
https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection.
</p>

## in-context
### Title: ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation. (arXiv:2308.00906v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.00906](http://arxiv.org/abs/2308.00906)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.00906] ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation](http://arxiv.org/abs/2308.00906) #in-context`
* Summary: <p>While language-guided image manipulation has made remarkable progress, the
challenge of how to instruct the manipulation process faithfully reflecting
human intentions persists. An accurate and comprehensive description of a
manipulation task using natural language is laborious and sometimes even
impossible, primarily due to the inherent uncertainty and ambiguity present in
linguistic expressions. Is it feasible to accomplish image manipulation without
resorting to external cross-modal language information? If this possibility
exists, the inherent modality gap would be effortlessly eliminated. In this
paper, we propose a novel manipulation methodology, dubbed ImageBrush, that
learns visual instructions for more accurate image editing. Our key idea is to
employ a pair of transformation images as visual instructions, which not only
precisely captures human intention but also facilitates accessibility in
real-world scenarios. Capturing visual instructions is particularly challenging
because it involves extracting the underlying intentions solely from visual
demonstrations and then applying this operation to a new image. To address this
challenge, we formulate visual instruction learning as a diffusion-based
inpainting problem, where the contextual information is fully exploited through
an iterative process of generation. A visual prompting encoder is carefully
devised to enhance the model's capacity in uncovering human intent behind the
visual instructions. Extensive experiments show that our method generates
engaging manipulation results conforming to the transformations entailed in
demonstrations. Moreover, our model exhibits robust generalization capabilities
on various downstream tasks such as pose transfer, image translation and video
inpainting.
</p>

