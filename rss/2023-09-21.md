## diffusion
### Title: PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement. (arXiv:2309.11125v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11125](http://arxiv.org/abs/2309.11125)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11125] PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement](http://arxiv.org/abs/2309.11125) #diffusion`
* Summary: <p>Dominant Person Search methods aim to localize and recognize query persons in
a unified network, which jointly optimizes two sub-tasks, \ie, detection and
Re-IDentification (ReID). Despite significant progress, two major challenges
remain: 1) Detection-prior modules in previous methods are suboptimal for the
ReID task. 2) The collaboration between two sub-tasks is ignored. To alleviate
these issues, we present a novel Person Search framework based on the Diffusion
model, PSDiff. PSDiff formulates the person search as a dual denoising process
from noisy boxes and ReID embeddings to ground truths. Unlike existing methods
that follow the Detection-to-ReID paradigm, our denoising paradigm eliminates
detection-prior modules to avoid the local-optimum of the ReID task. Following
the new paradigm, we further design a new Collaborative Denoising Layer (CDL)
to optimize detection and ReID sub-tasks in an iterative and collaborative way,
which makes two sub-tasks mutually beneficial. Extensive experiments on the
standard benchmarks show that PSDiff achieves state-of-the-art performance with
fewer parameters and elastic computing overhead.
</p>

### Title: Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates. (arXiv:2309.11281v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11281](http://arxiv.org/abs/2309.11281)
* Code URL: [https://github.com/kcshum/pose-conditioned-NeRF-object-fusion](https://github.com/kcshum/pose-conditioned-NeRF-object-fusion)
* Copy Paste: `<input type="checkbox">[[2309.11281] Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates](http://arxiv.org/abs/2309.11281) #diffusion`
* Summary: <p>Neural radiance field is an emerging rendering method that generates
high-quality multi-view consistent images from a neural scene representation
and volume rendering. Although neural radiance field-based techniques are
robust for scene reconstruction, their ability to add or remove objects remains
limited. This paper proposes a new language-driven approach for object
manipulation with neural radiance fields through dataset updates. Specifically,
to insert a new foreground object represented by a set of multi-view images
into a background radiance field, we use a text-to-image diffusion model to
learn and generate combined images that fuse the object of interest into the
given background across views. These combined images are then used for refining
the background radiance field so that we can render view-consistent images
containing both the object and the background. To ensure view consistency, we
propose a dataset updates strategy that prioritizes radiance field training
with camera views close to the already-trained views prior to propagating the
training to remaining views. We show that under the same dataset updates
strategy, we can easily adapt our method for object insertion using data from
text-to-3D models as well as object removal. Experimental results show that our
method generates photorealistic images of the edited scenes, and outperforms
state-of-the-art methods in 3D reconstruction and neural radiance field
blending.
</p>

### Title: FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11306](http://arxiv.org/abs/2309.11306)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11306] FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion](http://arxiv.org/abs/2309.11306) #diffusion`
* Summary: <p>Speech-driven 3D facial animation synthesis has been a challenging task both
in industry and research. Recent methods mostly focus on deterministic deep
learning methods meaning that given a speech input, the output is always the
same. However, in reality, the non-verbal facial cues that reside throughout
the face are non-deterministic in nature. In addition, majority of the
approaches focus on 3D vertex based datasets and methods that are compatible
with existing facial animation pipelines with rigged characters is scarce. To
eliminate these issues, we present FaceDiffuser, a non-deterministic deep
learning model to generate speech-driven facial animations that is trained with
both 3D vertex and blendshape based datasets. Our method is based on the
diffusion technique and uses the pre-trained large speech representation model
HuBERT to encode the audio input. To the best of our knowledge, we are the
first to employ the diffusion method for the task of speech-driven 3D facial
animation synthesis. We have run extensive objective and subjective analyses
and show that our approach achieves better or comparable results in comparison
to the state-of-the-art methods. We also introduce a new in-house dataset that
is based on a blendshape based rigged character. We recommend watching the
accompanying supplementary video. The code and the dataset will be publicly
available.
</p>

### Title: Face Aging via Diffusion-based Editing. (arXiv:2309.11321v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11321](http://arxiv.org/abs/2309.11321)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11321] Face Aging via Diffusion-based Editing](http://arxiv.org/abs/2309.11321) #diffusion`
* Summary: <p>In this paper, we address the problem of face aging: generating past or
future facial images by incorporating age-related changes to the given face.
Previous aging methods rely solely on human facial image datasets and are thus
constrained by their inherent scale and bias. This restricts their application
to a limited generatable age range and the inability to handle large age gaps.
We propose FADING, a novel approach to address Face Aging via DIffusion-based
editiNG. We go beyond existing methods by leveraging the rich prior of
large-scale language-image diffusion models. First, we specialize a pre-trained
diffusion model for the task of face age editing by using an age-aware
fine-tuning scheme. Next, we invert the input image to latent noise and obtain
optimized null text embeddings. Finally, we perform text-guided local age
editing via attention control. The quantitative and qualitative analyses
demonstrate that our method outperforms existing approaches with respect to
aging accuracy, attribute preservation, and aging quality.
</p>

### Title: FreeU: Free Lunch in Diffusion U-Net. (arXiv:2309.11497v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11497](http://arxiv.org/abs/2309.11497)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11497] FreeU: Free Lunch in Diffusion U-Net](http://arxiv.org/abs/2309.11497) #diffusion`
* Summary: <p>In this paper, we uncover the untapped potential of diffusion U-Net, which
serves as a "free lunch" that substantially improves the generation quality on
the fly. We initially investigate the key contributions of the U-Net
architecture to the denoising process and identify that its main backbone
primarily contributes to denoising, whereas its skip connections mainly
introduce high-frequency features into the decoder module, causing the network
to overlook the backbone semantics. Capitalizing on this discovery, we propose
a simple yet effective method-termed "FreeU" - that enhances generation quality
without additional training or finetuning. Our key insight is to strategically
re-weight the contributions sourced from the U-Net's skip connections and
backbone feature maps, to leverage the strengths of both components of the
U-Net architecture. Promising results on image and video generation tasks
demonstrate that our FreeU can be readily integrated to existing diffusion
models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion,
to improve the generation quality with only a few lines of code. All you need
is to adjust two scaling factors during inference. Project page:
https://chenyangsi.top/FreeU/.
</p>

### Title: Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models. (arXiv:2309.11420v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.11420](http://arxiv.org/abs/2309.11420)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11420] Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models](http://arxiv.org/abs/2309.11420) #diffusion`
* Summary: <p>We investigate the approximation efficiency of score functions by deep neural
networks in diffusion-based generative modeling. While existing approximation
theories utilize the smoothness of score functions, they suffer from the curse
of dimensionality for intrinsically high-dimensional data. This limitation is
pronounced in graphical models such as Markov random fields, common for image
distributions, where the approximation efficiency of score functions remains
unestablished.
</p>
<p>To address this, we observe score functions can often be well-approximated in
graphical models through variational inference denoising algorithms.
Furthermore, these algorithms are amenable to efficient neural network
representation. We demonstrate this in examples of graphical models, including
Ising models, conditional Ising models, restricted Boltzmann machines, and
sparse encoding models. Combined with off-the-shelf discretization error bounds
for diffusion-based sampling, we provide an efficient sample complexity bound
for diffusion-based generative modeling when the score function is learned by
deep neural networks.
</p>

## self-supervised
### Title: SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics. (arXiv:2309.10972v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.10972](http://arxiv.org/abs/2309.10972)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10972] SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics](http://arxiv.org/abs/2309.10972) #self-supervised`
* Summary: <p>Accurately determining salient regions of an image is challenging when
labeled data is scarce. DINO-based self-supervised approaches have recently
leveraged meaningful image semantics captured by patch-wise features for
locating foreground objects. Recent methods have also incorporated intuitive
priors and demonstrated value in unsupervised methods for object partitioning.
In this paper, we propose SEMPART, which jointly infers coarse and fine
bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART
preserves fine boundary details using graph-driven regularization and
successfully distills the coarse mask semantics into the fine mask. Our salient
object detection and single object localization findings suggest that SEMPART
produces high-quality masks rapidly without additional post-processing and
benefits from co-optimizing the coarse and fine branches.
</p>

### Title: Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11077](http://arxiv.org/abs/2309.11077)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11077] Weak Supervision for Label Efficient Visual Bug Detection](http://arxiv.org/abs/2309.11077) #self-supervised`
* Summary: <p>As video games evolve into expansive, detailed worlds, visual quality becomes
essential, yet increasingly challenging. Traditional testing methods, limited
by resources, face difficulties in addressing the plethora of potential bugs.
Machine learning offers scalable solutions; however, heavy reliance on large
labeled datasets remains a constraint. Addressing this challenge, we propose a
novel method, utilizing unlabeled gameplay and domain-specific augmentations to
generate datasets &amp; self-supervised objectives used during pre-training or
multi-task settings for downstream visual bug detection. Our methodology uses
weak-supervision to scale datasets for the crafted objectives and facilitates
both autonomous and interactive weak-supervision, incorporating unsupervised
clustering and/or an interactive approach based on text and geometric prompts.
We demonstrate on first-person player clipping/collision bugs (FPPC) within the
expansive Giantmap game world, that our approach is very effective, improving
over a strong supervised baseline in a practical, very low-prevalence, low data
regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled "good"
exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough
signal to outperform the low-labeled supervised settings. Building on
large-pretrained vision models, our approach is adaptable across various visual
bugs. Our results suggest applicability in curating datasets for broader image
and video tasks within video games beyond visual bugs.
</p>

### Title: Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval. (arXiv:2309.11091v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11091](http://arxiv.org/abs/2309.11091)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11091] Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval](http://arxiv.org/abs/2309.11091) #self-supervised`
* Summary: <p>With the explosive growth of web videos in recent years, large-scale
Content-Based Video Retrieval (CBVR) becomes increasingly essential in video
filtering, recommendation, and copyright protection. Segment-level CBVR
(S-CBVR) locates the start and end time of similar segments in finer
granularity, which is beneficial for user browsing efficiency and infringement
detection especially in long video scenarios. The challenge of S-CBVR task is
how to achieve high temporal alignment accuracy with efficient computation and
low storage consumption. In this paper, we propose a Segment Similarity and
Alignment Network (SSAN) in dealing with the challenge which is firstly trained
end-to-end in S-CBVR. SSAN is based on two newly proposed modules in video
retrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to
reduce redundant frame features, (2) A robust Similarity Pattern Detection
(SPD) module for temporal alignment. In comparison with uniform frame
extraction, SKE not only saves feature storage and search time, but also
introduces comparable accuracy and limited extra computation time. In terms of
temporal alignment, SPD localizes similar segments with higher accuracy and
efficiency than existing deep learning methods. Furthermore, we jointly train
SSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two
key modules SKE and SPD can also be effectively inserted into other video
retrieval pipelines and gain considerable performance improvements.
Experimental results on public datasets show that SSAN can obtain higher
alignment accuracy while saving storage and online query computational cost
compared to existing methods.
</p>

### Title: Self-supervised Domain-agnostic Domain Adaptation for Satellite Images. (arXiv:2309.11109v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11109](http://arxiv.org/abs/2309.11109)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11109] Self-supervised Domain-agnostic Domain Adaptation for Satellite Images](http://arxiv.org/abs/2309.11109) #self-supervised`
* Summary: <p>Domain shift caused by, e.g., different geographical regions or acquisition
conditions is a common issue in machine learning for global scale satellite
image processing. A promising method to address this problem is domain
adaptation, where the training and the testing datasets are split into two or
multiple domains according to their distributions, and an adaptation method is
applied to improve the generalizability of the model on the testing dataset.
However, defining the domain to which each satellite image belongs is not
trivial, especially under large-scale multi-temporal and multi-sensory
scenarios, where a single image mosaic could be generated from multiple data
sources. In this paper, we propose an self-supervised domain-agnostic domain
adaptation (SS(DA)2) method to perform domain adaptation without such a domain
definition. To achieve this, we first design a contrastive generative
adversarial loss to train a generative network to perform image-to-image
translation between any two satellite image patches. Then, we improve the
generalizability of the downstream models by augmenting the training data with
different testing spectral characteristics. The experimental results on public
benchmarks verify the effectiveness of SS(DA)2.
</p>

### Title: Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11354](http://arxiv.org/abs/2309.11354)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11354] Self-supervised learning unveils change in urban housing from street-level images](http://arxiv.org/abs/2309.11354) #self-supervised`
* Summary: <p>Cities around the world face a critical shortage of affordable and decent
housing. Despite its critical importance for policy, our ability to effectively
monitor and track progress in urban housing is limited. Deep learning-based
computer vision methods applied to street-level images have been successful in
the measurement of socioeconomic and environmental inequalities but did not
fully utilize temporal images to track urban change as time-varying labels are
often unavailable. We used self-supervised methods to measure change in London
using 15 million street images taken between 2008 and 2021. Our novel
adaptation of Barlow Twins, Street2Vec, embeds urban structure while being
invariant to seasonal and daily changes without manual annotations. It
outperformed generic embeddings, successfully identified point-level change in
London's housing supply from street-level images, and distinguished between
major and minor change. This capability can provide timely information for
urban planning and policy decisions toward more liveable, equitable, and
sustainable cities.
</p>

## foundation model
### Title: UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11065](http://arxiv.org/abs/2309.11065)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11065] UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt](http://arxiv.org/abs/2309.11065) #foundation model`
* Summary: <p>Recent research has shown that multi-task pre-training greatly improves the
model's robustness and transfer ability, which is crucial for building a
high-quality dialog system. However, most previous works on multi-task
pre-training rely heavily on human-defined input format or prompt, which is not
optimal in quality and quantity. In this work, we propose to use Task-based
Automatic Prompt generation (TAP) to automatically generate high-quality
prompts. Using the high-quality prompts generated, we scale the corpus of the
pre-trained conversation model to 122 datasets from 15 dialog-related tasks,
resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful
foundation model for various conversational tasks and different dialog systems.
Extensive experiments have shown that UniPCM is robust to input prompts and
capable of various dialog-related tasks. Moreover, UniPCM has strong transfer
ability and excels at low resource scenarios, achieving SOTA results on 9
different datasets ranging from task-oriented dialog to open-domain
conversation. Furthermore, we are amazed to find that TAP can generate prompts
on par with those collected with crowdsourcing. The code is released with the
paper.
</p>

## generative
### Title: Score Mismatching for Generative Modeling. (arXiv:2309.11043v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11043](http://arxiv.org/abs/2309.11043)
* Code URL: [https://github.com/senmaoy/Score-Mismatching](https://github.com/senmaoy/Score-Mismatching)
* Copy Paste: `<input type="checkbox">[[2309.11043] Score Mismatching for Generative Modeling](http://arxiv.org/abs/2309.11043) #generative`
* Summary: <p>We propose a new score-based model with one-step sampling. Previously,
score-based models were burdened with heavy computations due to iterative
sampling. For substituting the iterative process, we train a standalone
generator to compress all the time steps with the gradient backpropagated from
the score network. In order to produce meaningful gradients for the generator,
the score network is trained to simultaneously match the real data distribution
and mismatch the fake data distribution. This model has the following
advantages: 1) For sampling, it generates a fake image with only one step
forward. 2) For training, it only needs 10 diffusion steps.3) Compared with
consistency model, it is free of the ill-posed problem caused by consistency
loss. On the popular CIFAR-10 dataset, our model outperforms Consistency Model
and Denoising Score Matching, which demonstrates the potential of the
framework. We further provide more examples on the MINIST and LSUN datasets.
The code is available on GitHub.
</p>

### Title: Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11132](http://arxiv.org/abs/2309.11132)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11132] Contrastive Pseudo Learning for Open-World DeepFake Attribution](http://arxiv.org/abs/2309.11132) #generative`
* Summary: <p>The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.
</p>

### Title: DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11499](http://arxiv.org/abs/2309.11499)
* Code URL: [https://github.com/RunpeiDong/DreamLLM](https://github.com/RunpeiDong/DreamLLM)
* Copy Paste: `<input type="checkbox">[[2309.11499] DreamLLM: Synergistic Multimodal Comprehension and Creation](http://arxiv.org/abs/2309.11499) #generative`
* Summary: <p>This paper presents DreamLLM, a learning framework that first achieves
versatile Multimodal Large Language Models (MLLMs) empowered with frequently
overlooked synergy between multimodal comprehension and creation. DreamLLM
operates on two fundamental principles. The first focuses on the generative
modeling of both language and image posteriors by direct sampling in the raw
multimodal space. This approach circumvents the limitations and information
loss inherent to external feature extractors like CLIP, and a more thorough
multimodal understanding is obtained. Second, DreamLLM fosters the generation
of raw, interleaved documents, modeling both text and image contents, along
with unstructured layouts. This allows DreamLLM to learn all conditional,
marginal, and joint multimodal distributions effectively. As a result, DreamLLM
is the first MLLM capable of generating free-form interleaved content.
Comprehensive experiments highlight DreamLLM's superior performance as a
zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</p>

### Title: Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.10945](http://arxiv.org/abs/2309.10945)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10945] Benchmarks for Pir\'a 2](http://arxiv.org/abs/2309.10945) #generative`
* Summary: <p>Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian
coast, and climate change, built from a collection of scientific abstracts and
reports on these topics. This dataset represents a versatile language resource,
particularly useful for testing the ability of current machine learning models
to acquire expert scientific knowledge. Despite its potential, a detailed set
of baselines has not yet been developed for Pir\'a. By creating these
baselines, researchers can more easily utilize Pir\'a as a resource for testing
machine learning models across a wide range of question answering tasks. In
this paper, we define six benchmarks over the Pir\'a dataset, covering closed
generative question answering, machine reading comprehension, information
retrieval, open question answering, answer triggering, and multiple choice
question answering. As part of this effort, we have also produced a curated
version of the original dataset, where we fixed a number of grammar issues,
repetitions, and other shortcomings. Furthermore, the dataset has been extended
in several new directions, so as to face the aforementioned benchmarks:
translation of supporting texts from English into Portuguese, classification
labels for answerability, automatic paraphrases of questions and answers, and
multiple choice candidates. The results described in this paper provide several
points of reference for researchers interested in exploring the challenges
provided by the Pir\'a dataset.
</p>

### Title: Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11042](http://arxiv.org/abs/2309.11042)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11042] Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters](http://arxiv.org/abs/2309.11042) #generative`
* Summary: <p>Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with &lt;1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.
</p>

### Title: Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11049](http://arxiv.org/abs/2309.11049)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11049] Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables](http://arxiv.org/abs/2309.11049) #generative`
* Summary: <p>Question answering on tabular data (TableQA), which aims at generating
answers to questions grounded on a given table, has attracted increasing
attention in recent years. Existing work tends to generate factual short-form
answers by extracting information from one or a few table cells without
reasoning over selected table cells. However, the free-form TableQA, requiring
a more complex relevant table cell selection strategy and the complex
integration and inference of separate pieces of information, has been
under-explored. To this end, this paper proposes a generalized three-stage
approach: Table-to-Graph conversion and cell localizing, external knowledge
retrieval and table-text fusion (called TAG-QA), addressing the challenge of
inferring long free-form answer for generative TableQA. In particular, TAG-QA
(1) locates relevant table cells using a graph neural network to gather
intersecting cells between relevant rows and columns; (2) leverages external
knowledge from Wikipedia and (3) generates answers by integrating both tabular
data and natural linguistic information. Experiments with a human evaluation
demonstrate that TAG-QA is capable of generating more faithful and coherent
sentence when compared with several state-of-the-art baselines. Especially,
TAG-QA outperforms the strong pipeline-based baseline TAPAS by 17% and 14%, in
terms of BLEU-4 and PARENT F-score, respectively. Moreover, TAG-QA outperforms
end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score.
</p>

### Title: Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11259](http://arxiv.org/abs/2309.11259)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11259] Sequence-to-Sequence Spanish Pre-trained Language Models](http://arxiv.org/abs/2309.11259) #generative`
* Summary: <p>In recent years, substantial advancements in pre-trained language models have
paved the way for the development of numerous non-English language versions,
with a particular focus on encoder-only and decoder-only architectures. While
Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited
prowess in natural language understanding and generation, there remains a
scarcity of encoder-decoder models designed for sequence-to-sequence tasks
involving input-output pairs. This paper breaks new ground by introducing the
implementation and evaluation of renowned encoder-decoder architectures,
exclusively pre-trained on Spanish corpora. Specifically, we present Spanish
versions of BART, T5, and BERT2BERT-style models and subject them to a
comprehensive assessment across a diverse range of sequence-to-sequence tasks,
spanning summarization, rephrasing, and generative question answering. Our
findings underscore the competitive performance of all models, with BART and T5
emerging as top performers across all evaluated tasks. As an additional
contribution, we have made all models publicly available to the research
community, fostering future exploration and development in Spanish language
processing.
</p>

### Title: Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.11044](http://arxiv.org/abs/2309.11044)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11044] Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion](http://arxiv.org/abs/2309.11044) #generative`
* Summary: <p>Federated Learning (FL) is currently one of the most popular technologies in
the field of Artificial Intelligence (AI) due to its collaborative learning and
ability to preserve client privacy. However, it faces challenges such as
non-identically and non-independently distributed (non-IID) and data with
imbalanced labels among local clients. To address these limitations, the
research community has explored various approaches such as using local model
parameters, federated generative adversarial learning, and federated
representation learning. In our study, we propose a novel Clustered FedStack
framework based on the previously published Stacked Federated Learning
(FedStack) framework. The local clients send their model predictions and output
layer weights to a server, which then builds a robust global model. This global
model clusters the local clients based on their output layer weights using a
clustering mechanism. We adopt three clustering mechanisms, namely K-Means,
Agglomerative, and Gaussian Mixture Models, into the framework and evaluate
their performance. We use Bayesian Information Criterion (BIC) with the maximum
likelihood function to determine the number of clusters. The Clustered FedStack
models outperform baseline models with clustering mechanisms. To estimate the
convergence of our proposed framework, we use Cyclical learning rates.
</p>

### Title: Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.11427](http://arxiv.org/abs/2309.11427)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11427] Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing](http://arxiv.org/abs/2309.11427) #generative`
* Summary: <p>This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</p>

## anomaly
### Title: Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.10923](http://arxiv.org/abs/2309.10923)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10923] Semi-automatic staging area for high-quality structured data extraction from scientific literature](http://arxiv.org/abs/2309.10923) #anomaly`
* Summary: <p>In this study, we propose a staging area for ingesting new superconductors'
experimental data in SuperCon that is machine-collected from scientific
articles. Our objective is to enhance the efficiency of updating SuperCon while
maintaining or enhancing the data quality. We present a semi-automatic staging
area driven by a workflow combining automatic and manual processes on the
extracted database. An anomaly detection automatic process aims to pre-screen
the collected data. Users can then manually correct any errors through a user
interface tailored to simplify the data verification on the original PDF
documents. Additionally, when a record is corrected, its raw data is collected
and utilised to improve machine learning models as training data. Evaluation
experiments demonstrate that our staging area significantly improves curation
quality. We compare the interface with the traditional manual approach of
reading PDF documents and recording information in an Excel document. Using the
interface boosts the precision and recall by 6% and 50%, respectively to an
average increase of 40% in F1-score.
</p>

## in-context
### Title: In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.10954](http://arxiv.org/abs/2309.10954)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10954] In-Context Learning for Text Classification with Many Labels](http://arxiv.org/abs/2309.10954) #in-context`
* Summary: <p>In-context learning (ICL) using large language models for tasks with many
labels is challenging due to the limited context window, which makes it
difficult to fit a sufficient number of examples in the prompt. In this paper,
we use a pre-trained dense retrieval model to bypass this limitation, giving
the model only a partial view of the full label space for each inference call.
Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art
performance in few-shot settings for three common intent classification
datasets, with no finetuning. We also surpass fine-tuned performance on
fine-grained sentiment classification in certain cases. We analyze the
performance across number of in-context examples and different model scales,
showing that larger models are necessary to effectively and consistently make
use of larger context lengths for ICL. By running several ablations, we analyze
the model's use of: a) the similarity of the in-context examples to the current
input, b) the semantic content of the class names, and c) the correct
correspondence between examples and labels. We demonstrate that all three are
needed to varying degrees depending on the domain, contrary to certain recent
works.
</p>

## memory
### Title: Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.10834](http://arxiv.org/abs/2309.10834)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10834] Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization](http://arxiv.org/abs/2309.10834) #memory`
* Summary: <p>This work presents a new method for enhancing communication efficiency in
stochastic Federated Learning that trains over-parameterized random networks.
In this setting, a binary mask is optimized instead of the model weights, which
are kept fixed. The mask characterizes a sparse sub-network that is able to
generalize as good as a smaller target network. Importantly, sparse binary
masks are exchanged rather than the floating point weights in traditional
federated learning, reducing communication cost to at most 1 bit per parameter.
We show that previous state of the art stochastic methods fail to find the
sparse networks that can reduce the communication and storage overhead using
consistent loss objectives. To address this, we propose adding a regularization
term to local objectives that encourages sparser solutions by eliminating
redundant features across sub-networks. Extensive experiments demonstrate
significant improvements in communication and memory efficiency of up to five
magnitudes compared to the literature, with minimal performance degradation in
validation accuracy in some instances.
</p>

### Title: DeepliteRT: Computer Vision at the Edge. (arXiv:2309.10878v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.10878](http://arxiv.org/abs/2309.10878)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10878] DeepliteRT: Computer Vision at the Edge](http://arxiv.org/abs/2309.10878) #memory`
* Summary: <p>The proliferation of edge devices has unlocked unprecedented opportunities
for deep learning model deployment in computer vision applications. However,
these complex models require considerable power, memory and compute resources
that are typically not available on edge platforms. Ultra low-bit quantization
presents an attractive solution to this problem by scaling down the model
weights and activations from 32-bit to less than 8-bit. We implement highly
optimized ultra low-bit convolution operators for ARM-based targets that
outperform existing methods by up to 4.34x. Our operator is implemented within
Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation,
tuning, and inference of ultra low-bit models on ARM devices. Compiler passes
in DeepliteRT automatically convert a fake-quantized model in full precision to
a compact ultra low-bit representation, easing the process of quantized model
deployment on commodity hardware. We analyze the performance of DeepliteRT on
classification and detection models against optimized 32-bit floating-point,
8-bit integer, and 2-bit baselines, achieving significant speedups of up to
2.20x, 2.33x and 2.17x, respectively.
</p>

### Title: Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text. (arXiv:2309.11248v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11248](http://arxiv.org/abs/2309.11248)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11248] Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text](http://arxiv.org/abs/2309.11248) #memory`
* Summary: <p>Recently, Transformer-based text detection techniques have sought to predict
polygons by encoding the coordinates of individual boundary vertices using
distinct query features. However, this approach incurs a significant memory
overhead and struggles to effectively capture the intricate relationships
between vertices belonging to the same instance. Consequently, irregular text
layouts often lead to the prediction of outlined vertices, diminishing the
quality of results. To address these challenges, we present an innovative
approach rooted in Sparse R-CNN: a cascade decoding pipeline for polygon
prediction. Our method ensures precision by iteratively refining polygon
predictions, considering both the scale and location of preceding results.
Leveraging this stabilized regression pipeline, even employing just a single
feature vector to guide polygon instance regression yields promising detection
results. Simultaneously, the leverage of instance-level feature proposal
substantially enhances memory efficiency (&gt;50% less vs. the state-of-the-art
method DPText-DETR) and reduces inference speed (&gt;40% less vs. DPText-DETR)
with minor performance drop on benchmarks.
</p>

### Title: CNNs for JPEGs: A Study in Computational Cost. (arXiv:2309.11417v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11417](http://arxiv.org/abs/2309.11417)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11417] CNNs for JPEGs: A Study in Computational Cost](http://arxiv.org/abs/2309.11417) #memory`
* Summary: <p>Convolutional neural networks (CNNs) have achieved astonishing advances over
the past decade, defining state-of-the-art in several computer vision tasks.
CNNs are capable of learning robust representations of the data directly from
the RGB pixels. However, most image data are usually available in compressed
format, from which the JPEG is the most widely used due to transmission and
storage purposes demanding a preliminary decoding process that have a high
computational load and memory usage. For this reason, deep learning methods
capable of learning directly from the compressed domain have been gaining
attention in recent years. Those methods usually extract a frequency domain
representation of the image, like DCT, by a partial decoding, and then make
adaptation to typical CNNs architectures to work with them. One limitation of
these current works is that, in order to accommodate the frequency domain data,
the modifications made to the original model increase significantly their
amount of parameters and computational complexity. On one hand, the methods
have faster preprocessing, since the cost of fully decoding the images is
avoided, but on the other hand, the cost of passing the images though the model
is increased, mitigating the possible upside of accelerating the method. In
this paper, we propose a further study of the computational cost of deep models
designed for the frequency domain, evaluating the cost of decoding and passing
the images through the network. We also propose handcrafted and data-driven
techniques for reducing the computational complexity and the number of
parameters for these models in order to keep them similar to their RGB
baselines, leading to efficient models with a better trade off between
computational cost and accuracy.
</p>

### Title: Prototype of a robotic system to assist the learning process of English language with text-generation through DNN. (arXiv:2309.11142v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11142](http://arxiv.org/abs/2309.11142)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11142] Prototype of a robotic system to assist the learning process of English language with text-generation through DNN](http://arxiv.org/abs/2309.11142) #memory`
* Summary: <p>In the last ongoing years, there has been a significant ascending on the
field of Natural Language Processing (NLP) for performing multiple tasks
including English Language Teaching (ELT). An effective strategy to favor the
learning process uses interactive devices to engage learners in their
self-learning process. In this work, we present a working prototype of a
humanoid robotic system to assist English language self-learners through text
generation using Long Short Term Memory (LSTM) Neural Networks. The learners
interact with the system using a Graphic User Interface that generates text
according to the English level of the user. The experimentation was conducted
using English learners and the results were measured accordingly to
International English Language Testing System (IELTS) rubric. Preliminary
results show an increment in the Grammatical Range of learners who interacted
with the system.
</p>

### Title: Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.11271](http://arxiv.org/abs/2309.11271)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11271] Grounded Complex Task Segmentation for Conversational Assistants](http://arxiv.org/abs/2309.11271) #memory`
* Summary: <p>Following complex instructions in conversational assistants can be quite
daunting due to the shorter attention and memory spans when compared to reading
the same instructions. Hence, when conversational assistants walk users through
the steps of complex tasks, there is a need to structure the task into
manageable pieces of information of the right length and complexity. In this
paper, we tackle the recipes domain and convert reading structured instructions
into conversational structured ones. We annotated the structure of instructions
according to a conversational scenario, which provided insights into what is
expected in this setting. To computationally model the conversational step's
characteristics, we tested various Transformer-based architectures, showing
that a token-based approach delivers the best results. A further user study
showed that users tend to favor steps of manageable complexity and length, and
that the proposed methodology can improve the original web-based instructional
text. Specifically, 86% of the evaluated tasks were improved from a
conversational suitability point of view.
</p>

### Title: GME: GPU-based Microarchitectural Extensions to Accelerate Homomorphic Encryption. (arXiv:2309.11001v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.11001](http://arxiv.org/abs/2309.11001)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11001] GME: GPU-based Microarchitectural Extensions to Accelerate Homomorphic Encryption](http://arxiv.org/abs/2309.11001) #memory`
* Summary: <p>Fully Homomorphic Encryption (FHE) enables the processing of encrypted data
without decrypting it. FHE has garnered significant attention over the past
decade as it supports secure outsourcing of data processing to remote cloud
services. Despite its promise of strong data privacy and security guarantees,
FHE introduces a slowdown of up to five orders of magnitude as compared to the
same computation using plaintext data. This overhead is presently a major
barrier to the commercial adoption of FHE.
</p>
<p>In this work, we leverage GPUs to accelerate FHE, capitalizing on a
well-established GPU ecosystem available in the cloud. We propose GME, which
combines three key microarchitectural extensions along with a compile-time
optimization to the current AMD CDNA GPU architecture. First, GME integrates a
lightweight on-chip compute unit (CU)-side hierarchical interconnect to retain
ciphertext in cache across FHE kernels, thus eliminating redundant memory
transactions. Second, to tackle compute bottlenecks, GME introduces special
MOD-units that provide native custom hardware support for modular reduction
operations, one of the most commonly executed sets of operations in FHE. Third,
by integrating the MOD-unit with our novel pipelined $64$-bit integer
arithmetic cores (WMAC-units), GME further accelerates FHE workloads by $19\%$.
Finally, we propose a Locality-Aware Block Scheduler (LABS) that exploits the
temporal locality available in FHE primitive blocks. Incorporating these
microarchitectural features and compiler optimizations, we create a synergistic
approach achieving average speedups of $796\times$, $14.2\times$, and
$2.3\times$ over Intel Xeon CPU, NVIDIA V100 GPU, and Xilinx FPGA
implementations, respectively.
</p>

### Title: Capacity: Cryptographically-Enforced In-Process Capabilities for Modern ARM Architectures (Extended Version). (arXiv:2309.11151v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.11151](http://arxiv.org/abs/2309.11151)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11151] Capacity: Cryptographically-Enforced In-Process Capabilities for Modern ARM Architectures (Extended Version)](http://arxiv.org/abs/2309.11151) #memory`
* Summary: <p>In-process compartmentalization and access control have been actively
explored to provide in-place and efficient isolation of in-process security
domains. Many works have proposed compartmentalization schemes that leverage
hardware features, most notably using the new page-based memory isolation
feature called Protection Keys for Userspace (PKU) on x86. Unfortunately, the
modern ARM architecture does not have an equivalent feature. Instead, newer ARM
architectures introduced Pointer Authentication (PA) and Memory Tagging
Extension (MTE), adapting the reference validation model for memory safety and
runtime exploit mitigation. We argue that those features have been
underexplored in the context of compartmentalization and that they can be
retrofitted to implement a capability-based in-process access control scheme.
This paper presents Capacity, a novel hardware-assisted intra-process access
control design that embraces capability-based security principles. Capacity
coherently incorporates the new hardware security features on ARM that already
exhibit inherent characteristics of capability. It supports the life-cycle
protection of the domain's sensitive objects -- starting from their import from
the file system to their place in memory. With intra-process domains
authenticated with unique PA keys, Capacity transforms file descriptors and
memory pointers into cryptographically-authenticated references and completely
mediates reference usage with its program instrumentation framework and an
efficient system call monitor. We evaluate our Capacity-enabled NGINX web
server prototype and other common applications in which sensitive resources are
isolated into different domains. Our evaluation shows that Capacity incurs a
low-performance overhead of approximately 17% for the single-threaded and
13.54% for the multi-threaded webserver.
</p>

### Title: Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks. (arXiv:2309.11048v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.11048](http://arxiv.org/abs/2309.11048)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11048] Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks](http://arxiv.org/abs/2309.11048) #memory`
* Summary: <p>Edge computing is a promising solution for handling high-dimensional,
multispectral analog data from sensors and IoT devices for applications such as
autonomous drones. However, edge devices' limited storage and computing
resources make it challenging to perform complex predictive modeling at the
edge. Compute-in-memory (CiM) has emerged as a principal paradigm to minimize
energy for deep learning-based inference at the edge. Nevertheless, integrating
storage and processing complicates memory cells and/or memory peripherals,
essentially trading off area efficiency for energy efficiency. This paper
proposes a novel solution to improve area efficiency in deep learning inference
tasks. The proposed method employs two key strategies. Firstly, a Frequency
domain learning approach uses binarized Walsh-Hadamard Transforms, reducing the
necessary parameters for DNN (by 87% in MobileNetV2) and enabling
compute-in-SRAM, which better utilizes parallelism during inference. Secondly,
a memory-immersed collaborative digitization method is described among CiM
arrays to reduce the area overheads of conventional ADCs. This facilitates more
CiM arrays in limited footprint designs, leading to better parallelism and
reduced external memory accesses. Different networking configurations are
explored, where Flash, SA, and their hybrid digitization steps can be
implemented using the memory-immersed scheme. The results are demonstrated
using a 65 nm CMOS test chip, exhibiting significant area and energy savings
compared to a 40 nm-node 5-bit SAR ADC and 5-bit Flash ADC. By processing
analog data more efficiently, it is possible to selectively retain valuable
data from sensors and alleviate the challenges posed by the analog data deluge.
</p>

### Title: InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update. (arXiv:2309.11071v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.11071](http://arxiv.org/abs/2309.11071)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11071] InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update](http://arxiv.org/abs/2309.11071) #memory`
* Summary: <p>Classic Graph Neural Network (GNN) inference approaches, designed for static
graphs, are ill-suited for streaming graphs that evolve with time. The dynamism
intrinsic to streaming graphs necessitates constant updates, posing unique
challenges to acceleration on GPU. We address these challenges based on two key
insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the
nodes is not impacted by the modified edges when the model uses min or max as
aggregation function; (2) When the model weights remain static while the graph
structure changes, node embeddings can incrementally evolve over time by
computing only the impacted part of the neighborhood. With these insights, we
propose a novel method, InkStream, designed for real-time inference with
minimal memory access and computation, while ensuring an identical output to
conventional methods. InkStream operates on the principle of propagating and
fetching data only when necessary. It uses an event-based system to control
inter-layer effect propagation and intra-layer incremental updates of node
embedding. InkStream is highly extensible and easily configurable by allowing
users to create and process customized events. We showcase that less than 10
lines of additional user code are needed to support popular GNN models such as
GCN, GraphSAGE, and GIN. Our experiments with three GNN models on four large
graphs demonstrate that InkStream accelerates by 2.5-427$\times$ on a CPU
cluster and 2.4-343$\times$ on two different GPU clusters while producing
identical outputs as GNN model inference on the latest graph snapshot.
</p>

## few-shot
### Title: Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation. (arXiv:2309.11160v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11160](http://arxiv.org/abs/2309.11160)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11160] Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation](http://arxiv.org/abs/2309.11160) #few-shot`
* Summary: <p>Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a query
video with the same category defined by a few annotated support images.
However, this task was seldom explored. In this work, based on IPMT, a
state-of-the-art few-shot image segmentation method that combines external
support guidance information with adaptive query guidance cues, we propose to
leverage multi-grained temporal guidance information for handling the temporal
correlation nature of video data. We decompose the query video information into
a clip prototype and a memory prototype for capturing local and long-term
internal temporal guidance, respectively. Frame prototypes are further used for
each frame independently to handle fine-grained adaptive guidance and enable
bidirectional clip-frame prototype communication. To reduce the influence of
noisy memory, we propose to leverage the structural similarity relation among
different predicted regions and the support for selecting reliable memory
frames. Furthermore, a new segmentation loss is also proposed to enhance the
category discriminability of the learned prototypes. Experimental results
demonstrate that our proposed video IPMT model significantly outperforms
previous models on two benchmark datasets. Code is available at
https://github.com/nankepan/VIPMT.
</p>

### Title: Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation. (arXiv:2309.11172v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11172](http://arxiv.org/abs/2309.11172)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11172] Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation](http://arxiv.org/abs/2309.11172) #few-shot`
* Summary: <p>Few-shot Medical Image Segmentation (FSMIS) is a more promising solution for
medical image segmentation tasks where high-quality annotations are naturally
scarce. However, current mainstream methods primarily focus on extracting
holistic representations from support images with large intra-class variations
in appearance and background, and encounter difficulties in adapting to query
images. In this work, we present an approach to extract multiple representative
sub-regions from a given support medical image, enabling fine-grained selection
over the generated image regions. Specifically, the foreground of the support
image is decomposed into distinct regions, which are subsequently used to
derive region-level representations via a designed Regional Prototypical
Learning (RPL) module. We then introduce a novel Prototypical Representation
Debiasing (PRD) module based on a two-way elimination mechanism which
suppresses the disturbance of regional representations by a self-support,
Multi-direction Self-debiasing (MS) block, and a support-query, Interactive
Debiasing (ID) block. Finally, an Assembled Prediction (AP) module is devised
to balance and integrate predictions of multiple prototypical representations
learned using stacked PRD modules. Results obtained through extensive
experiments on three publicly accessible medical imaging datasets demonstrate
consistent improvements over the leading FSMIS methods. The source code is
available at https://github.com/YazhouZhu19/PAMI.
</p>

### Title: Generalized Few-Shot Point Cloud Segmentation Via Geometric Words. (arXiv:2309.11222v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11222](http://arxiv.org/abs/2309.11222)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11222] Generalized Few-Shot Point Cloud Segmentation Via Geometric Words](http://arxiv.org/abs/2309.11222) #few-shot`
* Summary: <p>Existing fully-supervised point cloud segmentation methods suffer in the
dynamic testing environment with emerging new classes. Few-shot point cloud
segmentation algorithms address this problem by learning to adapt to new
classes at the sacrifice of segmentation accuracy for the base classes, which
severely impedes its practicality. This largely motivates us to present the
first attempt at a more practical paradigm of generalized few-shot point cloud
segmentation, which requires the model to generalize to new categories with
only a few support point clouds and simultaneously retain the capability to
segment base classes. We propose the geometric words to represent geometric
components shared between the base and novel classes, and incorporate them into
a novel geometric-aware semantic representation to facilitate better
generalization to the new classes without forgetting the old ones. Moreover, we
introduce geometric prototypes to guide the segmentation with geometric prior
knowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate
the superior performance of our method over baseline methods. Our code is
available at: https://github.com/Pixie8888/GFS-3DSeg_GWs.
</p>

### Title: Towards Robust Few-shot Point Cloud Semantic Segmentation. (arXiv:2309.11228v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11228](http://arxiv.org/abs/2309.11228)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11228] Towards Robust Few-shot Point Cloud Semantic Segmentation](http://arxiv.org/abs/2309.11228) #few-shot`
* Summary: <p>Few-shot point cloud semantic segmentation aims to train a model to quickly
adapt to new unseen classes with only a handful of support set samples.
However, the noise-free assumption in the support set can be easily violated in
many practical real-world settings. In this paper, we focus on improving the
robustness of few-shot point cloud segmentation under the detrimental influence
of noisy support sets during testing time. To this end, we first propose a
Component-level Clean Noise Separation (CCNS) representation learning to learn
discriminative feature representations that separates the clean samples of the
target classes from the noisy samples. Leveraging the well separated clean and
noisy support samples from our CCNS, we further propose a Multi-scale
Degree-based Noise Suppression (MDNS) scheme to remove the noisy shots from the
support set. We conduct extensive experiments on various noise settings on two
benchmark datasets. Our results show that the combination of CCNS and MDNS
significantly improves the performance. Our code is available at
https://github.com/Pixie8888/R3DFSSeg.
</p>

### Title: A Systematic Review of Few-Shot Learning in Medical Imaging. (arXiv:2309.11433v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.11433](http://arxiv.org/abs/2309.11433)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.11433] A Systematic Review of Few-Shot Learning in Medical Imaging](http://arxiv.org/abs/2309.11433) #few-shot`
* Summary: <p>The lack of annotated medical images limits the performance of deep learning
models, which usually need large-scale labelled datasets. Few-shot learning
techniques can reduce data scarcity issues and enhance medical image analysis,
especially with meta-learning. This systematic review gives a comprehensive
overview of few-shot learning in medical imaging. We searched the literature
systematically and selected 80 relevant articles published from 2018 to 2023.
We clustered the articles based on medical outcomes, such as tumour
segmentation, disease classification, and image registration; anatomical
structure investigated (i.e. heart, lung, etc.); and the meta-learning method
used. For each cluster, we examined the papers' distributions and the results
provided by the state-of-the-art. In addition, we identified a generic pipeline
shared among all the studies. The review shows that few-shot learning can
overcome data scarcity in most outcomes and that meta-learning is a popular
choice to perform few-shot learning because it can adapt to new tasks with few
labelled samples. In addition, following meta-learning, supervised learning and
semi-supervised learning stand out as the predominant techniques employed to
tackle few-shot learning challenges in medical imaging and also best
performing. Lastly, we observed that the primary application areas
predominantly encompass cardiac, pulmonary, and abdominal domains. This
systematic review aims to inspire further research to improve medical image
analysis and patient care.
</p>

### Title: Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.10929](http://arxiv.org/abs/2309.10929)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.10929] Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training](http://arxiv.org/abs/2309.10929) #few-shot`
* Summary: <p>In this work, we introduce the concept of complex text style transfer tasks,
and constructed complex text datasets based on two widely applicable scenarios.
Our dataset is the first large-scale data set of its kind, with 700 rephrased
sentences and 1,000 sentences from the game Genshin Impact. While large
language models (LLM) have shown promise in complex text style transfer, they
have drawbacks such as data privacy concerns, network instability, and high
deployment costs. To address these issues, we explore the effectiveness of
small models (less than T5-3B) with implicit style pre-training through
contrastive learning. We also propose a method for automated evaluation of text
generation quality based on alignment with human evaluations using ChatGPT.
Finally, we compare our approach with existing methods and show that our model
achieves state-of-art performances of few-shot text style transfer models.
</p>

