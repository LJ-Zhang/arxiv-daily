## diffusion
### Title: Synthetic Image Detection: Highlights from the IEEE Video and Image Processing Cup 2022 Student Competition. (arXiv:2309.12428v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12428](http://arxiv.org/abs/2309.12428)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12428] Synthetic Image Detection: Highlights from the IEEE Video and Image Processing Cup 2022 Student Competition](http://arxiv.org/abs/2309.12428) #diffusion`
* Summary: <p>The Video and Image Processing (VIP) Cup is a student competition that takes
place each year at the IEEE International Conference on Image Processing. The
2022 IEEE VIP Cup asked undergraduate students to develop a system capable of
distinguishing pristine images from generated ones. The interest in this topic
stems from the incredible advances in the AI-based generation of visual data,
with tools that allows the synthesis of highly realistic images and videos.
While this opens up a large number of new opportunities, it also undermines the
trustworthiness of media content and fosters the spread of disinformation on
the internet. Recently there was strong concern about the generation of
extremely realistic images by means of editing software that includes the
recent technology on diffusion models. In this context, there is a need to
develop robust and automatic tools for synthetic image detection.
</p>

### Title: License Plate Super-Resolution Using Diffusion Models. (arXiv:2309.12506v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12506](http://arxiv.org/abs/2309.12506)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12506] License Plate Super-Resolution Using Diffusion Models](http://arxiv.org/abs/2309.12506) #diffusion`
* Summary: <p>In surveillance, accurately recognizing license plates is hindered by their
often low quality and small dimensions, compromising recognition precision.
Despite advancements in AI-based image super-resolution, methods like
Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs)
still fall short in enhancing license plate images. This study leverages the
cutting-edge diffusion model, which has consistently outperformed other deep
learning techniques in image restoration. By training this model using a
curated dataset of Saudi license plates, both in low and high resolutions, we
discovered the diffusion model's superior efficacy. The method achieves a
12.55\% and 37.32% improvement in Peak Signal-to-Noise Ratio (PSNR) over SwinIR
and ESRGAN, respectively. Moreover, our method surpasses these techniques in
terms of Structural Similarity Index (SSIM), registering a 4.89% and 17.66%
improvement over SwinIR and ESRGAN, respectively. Furthermore, 92% of human
evaluators preferred our images over those from other algorithms. In essence,
this research presents a pioneering solution for license plate
super-resolution, with tangible potential for surveillance systems.
</p>

### Title: Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12829](http://arxiv.org/abs/2309.12829)
* Code URL: [https://github.com/naamiinepal/synthetic-boost](https://github.com/naamiinepal/synthetic-boost)
* Copy Paste: `<input type="checkbox">[[2309.12829] Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography](http://arxiv.org/abs/2309.12829) #diffusion`
* Summary: <p>Accurate segmentation is essential for echocardiography-based assessment of
cardiovascular diseases (CVDs). However, the variability among sonographers and
the inherent challenges of ultrasound images hinder precise segmentation. By
leveraging the joint representation of image and text modalities,
Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual
information, potentially aiding in accurate and explainable segmentation.
However, the lack of readily available data in echocardiography hampers the
training of VLSMs. In this study, we explore using synthetic datasets from
Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography
segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS)
using seven different kinds of language prompts derived from several
attributes, automatically extracted from echocardiography images, segmentation
masks, and their metadata. Our results show improved metrics and faster
convergence when pretraining VLSMs on SDM-generated synthetic images before
finetuning on real images. The code, configs, and prompts are available at
https://github.com/naamiinepal/synthetic-boost.
</p>

### Title: MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.13042](http://arxiv.org/abs/2309.13042)
* Code URL: [https://github.com/jiahao000/mosaicfusion](https://github.com/jiahao000/mosaicfusion)
* Copy Paste: `<input type="checkbox">[[2309.13042] MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation](http://arxiv.org/abs/2309.13042) #diffusion`
* Summary: <p>We present MosaicFusion, a simple yet effective diffusion-based data
augmentation approach for large vocabulary instance segmentation. Our method is
training-free and does not rely on any label supervision. Two key designs
enable us to employ an off-the-shelf text-to-image diffusion model as a useful
dataset generator for object instances and mask annotations. First, we divide
an image canvas into several regions and perform a single round of diffusion
process to generate multiple instances simultaneously, conditioning on
different text prompts. Second, we obtain corresponding instance masks by
aggregating cross-attention maps associated with object prompts across layers
and diffusion time steps, followed by simple thresholding and edge-aware
refinement processing. Without bells and whistles, our MosaicFusion can produce
a significant amount of synthetic labeled data for both rare and novel
categories. Experimental results on the challenging LVIS long-tailed and
open-vocabulary benchmarks demonstrate that MosaicFusion can significantly
improve the performance of existing instance segmentation models, especially
for rare and novel categories. Code will be released at
https://github.com/Jiahao000/MosaicFusion.
</p>

### Title: A Diffusion-Model of Joint Interactive Navigation. (arXiv:2309.12508v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12508](http://arxiv.org/abs/2309.12508)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12508] A Diffusion-Model of Joint Interactive Navigation](http://arxiv.org/abs/2309.12508) #diffusion`
* Summary: <p>Simulation of autonomous vehicle systems requires that simulated traffic
participants exhibit diverse and realistic behaviors. The use of prerecorded
real-world traffic scenarios in simulation ensures realism but the rarity of
safety critical events makes large scale collection of driving scenarios
expensive. In this paper, we present DJINN - a diffusion based method of
generating traffic scenarios. Our approach jointly diffuses the trajectories of
all agents, conditioned on a flexible set of state observations from the past,
present, or future. On popular trajectory forecasting datasets, we report state
of the art performance on joint trajectory metrics. In addition, we demonstrate
how DJINN flexibly enables direct test-time sampling from a variety of valuable
conditional distributions including goal-based sampling, behavior-class
sampling, and scenario editing.
</p>

## self-supervised
### Title: Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12757](http://arxiv.org/abs/2309.12757)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12757] Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where](http://arxiv.org/abs/2309.12757) #self-supervised`
* Summary: <p>While image data starts to enjoy the simple-but-effective self-supervised
learning scheme built upon masking and self-reconstruction objective thanks to
the introduction of tokenization procedure and vision transformer backbone,
convolutional neural networks as another important and widely-adopted
architecture for image data, though having contrastive-learning techniques to
drive the self-supervised learning, still face the difficulty of leveraging
such straightforward and general masking operation to benefit their learning
process significantly. In this work, we aim to alleviate the burden of
including masking operation into the contrastive-learning framework for
convolutional neural networks as an extra augmentation method. In addition to
the additive but unwanted edges (between masked and unmasked regions) as well
as other adverse effects caused by the masking operations for ConvNets, which
have been discussed by prior works, we particularly identify the potential
problem where for one view in a contrastive sample-pair the randomly-sampled
masking regions could be overly concentrated on important/salient objects thus
resulting in misleading contrastiveness to the other view. To this end, we
propose to explicitly take the saliency constraint into consideration in which
the masked regions are more evenly distributed among the foreground and
background for realizing the masking-based augmentation. Moreover, we introduce
hard negative samples by masking larger regions of salient patches in an input
image. Extensive experiments conducted on various datasets, contrastive
learning mechanisms, and downstream tasks well verify the efficacy as well as
the superior performance of our proposed method with respect to several
state-of-the-art baselines.
</p>

### Title: On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12931](http://arxiv.org/abs/2309.12931)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12931] On Separate Normalization in Self-supervised Transformers](http://arxiv.org/abs/2309.12931) #self-supervised`
* Summary: <p>Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.
</p>

## foundation model
### Title: BayesDLL: Bayesian Deep Learning Library. (arXiv:2309.12928v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12928](http://arxiv.org/abs/2309.12928)
* Code URL: [https://github.com/minyoungkim21/bayesdll](https://github.com/minyoungkim21/bayesdll)
* Copy Paste: `<input type="checkbox">[[2309.12928] BayesDLL: Bayesian Deep Learning Library](http://arxiv.org/abs/2309.12928) #foundation model`
* Summary: <p>We release a new Bayesian neural network library for PyTorch for large-scale
deep networks. Our library implements mainstream approximate Bayesian inference
algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and
Laplace approximation. The main differences from other existing Bayesian neural
network libraries are as follows: 1) Our library can deal with very large-scale
deep networks including Vision Transformers (ViTs). 2) We need virtually zero
code modifications for users (e.g., the backbone network definition codes do
not neet to be modified at all). 3) Our library also allows the pre-trained
model weights to serve as a prior mean, which is very useful for performing
Bayesian inference with the large-scale foundation models like ViTs that are
hard to optimise from scratch with the downstream data alone. Our code is
publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A
mirror repository is also available at:
\url{https://github.com/minyoungkim21/BayesDLL}.}.
</p>

## generative
### Title: Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. (arXiv:2309.12444v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12444](http://arxiv.org/abs/2309.12444)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12444] Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI](http://arxiv.org/abs/2309.12444) #generative`
* Summary: <p>Generative Artificial Intelligence is set to revolutionize healthcare
delivery by transforming traditional patient care into a more personalized,
efficient, and proactive process. Chatbots, serving as interactive
conversational models, will probably drive this patient-centered transformation
in healthcare. Through the provision of various services, including diagnosis,
personalized lifestyle recommendations, and mental health support, the
objective is to substantially augment patient health outcomes, all the while
mitigating the workload burden on healthcare providers. The life-critical
nature of healthcare applications necessitates establishing a unified and
comprehensive set of evaluation metrics for conversational models. Existing
evaluation metrics proposed for various generic large language models (LLMs)
demonstrate a lack of comprehension regarding medical and health concepts and
their significance in promoting patients' well-being. Moreover, these metrics
neglect pivotal user-centered aspects, including trust-building, ethics,
personalization, empathy, user comprehension, and emotional support. The
purpose of this paper is to explore state-of-the-art LLM-based evaluation
metrics that are specifically applicable to the assessment of interactive
conversational models in healthcare. Subsequently, we present an comprehensive
set of evaluation metrics designed to thoroughly assess the performance of
healthcare chatbots from an end-user perspective. These metrics encompass an
evaluation of language processing abilities, impact on real-world clinical
tasks, and effectiveness in user-interactive conversations. Finally, we engage
in a discussion concerning the challenges associated with defining and
implementing these metrics, with particular emphasis on confounding factors
such as the target audience, evaluation methods, and prompt techniques involved
in the evaluation process.
</p>

### Title: Learning to Diversify Neural Text Generation via Degenerative Model. (arXiv:2309.12619v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12619](http://arxiv.org/abs/2309.12619)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12619] Learning to Diversify Neural Text Generation via Degenerative Model](http://arxiv.org/abs/2309.12619) #generative`
* Summary: <p>Neural language models often fail to generate diverse and informative texts,
limiting their applicability in real-world problems. While previous approaches
have proposed to address these issues by identifying and penalizing undesirable
behaviors (e.g., repetition, overuse of frequent words) from language models,
we propose an alternative approach based on an observation: models primarily
learn attributes within examples that are likely to cause degeneration
problems. Based on this observation, we propose a new approach to prevent
degeneration problems by training two models. Specifically, we first train a
model that is designed to amplify undesirable patterns. We then enhance the
diversity of the second model by focusing on patterns that the first model
fails to learn. Extensive experiments on two tasks, namely language modeling
and dialogue generation, demonstrate the effectiveness of our approach.
</p>

### Title: Change Management using Generative Modeling on Digital Twins. (arXiv:2309.12421v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.12421](http://arxiv.org/abs/2309.12421)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12421] Change Management using Generative Modeling on Digital Twins](http://arxiv.org/abs/2309.12421) #generative`
* Summary: <p>A key challenge faced by small and medium-sized business entities is securely
managing software updates and changes. Specifically, with rapidly evolving
cybersecurity threats, changes/updates/patches to software systems are
necessary to stay ahead of emerging threats and are often mandated by
regulators or statutory authorities to counter these. However, security
patches/updates require stress testing before they can be released in the
production system. Stress testing in production environments is risky and poses
security threats. Large businesses usually have a non-production environment
where such changes can be made and tested before being released into
production. Smaller businesses do not have such facilities. In this work, we
show how "digital twins", especially for a mix of IT and IoT environments, can
be created on the cloud. These digital twins act as a non-production
environment where changes can be applied, and the system can be securely tested
before patch release. Additionally, the non-production digital twin can be used
to collect system data and run stress tests on the environment, both manually
and automatically. In this paper, we show how using a small sample of real
data/interactions, Generative Artificial Intelligence (AI) models can be used
to generate testing scenarios to check for points of failure.
</p>

## anomaly
### Title: mixed attention auto encoder for multi-class industrial anomaly detection. (arXiv:2309.12700v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12700](http://arxiv.org/abs/2309.12700)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12700] mixed attention auto encoder for multi-class industrial anomaly detection](http://arxiv.org/abs/2309.12700) #anomaly`
* Summary: <p>Most existing methods for unsupervised industrial anomaly detection train a
separate model for each object category. This kind of approach can easily
capture the category-specific feature distributions, but results in high
storage cost and low training efficiency. In this paper, we propose a unified
mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection
with a single model. To alleviate the performance degradation due to the
diverse distribution patterns of different categories, we employ spatial
attentions and channel attentions to effectively capture the global category
information and model the feature distributions of multiple classes.
Furthermore, to simulate the realistic noises on features and preserve the
surface semantics of objects from different categories which are essential for
detecting the subtle anomalies, we propose an adaptive noise generator and a
multi-scale fusion module for the pre-trained features. MAAE delivers
remarkable performances on the benchmark dataset compared with the
state-of-the-art methods.
</p>

### Title: On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures. (arXiv:2309.12955v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.12955](http://arxiv.org/abs/2309.12955)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12955] On Data Fabrication in Collaborative Vehicular Perception: Attacks and Countermeasures](http://arxiv.org/abs/2309.12955) #anomaly`
* Summary: <p>Collaborative perception, which greatly enhances the sensing capability of
connected and autonomous vehicles (CAVs) by incorporating data from external
resources, also brings forth potential security risks. CAVs' driving decisions
rely on remote untrusted data, making them susceptible to attacks carried out
by malicious participants in the collaborative perception system. However,
security analysis and countermeasures for such threats are absent. To
understand the impact of the vulnerability, we break the ground by proposing
various real-time data fabrication attacks in which the attacker delivers
crafted malicious data to victims in order to perturb their perception results,
leading to hard brakes or increased collision risks. Our attacks demonstrate a
high success rate of over 86\% on high-fidelity simulated scenarios and are
realizable in real-world experiments. To mitigate the vulnerability, we present
a systematic anomaly detection approach that enables benign vehicles to jointly
reveal malicious fabrication. It detects 91.5% of attacks with a false positive
rate of 3% in simulated scenarios and significantly mitigates attack impacts in
real-world scenarios.
</p>

## in-context
### Title: HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering. (arXiv:2309.12669v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12669](http://arxiv.org/abs/2309.12669)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12669] HRoT: Hybrid prompt strategy and Retrieval of Thought for Table-Text Hybrid Question Answering](http://arxiv.org/abs/2309.12669) #in-context`
* Summary: <p>Answering numerical questions over hybrid contents from the given tables and
text(TextTableQA) is a challenging task. Recently, Large Language Models (LLMs)
have gained significant attention in the NLP community. With the emergence of
large language models, In-Context Learning and Chain-of-Thought prompting have
become two particularly popular research topics in this field. In this paper,
we introduce a new prompting strategy called Hybrid prompt strategy and
Retrieval of Thought for TextTableQA. Through In-Context Learning, we prompt
the model to develop the ability of retrieval thinking when dealing with hybrid
data. Our method achieves superior performance compared to the fully-supervised
SOTA on the MultiHiertt dataset in the few-shot setting.
</p>

### Title: Affect Recognition in Conversations Using Large Language Models. (arXiv:2309.12881v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12881](http://arxiv.org/abs/2309.12881)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12881] Affect Recognition in Conversations Using Large Language Models](http://arxiv.org/abs/2309.12881) #in-context`
* Summary: <p>Affect recognition, encompassing emotions, moods, and feelings, plays a
pivotal role in human communication. In the realm of conversational artificial
intelligence (AI), the ability to discern and respond to human affective cues
is a critical factor for creating engaging and empathetic interactions. This
study delves into the capacity of large language models (LLMs) to recognise
human affect in conversations, with a focus on both open-domain chit-chat
dialogues and task-oriented dialogues. Leveraging three diverse datasets,
namely IEMOCAP, EmoWOZ, and DAIC-WOZ, covering a spectrum of dialogues from
casual conversations to clinical interviews, we evaluated and compared LLMs'
performance in affect recognition. Our investigation explores the zero-shot and
few-shot capabilities of LLMs through in-context learning (ICL) as well as
their model capacities through task-specific fine-tuning. Additionally, this
study takes into account the potential impact of automatic speech recognition
(ASR) errors on LLM predictions. With this work, we aim to shed light on the
extent to which LLMs can replicate human-like affect recognition capabilities
in conversations.
</p>

## memory
### Title: Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation. (arXiv:2309.12557v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12557](http://arxiv.org/abs/2309.12557)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12557] Triple-View Knowledge Distillation for Semi-Supervised Semantic Segmentation](http://arxiv.org/abs/2309.12557) #memory`
* Summary: <p>To alleviate the expensive human labeling, semi-supervised semantic
segmentation employs a few labeled images and an abundant of unlabeled images
to predict the pixel-level label map with the same size. Previous methods often
adopt co-training using two convolutional networks with the same architecture
but different initialization, which fails to capture the sufficiently diverse
features. This motivates us to use tri-training and develop the triple-view
encoder to utilize the encoders with different architectures to derive diverse
features, and exploit the knowledge distillation skill to learn the
complementary semantics among these encoders. Moreover, existing methods simply
concatenate the features from both encoder and decoder, resulting in redundant
features that require large memory cost. This inspires us to devise a
dual-frequency decoder that selects those important features by projecting the
features from the spatial domain to the frequency domain, where the
dual-frequency channel attention mechanism is introduced to model the feature
importance. Therefore, we propose a Triple-view Knowledge Distillation
framework, termed TriKD, for semi-supervised semantic segmentation, including
the triple-view encoder and the dual-frequency decoder. Extensive experiments
were conducted on two benchmarks, \ie, Pascal VOC 2012 and Cityscapes, whose
results verify the superiority of the proposed method with a good tradeoff
between precision and inference speed.
</p>

### Title: On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12673](http://arxiv.org/abs/2309.12673)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12673] On Sparse Modern Hopfield Model](http://arxiv.org/abs/2309.12673) #memory`
* Summary: <p>We introduce the sparse modern Hopfield model as a sparse extension of the
modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield
model equips a memory-retrieval dynamics whose one-step approximation
corresponds to the sparse attention mechanism. Theoretically, our key
contribution is a principled derivation of a closed-form sparse Hopfield energy
using the convex conjugate of the sparse entropic regularizer. Building upon
this, we derive the sparse memory retrieval dynamics from the sparse energy
function and show its one-step approximation is equivalent to the
sparse-structured attention. Importantly, we provide a sparsity-dependent
memory retrieval error bound which is provably tighter than its dense analog.
The conditions for the benefits of sparsity to arise are therefore identified
and discussed. In addition, we show that the sparse modern Hopfield model
maintains the robust theoretical properties of its dense counterpart, including
rapid fixed point convergence and exponential memory capacity. Empirically, we
use both synthetic and real-world datasets to demonstrate that the sparse
Hopfield model outperforms its dense counterpart in many situations.
</p>

### Title: PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12708](http://arxiv.org/abs/2309.12708)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12708] PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion](http://arxiv.org/abs/2309.12708) #memory`
* Summary: <p>Semantic Scene Completion (SSC) aims to jointly generate space occupancies
and semantic labels for complex 3D scenes. Most existing SSC models focus on
volumetric representations, which are memory-inefficient for large outdoor
spaces. Point clouds provide a lightweight alternative but existing benchmarks
lack outdoor point cloud scenes with semantic labels. To address this, we
introduce PointSSC, the first cooperative vehicle-infrastructure point cloud
benchmark for semantic scene completion. These scenes exhibit long-range
perception and minimal occlusion. We develop an automated annotation pipeline
leveraging Segment Anything to efficiently assign semantics. To benchmark
progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for
global and local feature extraction and a Completion and Segmentation
Cooperative Module for joint completion and segmentation. PointSSC provides a
challenging testbed to drive advances in semantic point cloud completion for
real-world navigation.
</p>

### Title: Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12862](http://arxiv.org/abs/2309.12862)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12862] Associative Transformer Is A Sparse Representation Learner](http://arxiv.org/abs/2309.12862) #memory`
* Summary: <p>Emerging from the monolithic pairwise attention mechanism in conventional
Transformer models, there is a growing interest in leveraging sparse
interactions that align more closely with biological principles. Approaches
including the Set Transformer and the Perceiver employ cross-attention
consolidated with a latent space that forms an attention bottleneck with
limited capacity. Building upon recent neuroscience studies of Global Workspace
Theory and associative memory, we propose the Associative Transformer (AiT).
AiT induces low-rank explicit memory that serves as both priors to guide
bottleneck attention in the shared workspace and attractors within associative
memory of a Hopfield network. Through joint end-to-end training, these priors
naturally develop module specialization, each contributing a distinct inductive
bias to form attention bottlenecks. A bottleneck can foster competition among
inputs for writing information into the memory. We show that AiT is a sparse
representation learner, learning distinct priors through the bottlenecks that
are complexity-invariant to input quantities and dimensions. AiT demonstrates
its superiority over methods such as the Set Transformer, Vision Transformer,
and Coordination in various vision tasks.
</p>

### Title: A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them. (arXiv:2309.12925v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2309.12925](http://arxiv.org/abs/2309.12925)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12925] A New Security Threat in MCUs -- SoC-wide timing side channels and how to find them](http://arxiv.org/abs/2309.12925) #memory`
* Summary: <p>Microarchitectural timing side channels have been thoroughly investigated as
a security threat in hardware designs featuring shared buffers (e.g., caches)
and/or parallelism between attacker and victim task execution. Contradicting
common intuitions, recent activities demonstrate, however, that this threat is
real also in microcontroller SoCs without such features. In this paper, we
describe SoC-wide timing side channels previously neglected by security
analysis and present a new formal method to close this gap. In a case study
with the RISC-V Pulpissimo SoC platform, our method found a vulnerability to a
so far unknown attack variant that allows an attacker to obtain information
about a victim's memory access behavior. After implementing a conservative fix,
we were able to verify that the SoC is now secure w.r.t. timing side channels.
</p>

### Title: Memory Efficient Mixed-Precision Optimizers. (arXiv:2309.12381v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12381](http://arxiv.org/abs/2309.12381)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12381] Memory Efficient Mixed-Precision Optimizers](http://arxiv.org/abs/2309.12381) #memory`
* Summary: <p>Traditional optimization methods rely on the use of single-precision floating
point arithmetic, which can be costly in terms of memory size and computing
power. However, mixed precision optimization techniques leverage the use of
both single and half-precision floating point arithmetic to reduce memory
requirements while maintaining model accuracy. We provide here an algorithm to
further reduce memory usage during the training of a model by getting rid of
the floating point copy of the parameters, virtually keeping only
half-precision numbers. We also explore the benefits of getting rid of the
gradient's value by executing the optimizer step during the back-propagation.
In practice, we achieve up to 25% lower peak memory use and 15% faster training
while maintaining the same level of accuracy.
</p>

### Title: SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. (arXiv:2309.12578v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2309.12578](http://arxiv.org/abs/2309.12578)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12578] SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling](http://arxiv.org/abs/2309.12578) #memory`
* Summary: <p>Sparsifying the Transformer has garnered considerable interest, as training
the Transformer is very computationally demanding. Prior efforts to sparsify
the Transformer have either used a fixed pattern or data-driven approach to
reduce the number of operations involving the computation of multi-head
attention, which is the main bottleneck of the Transformer. However, existing
methods suffer from inevitable problems, such as the potential loss of
essential sequence features due to the uniform fixed pattern applied across all
layers, and an increase in the model size resulting from the use of additional
parameters to learn sparsity patterns in attention operations. In this paper,
we propose a novel sparsification scheme for the Transformer that integrates
convolution filters and the flood filling method to efficiently capture the
layer-wise sparse pattern in attention operations. Our sparsification approach
reduces the computational complexity and memory footprint of the Transformer
during training. Efficient implementations of the layer-wise sparsified
attention algorithm on GPUs are developed, demonstrating a new SPION that
achieves up to 3.08X speedup over existing state-of-the-art sparse Transformer
models, with better evaluation quality.
</p>

## few-shot
### Title: Domain Adaptive Few-Shot Open-Set Learning. (arXiv:2309.12814v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12814](http://arxiv.org/abs/2309.12814)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12814] Domain Adaptive Few-Shot Open-Set Learning](http://arxiv.org/abs/2309.12814) #few-shot`
* Summary: <p>Few-shot learning has made impressive strides in addressing the crucial
challenges of recognizing unknown samples from novel classes in target query
sets and managing visual shifts between domains. However, existing techniques
fall short when it comes to identifying target outliers under domain shifts by
learning to reject pseudo-outliers from the source domain, resulting in an
incomplete solution to both problems. To address these challenges
comprehensively, we propose a novel approach called Domain Adaptive Few-Shot
Open Set Recognition (DA-FSOS) and introduce a meta-learning-based architecture
named DAFOSNET. During training, our model learns a shared and discriminative
embedding space while creating a pseudo open-space decision boundary, given a
fully-supervised source domain and a label-disjoint few-shot target domain. To
enhance data density, we use a pair of conditional adversarial networks with
tunable noise variances to augment both domains closed and pseudo-open spaces.
Furthermore, we propose a domain-specific batch-normalized class prototypes
alignment strategy to align both domains globally while ensuring
class-discriminativeness through novel metric objectives. Our training approach
ensures that DAFOS-NET can generalize well to new scenarios in the target
domain. We present three benchmarks for DA-FSOS based on the Office-Home,
mini-ImageNet/CUB, and DomainNet datasets and demonstrate the efficacy of
DAFOS-NET through extensive experimentation
</p>

### Title: Detect Every Thing with Few Examples. (arXiv:2309.12969v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2309.12969](http://arxiv.org/abs/2309.12969)
* Code URL: [https://github.com/mlzxy/devit](https://github.com/mlzxy/devit)
* Copy Paste: `<input type="checkbox">[[2309.12969] Detect Every Thing with Few Examples](http://arxiv.org/abs/2309.12969) #few-shot`
* Summary: <p>Open-set object detection aims at detecting arbitrary categories beyond those
seen during training. Most recent advancements have adopted the open-vocabulary
paradigm, utilizing vision-language backbones to represent categories with
language. In this paper, we introduce DE-ViT, an open-set object detector that
employs vision-only DINOv2 backbones and learns new categories through example
images instead of language. To improve general detection ability, we transform
multi-classification tasks into binary classification tasks while bypassing
per-class inference, and propose a novel region propagation technique for
localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot
object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the
open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT
surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and
one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary
SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at
https://github.com/mlzxy/devit.
</p>

### Title: HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12481](http://arxiv.org/abs/2309.12481)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12481] HANS, are you clever? Clever Hans Effect Analysis of Neural Systems](http://arxiv.org/abs/2309.12481) #few-shot`
* Summary: <p>Instruction-tuned Large Language Models (It-LLMs) have been exhibiting
outstanding abilities to reason around cognitive states, intentions, and
reactions of all people involved, letting humans guide and comprehend
day-to-day social interactions effectively. In fact, several multiple-choice
questions (MCQ) benchmarks have been proposed to construct solid assessments of
the models' abilities. However, earlier works are demonstrating the presence of
inherent "order bias" in It-LLMs, posing challenges to the appropriate
evaluation. In this paper, we investigate It-LLMs' resilience abilities towards
a series of probing tests using four MCQ benchmarks. Introducing adversarial
examples, we show a significant performance gap, mainly when varying the order
of the choices, which reveals a selection bias and brings into discussion
reasoning abilities. Following a correlation between first positions and model
choices due to positional bias, we hypothesized the presence of structural
heuristics in the decision-making process of the It-LLMs, strengthened by
including significant examples in few-shot scenarios. Finally, by using the
Chain-of-Thought (CoT) technique, we elicit the model to reason and mitigate
the bias by obtaining more robust models.
</p>

### Title: Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models. (arXiv:2309.12940v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2309.12940](http://arxiv.org/abs/2309.12940)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2309.12940] Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models](http://arxiv.org/abs/2309.12940) #few-shot`
* Summary: <p>Task-oriented dialogue (TOD) systems facilitate users in executing various
activities via multi-turn dialogues, but Large Language Models (LLMs) often
struggle to comprehend these intricate contexts. In this study, we propose a
novel "Self-Explanation" prompting strategy to enhance the comprehension
abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires
the model to analyze each dialogue utterance before task execution, thereby
improving performance across various dialogue-centric tasks. Experimental
results from six benchmark datasets confirm that our method consistently
outperforms other zero-shot prompts and matches or exceeds the efficacy of
few-shot prompts, demonstrating its potential as a powerful tool in enhancing
LLMs' comprehension in complex dialogue tasks.
</p>

