## secure
### Title: Ride-Hailing for Autonomous Vehicles: Hyperledger Fabric-Based Secure and Decentralize Blockchain Platform. (arXiv:2207.03525v1 [cs.DC])
* Paper URL: [http://arxiv.org/abs/2207.03525](http://arxiv.org/abs/2207.03525)
* Code URL: null
* Summary: <p>Ride-hailing and ride-sharing applications have recently gained popularity as
a convenient alternative to traditional modes of travel. Current research into
autonomous vehicles is accelerating rapidly and will soon become a critical
component of a ride-hailing platforms architecture. Implementing an autonomous
vehicle ride-hailing platform proves a difficult challenge due to the
centralized nature of traditional ride-hailing architectures. In a traditional
ride-hailing environment the drivers operate their own personal vehicles so it
follows that a fleet of autonomous vehicles would be required for a centralized
ride-hailing platform to succeed. Decentralization of the ride-hailing platform
would remove a roadblock along the way to an autonomous vehicle ride-hailing
platform by allowing owners of autonomous vehicles to add their vehicles to a
community-driven fleet when not in use. Blockchain technology is an attractive
choice for this decentralized architecture due to its immutability and fault
tolerance. This thesis proposes a framework for developing a decentralized
ride-hailing architecture that is verifiably secure. This framework is
implemented on the Hyperledger Fabric blockchain platform. The evaluation of
the implementation is done by applying known security models, utilizing a
static analysis tool, and performing a performance analysis under heavy network
load.
</p>

### Title: Post-quantum hash functions using $\mathrm{SL}_n(\mathbb{F}_p)$. (arXiv:2207.03987v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2207.03987](http://arxiv.org/abs/2207.03987)
* Code URL: null
* Summary: <p>We define new families of Tillich-Z\'emor hash functions, using higher
dimensional special linear groups over finite fields as platforms. The Cayley
graphs of these groups combine fast mixing properties and high girth, which
together give rise to good preimage and collision resistance of the
corresponding hash functions. We justify the claim that the resulting hash
functions are post-quantum secure.
</p>

### Title: A methodology for training homomorphicencryption friendly neural networks. (arXiv:2111.03362v3 [cs.CR] UPDATED)
* Paper URL: [http://arxiv.org/abs/2111.03362](http://arxiv.org/abs/2111.03362)
* Code URL: null
* Summary: <p>Privacy-preserving deep neural network (DNN) inference is a necessity in
different regulated industries such as healthcare, finance and retail.
Recently, homomorphic encryption (HE) has been used as a method to enable
analytics while addressing privacy concerns. HE enables secure predictions over
encrypted data. However, there are several challenges related to the use of HE,
including DNN size limitations and the lack of support for some operation
types. Most notably, the commonly used ReLU activation is not supported under
some HE schemes. We propose a structured methodology to replace ReLU with a
quadratic polynomial activation. To address the accuracy degradation issue, we
use a pre-trained model that trains another HE-friendly model, using techniques
such as trainable activation functions and knowledge distillation. We
demonstrate our methodology on the AlexNet architecture, using the chest X-Ray
and CT datasets for COVID-19 detection. Experiments using our approach reduced
the gap between the F1 score and accuracy of the models trained with ReLU and
the HE-friendly model to within a mere 0.32-5.3 percent degradation. We also
demonstrate our methodology using the SqueezeNet architecture, for which we
observed 7 percent accuracy and F1 improvements over training similar networks
with other HE-friendly training methods.
</p>

### Title: A Survey on DNS Encryption: Current Development, Malware Misuse, and Inference Techniques. (arXiv:2201.00900v2 [cs.CR] UPDATED)
* Paper URL: [http://arxiv.org/abs/2201.00900](http://arxiv.org/abs/2201.00900)
* Code URL: null
* Summary: <p>The domain name system (DNS) that maps alphabetic names to numeric Internet
Protocol (IP) addresses plays a foundational role for Internet communications.
By default, DNS queries and responses are exchanged in unencrypted plaintext,
and hence, can be read and/or hijacked by third parties. To protect user
privacy, the networking community has proposed standard encryption technologies
such as DNS over TLS (DoT), DNS over HTTPS (DoH), and DNS over QUIC (DoQ) for
DNS communications, enabling clients to perform secure and private domain name
lookups. We survey the DNS encryption literature published since 2016, focusing
on its current landscape and how it is misused by malware, and highlighting the
existing techniques developed to make inferences from encrypted DNS traffic.
First, we provide an overview of various standards developed in the space of
DNS encryption and their adoption status, performance, benefits, and security
issues. Second, we highlight ways that various malware families can exploit DNS
encryption to their advantage for botnet communications and/or data
exfiltration. Third, we discuss existing inference methods for profiling normal
patterns and/or detecting malicious encrypted DNS traffic. Several directions
are presented to motivate future research in enhancing the performance and
security of DNS encryption.
</p>

### Title: Secure Joint Communication and Sensing. (arXiv:2202.10790v4 [cs.IT] UPDATED)
* Paper URL: [http://arxiv.org/abs/2202.10790](http://arxiv.org/abs/2202.10790)
* Code URL: null
* Summary: <p>This work considers mitigation of information leakage between communication
and sensing operations in joint communication and sensing systems.
Specifically, a discrete memoryless state-dependent broadcast channel model is
studied in which (i) the presence of feedback enables a transmitter to
simultaneously achieve reliable communication and channel state estimation;
(ii) one of the receivers is treated as an eavesdropper whose state should be
estimated but which should remain oblivious to a part of the transmitted
information. The model abstracts the challenges behind security for joint
communication and sensing if one views the channel state as a characteristic of
the receiver, e.g., its location. For independent identically distributed
(i.i.d.) states, perfect output feedback, and when part of the transmitted
message should be kept secret, a partial characterization of the
secrecy-distortion region is developed. The characterization is exact when the
broadcast channel is either physically-degraded or
reversely-physically-degraded. The characterization is also extended to the
situation in which the entire transmitted message should be kept secret. The
benefits of a joint approach compared to separation-based secure communication
and state-sensing methods are illustrated with a binary joint communication and
sensing model.
</p>

## security
### Title: GaitTAKE: Gait Recognition by Temporal Attention \\and Keypoint-guided Embedding. (arXiv:2207.03608v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03608](http://arxiv.org/abs/2207.03608)
* Code URL: null
* Summary: <p>Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.
</p>

### Title: A Review of Quantum Cybersecurity: Threats, Risks and Opportunities. (arXiv:2207.03534v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2207.03534](http://arxiv.org/abs/2207.03534)
* Code URL: null
* Summary: <p>The promise of quantum computing is not speeding up conventional computing
rather delivering an exponential advantage for certain classes of problems,
with profound implications for cybersecurity for instance. With the advent and
development of quantum computers, cyberspace security can surely become the
most critical problem for the Internet in near future. On contrary, prosaic
quantum technology can be promising to transform cybersecurity. This research
aims to synthesize basic and fundamental studies concerning quantum
cybersecurity that can be emerged both as a threat and solution to critical
cybersecurity issues based on a systematic study. We provide a comprehensive,
illustrative description of the current state-of-the-art quantum computing and
cybersecurity and present the proposed approaches to date. Findings in quantum
computing cybersecurity suggest that quantum computing can be adopted for the
betterment of cybersecurity threats while it poses the most unexpected threats
to cybersecurity. The focus and depth of this systematic survey not only
provide quantum and cybersecurity practitioners and researchers with a
consolidated body of knowledge about current trends in this area but also
underpins a starting point for further research in this field.
</p>

### Title: Active Learning-based Isolation Forest (ALIF): Enhancing Anomaly Detection in Decision Support Systems. (arXiv:2207.03934v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03934](http://arxiv.org/abs/2207.03934)
* Code URL: null
* Summary: <p>The detection of anomalous behaviours is an emerging need in many
applications, particularly in contexts where security and reliability are
critical aspects. While the definition of anomaly strictly depends on the
domain framework, it is often impractical or too time consuming to obtain a
fully labelled dataset. The use of unsupervised models to overcome the lack of
labels often fails to catch domain specific anomalies as they rely on general
definitions of outlier. This paper suggests a new active learning based
approach, ALIF, to solve this problem by reducing the number of required labels
and tuning the detector towards the definition of anomaly provided by the user.
The proposed approach is particularly appealing in the presence of a Decision
Support System (DSS), a case that is increasingly popular in real-world
scenarios. While it is common that DSS embedded with anomaly detection
capabilities rely on unsupervised models, they don't have a way to improve
their performance: ALIF is able to enhance the capabilities of DSS by
exploiting the user feedback during common operations. ALIF is a lightweight
modification of the popular Isolation Forest that proved superior performances
with respect to other state-of-art algorithms in a multitude of real anomaly
detection datasets.
</p>

### Title: GCN-based Multi-task Representation Learning for Anomaly Detection in Attributed Networks. (arXiv:2207.03688v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03688](http://arxiv.org/abs/2207.03688)
* Code URL: null
* Summary: <p>Anomaly detection in attributed networks has received a considerable
attention in recent years due to its applications in a wide range of domains
such as finance, network security, and medicine. Traditional approaches cannot
be adopted on attributed networks' settings to solve the problem of anomaly
detection. The main limitation of such approaches is that they inherently
ignore the relational information between data features. With a rapid explosion
in deep learning- and graph neural networks-based techniques, spotting rare
objects on attributed networks has significantly stepped forward owing to the
potentials of deep techniques in extracting complex relationships. In this
paper, we propose a new architecture on anomaly detection. The main goal of
designing such an architecture is to utilize multi-task learning which would
enhance the detection performance. Multi-task learning-based anomaly detection
is still in its infancy and only a few studies in the existing literature have
catered to the same. We incorporate both community detection and multi-view
representation learning techniques for extracting distinct and complementary
information from attributed networks and subsequently fuse the captured
information for achieving a better detection result. The mutual collaboration
between two main components employed in this architecture, i.e.,
community-specific learning and multi-view representation learning, exhibits a
promising solution to reach more effective results.
</p>

## privacy
### Title: Deepfake Face Traceability with Disentangling Reversing Network. (arXiv:2207.03666v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03666](http://arxiv.org/abs/2207.03666)
* Code URL: null
* Summary: <p>Deepfake face not only violates the privacy of personal identity, but also
confuses the public and causes huge social harm. The current deepfake detection
only stays at the level of distinguishing true and false, and cannot trace the
original genuine face corresponding to the fake face, that is, it does not have
the ability to trace the source of evidence. The deepfake countermeasure
technology for judicial forensics urgently calls for deepfake traceability.
This paper pioneers an interesting question about face deepfake, active
forensics that "know it and how it happened". Given that deepfake faces do not
completely discard the features of original faces, especially facial
expressions and poses, we argue that original faces can be approximately
speculated from their deepfake counterparts. Correspondingly, we design a
disentangling reversing network that decouples latent space features of
deepfake faces under the supervision of fake-original face pair samples to
infer original faces in reverse.
</p>

### Title: Private independence testing across two parties. (arXiv:2207.03652v1 [math.ST])
* Paper URL: [http://arxiv.org/abs/2207.03652](http://arxiv.org/abs/2207.03652)
* Code URL: null
* Summary: <p>We introduce $\pi$-test, a privacy-preserving algorithm for testing
statistical independence between data distributed across multiple parties. Our
algorithm relies on privately estimating the distance correlation between
datasets, a quantitative measure of independence introduced in Sz\'ekely et al.
[2007]. We establish both additive and multiplicative error bounds on the
utility of our differentially private test, which we believe will find
applications in a variety of distributed hypothesis testing settings involving
sensitive data.
</p>

### Title: Frequency-based Randomization for Guaranteeing Differential Privacy in Spatial Trajectories. (arXiv:2207.03722v1 [cs.DB])
* Paper URL: [http://arxiv.org/abs/2207.03722](http://arxiv.org/abs/2207.03722)
* Code URL: null
* Summary: <p>With the popularity of GPS-enabled devices, a huge amount of trajectory data
has been continuously collected and a variety of location-based services have
been developed that greatly benefit our daily life. However, the released
trajectories also bring severe concern about personal privacy, and several
recent studies have demonstrated the existence of personally-identifying
information in spatial trajectories. Trajectory anonymization is nontrivial due
to the trade-off between privacy protection and utility preservation.
Furthermore, recovery attack has not been well studied in the current
literature. To tackle these issues, we propose a frequency-based randomization
model with a rigorous differential privacy guarantee for trajectory data
publishing. In particular, we introduce two randomized mechanisms to perturb
the local/global frequency distributions of significantly important locations
in trajectories by injecting Laplace noise. We design a hierarchical indexing
along with a novel search algorithm to support efficient trajectory
modification, ensuring the modified trajectories satisfy the perturbed
distributions without compromising privacy guarantee or data utility. Extensive
experiments on a real-world trajectory dataset verify the effectiveness of our
approaches in resisting individual re-identification and recovery attacks and
meanwhile preserving desirable data utility as well as the feasibility in
practice.
</p>

### Title: Bistochastic privacy. (arXiv:2207.03940v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2207.03940](http://arxiv.org/abs/2207.03940)
* Code URL: null
* Summary: <p>We introduce a new privacy model relying on bistochastic matrices, that is,
matrices whose components are nonnegative and sum to 1 both row-wise and
column-wise. This class of matrices is used to both define privacy guarantees
and a tool to apply protection on a data set. The bistochasticity assumption
happens to connect several fields of the privacy literature, including the two
most popular models, k-anonymity and differential privacy. Moreover, it
establishes a bridge with information theory, which simplifies the thorny issue
of evaluating the utility of a protected data set. Bistochastic privacy also
clarifies the trade-off between protection and utility by using bits, which can
be viewed as a natural currency to comprehend and operationalize this
trade-off, in the same way than bits are used in information theory to capture
uncertainty. A discussion on the suitable parameterization of bistochastic
matrices to achieve the privacy guarantees of this new model is also provided.
</p>

### Title: Uncertainty-aware Personal Assistant for Making Personalized Privacy Decisions. (arXiv:2205.06544v3 [cs.AI] UPDATED)
* Paper URL: [http://arxiv.org/abs/2205.06544](http://arxiv.org/abs/2205.06544)
* Code URL: null
* Summary: <p>Many software systems, such as online social networks enable users to share
information about themselves. While the action of sharing is simple, it
requires an elaborate thought process on privacy: what to share, with whom to
share, and for what purposes. Thinking about these for each piece of content to
be shared is tedious. Recent approaches to tackle this problem build personal
assistants that can help users by learning what is private over time and
recommending privacy labels such as private or public to individual content
that a user considers sharing. However, privacy is inherently ambiguous and
highly personal. Existing approaches to recommend privacy decisions do not
address these aspects of privacy sufficiently. Ideally, a personal assistant
should be able to adjust its recommendation based on a given user, considering
that user's privacy understanding. Moreover, the personal assistant should be
able to assess when its recommendation would be uncertain and let the user make
the decision on her own. Accordingly, this paper proposes a personal assistant
that uses evidential deep learning to classify content based on its privacy
label. An important characteristic of the personal assistant is that it can
model its uncertainty in its decisions explicitly, determine that it does not
know the answer, and delegate from making a recommendation when its uncertainty
is high. By factoring in the user's own understanding of privacy, such as risk
factors or own labels, the personal assistant can personalize its
recommendations per user. We evaluate our proposed personal assistant using a
well-known data set. Our results show that our personal assistant can
accurately identify uncertain cases, personalize them to its user's needs, and
thus helps users preserve their privacy well.
</p>

### Title: Encoding NetFlows for State-Machine Learning. (arXiv:2207.03890v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03890](http://arxiv.org/abs/2207.03890)
* Code URL: null
* Summary: <p>NetFlow data is a well-known network log format used by many network analysts
and researchers. The advantages of using this format compared to pcap are that
it contains fewer data, is less privacy intrusive, and is easier to collect and
process. However, having less data does mean that this format might not be able
to capture important network behaviour as all information is summarised into
statistics. Much research aims to overcome this disadvantage through the use of
machine learning, for instance, to detect attacks within a network. Many
approaches can be used to pre-process the NetFlow data before it is used to
train the machine learning algorithms. However, many of these approaches simply
apply existing methods to the data, not considering the specific properties of
network data. We argue that for data originating from software systems, such as
NetFlow or software logs, similarities in frequency and contexts of feature
values are more important than similarities in the value itself. In this work,
we, therefore, propose an encoding algorithm that directly takes the frequency
and the context of the feature values into account when the data is being
processed. Different types of network behaviours can be clustered using this
encoding, thus aiding the process of detecting anomalies within the network.
From windows of these clusters obtained from monitoring a clean system, we
learn state machine behavioural models for anomaly detection. These models are
very well-suited to modelling the cyclic and repetitive patterns present in
NetFlow data. We evaluate our encoding on a new dataset that we created for
detecting problems in Kubernetes clusters and on two well-known public NetFlow
datasets. The obtained performance results of the state machine models are
comparable to existing works that use many more features and require both clean
and infected data as training input.
</p>

## protect
### Title: Video-based Smoky Vehicle Detection with A Coarse-to-Fine Framework. (arXiv:2207.03708v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03708](http://arxiv.org/abs/2207.03708)
* Code URL: null
* Summary: <p>Automatic smoky vehicle detection in videos is a superior solution to the
traditional expensive remote sensing one with ultraviolet-infrared light
devices for environmental protection agencies. However, it is challenging to
distinguish vehicle smoke from shadow and wet regions coming from rear vehicle
or clutter roads, and could be worse due to limited annotated data. In this
paper, we first introduce a real-world large-scale smoky vehicle dataset with
75,000 annotated smoky vehicle images, facilitating the effective training of
advanced deep learning models. To enable fair algorithm comparison, we also
build a smoky vehicle video dataset including 163 long videos with
segment-level annotations. Moreover, we present a new Coarse-to-fine Deep Smoky
vehicle detection (CoDeS) framework for efficient smoky vehicle detection. The
CoDeS first leverages a light-weight YOLO detector for fast smoke detection
with high recall rate, and then applies a smoke-vehicle matching strategy to
eliminate non-vehicle smoke, and finally uses a elaborately-designed 3D model
to further refine the results in spatial temporal space. Extensive experiments
in four metrics demonstrate that our framework is significantly superior to
those hand-crafted feature based methods and recent advanced methods. The code
and dataset will be released at https://github.com/pengxj/smokyvehicle.
</p>

### Title: Satellite-based high-resolution maps of cocoa for C\^ote d'Ivoire and Ghana. (arXiv:2206.06119v2 [cs.CV] UPDATED)
* Paper URL: [http://arxiv.org/abs/2206.06119](http://arxiv.org/abs/2206.06119)
* Code URL: null
* Summary: <p>C\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account
for two thirds of the global cocoa production. In both countries, cocoa is the
primary perennial crop, providing income to almost two million farmers. Yet
precise maps of cocoa planted area are missing, hindering accurate
quantification of expansion in protected areas, production and yields, and
limiting information available for improved sustainability governance. Here, we
combine cocoa plantation data with publicly available satellite imagery in a
deep learning framework and create high-resolution maps of cocoa plantations
for both countries, validated in situ. Our results suggest that cocoa
cultivation is an underlying driver of over 37% and 13% of forest loss in
protected areas in C\^ote d'Ivoire and Ghana, respectively, and that official
reports substantially underestimate the planted area, up to 40% in Ghana. These
maps serve as a crucial building block to advance understanding of conservation
and economic development in cocoa producing regions.
</p>

## defense
### Title: Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2207.03574](http://arxiv.org/abs/2207.03574)
* Code URL: null
* Summary: <p>Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagnergroup/demystify-random-transform.
</p>

### Title: Defense Against Multi-target Trojan Attacks. (arXiv:2207.03895v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03895](http://arxiv.org/abs/2207.03895)
* Code URL: null
* Summary: <p>Adversarial attacks on deep learning-based models pose a significant threat
to the current AI infrastructure. Among them, Trojan attacks are the hardest to
defend against. In this paper, we first introduce a variation of the Badnet
kind of attacks that introduces Trojan backdoors to multiple target classes and
allows triggers to be placed anywhere in the image. The former makes it more
potent and the latter makes it extremely easy to carry out the attack in the
physical space. The state-of-the-art Trojan detection methods fail with this
threat model. To defend against this attack, we first introduce a trigger
reverse-engineering mechanism that uses multiple images to recover a variety of
potential triggers. We then propose a detection mechanism by measuring the
transferability of such recovered triggers. A Trojan trigger will have very
high transferability i.e. they make other images also go to the same class. We
study many practical advantages of our attack method and then demonstrate the
detection performance using a variety of image datasets. The experimental
results show the superior detection performance of our method over the
state-of-the-arts.
</p>

### Title: Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)
* Paper URL: [http://arxiv.org/abs/2202.12154](http://arxiv.org/abs/2202.12154)
* Code URL: null
* Summary: <p>Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a single
input-agnostic trigger and targeting only one class to using multiple,
input-specific triggers and targeting multiple classes. However, Trojan
defenses have not caught up with this development. Most defense methods still
make inadequate assumptions about Trojan triggers and target classes, thus, can
be easily circumvented by modern Trojan attacks. To deal with this problem, we
propose two novel "filtering" defenses called Variational Input Filtering (VIF)
and Adversarial Input Filtering (AIF) which leverage lossy data compression and
adversarial learning respectively to effectively purify potential Trojan
triggers in the input at run time without making assumptions about the number
of triggers/target classes or the input dependence property of triggers. In
addition, we introduce a new defense mechanism called
"Filtering-then-Contrasting" (FtC) which helps avoid the drop in classification
accuracy on clean data caused by "filtering", and combine it with VIF/AIF to
derive new defenses of this kind. Extensive experimental results and ablation
studies show that our proposed defenses significantly outperform well-known
baseline defenses in mitigating five advanced Trojan attacks including two
recent state-of-the-art while being quite robust to small amounts of training
data and large-norm triggers.
</p>

## attack
### Title: Neighbors From Hell: Voltage Attacks Against Deep Learning Accelerators on Multi-Tenant FPGAs. (arXiv:2012.07242v2 [cs.CR] UPDATED)
* Paper URL: [http://arxiv.org/abs/2012.07242](http://arxiv.org/abs/2012.07242)
* Code URL: null
* Summary: <p>Field-programmable gate arrays (FPGAs) are becoming widely used accelerators
for a myriad of datacenter applications due to their flexibility and energy
efficiency. Among these applications, FPGAs have shown promising results in
accelerating low-latency real-time deep learning (DL) inference, which is
becoming an indispensable component of many end-user applications. With the
emerging research direction towards virtualized cloud FPGAs that can be shared
by multiple users, the security aspect of FPGA-based DL accelerators requires
careful consideration. In this work, we evaluate the security of DL
accelerators against voltage-based integrity attacks in a multitenant FPGA
scenario. We first demonstrate the feasibility of such attacks on a
state-of-the-art Stratix 10 card using different attacker circuits that are
logically and physically isolated in a separate attacker role, and cannot be
flagged as malicious circuits by conventional bitstream checkers. We show that
aggressive clock gating, an effective power-saving technique, can also be a
potential security threat in modern FPGAs. Then, we carry out the attack on a
DL accelerator running ImageNet classification in the victim role to evaluate
the inherent resilience of DL models against timing faults induced by the
adversary. We find that even when using the strongest attacker circuit, the
prediction accuracy of the DL accelerator is not compromised when running at
its safe operating frequency. Furthermore, we can achieve 1.18-1.31x higher
inference performance by over-clocking the DL accelerator without affecting its
prediction accuracy.
</p>

## robust
### Title: RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments. (arXiv:2207.03539v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03539](http://arxiv.org/abs/2207.03539)
* Code URL: null
* Summary: <p>As a fundamental task for intelligent robots, visual SLAM has made great
progress over the past decades. However, robust SLAM under highly weak-textured
environments still remains very challenging. In this paper, we propose a novel
visual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR
network which is able to produce dense point matching under low-textured scenes
to generate feature descriptors. To integrate the new features into the popular
ORB-SLAM framework, we develop feature masks to filter out the unreliable
features and employ KNN strategy to strengthen the matching robustness. We also
retrained visual vocabulary upon new descriptors for efficient loop closing.
The resulting RWT-SLAM is tested in various public datasets such as TUM and
OpenLORIS, as well as our own data. The results shows very promising
performance under highly weak-textured environments.
</p>

### Title: Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection. (arXiv:2207.03558v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03558](http://arxiv.org/abs/2207.03558)
* Code URL: [https://github.com/jxr326/swinmcnet](https://github.com/jxr326/swinmcnet)
* Summary: <p>RGB-thermal salient object detection (RGB-T SOD) aims to locate the common
prominent objects of an aligned visible and thermal infrared image pair and
accurately segment all the pixels belonging to those objects. It is promising
in challenging scenes such as nighttime and complex backgrounds due to the
insensitivity to lighting conditions of thermal images. Thus, the key problem
of RGB-T SOD is to make the features from the two modalities complement and
adjust each other flexibly, since it is inevitable that any modalities of RGB-T
image pairs failure due to challenging scenes such as extreme light conditions
and thermal crossover. In this paper, we propose a novel mirror complementary
Transformer network (MCNet) for RGB-T SOD. Specifically, we introduce a
Transformer-based feature extraction module to effective extract hierarchical
features of RGB and thermal images. Then, through the attention-based feature
interaction and serial multiscale dilated convolution (SDC) based feature
fusion modules, the proposed model achieves the complementary interaction of
low-level features and the semantic fusion of deep features. Finally, based on
the mirror complementary structure, the salient regions of the two modalities
can be accurately extracted even one modality is invalid. To demonstrate the
robustness of the proposed model under challenging scenes in real world, we
build a novel RGB-T SOD dataset VT723 based on a large public semantic
segmentation RGB-T dataset used in the autonomous driving domain. Expensive
experiments on benchmark and VT723 datasets show that the proposed method
outperforms state-of-the-art approaches, including CNN-based and
Transformer-based methods. The code and dataset will be released later at
https://github.com/jxr326/SwinMCNet.
</p>

### Title: A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm. (arXiv:2207.03638v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03638](http://arxiv.org/abs/2207.03638)
* Code URL: null
* Summary: <p>The tree pruning process is the key to promoting fruits' growth and improving
their productions due to effects on the photosynthesis efficiency of fruits and
nutrition transportation in branches. Currently, pruning is still highly
dependent on human labor. The workers' experience will strongly affect the
robustness of the performance of the tree pruning. Thus, it is a challenge for
workers and farmers to evaluate the pruning performance. Intended for a better
solution to the problem, this paper presents a novel pruning classification
strategy model called "OTSU-SVM" to evaluate the pruning performance based on
the shadows of branches and leaves. This model considers not only the available
illuminated area of the tree but also the uniformity of the illuminated area of
the tree. More importantly, our group implements OTSU algorithm into the model,
which highly reinforces robustness of the evaluation of this model. In
addition, the data from the pear trees in the Yuhang District, Hangzhou is also
used in the experiment. In this experiment, we prove that the OTSU-SVM has good
accuracy with 80% and high performance in the evaluation of the pruning for the
pear trees. It can provide more successful pruning if applied into the orchard.
A successful pruning can broaden the illuminated area of individual fruit, and
increase nutrition transportation from the target branch, dramatically
elevating the weights and production of the fruits.
</p>

### Title: Neural Implicit Dictionary via Mixture-of-Expert Training. (arXiv:2207.03691v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03691](http://arxiv.org/abs/2207.03691)
* Code URL: [https://github.com/vita-group/neural-implicit-dict](https://github.com/vita-group/neural-implicit-dict)
* Summary: <p>Representing visual signals by coordinate-based deep fully-connected networks
has been shown advantageous in fitting complex details and solving inverse
problems than discrete grid-based representation. However, acquiring such a
continuous Implicit Neural Representation (INR) requires tedious per-scene
training on tons of signal measurements, which limits its practicality. In this
paper, we present a generic INR framework that achieves both data and training
efficiency by learning a Neural Implicit Dictionary (NID) from a data
collection and representing INR as a functional combination of basis sampled
from the dictionary. Our NID assembles a group of coordinate-based subnetworks
which are tuned to span the desired function space. After training, one can
instantly and robustly acquire an unseen scene representation by solving the
coding coefficients. To parallelly optimize a large group of networks, we
borrow the idea from Mixture-of-Expert (MoE) to design and train our network
with a sparse gating mechanism. Our experiments show that, NID can improve
reconstruction of 2D images or 3D scenes by 2 orders of magnitude faster with
up to 98% less input data. We further demonstrate various applications of NID
in image inpainting and occlusion removal, which are considered to be
challenging with vanilla INR. Our codes are available in
https://github.com/VITA-Group/Neural-Implicit-Dict.
</p>

### Title: Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation. (arXiv:2207.03714v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03714](http://arxiv.org/abs/2207.03714)
* Code URL: null
* Summary: <p>Sign language is the window for people differently-abled to express their
feelings as well as emotions. However, it remains challenging for people to
learn sign language in a short time. To address this real-world challenge, in
this work, we study the motion transfer system, which can transfer the user
photo to the sign language video of specific words. In particular, the
appearance content of the output video comes from the provided user image,
while the motion of the video is extracted from the specified tutorial video.
We observe two primary limitations in adopting the state-of-the-art motion
transfer methods to sign language generation:(1) Existing motion transfer works
ignore the prior geometrical knowledge of the human body. (2) The previous
image animation methods only take image pairs as input in the training stage,
which could not fully exploit the temporal information within videos. In an
attempt to address the above-mentioned limitations, we propose Structure-aware
Temporal Consistency Network (STCNet) to jointly optimize the prior structure
of human with the temporal consistency for sign language video generation.
There are two main contributions in this paper. (1) We harness a fine-grained
skeleton detector to provide prior knowledge of the body keypoints. In this
way, we ensure the keypoint movement in a valid range and make the model become
more explainable and robust. (2) We introduce two cycle-consistency losses,
i.e., short-term cycle loss and long-term cycle loss, which are conducted to
assure the continuity of the generated video. We optimize the two losses and
keypoint detector network in an end-to-end manner.
</p>

### Title: TGRMPT: A Head-Shoulder Aided Multi-Person Tracker and a New Large-Scale Dataset for Tour-Guide Robot. (arXiv:2207.03726v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03726](http://arxiv.org/abs/2207.03726)
* Code URL: [https://github.com/wenwenzju/TGRMPT](https://github.com/wenwenzju/TGRMPT)
* Summary: <p>A service robot serving safely and politely needs to track the surrounding
people robustly, especially for Tour-Guide Robot (TGR). However, existing
multi-object tracking (MOT) or multi-person tracking (MPT) methods are not
applicable to TGR for the following reasons: 1. lacking relevant large-scale
datasets; 2. lacking applicable metrics to evaluate trackers. In this work, we
target the visual perceptual tasks for TGR and present the TGRDB dataset, a
novel large-scale multi-person tracking dataset containing roughly 5.6 hours of
annotated videos and over 450 long-term trajectories. Besides, we propose a
more applicable metric to evaluate trackers using our dataset. As part of our
work, we present TGRMPT, a novel MPT system that incorporates information from
head shoulder and whole body, and achieves state-of-the-art performance. We
have released our codes and dataset in https://github.com/wenwenzju/TGRMPT.
</p>

### Title: Learning Sequential Descriptors for Sequence-based Visual Place Recognition. (arXiv:2207.03868v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03868](http://arxiv.org/abs/2207.03868)
* Code URL: [https://github.com/vandal-vpr/vg-transformers](https://github.com/vandal-vpr/vg-transformers)
* Summary: <p>In robotics, Visual Place Recognition is a continuous process that receives
as input a video stream to produce a hypothesis of the robot's current position
within a map of known places. This task requires robust, scalable, and
efficient techniques for real applications. This work proposes a detailed
taxonomy of techniques using sequential descriptors, highlighting different
mechanism to fuse the information from the individual images. This
categorization is supported by a complete benchmark of experimental results
that provides evidence on the strengths and weaknesses of these different
architectural choices. In comparison to existing sequential descriptors
methods, we further investigate the viability of Transformers instead of CNN
backbones, and we propose a new ad-hoc sequence-level aggregator called
SeqVLAD, which outperforms prior state of the art on different datasets. The
code is available at https://github.com/vandal-vpr/vg-transformers.
</p>

### Title: RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection. (arXiv:2207.03917v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03917](http://arxiv.org/abs/2207.03917)
* Code URL: null
* Summary: <p>This paper presents a Refinement Pyramid Transformer (RePFormer) for robust
facial landmark detection. Most facial landmark detectors focus on learning
representative image features. However, these CNN-based feature representations
are not robust enough to handle complex real-world scenarios due to ignoring
the internal structure of landmarks, as well as the relations between landmarks
and context. In this work, we formulate the facial landmark detection task as
refining landmark queries along pyramid memories. Specifically, a pyramid
transformer head (PTH) is introduced to build both homologous relations among
landmarks and heterologous relations between landmarks and cross-scale
contexts. Besides, a dynamic landmark refinement (DLR) module is designed to
decompose the landmark regression into an end-to-end refinement procedure,
where the dynamically aggregated queries are transformed to residual
coordinates predictions. Extensive experimental results on four facial landmark
detection benchmarks and their various subsets demonstrate the superior
performance and high robustness of our framework.
</p>

### Title: diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v2 [cs.CV] UPDATED)
* Paper URL: [http://arxiv.org/abs/2111.14658](http://arxiv.org/abs/2111.14658)
* Code URL: [https://github.com/mmmmimic/diffconvnet](https://github.com/mmmmimic/diffconvnet)
* Summary: <p>Standard spatial convolutions assume input data with a regular neighborhood
structure. Existing methods typically generalize convolution to the irregular
point cloud domain by fixing a regular "view" through e.g. a fixed neighborhood
size, where the convolution kernel size remains the same for each point.
However, since point clouds are not as structured as images, the fixed neighbor
number gives an unfortunate inductive bias. We present a novel graph
convolution named Difference Graph Convolution (diffConv), which does not rely
on a regular view. diffConv operates on spatially-varying and density-dilated
neighborhoods, which are further adapted by a learned masked attention
mechanism. Experiments show that our model is very robust to the noise,
obtaining state-of-the-art performance in 3D shape classification and scene
understanding tasks, along with a faster inference speed.
</p>

### Title: Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)
* Paper URL: [http://arxiv.org/abs/2201.03145](http://arxiv.org/abs/2201.03145)
* Code URL: null
* Summary: <p>Images captured in the low-light condition suffer from low visibility and
various imaging artifacts, e.g., real noise. Existing supervised enlightening
algorithms require a large set of pixel-aligned training image pairs, which are
hard to prepare in practice. Though weakly-supervised or unsupervised methods
can alleviate such challenges without using paired training images, some
real-world artifacts inevitably get falsely amplified because of the lack of
corresponded supervision. In this paper, instead of using perfectly aligned
images for training, we creatively employ the misaligned real-world images as
the guidance, which are considerably easier to collect. Specifically, we
propose a Cross-Image Disentanglement Network (CIDN) to separately extract
cross-image brightness and image-specific content features from
low/normal-light images. Based on that, CIDN can simultaneously correct the
brightness and suppress image artifacts in the feature domain, which largely
increases the robustness to the pixel shifts. Furthermore, we collect a new
low-light image enhancement dataset consisting of misaligned training images
with real-world corruptions. Experimental results show that our model achieves
state-of-the-art performances on both the newly proposed dataset and other
popular low-light datasets.
</p>

### Title: How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)
* Paper URL: [http://arxiv.org/abs/2206.13673](http://arxiv.org/abs/2206.13673)
* Code URL: null
* Summary: <p>Event cameras continue to attract interest due to desirable characteristics
such as high dynamic range, low latency, virtually no motion blur, and high
energy efficiency. One of the potential applications that would benefit from
these characteristics lies in visual place recognition for robot localization,
i.e. matching a query observation to the corresponding reference place in the
database. In this letter, we explore the distinctiveness of event streams from
a small subset of pixels (in the tens or hundreds). We demonstrate that the
absolute difference in the number of events at those pixel locations
accumulated into event frames can be sufficient for the place recognition task,
when pixels that display large variations in the reference set are used. Using
such sparse (over image coordinates) but varying (variance over the number of
events per pixel location) pixels enables frequent and computationally cheap
updates of the location estimates. Furthermore, when event frames contain a
constant number of events, our method takes full advantage of the event-driven
nature of the sensory stream and displays promising robustness to changes in
velocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset
in an outdoor driving scenario, as well as the newly contributed indoor
QCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a
mobile robotic platform. Our results show that our approach achieves
competitive performance when compared to several baseline methods on those
datasets, and is particularly well suited for compute- and energy-constrained
platforms such as interplanetary rovers.
</p>

### Title: Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems. (arXiv:2207.03576v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2207.03576](http://arxiv.org/abs/2207.03576)
* Code URL: null
* Summary: <p>Recently, advances in deep learning have been observed in various fields,
including computer vision, natural language processing, and cybersecurity.
Machine learning (ML) has demonstrated its ability as a potential tool for
anomaly detection-based intrusion detection systems to build secure computer
networks. Increasingly, ML approaches are widely adopted than heuristic
approaches for cybersecurity because they learn directly from data. Data is
critical for the development of ML systems, and becomes potential targets for
attackers. Basically, data poisoning or contamination is one of the most common
techniques used to fool ML models through data. This paper evaluates the
robustness of six recent deep learning algorithms for intrusion detection on
contaminated data. Our experiments suggest that the state-of-the-art algorithms
used in this study are sensitive to data contamination and reveal the
importance of self-defense against data perturbation when developing novel
models, especially for intrusion detection systems.
</p>

### Title: CausalAgents: A Robustness Benchmark for Motion Forecasting using Causal Relationships. (arXiv:2207.03586v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03586](http://arxiv.org/abs/2207.03586)
* Code URL: null
* Summary: <p>As machine learning models become increasingly prevalent in motion
forecasting systems for autonomous vehicles (AVs), it is critical that we
ensure that model predictions are safe and reliable. However, exhaustively
collecting and labeling the data necessary to fully test the long tail of rare
and challenging scenarios is difficult and expensive. In this work, we
construct a new benchmark for evaluating and improving model robustness by
applying perturbations to existing data. Specifically, we conduct an extensive
labeling effort to identify causal agents, or agents whose presence influences
human driver behavior in any way, in the Waymo Open Motion Dataset (WOMD), and
we use these labels to perturb the data by deleting non-causal agents from the
scene. We then evaluate a diverse set of state-of-the-art deep-learning model
architectures on our proposed benchmark and find that all models exhibit large
shifts under perturbation. Under non-causal perturbations, we observe a
$25$-$38\%$ relative change in minADE as compared to the original. We then
investigate techniques to improve model robustness, including increasing the
training dataset size and using targeted data augmentations that drop agents
throughout training. We plan to provide the causal agent labels as an
additional attribute to WOMD and release the robustness benchmarks to aid the
community in building more reliable and safe deep-learning models for motion
forecasting.
</p>

### Title: Rich Feature Construction for the Optimization-Generalization Dilemma. (arXiv:2203.15516v2 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2203.15516](http://arxiv.org/abs/2203.15516)
* Code URL: [https://github.com/tjujianyu/rfc](https://github.com/tjujianyu/rfc)
* Summary: <p>There often is a dilemma between ease of optimization and robust
out-of-distribution (OoD) generalization. For instance, many OoD methods rely
on penalty terms whose optimization is challenging. They are either too strong
to optimize reliably or too weak to achieve their goals.
</p>
<p>We propose to initialize the networks with a rich representation containing a
palette of potentially useful features, ready to be used by even simple models.
On the one hand, a rich representation provides a good initialization for the
optimizer. On the other hand, it also provides an inductive bias that helps OoD
generalization. Such a representation is constructed with the Rich Feature
Construction (RFC) algorithm, also called the Bonsai algorithm, which consists
of a succession of training episodes. During discovery episodes, we craft a
multi-objective optimization criterion and its associated datasets in a manner
that prevents the network from using the features constructed in the previous
iterations. During synthesis episodes, we use knowledge distillation to force
the network to simultaneously represent all the previously discovered features.
</p>
<p>Initializing the networks with Bonsai representations consistently helps six
OoD methods achieve top performance on ColoredMNIST benchmark. The same
technique substantially outperforms comparable results on the Wilds Camelyon17
task, eliminates the high result variance that plagues other methods, and makes
hyperparameter tuning and model selection more reliable.
</p>

### Title: Deep Learning to Jointly Schema Match, Impute, and Transform Databases. (arXiv:2207.03536v1 [cs.DB])
* Paper URL: [http://arxiv.org/abs/2207.03536](http://arxiv.org/abs/2207.03536)
* Code URL: null
* Summary: <p>An applied problem facing all areas of data science is harmonizing data
sources. Joining data from multiple origins with unmapped and only partially
overlapping features is a prerequisite to developing and testing robust,
generalizable algorithms, especially in health care. We approach this issue in
the common but difficult case of numeric features such as nearly Gaussian and
binary features, where unit changes and variable shift make simple matching of
univariate summaries unsuccessful. We develop two novel procedures to address
this problem. First, we demonstrate multiple methods of "fingerprinting" a
feature based on its associations to other features. In the setting of even
modest prior information, this allows most shared features to be accurately
identified. Second, we demonstrate a deep learning algorithm for translation
between databases. Unlike prior approaches, our algorithm takes advantage of
discovered mappings while identifying surrogates for unshared features and
learning transformations. In synthetic and real-world experiments using two
electronic health record databases, our algorithms outperform existing
baselines for matching variable sets, while jointly learning to impute unshared
or transformed variables.
</p>

### Title: Product Segmentation Newsvendor Problems: A Robust Learning Approach. (arXiv:2207.03801v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03801](http://arxiv.org/abs/2207.03801)
* Code URL: null
* Summary: <p>We propose and analyze a product segmentation newsvendor problem, which
generalizes the phenomenon of segmentation sales of a class of perishable
items. The product segmentation newsvendor problem is a new variant of the
newsvendor problem, reflecting that sellers maximize profits by determining the
inventory of the whole item in the context of uncertain demand for sub-items.
We derive the closed-form robust ordering decision by assuming that the means
and covariance matrix of stochastic demand are available but not the
distributions. However, robust approaches that always trade-off in the
worst-case demand scenario face a concern in solution conservatism; thus, the
traditional robust schemes offer unsatisfactory. In this paper, we integrate
robust and deep reinforcement learning (DRL) techniques and propose a new
paradigm termed robust learning to increase the attractiveness of robust
policies. Notably, we take the robust decision as human domain knowledge and
implement it into the training process of DRL by designing a full-process
human-machine collaborative mechanism of teaching experience, normative
decision, and regularization return. Simulation results confirm that our
approach effectively improves robust performance and can generalize to various
problems that require robust but less conservative solutions. Simultaneously,
fewer training episodes, increased training stability, and interpretability of
behavior may have the opportunity to facilitate the deployment of DRL
algorithms in operational practice. Furthermore, the successful attempt of
RLDQN to solve the 1000-dimensional demand scenarios reveals that the algorithm
provides a path to solve complex operational problems through human-machine
collaboration and may have potential significance for solving other complex
operational management problems.
</p>

### Title: High Performance Simulation for Scalable Multi-Agent Reinforcement Learning. (arXiv:2207.03945v1 [cs.MA])
* Paper URL: [http://arxiv.org/abs/2207.03945](http://arxiv.org/abs/2207.03945)
* Code URL: null
* Summary: <p>Multi-agent reinforcement learning experiments and open-source training
environments are typically limited in scale, supporting tens or sometimes up to
hundreds of interacting agents. In this paper we demonstrate the use of Vogue,
a high performance agent based model (ABM) framework. Vogue serves as a
multi-agent training environment, supporting thousands to tens of thousands of
interacting agents while maintaining high training throughput by running both
the environment and reinforcement learning (RL) agents on the GPU. High
performance multi-agent environments at this scale have the potential to enable
the learning of robust and flexible policies for use in ABMs and simulations of
complex systems. We demonstrate training performance with two newly developed,
large scale multi-agent training environments. Moreover, we show that these
environments can train shared RL policies on time-scales of minutes and hours.
</p>

### Title: Learning with Muscles: Benefits for Data-Efficiency and Robustness in Anthropomorphic Tasks. (arXiv:2207.03952v1 [cs.RO])
* Paper URL: [http://arxiv.org/abs/2207.03952](http://arxiv.org/abs/2207.03952)
* Code URL: null
* Summary: <p>Humans are able to outperform robots in terms of robustness, versatility, and
learning of new tasks in a wide variety of movements. We hypothesize that
highly nonlinear muscle dynamics play a large role in providing inherent
stability, which is favorable to learning. While recent advances have been made
in applying modern learning techniques to muscle-actuated systems both in
simulation as well as in robotics, so far, no detailed analysis has been
performed to show the benefits of muscles in this setting. Our study closes
this gap by investigating core robotics challenges and comparing the
performance of different actuator morphologies in terms of data-efficiency,
hyperparameter sensitivity, and robustness.
</p>

### Title: Distributed Saddle-Point Problems: Lower Bounds, Near-Optimal and Robust Algorithms. (arXiv:2010.13112v8 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2010.13112](http://arxiv.org/abs/2010.13112)
* Code URL: null
* Summary: <p>This paper focuses on the distributed optimization of stochastic saddle point
problems. The first part of the paper is devoted to lower bounds for the
cenralized and decentralized distributed methods for smooth (strongly)
convex-(strongly) concave saddle-point problems as well as the near-optimal
algorithms by which these bounds are achieved. Next, we present a new federated
algorithm for cenralized distributed saddle point problems - Extra Step Local
SGD. Theoretical analysis of the new method is carried out for strongly
convex-strongly concave and non-convex-non-concave problems. In the
experimental part of the paper, we show the effectiveness of our method in
practice. In particular, we train GANs in a distributed manner.
</p>

### Title: Evaluating Causal Inference Methods. (arXiv:2202.04208v3 [stat.ME] UPDATED)
* Paper URL: [http://arxiv.org/abs/2202.04208](http://arxiv.org/abs/2202.04208)
* Code URL: null
* Summary: <p>The fundamental challenge of drawing causal inference is that counterfactual
outcomes are not fully observed for any unit. Furthermore, in observational
studies, treatment assignment is likely to be confounded. Many statistical
methods have emerged for causal inference under unconfoundedness conditions
given pre-treatment covariates, including propensity score-based methods,
prognostic score-based methods, and doubly robust methods. Unfortunately for
applied researchers, there is no `one-size-fits-all' causal method that can
perform optimally universally. In practice, causal methods are primarily
evaluated quantitatively on handcrafted simulated data. Such data-generative
procedures can be of limited value because they are typically stylized models
of reality. They are simplified for tractability and lack the complexities of
real-world data. For applied researchers, it is critical to understand how well
a method performs for the data at hand. Our work introduces a deep generative
model-based framework, Credence, to validate causal inference methods. The
framework's novelty stems from its ability to generate synthetic data anchored
at the empirical distribution for the observed sample, and therefore virtually
indistinguishable from the latter. The approach allows the user to specify
ground truth for the form and magnitude of causal effects and confounding bias
as functions of covariates. Thus simulated data sets are used to evaluate the
potential performance of various causal estimation methods when applied to data
similar to the observed sample. We demonstrate Credence's ability to accurately
assess the relative performance of causal estimation techniques in an extensive
simulation study and two real-world data applications from Lalonde and Project
STAR studies.
</p>

### Title: Neuro-Inspired Deep Neural Networks with Sparse, Strong Activations. (arXiv:2202.13074v3 [cs.NE] UPDATED)
* Paper URL: [http://arxiv.org/abs/2202.13074](http://arxiv.org/abs/2202.13074)
* Code URL: [https://github.com/metehancekic/sparsestrongactivations](https://github.com/metehancekic/sparsestrongactivations)
* Summary: <p>While end-to-end training of Deep Neural Networks (DNNs) yields state of the
art performance in an increasing array of applications, it does not provide
insight into, or control over, the features being extracted. We report here on
a promising neuro-inspired approach to DNNs with sparser and stronger
activations. We use standard stochastic gradient training, supplementing the
end-to-end discriminative cost function with layer-wise costs promoting Hebbian
("fire together," "wire together") updates for highly active neurons, and
anti-Hebbian updates for the remaining neurons. Instead of batch norm, we use
divisive normalization of activations (suppressing weak outputs using strong
outputs), along with implicit $\ell_2$ normalization of neuronal weights.
Experiments with standard image classification tasks on CIFAR-10 demonstrate
that, relative to baseline end-to-end trained architectures, our proposed
architecture (a) leads to sparser activations (with only a slight compromise on
accuracy), (b) exhibits more robustness to noise (without being trained on
noisy data), (c) exhibits more robustness to adversarial perturbations (without
adversarial training).
</p>

### Title: GraphWorld: Fake Graphs Bring Real Insights for GNNs. (arXiv:2203.00112v2 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2203.00112](http://arxiv.org/abs/2203.00112)
* Code URL: [https://github.com/google-research/graphworld](https://github.com/google-research/graphworld)
* Summary: <p>Despite advances in the field of Graph Neural Networks (GNNs), only a small
number (~5) of datasets are currently used to evaluate new models. This
continued reliance on a handful of datasets provides minimal insight into the
performance differences between models, and is especially challenging for
industrial practitioners who are likely to have datasets which look very
different from those used as academic benchmarks. In the course of our work on
GNN infrastructure and open-source software at Google, we have sought to
develop improved benchmarks that are robust, tunable, scalable,and
generalizable. In this work we introduce GraphWorld, a novel methodology and
system for benchmarking GNN models on an arbitrarily-large population of
synthetic graphs for any conceivable GNN task. GraphWorld allows a user to
efficiently generate a world with millions of statistically diverse datasets.
It is accessible, scalable, and easy to use. GraphWorld can be run on a single
machine without specialized hardware, or it can be easily scaled up to run on
arbitrary clusters or cloud frameworks. Using GraphWorld, a user has
fine-grained control over graph generator parameters, and can benchmark
arbitrary GNN models with built-in hyperparameter tuning. We present insights
from GraphWorld experiments regarding the performance characteristics of tens
of thousands of GNN models over millions of benchmark datasets. We further show
that GraphWorld efficiently explores regions of benchmark dataset space
uncovered by standard benchmarks, revealing comparisons between models that
have not been historically obtainable. Using GraphWorld, we also are able to
study in-detail the relationship between graph properties and task performance
metrics, which is nearly impossible with the classic collection of real-world
benchmarks.
</p>

### Title: Your Policy Regularizer is Secretly an Adversary. (arXiv:2203.12592v4 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2203.12592](http://arxiv.org/abs/2203.12592)
* Code URL: null
* Summary: <p>Policy regularization methods such as maximum entropy regularization are
widely used in reinforcement learning to improve the robustness of a learned
policy. In this paper, we show how this robustness arises from hedging against
worst-case perturbations of the reward function, which are chosen from a
limited set by an imagined adversary. Using convex duality, we characterize
this robust set of adversarial reward perturbations under KL and
alpha-divergence regularization, which includes Shannon and Tsallis entropy
regularization as special cases. Importantly, generalization guarantees can be
given within this robust set. We provide detailed discussion of the worst-case
reward perturbations, and present intuitive empirical examples to illustrate
this robustness and its relationship with generalization. Finally, we discuss
how our analysis complements and extends previous results on adversarial reward
robustness and path consistency optimality conditions.
</p>

### Title: Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v8 [cs.RO] UPDATED)
* Paper URL: [http://arxiv.org/abs/2206.10397](http://arxiv.org/abs/2206.10397)
* Code URL: [https://github.com/rcl-nus/neuromhe](https://github.com/rcl-nus/neuromhe)
* Summary: <p>Estimating and reacting to external disturbances is crucial for robust flight
control of quadrotors. Existing estimators typically require significant tuning
for a specific flight scenario or training with extensive ground-truth
disturbance data to achieve satisfactory performance. In this paper, we propose
a neural moving horizon estimator (NeuroMHE) that can automatically tune the
MHE parameters modeled by a neural network and adapt to different flight
scenarios. We achieve this by deriving the analytical gradients of the MHE
estimates with respect to the tuning parameters, which enable a seamless
embedding of an MHE as a learnable layer into the neural network for highly
effective learning. Most interestingly, we show that the gradients can be
obtained efficiently from a Kalman filter in a recursive form. Moreover, we
develop a model-based policy gradient algorithm to train NeuroMHE directly from
the trajectory tracking error without the need for the ground-truth disturbance
data. The effectiveness of NeuroMHE is verified extensively via both
simulations and physical experiments on a quadrotor in various challenging
flights. Notably, NeuroMHE outperforms the state-of-the-art estimator with
force estimation error reductions of up to 49.4% by using only a 2.5% amount of
the neural network parameters. The proposed method is general and can be
applied to robust adaptive control for other robotic systems.
</p>

## biometric
## steal
## extraction
### Title: Detection of Furigana Text in Images. (arXiv:2207.03960v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2207.03960](http://arxiv.org/abs/2207.03960)
* Code URL: [https://github.com/nikolajkb/furiganadetection](https://github.com/nikolajkb/furiganadetection)
* Summary: <p>Furigana are pronunciation notes used in Japanese writing. Being able to
detect these can help improve optical character recognition (OCR) performance
or make more accurate digital copies of Japanese written media by correctly
displaying furigana. This project focuses on detecting furigana in Japanese
books and comics. While there has been research into the detection of Japanese
text in general, there are currently no proposed methods for detecting
furigana.
</p>
<p>We construct a new dataset containing Japanese written media and annotations
of furigana. We propose an evaluation metric for such data which is similar to
the evaluation protocols used in object detection except that it allows groups
of objects to be labeled by one annotation. We propose a method for detection
of furigana that is based on mathematical morphology and connected component
analysis. We evaluate the detections of the dataset and compare different
methods for text extraction. We also evaluate different types of images such as
books and comics individually and discuss the challenges of each type of image.
</p>
<p>The proposed method reaches an F1-score of 76\% on the dataset. The method
performs well on regular books, but less so on comics, and books of irregular
format. Finally, we show that the proposed method can improve the performance
of OCR by 5\% on the manga109 dataset.
</p>
<p>Source code is available via
\texttt{\url{https://github.com/nikolajkb/FuriganaDetection}}
</p>

### Title: Quote Erat Demonstrandum: A Web Interface for Exploring the Quotebank Corpus. (arXiv:2207.03592v1 [cs.IR])
* Paper URL: [http://arxiv.org/abs/2207.03592](http://arxiv.org/abs/2207.03592)
* Code URL: null
* Summary: <p>The use of attributed quotes is the most direct and least filtered pathway of
information propagation in news. Consequently, quotes play a central role in
the conception, reception, and analysis of news stories. Since quotes provide a
more direct window into a speaker's mind than regular reporting, they are a
valuable resource for journalists and researchers alike. While substantial
research efforts have been devoted to methods for the automated extraction of
quotes from news and their attribution to speakers, few comprehensive corpora
of attributed quotes from contemporary sources are available to the public.
Here, we present an adaptive web interface for searching Quotebank, a massive
collection of quotes from the news, which we make available at
https://quotebank.dlab.tools.
</p>

### Title: SETSum: Summarization and Visualization of Student Evaluations of Teaching. (arXiv:2207.03640v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2207.03640](http://arxiv.org/abs/2207.03640)
* Code URL: [https://github.com/evahuyn/setsum](https://github.com/evahuyn/setsum)
* Summary: <p>Student Evaluations of Teaching (SETs) are widely used in colleges and
universities. Typically SET results are summarized for instructors in a static
PDF report. The report often includes summary statistics for quantitative
ratings and an unsorted list of open-ended student comments. The lack of
organization and summarization of the raw comments hinders those interpreting
the reports from fully utilizing informative feedback, making accurate
inferences, and designing appropriate instructional improvements. In this work,
we introduce a novel system, SETSum, that leverages sentiment analysis, aspect
extraction, summarization, and visualization techniques to provide organized
illustrations of SET findings to instructors and other reviewers. Ten
university professors from diverse departments serve as evaluators of the
system and all agree that SETSum helps them interpret SET results more
efficiently; and 6 out of 10 instructors prefer our system over the standard
static PDF report (while the remaining 4 would like to have both). This
demonstrates that our work holds the potential to reform the SET reporting
conventions in the future. Our code is available at
https://github.com/evahuyn/SETSum
</p>

### Title: Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base. (arXiv:2207.03680v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2207.03680](http://arxiv.org/abs/2207.03680)
* Code URL: [https://github.com/aozmh/crake](https://github.com/aozmh/crake)
* Summary: <p>Semantic parsing solves knowledge base (KB) question answering (KBQA) by
composing a KB query, which generally involves node extraction (NE) and graph
composition (GC) to detect and connect related nodes in a query. Despite the
strong causal effects between NE and GC, previous works fail to directly model
such causalities in their pipeline, hindering the learning of subtask
correlations. Also, the sequence-generation process for GC in previous works
induces ambiguity and exposure bias, which further harms accuracy. In this
work, we formalize semantic parsing into two stages. In the first stage (graph
structure generation), we propose a causal-enhanced table-filler to overcome
the issues in sequence-modelling and to learn the internal causalities. In the
second stage (relation extraction), an efficient beam-search algorithm is
presented to scale complex queries on large-scale KBs. Experiments on LC-QuAD
1.0 indicate that our method surpasses previous state-of-the-arts by a large
margin (17%) while remaining time and space efficiency. The code and models are
available at https://github.com/AOZMH/Crake.
</p>

### Title: A Medical Information Extraction Workbench to Process German Clinical Text. (arXiv:2207.03885v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2207.03885](http://arxiv.org/abs/2207.03885)
* Code URL: null
* Summary: <p>Background: In the information extraction and natural language processing
domain, accessible datasets are crucial to reproduce and compare results.
Publicly available implementations and tools can serve as benchmark and
facilitate the development of more complex applications. However, in the
context of clinical text processing the number of accessible datasets is scarce
-- and so is the number of existing tools. One of the main reasons is the
sensitivity of the data. This problem is even more evident for non-English
languages.
</p>
<p>Approach: In order to address this situation, we introduce a workbench: a
collection of German clinical text processing models. The models are trained on
a de-identified corpus of German nephrology reports.
</p>
<p>Result: The presented models provide promising results on in-domain data.
Moreover, we show that our models can be also successfully applied to other
biomedical text in German. Our workbench is made publicly available so it can
be used out of the box, as a benchmark or transferred to related problems.
</p>

### Title: Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning. (arXiv:2205.00731v2 [cs.CL] UPDATED)
* Paper URL: [http://arxiv.org/abs/2205.00731](http://arxiv.org/abs/2205.00731)
* Code URL: [https://github.com/xufangzhi/logiformer](https://github.com/xufangzhi/logiformer)
* Summary: <p>Machine reading comprehension has aroused wide concerns, since it explores
the potential of model for text understanding. To further equip the machine
with the reasoning capability, the challenging task of logical reasoning is
proposed. Previous works on logical reasoning have proposed some strategies to
extract the logical units from different aspects. However, there still remains
a challenge to model the long distance dependency among the logical units.
Also, it is demanding to uncover the logical structures of the text and further
fuse the discrete logic to the continuous text embedding. To tackle the above
issues, we propose an end-to-end model Logiformer which utilizes a two-branch
graph transformer network for logical reasoning of text. Firstly, we introduce
different extraction strategies to split the text into two sets of logical
units, and construct the logical graph and the syntax graph respectively. The
logical graph models the causal relations for the logical branch while the
syntax graph captures the co-occurrence relations for the syntax branch.
Secondly, to model the long distance dependency, the node sequence from each
graph is fed into the fully connected graph transformer structures. The two
adjacent matrices are viewed as the attention biases for the graph transformer
layers, which map the discrete logical structures to the continuous text
embedding space. Thirdly, a dynamic gate mechanism and a question-aware
self-attention module are introduced before the answer prediction to update the
features. The reasoning process provides the interpretability by employing the
logical units, which are consistent with human cognition. The experimental
results show the superiority of our model, which outperforms the
state-of-the-art single model on two logical reasoning benchmarks.
</p>

### Title: Healthcare Knowledge Graph Construction: State-of-the-art, open issues, and opportunities. (arXiv:2207.03771v1 [cs.AI])
* Paper URL: [http://arxiv.org/abs/2207.03771](http://arxiv.org/abs/2207.03771)
* Code URL: null
* Summary: <p>The incorporation of data analytics in the healthcare industry has made
significant progress, driven by the demand for efficient and effective big data
analytics solutions. Knowledge graphs (KGs) have proven utility in this arena
and are rooted in a number of healthcare applications to furnish better data
representation and knowledge inference. However, in conjunction with a lack of
a representative KG construction taxonomy, several existing approaches in this
designated domain are inadequate and inferior. This paper is the first to
provide a comprehensive taxonomy and a bird's eye view of healthcare KG
construction. Additionally, a thorough examination of the current
state-of-the-art techniques drawn from academic works relevant to various
healthcare contexts is carried out. These techniques are critically evaluated
in terms of methods used for knowledge extraction, types of the knowledge base
and sources, and the incorporated evaluation protocols. Finally, several
research findings and existing issues in the literature are reported and
discussed, opening horizons for future research in this vibrant area.
</p>

### Title: Lessons from Deep Learning applied to Scholarly Information Extraction: What Works, What Doesn't, and Future Directions. (arXiv:2207.04029v1 [cs.IR])
* Paper URL: [http://arxiv.org/abs/2207.04029](http://arxiv.org/abs/2207.04029)
* Code URL: null
* Summary: <p>Understanding key insights from full-text scholarly articles is essential as
it enables us to determine interesting trends, give insight into the research
and development, and build knowledge graphs. However, some of the interesting
key insights are only available when considering full-text. Although
researchers have made significant progress in information extraction from short
documents, extraction of scientific entities from full-text scholarly
literature remains a challenging problem. This work presents an automated
End-to-end Research Entity Extractor called EneRex to extract technical facets
such as dataset usage, objective task, method from full-text scholarly research
articles. Additionally, we extracted three novel facets, e.g., links to source
code, computing resources, programming language/libraries from full-text
articles. We demonstrate how EneRex is able to extract key insights and trends
from a large-scale dataset in the domain of computer science. We further test
our pipeline on multiple datasets and found that the EneRex improves upon a
state of the art model. We highlight how the existing datasets are limited in
their capacity and how EneRex may fit into an existing knowledge graph. We also
present a detailed discussion with pointers for future research. Our code and
data are publicly available at
https://github.com/DiscoveryAnalyticsCenter/EneRex.
</p>

## membership infer
## federate
### Title: AVDDPG: Federated reinforcement learning applied to autonomous platoon control. (arXiv:2207.03484v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03484](http://arxiv.org/abs/2207.03484)
* Code URL: null
* Summary: <p>Since 2016 federated learning (FL) has been an evolving topic of discussion
in the artificial intelligence (AI) research community. Applications of FL led
to the development and study of federated reinforcement learning (FRL). Few
works exist on the topic of FRL applied to autonomous vehicle (AV) platoons. In
addition, most FRL works choose a single aggregation method (usually weight or
gradient aggregation). We explore FRL's effectiveness as a means to improve AV
platooning by designing and implementing an FRL framework atop a custom AV
platoon environment. The application of FRL in AV platooning is studied under
two scenarios: (1) Inter-platoon FRL (Inter-FRL) where FRL is applied to AVs
across different platoons; (2) Intra-platoon FRL (Intra-FRL) where FRL is
applied to AVs within a single platoon. Both Inter-FRL and Intra-FRL are
applied to a custom AV platooning environment using both gradient and weight
aggregation to observe the performance effects FRL can have on AV platoons
relative to an AV platooning environment trained without FRL. It is concluded
that Intra-FRL using weight aggregation (Intra-FRLWA) provides the best
performance for controlling an AV platoon. In addition, we found that weight
aggregation in FRL for AV platooning provides increases in performance relative
to gradient aggregation. Finally, a performance analysis is conducted for
Intra-FRLWA versus a platooning environment without FRL for platoons of length
3, 4 and 5 vehicles. It is concluded that Intra-FRLWA largely out-performs the
platooning environment that is trained without FRL.
</p>

### Title: A Survey on Participant Selection for Federated Learning in Mobile Networks. (arXiv:2207.03681v1 [cs.DC])
* Paper URL: [http://arxiv.org/abs/2207.03681](http://arxiv.org/abs/2207.03681)
* Code URL: null
* Summary: <p>Federated Learning (FL) is an efficient distributed machine learning paradigm
that employs private datasets in a privacy-preserving manner. The main
challenges of FL is that end devices usually possess various computation and
communication capabilities and their training data are not independent and
identically distributed (non-IID). Due to limited communication bandwidth and
unstable availability of such devices in a mobile network, only a fraction of
end devices (also referred to as the participants or clients in a FL process)
can be selected in each round. Hence, it is of paramount importance to utilize
an efficient participant selection scheme to maximize the performance of FL
including final model accuracy and training time. In this paper, we provide a
review of participant selection techniques for FL. First, we introduce FL and
highlight the main challenges during participant selection. Then, we review the
existing studies and categorize them based on their solutions. Finally, we
provide some future directions on participant selection for FL based on our
analysis of the state-of-the-art in this topic area.
</p>

### Title: Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with Inexact Prox. (arXiv:2207.03957v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03957](http://arxiv.org/abs/2207.03957)
* Code URL: null
* Summary: <p>Inspired by a recent breakthrough of Mishchenko et al (2022), who for the
first time showed that local gradient steps can lead to provable communication
acceleration, we propose an alternative algorithm which obtains the same
communication acceleration as their method (ProxSkip). Our approach is very
different, however: it is based on the celebrated method of Chambolle and Pock
(2011), with several nontrivial modifications: i) we allow for an inexact
computation of the prox operator of a certain smooth strongly convex function
via a suitable gradient-based method (e.g., GD, Fast GD or FSFOM), ii) we
perform a careful modification of the dual update step in order to retain
linear convergence. Our general results offer the new state-of-the-art rates
for the class of strongly convex-concave saddle-point problems with bilinear
coupling characterized by the absence of smoothness in the dual function. When
applied to federated learning, we obtain a theoretically better alternative to
ProxSkip: our method requires fewer local steps ($O(\kappa^{1/3})$ or
$O(\kappa^{1/4})$, compared to $O(\kappa^{1/2})$ of ProxSkip), and performs a
deterministic number of local steps instead. Like ProxSkip, our method can be
applied to optimization over a connected network, and we obtain theoretical
improvements here as well.
</p>

### Title: Architecture Agnostic Federated Learning for Neural Networks. (arXiv:2202.07757v3 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2202.07757](http://arxiv.org/abs/2202.07757)
* Code URL: null
* Summary: <p>With growing concerns regarding data privacy and rapid increase in data
volume, Federated Learning(FL) has become an important learning paradigm.
However, jointly learning a deep neural network model in a FL setting proves to
be a non-trivial task because of the complexities associated with the neural
networks, such as varied architectures across clients, permutation invariance
of the neurons, and presence of non-linear transformations in each layer. This
work introduces a novel Federated Heterogeneous Neural Networks (FedHeNN)
framework that allows each client to build a personalised model without
enforcing a common architecture across clients. This allows each client to
optimize with respect to local data and compute constraints, while still
benefiting from the learnings of other (potentially more powerful) clients. The
key idea of FedHeNN is to use the instance-level representations obtained from
peer clients to guide the simultaneous training on each client. The extensive
experimental results demonstrate that the FedHeNN framework is capable of
learning better performing models on clients in both the settings of
homogeneous and heterogeneous architectures across clients.
</p>

## fair
### Title: An Approach to Ensure Fairness in News Articles. (arXiv:2207.03938v1 [cs.IR])
* Paper URL: [http://arxiv.org/abs/2207.03938](http://arxiv.org/abs/2207.03938)
* Code URL: null
* Summary: <p>Recommender systems, information retrieval, and other information access
systems present unique challenges for examining and applying concepts of
fairness and bias mitigation in unstructured text. This paper introduces Dbias,
which is a Python package to ensure fairness in news articles. Dbias is a
trained Machine Learning (ML) pipeline that can take a text (e.g., a paragraph
or news story) and detects if the text is biased or not. Then, it detects the
biased words in the text, masks them, and recommends a set of sentences with
new words that are bias-free or at least less biased. We incorporate the
elements of data science best practices to ensure that this pipeline is
reproducible and usable. We show in experiments that this pipeline can be
effective for mitigating biases and outperforms the common neural network
architectures in ensuring fairness in the news articles.
</p>

### Title: Individual Preference Stability for Clustering. (arXiv:2207.03600v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03600](http://arxiv.org/abs/2207.03600)
* Code URL: [https://github.com/amazon-research/ip-stability-for-clustering](https://github.com/amazon-research/ip-stability-for-clustering)
* Summary: <p>In this paper, we propose a natural notion of individual preference (IP)
stability for clustering, which asks that every data point, on average, is
closer to the points in its own cluster than to the points in any other
cluster. Our notion can be motivated from several perspectives, including game
theory and algorithmic fairness. We study several questions related to our
proposed notion. We first show that deciding whether a given data set allows
for an IP-stable clustering in general is NP-hard. As a result, we explore the
design of efficient algorithms for finding IP-stable clusterings in some
restricted metric spaces. We present a polytime algorithm to find a clustering
satisfying exact IP-stability on the real line, and an efficient algorithm to
find an IP-stable 2-clustering for a tree metric. We also consider relaxing the
stability constraint, i.e., every data point should not be too far from its own
cluster compared to any other cluster. For this case, we provide polytime
algorithms with different guarantees. We evaluate some of our algorithms and
several standard clustering approaches on real data sets.
</p>

### Title: Fair Exploration via Axiomatic Bargaining. (arXiv:2106.02553v2 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2106.02553](http://arxiv.org/abs/2106.02553)
* Code URL: null
* Summary: <p>Exploration is often necessary in online learning to maximize long-term
reward, but it comes at the cost of short-term 'regret'. We study how this cost
of exploration is shared across multiple groups. For example, in a clinical
trial setting, patients who are assigned a sub-optimal treatment effectively
incur the cost of exploration. When patients are associated with natural groups
on the basis of, say, race or age, it is natural to ask whether the cost of
exploration borne by any single group is 'fair'. So motivated, we introduce the
'grouped' bandit model. We leverage the theory of axiomatic bargaining, and the
Nash bargaining solution in particular, to formalize what might constitute a
fair division of the cost of exploration across groups. On the one hand, we
show that any regret-optimal policy strikingly results in the least fair
outcome: such policies will perversely leverage the most 'disadvantaged' groups
when they can. More constructively, we derive policies that are optimally fair
and simultaneously enjoy a small 'price of fairness'. We illustrate the
relative merits of our algorithmic framework with a case study on contextual
bandits for warfarin dosing where we are concerned with the cost of exploration
across multiple races and age groups.
</p>

### Title: Flexible Group Fairness Metrics for Survival Analysis. (arXiv:2206.03256v2 [cs.CY] UPDATED)
* Paper URL: [http://arxiv.org/abs/2206.03256](http://arxiv.org/abs/2206.03256)
* Code URL: [https://github.com/vollmer-lab/survival_fairness](https://github.com/vollmer-lab/survival_fairness)
* Summary: <p>Algorithmic fairness is an increasingly important field concerned with
detecting and mitigating biases in machine learning models. There has been a
wealth of literature for algorithmic fairness in regression and classification
however there has been little exploration of the field for survival analysis.
Survival analysis is the prediction task in which one attempts to predict the
probability of an event occurring over time. Survival predictions are
particularly important in sensitive settings such as when utilising machine
learning for diagnosis and prognosis of patients. In this paper we explore how
to utilise existing survival metrics to measure bias with group fairness
metrics. We explore this in an empirical experiment with 29 survival datasets
and 8 measures. We find that measures of discrimination are able to capture
bias well whereas there is less clarity with measures of calibration and
scoring rules. We suggest further areas for research including prediction-based
fairness metrics for distribution predictions.
</p>

## interpretability
### Title: LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v2 [cs.CV] UPDATED)
* Paper URL: [http://arxiv.org/abs/2106.12102](http://arxiv.org/abs/2106.12102)
* Code URL: [https://github.com/faridyagubbayli/LegoFormer](https://github.com/faridyagubbayli/LegoFormer)
* Summary: <p>Most modern deep learning-based multi-view 3D reconstruction techniques use
RNNs or fusion modules to combine information from multiple images after
independently encoding them. These two separate steps have loose connections
and do not allow easy information sharing among views. We propose LegoFormer, a
transformer model for voxel-based 3D reconstruction that uses the attention
layers to share information among views during all computational stages.
Moreover, instead of predicting each voxel independently, we propose to
parametrize the output with a series of low-rank decomposition factors. This
reformulation allows the prediction of an object as a set of independent
regular structures then aggregated to obtain the final reconstruction.
Experiments conducted on ShapeNet demonstrate the competitive performance of
our model with respect to the state of the art while having increased
interpretability thanks to the self-attention layers. We also show promising
generalization results to real data.
</p>

### Title: Hidden Schema Networks. (arXiv:2207.03777v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2207.03777](http://arxiv.org/abs/2207.03777)
* Code URL: [https://github.com/ramsesjsf/hiddenschemanetworks](https://github.com/ramsesjsf/hiddenschemanetworks)
* Summary: <p>Most modern language models infer representations that, albeit powerful, lack
both compositionality and semantic interpretability. Starting from the
assumption that a large proportion of semantic content is necessarily
relational, we introduce a neural language model that discovers networks of
symbols (schemata) from text datasets. Using a variational autoencoder (VAE)
framework, our model encodes sentences into sequences of symbols (composed
representation), which correspond to the nodes visited by biased random walkers
on a global latent graph. Sentences are then generated back, conditioned on the
selected symbol sequences. We first demonstrate that the model is able to
uncover ground-truth graphs from artificially generated datasets of random
token sequences. Next we leverage pretrained BERT and GPT-2 language models as
encoder and decoder, respectively, to train our model on language modelling
tasks. Qualitatively, our results show that the model is able to infer schema
networks encoding different aspects of natural language. Quantitatively, the
model achieves state-of-the-art scores on VAE language modeling benchmarks.
Source code to reproduce our experiments is available at
https://github.com/ramsesjsf/HiddenSchemaNetworks
</p>

### Title: Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning. (arXiv:2207.03902v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03902](http://arxiv.org/abs/2207.03902)
* Code URL: null
* Summary: <p>Deep cooperative multi-agent reinforcement learning has demonstrated its
remarkable success over a wide spectrum of complex control tasks. However,
recent advances in multi-agent learning mainly focus on value decomposition
while leaving entity interactions still intertwined, which easily leads to
over-fitting on noisy interactions between entities. In this work, we introduce
a novel interactiOn Pattern disenTangling (OPT) method, to disentangle not only
the joint value function into agent-wise value functions for decentralized
execution, but also the entity interactions into interaction prototypes, each
of which represents an underlying interaction pattern within a sub-group of the
entities. OPT facilitates filtering the noisy interactions between irrelevant
entities and thus significantly improves generalizability as well as
interpretability. Specifically, OPT introduces a sparse disagreement mechanism
to encourage sparsity and diversity among discovered interaction prototypes.
Then the model selectively restructures these prototypes into a compact
interaction pattern by an aggregator with learnable weights. To alleviate the
training instability issue caused by partial observability, we propose to
maximize the mutual information between the aggregation weights and the history
behaviors of each agent. Experiments on both single-task and multi-task
benchmarks demonstrate that the proposed method yields results superior to the
state-of-the-art counterparts. Our code will be made publicly available.
</p>

### Title: Towards Semantic Communication Protocols: A Probabilistic Logic Perspective. (arXiv:2207.03920v1 [cs.IT])
* Paper URL: [http://arxiv.org/abs/2207.03920](http://arxiv.org/abs/2207.03920)
* Code URL: null
* Summary: <p>Classical medium access control (MAC) protocols are interpretable, yet their
task-agnostic control signaling messages (CMs) are ill-suited for emerging
mission-critical applications. By contrast, neural network (NN) based protocol
models (NPMs) learn to generate task-specific CMs, but their rationale and
impact lack interpretability. To fill this void, in this article we propose,
for the first time, a semantic protocol model (SPM) constructed by transforming
an NPM into an interpretable symbolic graph written in the probabilistic logic
programming language (ProbLog). This transformation is viable by extracting and
merging common CMs and their connections while treating the NPM as a CM
generator. By extensive simulations, we corroborate that the SPM tightly
approximates its original NPM while occupying only 0.02% memory. By leveraging
its interpretability and memory-efficiency, we demonstrate several SPM-enabled
applications such as SPM reconfiguration for collision-avoidance, as well as
comparing different SPMs via semantic entropy calculation and storing multiple
SPMs to cope with non-stationary environments.
</p>

### Title: UDRN: Unified Dimensional Reduction Neural Network for Feature Selection and Feature Projection. (arXiv:2207.03809v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2207.03809](http://arxiv.org/abs/2207.03809)
* Code URL: null
* Summary: <p>Dimensional reduction~(DR) maps high-dimensional data into a lower dimensions
latent space with minimized defined optimization objectives. The DR method
usually falls into feature selection~(FS) and feature projection~(FP). FS
focuses on selecting a critical subset of dimensions but risks destroying the
data distribution (structure). On the other hand, FP combines all the input
features into lower dimensions space, aiming to maintain the data structure;
but lacks interpretability and sparsity. FS and FP are traditionally
incompatible categories; thus, they have not been unified into an amicable
framework. We propose that the ideal DR approach combines both FS and FP into a
unified end-to-end manifold learning framework, simultaneously performing
fundamental feature discovery while maintaining the intrinsic relationships
between data samples in the latent space. In this work, we develop a unified
framework, Unified Dimensional Reduction Neural-network~(UDRN), that integrates
FS and FP in a compatible, end-to-end way. We improve the neural network
structure by implementing FS and FP tasks separately using two stacked
sub-networks. In addition, we designed data augmentation of the DR process to
improve the generalization ability of the method when dealing with extensive
feature datasets and designed loss functions that can cooperate with the data
augmentation. Extensive experimental results on four image and four biological
datasets, including very high-dimensional data, demonstrate the advantages of
DRN over existing methods~(FS, FP, and FS\&amp;FP pipeline), especially in
downstream tasks such as classification and visualization.
</p>

### Title: ControlBurn: Nonlinear Feature Selection with Sparse Tree Ensembles. (arXiv:2207.03935v1 [stat.ML])
* Paper URL: [http://arxiv.org/abs/2207.03935](http://arxiv.org/abs/2207.03935)
* Code URL: [https://github.com/udellgroup/controlburn](https://github.com/udellgroup/controlburn)
* Summary: <p>ControlBurn is a Python package to construct feature-sparse tree ensembles
that support nonlinear feature selection and interpretable machine learning.
The algorithms in this package first build large tree ensembles that prioritize
basis functions with few features and then select a feature-sparse subset of
these basis functions using a weighted lasso optimization criterion. The
package includes visualizations to analyze the features selected by the
ensemble and their impact on predictions. Hence ControlBurn offers the accuracy
and flexibility of tree-ensemble models and the interpretability of sparse
generalized additive models.
</p>
<p>ControlBurn is scalable and flexible: for example, it can use warm-start
continuation to compute the regularization path (prediction error for any
number of selected features) for a dataset with tens of thousands of samples
and hundreds of features in seconds. For larger datasets, the runtime scales
linearly in the number of samples and features (up to a log factor), and the
package support acceleration using sketching. Moreover, the ControlBurn
framework accommodates feature costs, feature groupings, and $\ell_0$-based
regularizers. The package is user-friendly and open-source: its documentation
and source code appear on https://pypi.org/project/ControlBurn/ and
https://github.com/udellgroup/controlburn/.
</p>

### Title: Feature Selection Methods for Uplift Modeling and Heterogeneous Treatment Effect. (arXiv:2005.03447v2 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2005.03447](http://arxiv.org/abs/2005.03447)
* Code URL: null
* Summary: <p>Uplift modeling is a causal learning technique that estimates subgroup-level
treatment effects. It is commonly used in industry and elsewhere for tasks such
as targeting ads. In a typical setting, uplift models can take thousands of
features as inputs, which is costly and results in problems such as overfitting
and poor model interpretability. Consequently, there is a need to select a
subset of the most important features for modeling. However, traditional
methods for doing feature selection are not fit for the task because they are
designed for standard machine learning models whose target is importantly
different from uplift models. To address this, we introduce a set of feature
selection methods explicitly designed for uplift modeling, drawing inspiration
from statistics and information theory. We conduct empirical evaluations on the
proposed methods on publicly available datasets, demonstrating the advantages
of the proposed methods compared to traditional feature selection. We make the
proposed methods publicly available as a part of the CausalML open-source
package.
</p>

### Title: An AO-ADMM approach to constraining PARAFAC2 on all modes. (arXiv:2110.01278v3 [cs.LG] UPDATED)
* Paper URL: [http://arxiv.org/abs/2110.01278](http://arxiv.org/abs/2110.01278)
* Code URL: [https://github.com/marieroald/parafac2-aoadmm-simods](https://github.com/marieroald/parafac2-aoadmm-simods)
* Summary: <p>Analyzing multi-way measurements with variations across one mode of the
dataset is a challenge in various fields including data mining, neuroscience
and chemometrics. For example, measurements may evolve over time or have
unaligned time profiles. The PARAFAC2 model has been successfully used to
analyze such data by allowing the underlying factor matrices in one mode (i.e.,
the evolving mode) to change across slices. The traditional approach to fit a
PARAFAC2 model is to use an alternating least squares-based algorithm, which
handles the constant cross-product constraint of the PARAFAC2 model by
implicitly estimating the evolving factor matrices. This approach makes
imposing regularization on these factor matrices challenging. There is
currently no algorithm to flexibly impose such regularization with general
penalty functions and hard constraints. In order to address this challenge and
to avoid the implicit estimation, in this paper, we propose an algorithm for
fitting PARAFAC2 based on alternating optimization with the alternating
direction method of multipliers (AO-ADMM). With numerical experiments on
simulated data, we show that the proposed PARAFAC2 AO-ADMM approach allows for
flexible constraints, recovers the underlying patterns accurately, and is
computationally efficient compared to the state-of-the-art. We also apply our
model to two real-world datasets from neuroscience and chemometrics, and show
that constraining the evolving mode improves the interpretability of the
extracted patterns.
</p>

## exlainability
## watermark
