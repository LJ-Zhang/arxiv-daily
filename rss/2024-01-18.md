## diffusion
### Title: One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08639](http://arxiv.org/abs/2401.08639)
* Code URL: [https://github.com/locuslab/get](https://github.com/locuslab/get)
* Copy Paste: `<input type="checkbox">[[2401.08639] One-Step Diffusion Distillation via Deep Equilibrium Models](http://arxiv.org/abs/2401.08639) #diffusion`
* Summary: <p>Diffusion models excel at producing high-quality samples but naively require
hundreds of iterations, prompting multiple attempts to distill the generation
process into a faster network. However, many existing approaches suffer from a
variety of challenges: the process for distillation training can be complex,
often requiring multiple training stages, and the resulting models perform
poorly when utilized in single-step generative applications. In this paper, we
introduce a simple yet effective means of distilling diffusion models directly
from initial noise to the resulting image. Of particular importance to our
approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled
architecture: the Generative Equilibrium Transformer (GET). Our method enables
fully offline training with just noise/image pairs from the diffusion model
while achieving superior performance compared to existing one-step methods on
comparable training budgets. We demonstrate that the DEQ architecture is
crucial to this capability, as GET matches a $5\times$ larger ViT in terms of
FID scores while striking a critical balance of computational cost and image
quality. Code, checkpoints, and datasets are available.
</p>

### Title: SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08655](http://arxiv.org/abs/2401.08655)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08655] SAiD: Speech-driven Blendshape Facial Animation with Diffusion](http://arxiv.org/abs/2401.08655) #diffusion`
* Summary: <p>Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
</p>

### Title: NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08689](http://arxiv.org/abs/2401.08689)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08689] NODI: Out-Of-Distribution Detection with Noise from Diffusion](http://arxiv.org/abs/2401.08689) #diffusion`
* Summary: <p>Out-of-distribution (OOD) detection is a crucial part of deploying machine
learning models safely. It has been extensively studied with a plethora of
methods developed in the literature. This problem is tackled with an OOD score
computation, however, previous methods compute the OOD scores with limited
usage of the in-distribution dataset. For instance, the OOD scores are computed
with information from a small portion of the in-distribution data. Furthermore,
these methods encode images with a neural image encoder. The robustness of
these methods is rarely checked with respect to image encoders of different
training methods and architectures. In this work, we introduce the diffusion
process into the OOD task. The diffusion model integrates information on the
whole training set into the predicted noise vectors. What's more, we deduce a
closed-form solution for the noise vector (stable point). Then the noise vector
is converted into our OOD score, we test both the deep model predicted noise
vector and the closed-form noise vector on the OOD benchmarks \cite{openood}.
Our method outperforms previous OOD methods across all types of image encoders
(Table. \ref{main}). A $3.5\%$ performance gain is achieved with the MAE-based
image encoder. Moreover, we studied the robustness of OOD methods by applying
different types of image encoders. Some OOD methods failed to generalize well
when switching image encoders from ResNet to Vision Transformers, our method
performs exhibits good robustness with all the image encoders.
</p>

### Title: Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks. (arXiv:2401.08725v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08725](http://arxiv.org/abs/2401.08725)
* Code URL: [https://github.com/datar001/revealing-vulnerabilities-in-stable-diffusion-via-targeted-attacks](https://github.com/datar001/revealing-vulnerabilities-in-stable-diffusion-via-targeted-attacks)
* Copy Paste: `<input type="checkbox">[[2401.08725] Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks](http://arxiv.org/abs/2401.08725) #diffusion`
* Summary: <p>Recent developments in text-to-image models, particularly Stable Diffusion,
have marked significant achievements in various applications. With these
advancements, there are growing safety concerns about the vulnerability of the
model that malicious entities exploit to generate targeted harmful images.
However, the existing methods in the vulnerability of the model mainly evaluate
the alignment between the prompt and generated images, but fall short in
revealing the vulnerability associated with targeted image generation. In this
study, we formulate the problem of targeted adversarial attack on Stable
Diffusion and propose a framework to generate adversarial prompts.
Specifically, we design a gradient-based embedding optimization method to craft
reliable adversarial prompts that guide stable diffusion to generate specific
images. Furthermore, after obtaining successful adversarial prompts, we reveal
the mechanisms that cause the vulnerability of the model. Extensive experiments
on two targeted attack tasks demonstrate the effectiveness of our method in
targeted attacks. The code can be obtained in
https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks.
</p>

### Title: SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08740](http://arxiv.org/abs/2401.08740)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08740] SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers](http://arxiv.org/abs/2401.08740) #diffusion`
* Summary: <p>We present Scalable Interpolant Transformers (SiT), a family of generative
models built on the backbone of Diffusion Transformers (DiT). The interpolant
framework, which allows for connecting two distributions in a more flexible way
than standard diffusion models, makes possible a modular study of various
design choices impacting generative models built on dynamical transport: using
discrete vs. continuous time learning, deciding the objective for the model to
learn, choosing the interpolant connecting the distributions, and deploying a
deterministic or stochastic sampler. By carefully introducing the above
ingredients, SiT surpasses DiT uniformly across model sizes on the conditional
ImageNet 256x256 benchmark using the exact same backbone, number of parameters,
and GFLOPs. By exploring various diffusion coefficients, which can be tuned
separately from learning, SiT achieves an FID-50K score of 2.06.
</p>

### Title: Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08741](http://arxiv.org/abs/2401.08741)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08741] Fixed Point Diffusion Models](http://arxiv.org/abs/2401.08741) #diffusion`
* Summary: <p>We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to
image generation that integrates the concept of fixed point solving into the
framework of diffusion-based generative modeling. Our approach embeds an
implicit fixed point solving layer into the denoising network of a diffusion
model, transforming the diffusion process into a sequence of closely-related
fixed point problems. Combined with a new stochastic training method, this
approach significantly reduces model size, reduces memory usage, and
accelerates training. Moreover, it enables the development of two new
techniques to improve sampling efficiency: reallocating computation across
timesteps and reusing fixed point solutions between timesteps. We conduct
extensive experiments with state-of-the-art models on ImageNet, FFHQ,
CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in
performance and efficiency. Compared to the state-of-the-art DiT model, FPDM
contains 87% fewer parameters, consumes 60% less memory during training, and
improves image generation quality in situations where sampling computation or
time is limited. Our code and pretrained models are available at
https://lukemelas.github.io/fixed-point-diffusion-models.
</p>

### Title: Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08815](http://arxiv.org/abs/2401.08815)
* Code URL: [https://github.com/boschresearch/aldm](https://github.com/boschresearch/aldm)
* Copy Paste: `<input type="checkbox">[[2401.08815] Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive](http://arxiv.org/abs/2401.08815) #diffusion`
* Summary: <p>Despite the recent advances in large-scale diffusion models, little progress
has been made on the layout-to-image (L2I) synthesis task. Current L2I models
either suffer from poor editability via text or weak alignment between the
generated image and the input layout. This limits their usability in practice.
To mitigate this, we propose to integrate adversarial supervision into the
conventional training pipeline of L2I diffusion models (ALDM). Specifically, we
employ a segmentation-based discriminator which provides explicit feedback to
the diffusion generator on the pixel-level alignment between the denoised image
and the input layout. To encourage consistent adherence to the input layout
over the sampling steps, we further introduce the multistep unrolling strategy.
Instead of looking at a single timestep, we unroll a few steps recursively to
imitate the inference process, and ask the discriminator to assess the
alignment of denoised images with the layout over a certain time window. Our
experiments show that ALDM enables layout faithfulness of the generated images,
while allowing broad editability via text prompts. Moreover, we showcase its
usefulness for practical applications: by synthesizing target distribution
samples via text control, we improve domain generalization of semantic
segmentation models by a large margin (~12 mIoU points).
</p>

### Title: 3D Human Pose Analysis via Diffusion Synthesis. (arXiv:2401.08930v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08930](http://arxiv.org/abs/2401.08930)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08930] 3D Human Pose Analysis via Diffusion Synthesis](http://arxiv.org/abs/2401.08930) #diffusion`
* Summary: <p>Diffusion models have demonstrated remarkable success in generative modeling.
In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel
framework designed to address various challenges in 3D human pose analysis
through a unified pipeline. Central to PADS are two distinctive strategies: i)
learning a task-agnostic pose prior using a diffusion synthesis process to
effectively capture the kinematic constraints in human pose data, and ii)
unifying multiple pose analysis tasks like estimation, completion, denoising,
etc, as instances of inverse problems. The learned pose prior will be treated
as a regularization imposing on task-specific constraints, guiding the
optimization process through a series of conditional denoising steps. PADS
represents the first diffusion-based framework for tackling general 3D human
pose analysis within the inverse problem framework. Its performance has been
validated on different benchmarks, signaling the adaptability and robustness of
this pipeline.
</p>

### Title: VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. (arXiv:2401.09047v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09047](http://arxiv.org/abs/2401.09047)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09047] VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models](http://arxiv.org/abs/2401.09047) #diffusion`
* Summary: <p>Text-to-video generation aims to produce a video based on a given prompt.
Recently, several commercial video models have been able to generate plausible
videos with minimal noise, excellent details, and high aesthetic scores.
However, these models rely on large-scale, well-filtered, high-quality videos
that are not accessible to the community. Many existing research works, which
train models using the low-quality WebVid-10M dataset, struggle to generate
high-quality videos because the models are optimized to fit WebVid-10M. In this
work, we explore the training scheme of video models extended from Stable
Diffusion and investigate the feasibility of leveraging low-quality videos and
synthesized high-quality images to obtain a high-quality video model. We first
analyze the connection between the spatial and temporal modules of video models
and the distribution shift to low-quality videos. We observe that full training
of all modules results in a stronger coupling between spatial and temporal
modules than only training temporal modules. Based on this stronger coupling,
we shift the distribution to higher quality without motion degradation by
finetuning spatial modules with high-quality images, resulting in a generic
high-quality video model. Evaluations are conducted to demonstrate the
superiority of the proposed method, particularly in picture quality, motion,
and concept composition.
</p>

### Title: Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis. (arXiv:2401.09048v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09048](http://arxiv.org/abs/2401.09048)
* Code URL: [https://github.com/tomtom1103/compose-and-conquer](https://github.com/tomtom1103/compose-and-conquer)
* Copy Paste: `<input type="checkbox">[[2401.09048] Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis](http://arxiv.org/abs/2401.09048) #diffusion`
* Summary: <p>Addressing the limitations of text as a source of accurate layout
representation in text-conditional diffusion models, many works incorporate
additional signals to condition certain attributes within a generated image.
Although successful, previous works do not account for the specific
localization of said attributes extended into the three dimensional plane. In
this context, we present a conditional diffusion model that integrates control
over three-dimensional object placement with disentangled representations of
global stylistic semantics from multiple exemplar images. Specifically, we
first introduce \textit{depth disentanglement training} to leverage the
relative depth of objects as an estimator, allowing the model to identify the
absolute positions of unseen objects through the use of synthetic image
triplets. We also introduce \textit{soft guidance}, a method for imposing
global semantics onto targeted regions without the use of any additional
localization cues. Our integrated framework, \textsc{Compose and Conquer
(CnC)}, unifies these techniques to localize multiple conditions in a
disentangled manner. We demonstrate that our approach allows perception of
objects at varying depths while offering a versatile framework for composing
localized objects with different global semantics. Code:
https://github.com/tomtom1103/compose-and-conquer/
</p>

### Title: Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior. (arXiv:2401.09050v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09050](http://arxiv.org/abs/2401.09050)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09050] Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior](http://arxiv.org/abs/2401.09050) #diffusion`
* Summary: <p>Score distillation sampling (SDS) and its variants have greatly boosted the
development of text-to-3D generation, but are vulnerable to geometry collapse
and poor textures yet. To solve this issue, we first deeply analyze the SDS and
find that its distillation sampling process indeed corresponds to the
trajectory sampling of a stochastic differential equation (SDE): SDS samples
along an SDE trajectory to yield a less noisy sample which then serves as a
guidance to optimize a 3D model. However, the randomness in SDE sampling often
leads to a diverse and unpredictable sample which is not always less noisy, and
thus is not a consistently correct guidance, explaining the vulnerability of
SDS. Since for any SDE, there always exists an ordinary differential equation
(ODE) whose trajectory sampling can deterministically and consistently converge
to the desired target point as the SDE, we propose a novel and effective
"Consistent3D" method that explores the ODE deterministic sampling prior for
text-to-3D generation. Specifically, at each training iteration, given a
rendered image by a 3D model, we first estimate its desired 3D score function
by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling.
Next, we design a consistency distillation sampling loss which samples along
the ODE trajectory to generate two adjacent samples and uses the less noisy
sample to guide another more noisy one for distilling the deterministic prior
into the 3D model. Experimental results show the efficacy of our Consistent3D
in generating high-fidelity and diverse 3D objects and large-scale scenes, as
shown in Fig. 1. The codes are available at
https://github.com/sail-sg/Consistent3D.
</p>

### Title: UniVG: Towards UNIfied-modal Video Generation. (arXiv:2401.09084v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09084](http://arxiv.org/abs/2401.09084)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09084] UniVG: Towards UNIfied-modal Video Generation](http://arxiv.org/abs/2401.09084) #diffusion`
* Summary: <p>Diffusion based video generation has received extensive attention and
achieved considerable success within both the academic and industrial
communities. However, current efforts are mainly concentrated on
single-objective or single-task video generation, such as generation driven by
text, by image, or by a combination of text and image. This cannot fully meet
the needs of real-world application scenarios, as users are likely to input
images and text conditions in a flexible manner, either individually or in
combination. To address this, we propose a Unified-modal Video Genearation
system that is capable of handling multiple video generation tasks across text
and image modalities. To this end, we revisit the various video generation
tasks within our system from the perspective of generative freedom, and
classify them into high-freedom and low-freedom video generation categories.
For high-freedom video generation, we employ Multi-condition Cross Attention to
generate videos that align with the semantics of the input images or text. For
low-freedom video generation, we introduce Biased Gaussian Noise to replace the
pure random Gaussian Noise, which helps to better preserve the content of the
input conditions. Our method achieves the lowest Fr\'echet Video Distance (FVD)
on the public academic benchmark MSR-VTT, surpasses the current open-source
methods in human evaluations, and is on par with the current close-source
method Gen2. For more samples, visit https://univg-baidu.github.io.
</p>

### Title: Training-Free Semantic Video Composition via Pre-trained Diffusion Model. (arXiv:2401.09195v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09195](http://arxiv.org/abs/2401.09195)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09195] Training-Free Semantic Video Composition via Pre-trained Diffusion Model](http://arxiv.org/abs/2401.09195) #diffusion`
* Summary: <p>The video composition task aims to integrate specified foregrounds and
backgrounds from different videos into a harmonious composite. Current
approaches, predominantly trained on videos with adjusted foreground color and
lighting, struggle to address deep semantic disparities beyond superficial
adjustments, such as domain gaps. Therefore, we propose a training-free
pipeline employing a pre-trained diffusion model imbued with semantic prior
knowledge, which can process composite videos with broader semantic
disparities. Specifically, we process the video frames in a cascading manner
and handle each frame in two processes with the diffusion model. In the
inversion process, we propose Balanced Partial Inversion to obtain generation
initial points that balance reversibility and modifiability. Then, in the
generation process, we further propose Inter-Frame Augmented attention to
augment foreground continuity across frames. Experimental results reveal that
our pipeline successfully ensures the visual harmony and inter-frame coherence
of the outputs, demonstrating efficacy in managing broader semantic
disparities.
</p>

### Title: Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09031](http://arxiv.org/abs/2401.09031)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09031] Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation](http://arxiv.org/abs/2401.09031) #diffusion`
* Summary: <p>Data attribution methods trace model behavior back to its training dataset,
offering an effective approach to better understand ``black-box'' neural
networks. While prior research has established quantifiable links between model
output and training data in diverse settings, interpreting diffusion model
outputs in relation to training samples remains underexplored. In particular,
diffusion models operate over a sequence of timesteps instead of instantaneous
input-output relationships in previous contexts, posing a significant challenge
to extend existing frameworks to diffusion models directly. Notably, we present
Diffusion-TracIn that incorporates this temporal dynamics and observe that
samples' loss gradient norms are highly dependent on timestep. This trend leads
to a prominent bias in influence estimation, and is particularly noticeable for
samples trained on large-norm-inducing timesteps, causing them to be generally
influential. To mitigate this effect, we introduce Diffusion-ReTrac as a
re-normalized adaptation that enables the retrieval of training samples more
targeted to the test sample of interest, facilitating a localized measurement
of influence and considerably more intuitive visualization. We demonstrate the
efficacy of our approach through various evaluation metrics and auxiliary
tasks, reducing the amount of generally influential samples to $\frac{1}{3}$ of
its original quantity.
</p>

## self-supervised
### Title: Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08581](http://arxiv.org/abs/2401.08581)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08581] Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision](http://arxiv.org/abs/2401.08581) #self-supervised`
* Summary: <p>There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas. Temporal embeddings
transform sequential, spatiotemporal motion trajectory data into semantically
meaningful image-like tensor representations that can be combined (multimodal
fusion) with other data modalities that are or can be transformed into
image-like tensor representations (for e.g., RBG imagery, graph embeddings of
road networks, passively collected imagery like SAR, etc.) to facilitate
multimodal learning in geospatial computer vision. Multimodal computer vision
is critical for training machine learning models for geospatial feature
detection to keep a geospatial mapping service up-to-date in real-time and can
significantly improve user experience and above all, user safety.
</p>

### Title: Unsupervised Pre-Training for 3D Leaf Instance Segmentation. (arXiv:2401.08720v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08720](http://arxiv.org/abs/2401.08720)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08720] Unsupervised Pre-Training for 3D Leaf Instance Segmentation](http://arxiv.org/abs/2401.08720) #self-supervised`
* Summary: <p>Crops for food, feed, fiber, and fuel are key natural resources for our
society. Monitoring plants and measuring their traits is an important task in
agriculture often referred to as plant phenotyping. Traditionally, this task is
done manually, which is time- and labor-intensive. Robots can automate
phenotyping providing reproducible and high-frequency measurements. Today's
perception systems use deep learning to interpret these measurements, but
require a substantial amount of annotated data to work well. Obtaining such
labels is challenging as it often requires background knowledge on the side of
the labelers. This paper addresses the problem of reducing the labeling effort
required to perform leaf instance segmentation on 3D point clouds, which is a
first step toward phenotyping in 3D. Separating all leaves allows us to count
them and compute relevant traits as their areas, lengths, and widths. We
propose a novel self-supervised task-specific pre-training approach to
initialize the backbone of a network for leaf instance segmentation. We also
introduce a novel automatic postprocessing that considers the difficulty of
correctly segmenting the points close to the stem, where all the leaves petiole
overlap. The experiments presented in this paper suggest that our approach
boosts the performance over all the investigated scenarios. We also evaluate
the embeddings to assess the quality of the fully unsupervised approach and see
a higher performance of our domain-specific postprocessing.
</p>

### Title: Cross-Level Multi-Instance Distillation for Self-Supervised Fine-Grained Visual Categorization. (arXiv:2401.08860v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08860](http://arxiv.org/abs/2401.08860)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08860] Cross-Level Multi-Instance Distillation for Self-Supervised Fine-Grained Visual Categorization](http://arxiv.org/abs/2401.08860) #self-supervised`
* Summary: <p>High-quality annotation of fine-grained visual categories demands great
expert knowledge, which is taxing and time consuming. Alternatively, learning
fine-grained visual representation from enormous unlabeled images (e.g.,
species, brands) by self-supervised learning becomes a feasible solution.
However, recent researches find that existing self-supervised learning methods
are less qualified to represent fine-grained categories. The bottleneck lies in
that the pre-text representation is built from every patch-wise embedding,
while fine-grained categories are only determined by several key patches of an
image. In this paper, we propose a Cross-level Multi-instance Distillation
(CMD) framework to tackle the challenge. Our key idea is to consider the
importance of each image patch in determining the fine-grained pre-text
representation by multiple instance learning. To comprehensively learn the
relation between informative patches and fine-grained semantics, the
multi-instance knowledge distillation is implemented on both the region/image
crop pairs from the teacher and student net, and the region-image crops inside
the teacher / student net, which we term as intra-level multi-instance
distillation and inter-level multi-instance distillation. Extensive experiments
on CUB-200-2011, Stanford Cars and FGVC Aircraft show that the proposed method
outperforms the contemporary method by upto 10.14% and existing
state-of-the-art self-supervised learning approaches by upto 19.78% on both
top-1 accuracy and Rank-1 retrieval metric.
</p>

### Title: Hearing Loss Detection from Facial Expressions in One-on-one Conversations. (arXiv:2401.08972v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08972](http://arxiv.org/abs/2401.08972)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08972] Hearing Loss Detection from Facial Expressions in One-on-one Conversations](http://arxiv.org/abs/2401.08972) #self-supervised`
* Summary: <p>Individuals with impaired hearing experience difficulty in conversations,
especially in noisy environments. This difficulty often manifests as a change
in behavior and may be captured via facial expressions, such as the expression
of discomfort or fatigue. In this work, we build on this idea and introduce the
problem of detecting hearing loss from an individual's facial expressions
during a conversation. Building machine learning models that can represent
hearing-related facial expression changes is a challenge. In addition, models
need to disentangle spurious age-related correlations from hearing-driven
expressions. To this end, we propose a self-supervised pre-training strategy
tailored for the modeling of expression variations. We also use adversarial
representation learning to mitigate the age bias. We evaluate our approach on a
large-scale egocentric dataset with real-world conversational scenarios
involving subjects with hearing loss and show that our method for hearing loss
detection achieves superior performance over baselines.
</p>

### Title: CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding. (arXiv:2401.09057v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09057](http://arxiv.org/abs/2401.09057)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09057] CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding](http://arxiv.org/abs/2401.09057) #self-supervised`
* Summary: <p>This paper introduces a novel approach named CrossVideo, which aims to
enhance self-supervised cross-modal contrastive learning in the field of point
cloud video understanding. Traditional supervised learning methods encounter
limitations due to data scarcity and challenges in label acquisition. To
address these issues, we propose a self-supervised learning method that
leverages the cross-modal relationship between point cloud videos and image
videos to acquire meaningful feature representations. Intra-modal and
cross-modal contrastive learning techniques are employed to facilitate
effective comprehension of point cloud video. We also propose a multi-level
contrastive approach for both modalities. Through extensive experiments, we
demonstrate that our method significantly surpasses previous state-of-the-art
approaches, and we conduct comprehensive ablation studies to validate the
effectiveness of our proposed designs.
</p>

### Title: SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects. (arXiv:2401.09133v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09133](http://arxiv.org/abs/2401.09133)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09133] SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects](http://arxiv.org/abs/2401.09133) #self-supervised`
* Summary: <p>Reconstructing real-world objects and estimating their movable joint
structures are pivotal technologies within the field of robotics. Previous
research has predominantly focused on supervised approaches, relying on
extensively annotated datasets to model articulated objects within limited
categories. However, this approach falls short of effectively addressing the
diversity present in the real world. To tackle this issue, we propose a
self-supervised interaction perception method, referred to as SM$^3$, which
leverages multi-view RGB images captured before and after interaction to model
articulated objects, identify the movable parts, and infer the parameters of
their rotating joints. By constructing 3D geometries and textures from the
captured 2D images, SM$^3$ achieves integrated optimization of movable part and
joint parameters during the reconstruction process, obviating the need for
annotations. Furthermore, we introduce the MMArt dataset, an extension of
PartNet-Mobility, encompassing multi-view and multi-modal data of articulated
objects spanning diverse categories. Evaluations demonstrate that SM$^3$
surpasses existing benchmarks across various categories and objects, while its
adaptability in real-world scenarios has been thoroughly validated.
</p>

### Title: Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems. (arXiv:2401.08863v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08863](http://arxiv.org/abs/2401.08863)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08863] Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems](http://arxiv.org/abs/2401.08863) #self-supervised`
* Summary: <p>Using neural networks for localization of key fob within and surrounding a
car as a security feature for keyless entry is fast emerging. In this paper we
study: 1) the performance of pre-computed features of neural networks based UWB
(ultra wide band) localization classification forming the baseline of our
experiments. 2) Investigate the inherent robustness of various neural networks;
therefore, we include the study of robustness of the adversarial examples
without any adversarial training in this work. 3) Propose a multi-head
self-supervised neural network architecture which outperforms the baseline
neural networks without any adversarial training. The model's performance
improved by 67% at certain ranges of adversarial magnitude for fast gradient
sign method and 37% each for basic iterative method and projected gradient
descent method.
</p>

### Title: Contrastive Learning with Negative Sampling Correction. (arXiv:2401.08690v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08690](http://arxiv.org/abs/2401.08690)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08690] Contrastive Learning with Negative Sampling Correction](http://arxiv.org/abs/2401.08690) #self-supervised`
* Summary: <p>As one of the most effective self-supervised representation learning methods,
contrastive learning (CL) relies on multiple negative pairs to contrast against
each positive pair. In the standard practice of contrastive learning, data
augmentation methods are utilized to generate both positive and negative pairs.
While existing works have been focusing on improving the positive sampling, the
negative sampling process is often overlooked. In fact, the generated negative
samples are often polluted by positive samples, which leads to a biased loss
and performance degradation. To correct the negative sampling bias, we propose
a novel contrastive learning method named Positive-Unlabeled Contrastive
Learning (PUCL). PUCL treats the generated negative samples as unlabeled
samples and uses information from positive samples to correct bias in
contrastive loss. We prove that the corrected loss used in PUCL only incurs a
negligible bias compared to the unbiased contrastive loss. PUCL can be applied
to general contrastive learning problems and outperforms state-of-the-art
methods on various image and graph classification tasks. The code of PUCL is in
the supplementary file.
</p>

## foundation model
### Title: Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping. (arXiv:2401.08787v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08787](http://arxiv.org/abs/2401.08787)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08787] Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping](http://arxiv.org/abs/2401.08787) #foundation model`
* Summary: <p>This paper assesses trending AI foundation models, especially emerging
computer vision foundation models and their performance in natural landscape
feature segmentation. While the term foundation model has quickly garnered
interest from the geospatial domain, its definition remains vague. Hence, this
paper will first introduce AI foundation models and their defining
characteristics. Built upon the tremendous success achieved by Large Language
Models (LLMs) as the foundation models for language tasks, this paper discusses
the challenges of building foundation models for geospatial artificial
intelligence (GeoAI) vision tasks. To evaluate the performance of large AI
vision models, especially Meta's Segment Anything Model (SAM), we implemented
different instance segmentation pipelines that minimize the changes to SAM to
leverage its power as a foundation model. A series of prompt strategies was
developed to test SAM's performance regarding its theoretical upper bound of
predictive accuracy, zero-shot performance, and domain adaptability through
fine-tuning. The analysis used two permafrost feature datasets, ice-wedge
polygons and retrogressive thaw slumps because (1) these landform features are
more challenging to segment than manmade features due to their complicated
formation mechanisms, diverse forms, and vague boundaries; (2) their presence
and changes are important indicators for Arctic warming and climate change. The
results show that although promising, SAM still has room for improvement to
support AI-augmented terrain mapping. The spatial and domain generalizability
of this finding is further validated using a more general dataset EuroCrop for
agricultural field mapping. Finally, we discuss future research directions that
strengthen SAM's applicability in challenging geospatial domains.
</p>

### Title: Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models. (arXiv:2401.09083v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.09083](http://arxiv.org/abs/2401.09083)
* Code URL: [https://github.com/haonanguo/remote-sensing-chatgpt](https://github.com/haonanguo/remote-sensing-chatgpt)
* Copy Paste: `<input type="checkbox">[[2401.09083] Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models](http://arxiv.org/abs/2401.09083) #foundation model`
* Summary: <p>Recently, the flourishing large language models(LLM), especially ChatGPT,
have shown exceptional performance in language understanding, reasoning, and
interaction, attracting users and researchers from multiple fields and domains.
Although LLMs have shown great capacity to perform human-like task
accomplishment in natural language and natural image, their potential in
handling remote sensing interpretation tasks has not yet been fully explored.
Moreover, the lack of automation in remote sensing task planning hinders the
accessibility of remote sensing interpretation techniques, especially to
non-remote sensing experts from multiple research fields. To this end, we
present Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to
connect various AI-based remote sensing models to solve complicated
interpretation tasks. More specifically, given a user request and a remote
sensing image, we utilized ChatGPT to understand user requests, perform task
planning according to the tasks' functions, execute each subtask iteratively,
and generate the final response according to the output of each subtask.
Considering that LLM is trained with natural language and is not capable of
directly perceiving visual concepts as contained in remote sensing images, we
designed visual cues that inject visual information into ChatGPT. With Remote
Sensing ChatGPT, users can simply send a remote sensing image with the
corresponding request, and get the interpretation results as well as language
feedback from Remote Sensing ChatGPT. Experiments and examples show that Remote
Sensing ChatGPT can tackle a wide range of remote sensing tasks and can be
extended to more tasks with more sophisticated models such as the remote
sensing foundation model. The code and demo of Remote Sensing ChatGPT is
publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .
</p>

### Title: Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.08992](http://arxiv.org/abs/2401.08992)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08992] Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR](http://arxiv.org/abs/2401.08992) #foundation model`
* Summary: <p>The end-to-end ASR model is often desired in the streaming multilingual
scenario since it is easier to deploy and can benefit from pre-trained speech
models such as powerful foundation models. Meanwhile, the heterogeneous nature
and imbalanced data abundance of different languages may cause performance
degradation, leading to asynchronous peak performance for different languages
during training, especially on tail ones. Sometimes even the data itself may
become unavailable as a result of the enhanced privacy protection. Existing
work tend to significantly increase the model size or learn language-specific
decoders to accommodate each language separately. In this study, we explore
simple yet effective Language-Dependent Adapter (LDA) finetuning under a
cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for
tail languages in the streaming multilingual ASR. The adapter only accounts for
0.4% of the full model per language. It is plugged into the frozen foundation
model and is the only trainable module during the finetuning process with noisy
student training. The final model merges the adapter parameters from different
checkpoints for different languages. The model performance is validated on a
challenging multilingual dictation dataset, which includes 39 tail languages
across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error
rate reduction on average and up to 37.5% on a single locale. Furthermore, we
show that our parameter-efficient LDA can match the quality of the full model
finetuning, thus greatly alleviating the asynchronous peak performance issue.
</p>

## generative
### Title: Improved Pothole Detection Using YOLOv7 and ESRGAN. (arXiv:2401.08588v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08588](http://arxiv.org/abs/2401.08588)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08588] Improved Pothole Detection Using YOLOv7 and ESRGAN](http://arxiv.org/abs/2401.08588) #generative`
* Summary: <p>Potholes are common road hazards that is causing damage to vehicles and
posing a safety risk to drivers. The introduction of Convolutional Neural
Networks (CNNs) is widely used in the industry for object detection based on
Deep Learning methods and has achieved significant progress in hardware
improvement and software implementations. In this paper, a unique better
algorithm is proposed to warrant the use of low-resolution cameras or
low-resolution images and video feed for automatic pothole detection using
Super Resolution (SR) through Super Resolution Generative Adversarial Networks
(SRGANs). Then we have proceeded to establish a baseline pothole detection
performance on low quality and high quality dashcam images using a You Only
Look Once (YOLO) network, namely the YOLOv7 network. We then have illustrated
and examined the speed and accuracy gained above the benchmark after having
upscaling implementation on the low quality images.
</p>

### Title: COCO is "ALL'' You Need for Visual Instruction Fine-tuning. (arXiv:2401.08968v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08968](http://arxiv.org/abs/2401.08968)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08968] COCO is "ALL'' You Need for Visual Instruction Fine-tuning](http://arxiv.org/abs/2401.08968) #generative`
* Summary: <p>Multi-modal Large Language Models (MLLMs) are increasingly prominent in the
field of artificial intelligence. Visual instruction fine-tuning (IFT) is a
vital process for aligning MLLMs' output with user's intentions. High-quality
and diversified instruction following data is the key to this fine-tuning
process. Recent studies propose to construct visual IFT datasets through a
multifaceted approach: transforming existing datasets with rule-based
templates, employing GPT-4 for rewriting annotations, and utilizing GPT-4V for
visual dataset pseudo-labeling. LLaVA-1.5 adopted similar approach and
construct LLaVA-mix-665k, which is one of the simplest, most widely used, yet
most effective IFT datasets today. Notably, when properly fine-tuned with this
dataset, MLLMs can achieve state-of-the-art performance on several benchmarks.
However, we noticed that models trained with this dataset often struggle to
follow user instructions properly in multi-round dialog. In addition, tradition
caption and VQA evaluation benchmarks, with their closed-form evaluation
structure, are not fully equipped to assess the capabilities of modern
open-ended generative MLLMs. This problem is not unique to the LLaVA-mix-665k
dataset, but may be a potential issue in all IFT datasets constructed from
image captioning or VQA sources, though the extent of this issue may vary. We
argue that datasets with diverse and high-quality detailed instruction
following annotations are essential and adequate for MLLMs IFT. In this work,
we establish a new IFT dataset, with images sourced from the COCO dataset along
with more diverse instructions. Our experiments show that when fine-tuned with
out proposed dataset, MLLMs achieve better performance on open-ended evaluation
benchmarks in both single-round and multi-round dialog setting.
</p>

### Title: Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09180](http://arxiv.org/abs/2401.09180)
* Code URL: [https://github.com/antonioalmudevar/variational_translation](https://github.com/antonioalmudevar/variational_translation)
* Copy Paste: `<input type="checkbox">[[2401.09180] Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder](http://arxiv.org/abs/2401.09180) #generative`
* Summary: <p>Unsupervised Multiple Domain Translation is the task of transforming data
from one domain to other domains without having paired data to train the
systems. Typically, methods based on Generative Adversarial Networks (GANs) are
used to address this task. However, our proposal exclusively relies on a
modified version of a Variational Autoencoder. This modification consists of
the use of two latent variables disentangled in a controlled way by design. One
of this latent variables is imposed to depend exclusively on the domain, while
the other one must depend on the rest of the variability factors of the data.
Additionally, the conditions imposed over the domain latent variable allow for
better control and understanding of the latent space. We empirically
demonstrate that our approach works on different vision datasets improving the
performance of other well known methods. Finally, we prove that, indeed, one of
the latent variables stores all the information related to the domain and the
other one hardly contains any domain information.
</p>

### Title: Machine Learning for Healthcare-IoT Security: A Review and Risk Mitigation. (arXiv:2401.09124v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2401.09124](http://arxiv.org/abs/2401.09124)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09124] Machine Learning for Healthcare-IoT Security: A Review and Risk Mitigation](http://arxiv.org/abs/2401.09124) #generative`
* Summary: <p>The Healthcare Internet-of-Things (H-IoT), commonly known as Digital
Healthcare, is a data-driven infrastructure that highly relies on smart sensing
devices (i.e., blood pressure monitors, temperature sensors, etc.) for faster
response time, treatments, and diagnosis. However, with the evolving cyber
threat landscape, IoT devices have become more vulnerable to the broader risk
surface (e.g., risks associated with generative AI, 5G-IoT, etc.), which, if
exploited, may lead to data breaches, unauthorized access, and lack of command
and control and potential harm. This paper reviews the fundamentals of
healthcare IoT, its privacy, and data security challenges associated with
machine learning and H-IoT devices. The paper further emphasizes the importance
of monitoring healthcare IoT layers such as perception, network, cloud, and
application. Detecting and responding to anomalies involves various
cyber-attacks and protocols such as Wi-Fi 6, Narrowband Internet of Things
(NB-IoT), Bluetooth, ZigBee, LoRa, and 5G New Radio (5G NR). A robust
authentication mechanism based on machine learning and deep learning techniques
is required to protect and mitigate H-IoT devices from increasing cybersecurity
vulnerabilities. Hence, in this review paper, security and privacy challenges
and risk mitigation strategies for building resilience in H-IoT are explored
and reported.
</p>

### Title: ACT-GAN: Radio map construction based on generative adversarial networks with ACT blocks. (arXiv:2401.08976v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08976](http://arxiv.org/abs/2401.08976)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08976] ACT-GAN: Radio map construction based on generative adversarial networks with ACT blocks](http://arxiv.org/abs/2401.08976) #generative`
* Summary: <p>The radio map, serving as a visual representation of electromagnetic spatial
characteristics, plays a pivotal role in assessment of wireless communication
networks and radio monitoring coverage. Addressing the issue of low accuracy
existing in the current radio map construction, this paper presents a novel
radio map construction method based on generative adversarial network (GAN) in
which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block
Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied
to the generator, and we name it as ACT-GAN. It significantly improves the
reconstruction accuracy and local texture of the radio maps. The performance of
ACT-GAN across three different scenarios is demonstrated. Experiment results
reveal that in the scenario without sparse discrete observations, the proposed
method reduces the root mean square error (RMSE) by 14.6% in comparison to the
state-of-the-art models. In the scenario with sparse discrete observations, the
RMSE is diminished by 13.2%. Furthermore, the predictive results of the
proposed model show a more lucid representation of electromagnetic spatial
field distribution. To verify the universality of this model in radio map
construction tasks, the scenario of unknown radio emission source is
investigated. The results indicate that the proposed model is robust radio map
construction and accurate in predicting the location of the emission source.
</p>

## anomaly
### Title: Online Anomaly Detection over Live Social Video Streaming. (arXiv:2401.08615v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08615](http://arxiv.org/abs/2401.08615)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08615] Online Anomaly Detection over Live Social Video Streaming](http://arxiv.org/abs/2401.08615) #anomaly`
* Summary: <p>Social video anomaly is an observation in video streams that does not conform
to a common pattern of dataset's behaviour. Social video anomaly detection
plays a critical role in applications from e-commerce to e-learning.
Traditionally, anomaly detection techniques are applied to find anomalies in
video broadcasting. However, they neglect the live social video streams which
contain interactive talk, speech, or lecture with audience. In this paper, we
propose a generic framework for effectively online detecting Anomalies Over
social Video LIve Streaming (AOVLIS). Specifically, we propose a novel deep
neural network model called Coupling Long Short-Term Memory (CLSTM) that
adaptively captures the history behaviours of the presenters and audience, and
their mutual interactions to predict their behaviour at next time point over
streams. Then we well integrate the CLSTM with a decoder layer, and propose a
new reconstruction error-based scoring function $RE_{IA}$ to calculate the
anomaly score of each video segment for anomaly detection. After that, we
propose a novel model update scheme that incrementally maintains CLSTM and
decoder. Moreover, we design a novel upper bound and ADaptive Optimisation
Strategy (ADOS) for improving the efficiency of our solution. Extensive
experiments are conducted to prove the superiority of AOVLIS.
</p>

### Title: Attention Modules Improve Modern Image-Level Anomaly Detection: A DifferNet Case Study. (arXiv:2401.08686v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2401.08686](http://arxiv.org/abs/2401.08686)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08686] Attention Modules Improve Modern Image-Level Anomaly Detection: A DifferNet Case Study](http://arxiv.org/abs/2401.08686) #anomaly`
* Summary: <p>Within (semi-)automated visual inspection, learning-based approaches for
assessing visual defects, including deep neural networks, enable the processing
of otherwise small defect patterns in pixel size on high-resolution imagery.
The emergence of these often rarely occurring defect patterns explains the
general need for labeled data corpora. To not only alleviate this issue but to
furthermore advance the current state of the art in unsupervised visual
inspection, this contribution proposes a DifferNet-based solution enhanced with
attention modules utilizing SENet and CBAM as backbone - AttentDifferNet - to
improve the detection and classification capabilities on three different visual
inspection and anomaly detection datasets: MVTec AD, InsPLAD-fault, and
Semiconductor Wafer. In comparison to the current state of the art, it is shown
that AttentDifferNet achieves improved results, which are, in turn, highlighted
throughout our quantitative as well as qualitative evaluation, indicated by a
general improvement in AUC of 94.34 vs. 92.46, 96.67 vs. 94.69, and 90.20 vs.
88.74%. As our variants to AttentDifferNet show great prospects in the context
of currently investigated approaches, a baseline is formulated, emphasizing the
importance of attention for anomaly detection.
</p>

### Title: A GAN-based data poisoning framework against anomaly detection in vertical federated learning. (arXiv:2401.08984v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08984](http://arxiv.org/abs/2401.08984)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08984] A GAN-based data poisoning framework against anomaly detection in vertical federated learning](http://arxiv.org/abs/2401.08984) #anomaly`
* Summary: <p>In vertical federated learning (VFL), commercial entities collaboratively
train a model while preserving data privacy. However, a malicious participant's
poisoning attack may degrade the performance of this collaborative model. The
main challenge in achieving the poisoning attack is the absence of access to
the server-side top model, leaving the malicious participant without a clear
target model. To address this challenge, we introduce an innovative end-to-end
poisoning framework P-GAN. Specifically, the malicious participant initially
employs semi-supervised learning to train a surrogate target model.
Subsequently, this participant employs a GAN-based method to produce
adversarial perturbations to degrade the surrogate target model's performance.
Finally, the generator is obtained and tailored for VFL poisoning. Besides, we
develop an anomaly detection algorithm based on a deep auto-encoder (DAE),
offering a robust defense mechanism to VFL scenarios. Through extensive
experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the
factors that influence their performance.
</p>

## in-context
### Title: HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance. (arXiv:2401.08772v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2401.08772](http://arxiv.org/abs/2401.08772)
* Code URL: [https://github.com/internlm/huixiangdou](https://github.com/internlm/huixiangdou)
* Copy Paste: `<input type="checkbox">[[2401.08772] HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance](http://arxiv.org/abs/2401.08772) #in-context`
* Summary: <p>In this work, we present HuixiangDou, a technical assistant powered by Large
Language Models (LLM). This system is designed to assist algorithm developers
by providing insightful responses to questions related to open-source algorithm
projects, such as computer vision and deep learning projects from OpenMMLab. We
further explore the integration of this assistant into the group chats of
instant messaging (IM) tools such as WeChat and Lark. Through several iterative
improvements and trials, we have developed a sophisticated technical chat
assistant capable of effectively answering users' technical questions without
causing message flooding. This paper's contributions include: 1) Designing an
algorithm pipeline specifically for group chat scenarios; 2) Verifying the
reliable performance of text2vec in task rejection; 3) Identifying three
critical requirements for LLMs in technical-assistant-like products, namely
scoring ability, In-Context Learning (ICL), and Long Context. We have made the
software and source code available at https://github.com/internlm/huixiangdou
to aid in future research and application. HuixiangDou is applicable to any
group chat within IM tools.
</p>

## memory
### Title: HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2401.08723](http://arxiv.org/abs/2401.08723)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08723] HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing](http://arxiv.org/abs/2401.08723) #memory`
* Summary: <p>Federated Learning is a promising approach for learning from user data while
preserving data privacy. However, the high requirements of the model training
process make it difficult for clients with limited memory or bandwidth to
participate. To tackle this problem, Split Federated Learning is utilized,
where clients upload their intermediate model training outcomes to a cloud
server for collaborative server-client model training. This methodology
facilitates resource-constrained clients' participation in model training but
also increases the training time and communication overhead. To overcome these
limitations, we propose a novel algorithm, called Hierarchical Split Federated
Learning (HierSFL), that amalgamates models at the edge and cloud phases,
presenting qualitative directives for determining the best aggregation
timeframes to reduce computation and communication expenses. By implementing
local differential privacy at the client and edge server levels, we enhance
privacy during local model parameter updates. Our experiments using CIFAR-10
and MNIST datasets show that HierSFL outperforms standard FL approaches with
better training accuracy, training time, and communication-computing
trade-offs. HierSFL offers a promising solution to mobile edge computing's
challenges, ultimately leading to faster content delivery and improved mobile
service quality.
</p>

### Title: Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09067](http://arxiv.org/abs/2401.09067)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.09067] Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding](http://arxiv.org/abs/2401.09067) #memory`
* Summary: <p>Deep neural networks are susceptible to catastrophic forgetting when trained
on sequential tasks. Various continual learning (CL) methods often rely on
exemplar buffers or/and network expansion for balancing model stability and
plasticity, which, however, compromises their practical value due to privacy
and memory concerns. Instead, this paper considers a strict yet realistic
setting, where the training data from previous tasks is unavailable and the
model size remains relatively constant during sequential training. To achieve
such desiderata, we propose a conceptually simple yet effective method that
attributes forgetting to layer-wise parameter overwriting and the resulting
decision boundary distortion. This is achieved by the synergy between two key
components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten
parameter updates mediated by Hilbert-Schmidt independence criterion in an
orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary
adaptation between old and new tasks with predefined basis vectors. Extensive
experiments demonstrate that our method achieves competitive accuracy
performance, even with absolute superiority of zero exemplar buffer and 1.02x
the base model.
</p>

### Title: Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08851](http://arxiv.org/abs/2401.08851)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08851] Using i-vectors for subject-independent cross-session EEG transfer learning](http://arxiv.org/abs/2401.08851) #memory`
* Summary: <p>Cognitive load classification is the task of automatically determining an
individual's utilization of working memory resources during performance of a
task based on physiologic measures such as electroencephalography (EEG). In
this paper, we follow a cross-disciplinary approach, where tools and
methodologies from speech processing are used to tackle this problem. The
corpus we use was released publicly in 2021 as part of the first passive
brain-computer interface competition on cross-session workload estimation. We
present our approach which used i-vector-based neural network classifiers to
accomplish inter-subject cross-session EEG transfer learning, achieving 18%
relative improvement over equivalent subject-dependent models. We also report
experiments showing how our subject-independent models perform competitively on
held-out subjects and improve with additional subject data, suggesting that
subject-dependent training is not required for effective cognitive load
determination.
</p>

### Title: HasTEE+ : Confidential Cloud Computing and Analytics with Haskell. (arXiv:2401.08901v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2401.08901](http://arxiv.org/abs/2401.08901)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08901] HasTEE+ : Confidential Cloud Computing and Analytics with Haskell](http://arxiv.org/abs/2401.08901) #memory`
* Summary: <p>Confidential computing is a security paradigm that enables the protection of
confidential code and data in a co-tenanted cloud deployment using specialized
hardware isolation units called Trusted Execution Environments (TEEs). By
integrating TEEs with a Remote Attestation protocol, confidential computing
allows a third party to establish the integrity of an \textit{enclave} hosted
within an untrusted cloud. However, TEE solutions, such as Intel SGX and ARM
TrustZone, offer low-level C/C++-based toolchains that are susceptible to
inherent memory safety vulnerabilities and lack language constructs to monitor
explicit and implicit information-flow leaks. Moreover, the toolchains involve
complex multi-project hierarchies and the deployment of hand-written
attestation protocols for verifying \textit{enclave} integrity.
</p>
<p>We address the above with HasTEE+, a domain-specific language (DSL) embedded
in Haskell that enables programming TEEs in a high-level language with strong
type-safety. HasTEE+ assists in multi-tier cloud application development by (1)
introducing a \textit{tierless} programming model for expressing distributed
client-server interactions as a single program, (2) integrating a general
remote-attestation architecture that removes the necessity to write
application-specific cross-cutting attestation code, and (3) employing a
dynamic information flow control mechanism to prevent explicit as well as
implicit data leaks. We demonstrate the practicality of HasTEE+ through a case
study on confidential data analytics, presenting a data-sharing pattern
applicable to mutually distrustful participants and providing overall
performance metrics.
</p>

### Title: Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08703](http://arxiv.org/abs/2401.08703)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2401.08703] Decoupled Prototype Learning for Reliable Test-Time Adaptation](http://arxiv.org/abs/2401.08703) #memory`
* Summary: <p>Test-time adaptation (TTA) is a task that continually adapts a pre-trained
source model to the target domain during inference. One popular approach
involves fine-tuning model with cross-entropy loss according to estimated
pseudo-labels. However, its performance is significantly affected by noisy
pseudo-labels. This study reveals that minimizing the classification error of
each sample causes the cross-entropy loss's vulnerability to label noise. To
address this issue, we propose a novel Decoupled Prototype Learning (DPL)
method that features prototype-centric loss computation. First, we decouple the
optimization of class prototypes. For each class prototype, we reduce its
distance with positive samples and enlarge its distance with negative samples
in a contrastive manner. This strategy prevents the model from overfitting to
noisy pseudo-labels. Second, we propose a memory-based strategy to enhance
DPL's robustness for the small batch sizes often encountered in TTA. We update
each class's pseudo-feature from a memory in a momentum manner and insert an
additional DPL loss. Finally, we introduce a consistency regularization-based
approach to leverage samples with unconfident pseudo-labels. This approach
transfers feature styles of samples with unconfident pseudo-labels to those
with confident pseudo-labels. Thus, more reliable samples for TTA are created.
The experimental results demonstrate that our methods achieve state-of-the-art
performance on domain generalization benchmarks, and reliably improve the
performance of self-training-based methods on image corruption benchmarks. The
code will be released.
</p>

### Title: CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation. (arXiv:2401.08940v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08940](http://arxiv.org/abs/2401.08940)
* Code URL: [https://github.com/aI-area/CEL](https://github.com/aI-area/CEL)
* Copy Paste: `<input type="checkbox">[[2401.08940] CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation](http://arxiv.org/abs/2401.08940) #memory`
* Summary: <p>Continual learning, the ability of a model to learn over time without
forgetting previous knowledge and, therefore, be adaptive to new data, is
paramount in dynamic fields such as disease outbreak prediction. Deep neural
networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This
study introduces a novel CEL model for continual learning by leveraging domain
adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate
the catastrophic forgetting phenomenon in a domain incremental setting. The
Fisher Information Matrix (FIM) is constructed with EWC to develop a
regularization term that penalizes changes to important parameters, namely, the
important previous knowledge. CEL's performance is evaluated on three distinct
diseases, Influenza, Mpox, and Measles, with different metrics. The high
R-squared values during evaluation and reevaluation outperform the other
state-of-the-art models in several contexts, indicating that CEL adapts to
incremental data well. CEL's robustness and reliability are underscored by its
minimal 65% forgetting rate and 18% higher memory stability compared to
existing benchmark studies. This study highlights CEL's versatility in disease
outbreak prediction, addressing evolving data with temporal patterns. It offers
a valuable model for proactive disease control with accurate, timely
predictions.
</p>

### Title: RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks. (arXiv:2401.09093v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.09093](http://arxiv.org/abs/2401.09093)
* Code URL: [https://github.com/howard-hou/rwkv-ts](https://github.com/howard-hou/rwkv-ts)
* Copy Paste: `<input type="checkbox">[[2401.09093] RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks](http://arxiv.org/abs/2401.09093) #memory`
* Summary: <p>Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and
GRU, have historically held prominence in time series tasks. However, they have
recently seen a decline in their dominant position across various time series
tasks. As a result, recent advancements in time series forecasting have seen a
notable shift away from RNNs towards alternative architectures such as
Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs,
we design an efficient RNN-based model for time series tasks, named RWKV-TS,
with three distinctive features: (i) A novel RNN architecture characterized by
$O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture
long-term sequence information compared to traditional RNNs. (iii) High
computational efficiency coupled with the capacity to scale up effectively.
Through extensive experimentation, our proposed RWKV-TS model demonstrates
competitive performance when compared to state-of-the-art Transformer-based or
CNN-based models. Notably, RWKV-TS exhibits not only comparable performance but
also demonstrates reduced latency and memory utilization. The success of
RWKV-TS encourages further exploration and innovation in leveraging RNN-based
approaches within the domain of Time Series. The combination of competitive
performance, low latency, and efficient memory usage positions RWKV-TS as a
promising avenue for future research in time series tasks. Code is available
at:\href{https://github.com/howard-hou/RWKV-TS}{
https://github.com/howard-hou/RWKV-TS}
</p>

## few-shot
### Title: Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2401.08732](http://arxiv.org/abs/2401.08732)
* Code URL: [https://github.com/iclr2024mcmi/iclrmcmi](https://github.com/iclr2024mcmi/iclrmcmi)
* Copy Paste: `<input type="checkbox">[[2401.08732] Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information](http://arxiv.org/abs/2401.08732) #few-shot`
* Summary: <p>It is believed that in knowledge distillation (KD), the role of the teacher
is to provide an estimate for the unknown Bayes conditional probability
distribution (BCPD) to be used in the student training process. Conventionally,
this estimate is obtained by training the teacher using maximum log-likelihood
(MLL) method. To improve this estimate for KD, in this paper we introduce the
concept of conditional mutual information (CMI) into the estimation of BCPD and
propose a novel estimator called the maximum CMI (MCMI) method. Specifically,
in MCMI estimation, both the log-likelihood and CMI of the teacher are
simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is
further shown that maximizing the teacher's CMI value allows the teacher to
capture more contextual information in an image cluster. Via conducting a
thorough set of experiments, we show that by employing a teacher trained via
MCMI estimation rather than one trained via MLL estimation in various
state-of-the-art KD frameworks, the student's classification accuracy
consistently increases, with the gain of up to 3.32\%. This suggests that the
teacher's BCPD estimate provided by MCMI method is more accurate than that
provided by MLL method. In addition, we show that such improvements in the
student's accuracy are more drastic in zero-shot and few-shot settings.
Notably, the student's accuracy increases with the gain of up to 5.72\% when
5\% of the training samples are available to the student (few-shot), and
increases from 0\% to as high as 84\% for an omitted class (zero-shot). The
code is available at \url{https://github.com/iclr2024mcmi/ICLRMCMI}.
</p>

