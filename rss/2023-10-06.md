## diffusion
### Title: EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. (arXiv:2310.03270v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03270](http://arxiv.org/abs/2310.03270)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03270] EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models](http://arxiv.org/abs/2310.03270) #diffusion`
* Summary: <p>Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</p>

### Title: Denoising Diffusion Step-aware Models. (arXiv:2310.03337v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03337](http://arxiv.org/abs/2310.03337)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03337] Denoising Diffusion Step-aware Models](http://arxiv.org/abs/2310.03337) #diffusion`
* Summary: <p>Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality. Our code and models will be
publicly available.
</p>

### Title: Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior. (arXiv:2310.03363v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03363](http://arxiv.org/abs/2310.03363)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03363] Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior](http://arxiv.org/abs/2310.03363) #diffusion`
* Summary: <p>Speech-to-face generation is an intriguing area of research that focuses on
generating realistic facial images based on a speaker's audio speech. However,
state-of-the-art methods employing GAN-based architectures lack stability and
cannot generate realistic face images. To fill this gap, we propose a novel
speech-to-face generation framework, which leverages a Speech-Conditioned
Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the
first work to harness the exceptional modeling capabilities of diffusion models
for speech-to-face generation. Preserving the shared identity information
between speech and face is crucial in generating realistic results. Therefore,
we employ contrastive pre-training for both the speech encoder and the face
encoder. This pre-training strategy facilitates effective alignment between the
attributes of speech, such as age and gender, and the corresponding facial
characteristics in the face images. Furthermore, we tackle the challenge posed
by excessive diversity in the synthesis process caused by the diffusion model.
To overcome this challenge, we introduce the concept of residuals by
integrating a statistical face prior to the diffusion process. This addition
helps to eliminate the shared component across the faces and enhances the
subtle variations captured by the speech condition. Extensive quantitative,
qualitative, and user study experiments demonstrate that our method can produce
more realistic face images while preserving the identity of the speaker better
than state-of-the-art methods. Highlighting the notable enhancements, our
method demonstrates significant gains in all metrics on the AVSpeech dataset
and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and
32.72 on the cosine distance metric for the two datasets, respectively.
</p>

### Title: ACT-Net: Anchor-context Action Detection in Surgery Videos. (arXiv:2310.03377v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03377](http://arxiv.org/abs/2310.03377)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03377] ACT-Net: Anchor-context Action Detection in Surgery Videos](http://arxiv.org/abs/2310.03377) #diffusion`
* Summary: <p>Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</p>

### Title: FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators. (arXiv:2310.03420v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03420](http://arxiv.org/abs/2310.03420)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03420] FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators](http://arxiv.org/abs/2310.03420) #diffusion`
* Summary: <p>Matching cross-modality features between images and point clouds is a
fundamental problem for image-to-point cloud registration. However, due to the
modality difference between images and points, it is difficult to learn robust
and discriminative cross-modality features by existing metric learning methods
for feature matching. Instead of applying metric learning on cross-modality
data, we propose to unify the modality between images and point clouds by
pretrained large-scale models first, and then establish robust correspondence
within the same modality. We show that the intermediate features, called
diffusion features, extracted by depth-to-image diffusion models are
semantically consistent between images and point clouds, which enables the
building of coarse but robust cross-modality correspondences. We further
extract geometric features on depth maps produced by the monocular depth
estimator. By matching such geometric features, we significantly improve the
accuracy of the coarse correspondences produced by diffusion features.
Extensive experiments demonstrate that without any task-specific training,
direct utilization of both features produces accurate image-to-point cloud
registration. On three public indoor and outdoor benchmarks, the proposed
method averagely achieves a 20.6 percent improvement in Inlier Ratio, a
three-fold higher Inlier Number, and a 48.6 percent improvement in Registration
Recall than existing state-of-the-arts.
</p>

### Title: Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion. (arXiv:2310.03502v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03502](http://arxiv.org/abs/2310.03502)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03502] Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion](http://arxiv.org/abs/2310.03502) #diffusion`
* Summary: <p>Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
</p>

### Title: Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints. (arXiv:2310.03602v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03602](http://arxiv.org/abs/2310.03602)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03602] Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints](http://arxiv.org/abs/2310.03602) #diffusion`
* Summary: <p>Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</p>

### Title: Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03218](http://arxiv.org/abs/2310.03218)
* Code URL: [https://github.com/yupeiyu98/diffusion-amortized-mcmc](https://github.com/yupeiyu98/diffusion-amortized-mcmc)
* Copy Paste: `<input type="checkbox">[[2310.03218] Learning Energy-Based Prior Model with Diffusion-Amortized MCMC](http://arxiv.org/abs/2310.03218) #diffusion`
* Summary: <p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in the field of generative modeling due to its
flexibility in the formulation and strong modeling power of the latent space.
However, the common practice of learning latent space EBMs with non-convergent
short-run MCMC for prior and posterior sampling is hindering the model from
further progress; the degenerate MCMC sampling quality in practice often leads
to degraded generation quality and instability in training, especially with
highly multi-modal and/or high-dimensional target distributions. To remedy this
sampling issue, in this paper we introduce a simple but effective
diffusion-based amortization method for long-run MCMC sampling and develop a
novel learning algorithm for the latent space EBM based on it. We provide
theoretical evidence that the learned amortization of MCMC is a valid long-run
MCMC sampler. Experiments on several image modeling benchmark datasets
demonstrate the superior performance of our method compared with strong
counterparts
</p>

### Title: Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03725](http://arxiv.org/abs/2310.03725)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03725] Stochastic interpolants with data-dependent couplings](http://arxiv.org/abs/2310.03725) #diffusion`
* Summary: <p>Generative models inspired by dynamical transport of measure -- such as flows
and diffusions -- construct a continuous-time map between two probability
densities. Conventionally, one of these is the target density, only accessible
through samples, while the other is taken as a simple base density that is
data-agnostic. In this work, using the framework of stochastic interpolants, we
formalize how to \textit{couple} the base and the target densities. This
enables us to incorporate information about class labels or continuous
embeddings to construct dynamical transport maps that serve as conditional
generative models. We show that these transport maps can be learned by solving
a simple square loss regression problem analogous to the standard independent
setting. We demonstrate the usefulness of constructing dependent couplings in
practice through experiments in super-resolution and in-painting.
</p>

## self-supervised
### Title: Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery. (arXiv:2310.03513v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03513](http://arxiv.org/abs/2310.03513)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03513] Exploring DINO: Emergent Properties and Limitations for Synthetic Aperture Radar Imagery](http://arxiv.org/abs/2310.03513) #self-supervised`
* Summary: <p>Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone, and compare them with the model's token embedding space.
We observe a small improvement in model performance with pre-training compared
to training from scratch, and discuss the limitations and opportunities of SSL
for remote sensing and land cover segmentation. Beyond small performance
increases, we show that ViT attention maps hold great intrinsic value for
remote sensing, and could provide useful inputs to other algorithms. With this,
our work lays the ground-work for bigger and better SSL models for Earth
Observation.
</p>

### Title: Regress Before Construct: Regress Autoencoder for Point Cloud Self-supervised Learning. (arXiv:2310.03670v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03670](http://arxiv.org/abs/2310.03670)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03670] Regress Before Construct: Regress Autoencoder for Point Cloud Self-supervised Learning](http://arxiv.org/abs/2310.03670) #self-supervised`
* Summary: <p>Masked Autoencoders (MAE) have demonstrated promising performance in
self-supervised learning for both 2D and 3D computer vision. Nevertheless,
existing MAE-based methods still have certain drawbacks. Firstly, the
functional decoupling between the encoder and decoder is incomplete, which
limits the encoder's representation learning ability. Secondly, downstream
tasks solely utilize the encoder, failing to fully leverage the knowledge
acquired through the encoder-decoder architecture in the pre-text task. In this
paper, we propose Point Regress AutoEncoder (Point-RAE), a new scheme for
regressive autoencoders for point cloud self-supervised learning. The proposed
method decouples functions between the decoder and the encoder by introducing a
mask regressor, which predicts the masked patch representation from the visible
patch representation encoded by the encoder and the decoder reconstructs the
target from the predicted masked patch representation. By doing so, we minimize
the impact of decoder updates on the representation space of the encoder.
Moreover, we introduce an alignment constraint to ensure that the
representations for masked patches, predicted from the encoded representations
of visible patches, are aligned with the masked patch presentations computed
from the encoder. To make full use of the knowledge learned in the pre-training
stage, we design a new finetune mode for the proposed Point-RAE. Extensive
experiments demonstrate that our approach is efficient during pre-training and
generalizes well on various downstream tasks. Specifically, our pre-trained
models achieve a high accuracy of \textbf{90.28\%} on the ScanObjectNN hardest
split and \textbf{94.1\%} accuracy on ModelNet40, surpassing all the other
self-supervised learning methods. Our code and pretrained model are public
available at: \url{https://github.com/liuyyy111/Point-RAE}.
</p>

### Title: OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03707](http://arxiv.org/abs/2310.03707)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03707] OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks](http://arxiv.org/abs/2310.03707) #self-supervised`
* Summary: <p>Evasion Attacks (EA) are used to test the robustness of trained neural
networks by distorting input data to misguide the model into incorrect
classifications. Creating these attacks is a challenging task, especially with
the ever-increasing complexity of models and datasets. In this work, we
introduce a self-supervised, computationally economical method for generating
adversarial examples, designed for the unseen black-box setting. Adapting
techniques from representation learning, our method generates on-manifold EAs
that are encouraged to resemble the data distribution. These attacks are
comparable in effectiveness compared to the state-of-the-art when attacking the
model trained on, but are significantly more effective when attacking unseen
models, as the attacks are more related to the data rather than the model
itself. Our experiments consistently demonstrate the method is effective across
various models, unseen data categories, and even defended models, suggesting a
significant role for on-manifold EAs when targeting unseen models.
</p>

### Title: Evaluating Self-Supervised Speech Representations for Indigenous American Languages. (arXiv:2310.03639v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03639](http://arxiv.org/abs/2310.03639)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03639] Evaluating Self-Supervised Speech Representations for Indigenous American Languages](http://arxiv.org/abs/2310.03639) #self-supervised`
* Summary: <p>The application of self-supervision to speech representation learning has
garnered significant interest in recent years, due to its scalability to large
amounts of unlabeled data. However, much progress, both in terms of
pre-training and downstream evaluation, has remained concentrated in
monolingual models that only consider English. Few models consider other
languages, and even fewer consider indigenous ones. In our submission to the
New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR
corpus for Quechua, an indigenous South American Language. We benchmark the
efficacy of large SSL models on Quechua, along with 6 other indigenous
languages such as Guarani and Bribri, on low-resource ASR. Our results show
surprisingly strong performance by state-of-the-art SSL models, showing the
potential generalizability of large-scale models to real-world data.
</p>

### Title: StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor. (arXiv:2310.03380v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2310.03380](http://arxiv.org/abs/2310.03380)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03380] StegGuard: Fingerprinting Self-supervised Pre-trained Encoders via Secrets Embeder and Extractor](http://arxiv.org/abs/2310.03380) #self-supervised`
* Summary: <p>In this work, we propose StegGuard, a novel fingerprinting mechanism to
verify the ownership of the suspect pre-trained encoder using steganography. A
critical perspective in StegGuard is that the unique characteristic of the
transformation from an image to an embedding, conducted by the pre-trained
encoder, can be equivalently exposed how an embeder embeds secrets into images
and how an extractor extracts the secrets from encoder's embeddings with a
tolerable error after the secrets are subjected to the encoder's
transformation. While each independent encoder has a distinct transformation,
the piracy encoder has a similar transformation to the victim. Based on these,
we learn a pair of secrets embeder and extractor as the fingerprint for the
victim encoder. We introduce a frequency-domain channel attention embedding
block into the embeder to adaptively embed secrets into suitable frequency
bands. During verification, if the secrets embedded into the query images can
be extracted with an acceptable error from the suspect encoder's embeddings,
the suspect encoder is determined as piracy, otherwise independent. Extensive
experiments demonstrate that depending on a very limited number of query
images, StegGuard can reliably identify across varied independent encoders, and
is robust against model stealing related attacks including model extraction,
fine-tuning, pruning, embedding noising and shuffle.
</p>

### Title: Fragment-based Pretraining and Finetuning on Molecular Graphs. (arXiv:2310.03274v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03274](http://arxiv.org/abs/2310.03274)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03274] Fragment-based Pretraining and Finetuning on Molecular Graphs](http://arxiv.org/abs/2310.03274) #self-supervised`
* Summary: <p>Property prediction on molecular graphs is an important application of Graph
Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant,
which facilitates the rapid development of self-supervised learning for GNNs in
the chemical domain. In this work, we propose pretraining GNNs at the fragment
level, which serves as a promising middle ground to overcome the limitations of
node-level and graph-level pretraining. Borrowing techniques from recent work
on principle subgraph mining, we obtain a compact vocabulary of prevalent
fragments that span a large pretraining dataset. From the extracted vocabulary,
we introduce several fragment-based contrastive and predictive pretraining
tasks. The contrastive learning task jointly pretrains two different GNNs: one
based on molecular graphs and one based on fragment graphs, which represents
high-order connectivity within molecules. By enforcing the consistency between
the fragment embedding and the aggregated embedding of the corresponding atoms
from the molecular graphs, we ensure that both embeddings capture structural
information at multiple resolutions. The structural information of the fragment
graphs is further exploited to extract auxiliary labels for the graph-level
predictive pretraining. We employ both the pretrained molecular-based and
fragment-based GNNs for downstream prediction, thus utilizing the fragment
information during finetuning. Our models advance the performances on 5 out of
8 common molecular benchmarks and improve the performances on long-range
biological benchmarks by at least 11.5%.
</p>

### Title: How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03494](http://arxiv.org/abs/2310.03494)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03494] How the level sampling process impacts zero-shot generalisation in deep reinforcement learning](http://arxiv.org/abs/2310.03494) #self-supervised`
* Summary: <p>A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent's internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.
</p>

## foundation model
### Title: Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03324](http://arxiv.org/abs/2310.03324)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03324] Investigating the Limitation of CLIP Models: The Worst-Performing Categories](http://arxiv.org/abs/2310.03324) #foundation model`
* Summary: <p>Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
</p>

### Title: Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03150](http://arxiv.org/abs/2310.03150)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03150] Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly](http://arxiv.org/abs/2310.03150) #foundation model`
* Summary: <p>Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
</p>
<p>This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.
</p>

### Title: BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03320](http://arxiv.org/abs/2310.03320)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03320] BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph](http://arxiv.org/abs/2310.03320) #foundation model`
* Summary: <p>Foundation models (FMs) are able to leverage large volumes of unlabeled data
to demonstrate superior performance across a wide range of tasks. However, FMs
developed for biomedical domains have largely remained unimodal, i.e.,
independently trained and used for tasks on protein sequences alone, small
molecule structures alone, or clinical data alone. To overcome this limitation
of biomedical FMs, we present BioBridge, a novel parameter-efficient learning
framework, to bridge independently trained unimodal FMs to establish multimodal
behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn
transformations between one unimodal FM and another without fine-tuning any
underlying unimodal FMs. Our empirical results demonstrate that BioBridge can
beat the best baseline KG embedding methods (on average by around 76.3%) in
cross-modal retrieval tasks. We also identify BioBridge demonstrates
out-of-domain generalization ability by extrapolating to unseen modalities or
relations. Additionally, we also show that BioBridge presents itself as a
general purpose retriever that can aid biomedical multimodal question answering
as well as enhance the guided generation of novel drugs.
</p>

### Title: TimeGPT-1. (arXiv:2310.03589v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03589](http://arxiv.org/abs/2310.03589)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03589] TimeGPT-1](http://arxiv.org/abs/2310.03589) #foundation model`
* Summary: <p>In this paper, we introduce TimeGPT, the first foundation model for time
series, capable of generating accurate predictions for diverse datasets not
seen during training. We evaluate our pre-trained model against established
statistical, machine learning, and deep learning methods, demonstrating that
TimeGPT zero-shot inference excels in performance, efficiency, and simplicity.
Our study provides compelling evidence that insights from other domains of
artificial intelligence can be effectively applied to time series analysis. We
conclude that large-scale time series models offer an exciting opportunity to
democratize access to precise predictions and reduce uncertainty by leveraging
the capabilities of contemporary advancements in deep learning.
</p>

## generative
### Title: Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation. (arXiv:2310.03125v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03125](http://arxiv.org/abs/2310.03125)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03125] Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation](http://arxiv.org/abs/2310.03125) #generative`
* Summary: <p>In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.
</p>

### Title: SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models. (arXiv:2310.03291v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03291](http://arxiv.org/abs/2310.03291)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03291] SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models](http://arxiv.org/abs/2310.03291) #generative`
* Summary: <p>In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
</p>

### Title: Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference. (arXiv:2310.03184v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03184](http://arxiv.org/abs/2310.03184)
* Code URL: [https://github.com/digitalharborfoundation/rag-for-math-qa](https://github.com/digitalharborfoundation/rag-for-math-qa)
* Copy Paste: `<input type="checkbox">[[2310.03184] Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference](http://arxiv.org/abs/2310.03184) #generative`
* Summary: <p>For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.
</p>

### Title: Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03376](http://arxiv.org/abs/2310.03376)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03376] Procedural Text Mining with Large Language Models](http://arxiv.org/abs/2310.03376) #generative`
* Summary: <p>Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.
</p>

### Title: TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03223](http://arxiv.org/abs/2310.03223)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03223] TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design](http://arxiv.org/abs/2310.03223) #generative`
* Summary: <p>We seek to automate the generation of drug-like compounds conditioned to
specific protein pocket targets. Most current methods approximate the
protein-molecule distribution of a finite dataset and, therefore struggle to
generate molecules with significant binding improvement over the training
dataset. We instead frame the pocket-conditioned molecular generation task as
an RL problem and develop TacoGFN, a target conditional Generative Flow Network
model. Our method is explicitly encouraged to generate molecules with desired
properties as opposed to fitting on a pre-existing data distribution. To this
end, we develop transformer-based docking score prediction to speed up docking
score computation and propose TacoGFN to explore molecule space efficiently.
Furthermore, we incorporate several rounds of active learning where generated
samples are queried using a docking oracle to improve the docking score
prediction. This approach allows us to accurately explore as much of the
molecule landscape as we can afford computationally. Empirically, molecules
generated using TacoGFN and its variants significantly outperform all baseline
methods across every property (Docking score, QED, SA, Lipinski), while being
orders of magnitude faster.
</p>

### Title: UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03266](http://arxiv.org/abs/2310.03266)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03266] UniPredict: Large Language Models are Universal Tabular Predictors](http://arxiv.org/abs/2310.03266) #generative`
* Summary: <p>Tabular data prediction is a fundamental machine learning task for many
applications. Existing methods predominantly employ discriminative modeling and
operate under the assumption of a fixed target column, necessitating
re-training for every new predictive task. Inspired by the generative power of
large language models (LLMs), this paper exploits the idea of building
universal tabular data predictors based on generative modeling, namely
UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets
with the capability of comprehending diverse tabular inputs and predicting for
target variables following the input instructions. Specifically, we train a
single LLM on an aggregation of 169 tabular datasets with diverse targets and
compare its performance against baselines that are trained on each dataset
separately. We observe this versatile UniPredict model demonstrates an
advantage over other models, ranging from 5.4% to 13.4%, when compared with the
best tree-boosting baseline and the best neural network baseline, respectively.
We further test UniPredict in few-shot learning settings on another 62 tabular
datasets. Our method achieves strong performance in quickly adapting to new
tasks, where our method outperforms XGBoost over 100% on the low-resource setup
and shows a significant margin over all baselines. We envision that UniPredict
sheds light on developing a universal tabular data prediction system that
learns from data at scale and serves a wide range of prediction tasks.
</p>

### Title: Learning Energy Decompositions for Partial Inference of GFlowNets. (arXiv:2310.03301v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03301](http://arxiv.org/abs/2310.03301)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03301] Learning Energy Decompositions for Partial Inference of GFlowNets](http://arxiv.org/abs/2310.03301) #generative`
* Summary: <p>This paper studies generative flow networks (GFlowNets) to sample objects
from the Boltzmann energy distribution via a sequence of actions. In
particular, we focus on improving GFlowNet with partial inference: training
flow functions with the evaluation of the intermediate states or transitions.
To this end, the recently developed forward-looking GFlowNet reparameterizes
the flow functions based on evaluating the energy of intermediate states.
However, such an evaluation of intermediate energies may (i) be too expensive
or impossible to evaluate and (ii) even provide misleading training signals
under large energy fluctuations along the sequence of actions. To resolve this
issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our
main idea is to (i) decompose the energy of an object into learnable potential
functions defined on state transitions and (ii) reparameterize the flow
functions using the potential functions. In particular, to produce informative
local credits, we propose to regularize the potential to change smoothly over
the sequence of actions. It is also noteworthy that training GFlowNet with our
learned potential can preserve the optimal policy. We empirically verify the
superiority of LED-GFN in five problems including the generation of
unstructured and maximum independent sets, molecular graphs, and RNA sequences.
</p>

### Title: Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03311](http://arxiv.org/abs/2310.03311)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03311] Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses](http://arxiv.org/abs/2310.03311) #generative`
* Summary: <p>Variational dimensionality reduction methods are known for their high
accuracy, generative abilities, and robustness. These methods have many
theoretical justifications. Here we introduce a unifying principle rooted in
information theory to rederive and generalize existing variational methods and
design new ones. We base our framework on an interpretation of the multivariate
information bottleneck, in which two Bayesian networks are traded off against
one another. We interpret the first network as an encoder graph, which
specifies what information to keep when compressing the data. We interpret the
second network as a decoder graph, which specifies a generative model for the
data. Using this framework, we rederive existing dimensionality reduction
methods such as the deep variational information bottleneck (DVIB), beta
variational auto-encoders (beta-VAE), and deep variational canonical
correlation analysis (DVCCA). The framework naturally introduces a trade-off
parameter between compression and reconstruction in the DVCCA family of
algorithms, resulting in the new beta-DVCCA family. In addition, we derive a
new variational dimensionality reduction method, deep variational symmetric
informational bottleneck (DVSIB), which simultaneously compresses two variables
to preserve information between their compressed representations. We implement
all of these algorithms and evaluate their ability to produce shared low
dimensional latent spaces on a modified noisy MNIST dataset. We show that
algorithms that are better matched to the structure of the data (beta-DVCCA and
DVSIB) produce better latent spaces as measured by classification accuracy and
the dimensionality of the latent variables. We believe that this framework can
be used to unify other multi-view representation learning algorithms.
Additionally, it provides a straightforward framework for deriving
problem-specific loss functions.
</p>

### Title: Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03419](http://arxiv.org/abs/2310.03419)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03419] Pre-Training and Fine-Tuning Generative Flow Networks](http://arxiv.org/abs/2310.03419) #generative`
* Summary: <p>Generative Flow Networks (GFlowNets) are amortized samplers that learn
stochastic policies to sequentially generate compositional objects from a given
unnormalized reward distribution. They can generate diverse sets of high-reward
objects, which is an important consideration in scientific discovery tasks.
However, as they are typically trained from a given extrinsic reward function,
it remains an important open challenge about how to leverage the power of
pre-training and train GFlowNets in an unsupervised fashion for efficient
adaptation to downstream tasks. Inspired by recent successes of unsupervised
pre-training in various domains, we introduce a novel approach for reward-free
pre-training of GFlowNets. By framing the training as a self-supervised
problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to
explore the candidate space. Specifically, OC-GFN learns to reach any targeted
outcomes, akin to goal-conditioned policies in reinforcement learning. We show
that the pre-trained OC-GFN model can allow for a direct extraction of a policy
capable of sampling from any new reward functions in downstream tasks.
Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an
intractable marginalization over possible outcomes. We propose a novel way to
approximate this marginalization by learning an amortized predictor enabling
efficient fine-tuning. Extensive experimental results validate the efficacy of
our approach, demonstrating the effectiveness of pre-training the OC-GFN, and
its ability to swiftly adapt to downstream tasks and discover modes more
efficiently. This work may serve as a foundation for further exploration of
pre-training strategies in the context of GFlowNets.
</p>

### Title: Multimarginal generative modeling with stochastic interpolants. (arXiv:2310.03695v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03695](http://arxiv.org/abs/2310.03695)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03695] Multimarginal generative modeling with stochastic interpolants](http://arxiv.org/abs/2310.03695) #generative`
* Summary: <p>Given a set of $K$ probability densities, we consider the multimarginal
generative modeling problem of learning a joint distribution that recovers
these densities as marginals. The structure of this joint distribution should
identify multi-way correspondences among the prescribed marginals. We formalize
an approach to this task within a generalization of the stochastic interpolant
framework, leading to efficient learning algorithms built upon dynamical
transport of measure. Our generative models are defined by velocity and score
fields that can be characterized as the minimizers of simple quadratic
objectives, and they are defined on a simplex that generalizes the time
variable in the usual dynamical transport framework. The resulting transport on
the simplex is influenced by all marginals, and we show that multi-way
correspondences can be extracted. The identification of such correspondences
has applications to style transfer, algorithmic fairness, and data
decorruption. In addition, the multimarginal perspective enables an efficient
algorithm for reducing the dynamical transport cost in the ordinary
two-marginal setting. We demonstrate these capacities with several numerical
examples.
</p>

## anomaly
## in-context
### Title: How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03051](http://arxiv.org/abs/2310.03051)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03051] How FaR Are Large Language Models From Agents with Theory-of-Mind?](http://arxiv.org/abs/2310.03051) #in-context`
* Summary: <p>"Thinking is for Doing." Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.
</p>

### Title: Fine-tune Language Models to Approximate Unbiased In-context Learning. (arXiv:2310.03331v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03331](http://arxiv.org/abs/2310.03331)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03331] Fine-tune Language Models to Approximate Unbiased In-context Learning](http://arxiv.org/abs/2310.03331) #in-context`
* Summary: <p>In-context learning (ICL) is an astonishing emergent ability of large
language models (LLMs). By presenting a prompt that includes multiple
input-output pairs as examples and introducing a new query input, models can
generate the corresponding output. However, the performance of models heavily
relies on the quality of the input prompt when implementing in-context
learning. Biased or imbalanced input prompts can significantly degrade the
performance of language models. To address this issue, we introduce a
reweighted algorithm called RICL (Reweighted In-context Learning). This
algorithm fine-tunes language models using an unbiased validation set to
determine the optimal weight for each input-output example to approximate
unbiased in-context learning. Furthermore, we also introduce a low-cost
reweighted algorithm, a linear optimal weight approximation algorithm called
LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm
requires minimal training cost while providing effective results. We prove the
convergence of our algorithm and validate its performance through experiments
conducted on a numerical dataset. The experimental findings reveal a
substantial improvement in comparison to benchmarks including the performance
of casual prompt-based in-context learning and the performance of a classic
fine-tuning method.
</p>

## memory
### Title: Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)
* Code URL: [https://github.com/EvenJoker/Point-PEFT](https://github.com/EvenJoker/Point-PEFT)
* Copy Paste: `<input type="checkbox">[[2310.03059] Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models](http://arxiv.org/abs/2310.03059) #memory`
* Summary: <p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
</p>

### Title: Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards. (arXiv:2310.03473v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03473](http://arxiv.org/abs/2310.03473)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03473] Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards](http://arxiv.org/abs/2310.03473) #memory`
* Summary: <p>Memory-efficient large language models are good at refining text input for
better readability. However, controllability is a matter of concern when it
comes to text generation tasks with long inputs, such as multi-document
summarization. In this work, we investigate for a generic controllable approach
for multi-document summarization that leverages the capabilities of LLMs to
refine the text. In particular, we train a controllable content extraction
scheme to extract the text that will be refined by an LLM. The scheme is
designed with a novel coverage and coherence intuitive policy, which is duly
rewarded by a passively trained LLM. Our approach yields competitive results in
the evaluation using ROUGE metrics and outperforms potential baselines in
coherence, as per human evaluation.
</p>

### Title: Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software. (arXiv:2310.03175v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2310.03175](http://arxiv.org/abs/2310.03175)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03175] Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software](http://arxiv.org/abs/2310.03175) #memory`
* Summary: <p>Discovering new vulnerabilities and implementing security and privacy
measures are important to protect systems and data against physical attacks.
One such vulnerability is impedance, an inherent property of a device that can
be exploited to leak information through an unintended side channel, thereby
posing significant security and privacy risks. Unlike traditional
vulnerabilities, impedance is often overlooked or narrowly explored, as it is
typically treated as a fixed value at a specific frequency in research and
design endeavors. Moreover, impedance has never been explored as a source of
information leakage. This paper demonstrates that the impedance of an embedded
device is not constant and directly relates to the programs executed on the
device. We define this phenomenon as impedance leakage and use this as a side
channel to extract software instructions from protected memory. Our experiment
on the ATmega328P microcontroller and the Artix 7 FPGA indicates that the
impedance side channel can detect software instructions with 96.1% and 92.6%
accuracy, respectively. Furthermore, we explore the dual nature of the
impedance side channel, highlighting the potential for beneficial purposes and
the associated risk of intellectual property theft. Finally, potential
countermeasures that specifically address impedance leakage are discussed.
</p>

### Title: Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03052](http://arxiv.org/abs/2310.03052)
* Code URL: [https://github.com/cosmoquester/memoria](https://github.com/cosmoquester/memoria)
* Copy Paste: `<input type="checkbox">[[2310.03052] Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing](http://arxiv.org/abs/2310.03052) #memory`
* Summary: <p>Transformers have demonstrated their success in various domains and tasks.
However, Transformers struggle with long input sequences due to their limited
capacity. While one solution is to increase input length, endlessly stretching
the length is unrealistic. Furthermore, humans selectively remember and use
only relevant information from inputs, unlike Transformers which process all
raw data from start to end. We introduce Memoria, a general memory network that
applies Hebbian theory which is a major theory explaining human memory
formulation to enhance long-term dependencies in neural networks. Memoria
stores and retrieves information called engram at multiple memory levels of
working memory, short-term memory, and long-term memory, using connection
weights that change according to Hebb's rule. Through experiments with popular
Transformer-based models like BERT and GPT, we present that Memoria
significantly improves the ability to consider long-term dependencies in
various tasks. Results show that Memoria outperformed existing methodologies in
sorting and language modeling, and long text classification.
</p>

### Title: Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03123](http://arxiv.org/abs/2310.03123)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03123] Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models](http://arxiv.org/abs/2310.03123) #memory`
* Summary: <p>With the blowout development of pre-trained models (PTMs), the efficient
tuning of these models for diverse downstream applications has emerged as a
pivotal research concern. Although recent investigations into prompt tuning
have provided promising avenues, three salient challenges persist: (1) memory
constraint: the continuous growth in the size of open-source PTMs renders
fine-tuning, even a fraction of their parameters, challenging for many
practitioners. (2) model privacy: existing PTMs often function as public API
services, with their parameters inaccessible for effective or tailored
fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates
high-quality datasets, which are typically localized and not shared to public.
To optimally harness each local dataset while navigating memory constraints and
preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT).
This innovative approach eschews reliance on parameter architectures and
private dataset access, instead capitalizing on a central server that aids
local users in collaboratively training a prompt generator through regular
aggregation. Local users leverage API-driven learning via a zero-order
optimizer, obviating the need for PTM deployment. Relative to extensive
fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM
storage and fine-tuning on local machines, tapping into comprehensive,
high-quality, yet private training datasets. A thorough evaluation across 40
datasets spanning CV and NLP tasks underscores the robustness of our proposed
model.
</p>

### Title: History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization. (arXiv:2310.03228v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03228](http://arxiv.org/abs/2310.03228)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03228] History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization](http://arxiv.org/abs/2310.03228) #memory`
* Summary: <p>History matching based on monitoring data will enable uncertainty reduction,
and thus improved aquifer management, in industrial-scale carbon storage
operations. In traditional model-based data assimilation, geomodel parameters
are modified to force agreement between flow simulation results and
observations. In data-space inversion (DSI), history-matched quantities of
interest, e.g., posterior pressure and saturation fields conditioned to
observations, are inferred directly, without constructing posterior geomodels.
This is accomplished efficiently using a set of O(1000) prior simulation
results, data parameterization, and posterior sampling within a Bayesian
setting. In this study, we develop and implement (in DSI) a deep-learning-based
parameterization to represent spatio-temporal pressure and CO2 saturation
fields at a set of time steps. The new parameterization uses an adversarial
autoencoder (AAE) for dimension reduction and a convolutional long short-term
memory (convLSTM) network to represent the spatial distribution and temporal
evolution of the pressure and saturation fields. This parameterization is used
with an ensemble smoother with multiple data assimilation (ESMDA) in the DSI
framework to enable posterior predictions. A realistic 3D system characterized
by prior geological realizations drawn from a range of geological scenarios is
considered. A local grid refinement procedure is introduced to estimate the
error covariance term that appears in the history matching formulation.
Extensive history matching results are presented for various quantities, for
multiple synthetic true models. Substantial uncertainty reduction in posterior
pressure and saturation fields is achieved in all cases. The framework is
applied to efficiently provide posterior predictions for a range of error
covariance specifications. Such an assessment would be expensive using a
model-based approach.
</p>

### Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03294](http://arxiv.org/abs/2310.03294)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03294] LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers](http://arxiv.org/abs/2310.03294) #memory`
* Summary: <p>Increasing the context length of large language models (LLMs) unlocks
fundamentally new capabilities, but also significantly increases the memory
footprints of training. Previous model-parallel systems such as Megatron-LM
partition and compute different attention heads in parallel, resulting in large
communication volumes, so they cannot scale beyond the number of attention
heads, thereby hindering its adoption. In this paper, we introduce a new
approach, LightSeq, for long-context LLMs training. LightSeq has many notable
advantages. First, LightSeq partitions over the sequence dimension, hence is
agnostic to model architectures and readily applicable for models with varying
numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query
attention. Second, LightSeq not only requires up to 4.7x less communication
than Megatron-LM on popular LLMs but also overlaps the communication with
computation. To further reduce the training time, LightSeq features a novel
gradient checkpointing scheme to bypass an forward computation for
memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants
with sequence lengths from 32K to 512K. Through comprehensive experiments on
single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x
end-to-end speedup, and a 2-8x longer sequence length on models with fewer
heads, compared to Megatron-LM. Codes will be available at
https://github.com/RulinShao/LightSeq.
</p>

### Title: Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs. (arXiv:2310.03339v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03339](http://arxiv.org/abs/2310.03339)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03339] Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs](http://arxiv.org/abs/2310.03339) #memory`
* Summary: <p>Accurate forecasts of electricity prices are crucial for the management of
electric power systems and the development of smart applications. European
electricity prices have risen substantially and became highly volatile after
the Russian invasion of Ukraine, challenging established forecasting methods.
Here, we present a Long Short-Term Memory (LSTM) model for the
German-Luxembourg day-ahead electricity prices addressing these challenges. The
recurrent structure of the LSTM allows the model to adapt to trends, while the
joint prediction of both mean and standard deviation enables a probabilistic
prediction. Using a physics-inspired approach - superstatistics - to derive an
explanation for the statistics of prices, we show that the LSTM model
faithfully reproduces both prices and their volatility.
</p>

### Title: GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03399](http://arxiv.org/abs/2310.03399)
* Code URL: [https://github.com/dfdazac/grapes](https://github.com/dfdazac/grapes)
* Copy Paste: `<input type="checkbox">[[2310.03399] GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks](http://arxiv.org/abs/2310.03399) #memory`
* Summary: <p>Graph neural networks (GNNs) learn the representation of nodes in a graph by
aggregating the neighborhood information in various ways. As these networks
grow in depth, their receptive field grows exponentially due to the increase in
neighborhood sizes, resulting in high memory costs. Graph sampling solves
memory issues in GNNs by sampling a small ratio of the nodes in the graph. This
way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed
sampling heuristics, which may not generalize to different structures or tasks.
We introduce GRAPES, an adaptive graph sampling method that learns to identify
sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet
to learn node sampling probabilities given the classification objectives. We
evaluate GRAPES across several small- and large-scale graph benchmarks and
demonstrate its effectiveness in accuracy and scalability. In contrast to
existing sampling methods, GRAPES maintains high accuracy even with small
sample sizes and, therefore, can scale to very large graphs. Our code is
publicly available at https://github.com/dfdazac/grapes.
</p>

### Title: Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2310.03675](http://arxiv.org/abs/2310.03675)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03675] Hadamard Domain Training with Integers for Class Incremental Quantized Learning](http://arxiv.org/abs/2310.03675) #memory`
* Summary: <p>Continual learning is a desirable feature in many modern machine learning
applications, which allows in-field adaptation and updating, ranging from
accommodating distribution shift, to fine-tuning, and to learning new tasks.
For applications with privacy and low latency requirements, the compute and
memory demands imposed by continual learning can be cost-prohibitive for
resource-constraint edge platforms. Reducing computational precision through
fully quantized training (FQT) simultaneously reduces memory footprint and
increases compute efficiency for both training and inference. However,
aggressive quantization especially integer FQT typically degrades model
accuracy to unacceptable levels. In this paper, we propose a technique that
leverages inexpensive Hadamard transforms to enable low-precision training with
only integer matrix multiplications. We further determine which tensors need
stochastic rounding and propose tiled matrix multiplication to enable low-bit
width accumulators. We demonstrate the effectiveness of our technique on
several human activity recognition datasets and CIFAR100 in a class incremental
learning setting. We achieve less than 0.5% and 3% accuracy degradation while
we quantize all matrix multiplications inputs down to 4-bits with 8-bit
accumulators.
</p>

## few-shot
### Title: OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03388](http://arxiv.org/abs/2310.03388)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03388] OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon](http://arxiv.org/abs/2310.03388) #few-shot`
* Summary: <p>Moving deep learning models from the laboratory setting to the open world
entails preparing them to handle unforeseen conditions. In several applications
the occurrence of novel classes during deployment poses a significant threat,
thus it is crucial to effectively detect them. Ideally, this skill should be
used when needed without requiring any further computational training effort at
every new task. Out-of-distribution detection has attracted significant
attention in the last years, however the majority of the studies deal with 2D
images ignoring the inherent 3D nature of the real-world and often confusing
between domain and semantic novelty. In this work, we focus on the latter,
considering the objects geometric structure captured by 3D point clouds
regardless of the specific domain. We advance the field by introducing
OpenPatch that builds on a large pre-trained model and simply extracts from its
intermediate features a set of patch representations that describe each known
class. For any new sample, we obtain a novelty score by evaluating whether it
can be recomposed mainly by patches of a single known class or rather via the
contribution of multiple classes. We present an extensive experimental
evaluation of our approach for the task of semantic novelty detection on
real-world point cloud samples when the reference known data are synthetic. We
demonstrate that OpenPatch excels in both the full and few-shot known sample
scenarios, showcasing its robustness across varying pre-training objectives and
network backbones. The inherent training-free nature of our method allows for
its immediate application to a wide array of real-world tasks, offering a
compelling advantage over approaches that need expensive retraining efforts.
</p>

### Title: PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification. (arXiv:2310.03517v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2310.03517](http://arxiv.org/abs/2310.03517)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03517] PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification](http://arxiv.org/abs/2310.03517) #few-shot`
* Summary: <p>Few-shot image classification has received considerable attention for
addressing the challenge of poor classification performance with limited
samples in novel classes. However, numerous studies have employed sophisticated
learning strategies and diversified feature extraction methods to address this
issue. In this paper, we propose our method called PrototypeFormer, which aims
to significantly advance traditional few-shot image classification approaches
by exploring prototype relationships. Specifically, we utilize a transformer
architecture to build a prototype extraction module, aiming to extract class
representations that are more discriminative for few-shot classification.
Additionally, during the model training process, we propose a contrastive
learning-based optimization approach to optimize prototype features in few-shot
learning scenarios. Despite its simplicity, the method performs remarkably
well, with no bells and whistles. We have experimented with our approach on
several popular few-shot image classification benchmark datasets, which shows
that our method outperforms all current state-of-the-art methods. In
particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way
1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with
accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</p>

### Title: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03214](http://arxiv.org/abs/2310.03214)
* Code URL: [https://github.com/freshllms/freshqa](https://github.com/freshllms/freshqa)
* Copy Paste: `<input type="checkbox">[[2310.03214] FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation](http://arxiv.org/abs/2310.03214) #few-shot`
* Summary: <p>Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.
</p>

### Title: Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning. (arXiv:2310.03249v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03249](http://arxiv.org/abs/2310.03249)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2310.03249] Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning](http://arxiv.org/abs/2310.03249) #few-shot`
* Summary: <p>Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies and BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to make long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.
</p>

### Title: DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2310.03714](http://arxiv.org/abs/2310.03714)
* Code URL: [https://github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)
* Copy Paste: `<input type="checkbox">[[2310.03714] DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](http://arxiv.org/abs/2310.03714) #few-shot`
* Summary: <p>The ML community is rapidly exploring techniques for prompting language
models (LMs) and for stacking them into pipelines that solve complex tasks.
Unfortunately, existing LM pipelines are typically implemented using hard-coded
"prompt templates", i.e. lengthy strings discovered via trial and error. Toward
a more systematic approach for developing and optimizing LM pipelines, we
introduce DSPy, a programming model that abstracts LM pipelines as text
transformation graphs, i.e. imperative computational graphs where LMs are
invoked through declarative modules. DSPy modules are parameterized, meaning
they can learn (by creating and collecting demonstrations) how to apply
compositions of prompting, finetuning, augmentation, and reasoning techniques.
We design a compiler that will optimize any DSPy pipeline to maximize a given
metric. We conduct two case studies, showing that succinct DSPy programs can
express and optimize sophisticated LM pipelines that reason about math word
problems, tackle multi-hop retrieval, answer complex questions, and control
agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
prompting (generally by over 25% and 65%, respectively) and pipelines with
expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
of that, DSPy programs compiled to open and relatively small LMs like
770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
https://github.com/stanfordnlp/dspy
</p>

