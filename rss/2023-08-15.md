## diffusion
### Title: ModelScope Text-to-Video Technical Report. (arXiv:2308.06571v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06571](http://arxiv.org/abs/2308.06571)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06571] ModelScope Text-to-Video Technical Report](http://arxiv.org/abs/2308.06571) #diffusion`
* Summary: <p>This paper introduces ModelScopeT2V, a text-to-video synthesis model that
evolves from a text-to-image synthesis model (i.e., Stable Diffusion).
ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame
generation and smooth movement transitions. The model could adapt to varying
frame numbers during training and inference, rendering it suitable for both
image-text and video-text datasets. ModelScopeT2V brings together three
components (i.e., VQGAN, a text encoder, and a denoising UNet), totally
comprising 1.7 billion parameters, in which 0.5 billion parameters are
dedicated to temporal capabilities. The model demonstrates superior performance
over state-of-the-art methods across three evaluation metrics. The code and an
online demo are available at
\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.
</p>

### Title: LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts. (arXiv:2308.06713v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06713](http://arxiv.org/abs/2308.06713)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06713] LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts](http://arxiv.org/abs/2308.06713) #diffusion`
* Summary: <p>Thanks to the rapid development of diffusion models, unprecedented progress
has been witnessed in image synthesis. Prior works mostly rely on pre-trained
linguistic models, but a text is often too abstract to properly specify all the
spatial properties of an image, e.g., the layout configuration of a scene,
leading to the sub-optimal results of complex scene generation. In this paper,
we achieve accurate complex scene generation by proposing a semantically
controllable Layout-AWare diffusion model, termed LAW-Diffusion. Distinct from
the previous Layout-to-Image generation (L2I) methods that only explore
category-aware relationships, LAW-Diffusion introduces a spatial dependency
parser to encode the location-aware semantic coherence across objects as a
layout embedding and produces a scene with perceptually harmonious object
styles and contextual relations. To be specific, we delicately instantiate each
object's regional semantics as an object region map and leverage a
location-aware cross-object attention module to capture the spatial
dependencies among those disentangled representations. We further propose an
adaptive guidance schedule for our layout guidance to mitigate the trade-off
between the regional semantic alignment and the texture fidelity of generated
objects. Moreover, LAW-Diffusion allows for instance reconfiguration while
maintaining the other regions in a synthesized image by introducing a
layout-aware latent grafting mechanism to recompose its local regional
semantics. To better verify the plausibility of generated scenes, we propose a
new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to
measure how the images preserve the rational and harmonious relations among
contextual objects. Comprehensive experiments demonstrate that our
LAW-Diffusion yields the state-of-the-art generative performance, especially
with coherent object relations.
</p>

### Title: White-box Membership Inference Attacks against Diffusion Models. (arXiv:2308.06405v1 [cs.CR])
* Paper URL: [http://arxiv.org/abs/2308.06405](http://arxiv.org/abs/2308.06405)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06405] White-box Membership Inference Attacks against Diffusion Models](http://arxiv.org/abs/2308.06405) #diffusion`
* Summary: <p>Diffusion models have begun to overshadow GANs and other generative models in
industrial applications due to their superior image generation performance. The
complex architecture of these models furnishes an extensive array of attack
features. In light of this, we aim to design membership inference attacks
(MIAs) catered to diffusion models. We first conduct an exhaustive analysis of
existing MIAs on diffusion models, taking into account factors such as
black-box/white-box models and the selection of attack features. We found that
white-box attacks are highly applicable in real-world scenarios, and the most
effective attacks presently are white-box. Departing from earlier research,
which employs model loss as the attack feature for white-box MIAs, we employ
model gradients in our attack, leveraging the fact that these gradients provide
a more profound understanding of model responses to various samples. We subject
these models to rigorous testing across a range of parameters, including
training steps, sampling frequency, diffusion steps, and data variance. Across
all experimental settings, our method consistently demonstrated near-flawless
attack performance, with attack success rate approaching $100\%$ and attack
AUCROC near $1.0$. We also evaluate our attack against common defense
mechanisms, and observe our attacks continue to exhibit commendable
performance.
</p>

### Title: Size Lowerbounds for Deep Operator Networks. (arXiv:2308.06338v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06338](http://arxiv.org/abs/2308.06338)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06338] Size Lowerbounds for Deep Operator Networks](http://arxiv.org/abs/2308.06338) #diffusion`
* Summary: <p>Deep Operator Networks are an increasingly popular paradigm for solving
regression in infinite dimensions and hence solve families of PDEs in one shot.
In this work, we aim to establish a first-of-its-kind data-dependent lowerbound
on the size of DeepONets required for them to be able to reduce empirical error
on noisy data. In particular, we show that for low training errors to be
obtained on $n$ data points it is necessary that the common output dimension of
the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right
)$. This inspires our experiments with DeepONets solving the
advection-diffusion-reaction PDE, where we demonstrate the possibility that at
a fixed model size, to leverage increase in this common output dimension and
get monotonic lowering of training error, the size of the training data might
necessarily need to scale quadratically with it.
</p>

### Title: Mirror Diffusion Models. (arXiv:2308.06342v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06342](http://arxiv.org/abs/2308.06342)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06342] Mirror Diffusion Models](http://arxiv.org/abs/2308.06342) #diffusion`
* Summary: <p>Diffusion models have successfully been applied to generative tasks in
various continuous domains. However, applying diffusion to discrete categorical
data remains a non-trivial task. Moreover, generation in continuous domains
often requires clipping in practice, which motivates the need for a theoretical
framework for adapting diffusion to constrained domains. Inspired by the mirror
Langevin algorithm for the constrained sampling problem, in this theoretical
report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the
context of simplex diffusion and propose natural extensions to popular domains
such as image and text generation.
</p>

### Title: EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction. (arXiv:2308.06564v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06564](http://arxiv.org/abs/2308.06564)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06564] EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction](http://arxiv.org/abs/2308.06564) #diffusion`
* Summary: <p>Accurate trajectory prediction is crucial for the safe and efficient
operation of autonomous vehicles. The growing popularity of deep learning has
led to the development of numerous methods for trajectory prediction. While
deterministic deep learning models have been widely used, deep generative
models have gained popularity as they learn data distributions from training
data and account for trajectory uncertainties. In this study, we propose
EquiDiff, a deep generative model for predicting future vehicle trajectories.
EquiDiff is based on the conditional diffusion model, which generates future
trajectories by incorporating historical information and random Gaussian noise.
The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully
utilizes the geometric properties of location coordinates. In addition, we
employ Recurrent Neural Networks and Graph Attention Networks to extract social
interactions from historical trajectories. To evaluate the performance of
EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results
demonstrate that EquiDiff outperforms other baseline models in short-term
prediction, but has slightly higher errors for long-term prediction.
Furthermore, we conduct an ablation study to investigate the contribution of
each component of EquiDiff to the prediction accuracy. Additionally, we present
a visualization of the generation process of our diffusion model, providing
insights into the uncertainty of the prediction.
</p>

### Title: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06644](http://arxiv.org/abs/2308.06644)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06644] Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation](http://arxiv.org/abs/2308.06644) #diffusion`
* Summary: <p>Graph-based diffusion models have shown promising results in terms of
generating high-quality solutions to NP-complete (NPC) combinatorial
optimization (CO) problems. However, those models are often inefficient in
inference, due to the iterative evaluation nature of the denoising diffusion
process. This paper proposes to use progressive distillation to speed up the
inference by taking fewer steps (e.g., forecasting two steps ahead within a
single step) during the denoising process. Our experimental results show that
the progressively distilled model can perform inference 16 times faster with
only 0.019% degradation in performance on the TSP-50 dataset.
</p>

### Title: Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06671](http://arxiv.org/abs/2308.06671)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06671] Law of Balance and Stationary Distribution of Stochastic Gradient Descent](http://arxiv.org/abs/2308.06671) #diffusion`
* Summary: <p>The stochastic gradient descent (SGD) algorithm is the algorithm we use to
train neural networks. However, it remains poorly understood how the SGD
navigates the highly nonlinear and degenerate loss landscape of a neural
network. In this work, we prove that the minibatch noise of SGD regularizes the
solution towards a balanced solution whenever the loss function contains a
rescaling symmetry. Because the difference between a simple diffusion process
and SGD dynamics is the most significant when symmetries are present, our
theory implies that the loss function symmetries constitute an essential probe
of how SGD works. We then apply this result to derive the stationary
distribution of stochastic gradient flow for a diagonal linear network with
arbitrary depth and width. The stationary distribution exhibits complicated
nonlinear phenomena such as phase transitions, broken ergodicity, and
fluctuation inversion. These phenomena are shown to exist uniquely in deep
networks, implying a fundamental difference between deep and shallow models.
</p>

### Title: Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model. (arXiv:2308.06708v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06708](http://arxiv.org/abs/2308.06708)
* Code URL: [https://github.com/yasahi-hpc/generative-enkf](https://github.com/yasahi-hpc/generative-enkf)
* Copy Paste: `<input type="checkbox">[[2308.06708] Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model](http://arxiv.org/abs/2308.06708) #diffusion`
* Summary: <p>This paper presents an ensemble data assimilation method using the pseudo
ensembles generated by denoising diffusion probabilistic model. Since the model
is trained against noisy and sparse observation data, this model can produce
divergent ensembles close to observations. Thanks to the variance in generated
ensembles, our proposed method displays better performance than the
well-established ensemble data assimilation method when the simulation model is
imperfect.
</p>

## self-supervised
### Title: Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06534](http://arxiv.org/abs/2308.06534)
* Code URL: [https://github.com/wolfda95/ssl-medicalimagining-cl-mae](https://github.com/wolfda95/ssl-medicalimagining-cl-mae)
* Copy Paste: `<input type="checkbox">[[2308.06534] Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models](http://arxiv.org/abs/2308.06534) #self-supervised`
* Summary: <p>Deep learning in medical imaging has the potential to minimize the risk of
diagnostic errors, reduce radiologist workload, and accelerate diagnosis.
Training such deep learning models requires large and accurate datasets, with
annotations for all training samples. However, in the medical imaging domain,
annotated datasets for specific tasks are often small due to the high
complexity of annotations, limited access, or the rarity of diseases. To
address this challenge, deep learning models can be pre-trained on large image
datasets without annotations using methods from the field of self-supervised
learning. After pre-training, small annotated datasets are sufficient to
fine-tune the models for a specific task, the so-called ``downstream task". The
most popular self-supervised pre-training approaches in medical imaging are
based on contrastive learning. However, recent studies in natural image
processing indicate a strong potential for masked autoencoder approaches. Our
work compares state-of-the-art contrastive learning methods with the recently
introduced masked autoencoder approach "SparK" for convolutional neural
networks (CNNs) on medical images. Therefore we pre-train on a large
unannotated CT image dataset and fine-tune on several downstream CT
classification tasks. Due to the challenge of obtaining sufficient annotated
training data in the medical imaging domain, it is of particular interest to
evaluate how the self-supervised pre-training methods perform on small
downstream datasets. By experimenting with gradually reducing the training
dataset size of our downstream tasks, we find that the reduction has different
effects depending on the type of pre-training chosen. The SparK pre-training
method is more robust to the training dataset size than the contrastive
methods. Based on our results, we propose the SparK pre-training for medical
downstream tasks with small datasets.
</p>

## foundation model
### Title: Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06668](http://arxiv.org/abs/2308.06668)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06668] Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges](http://arxiv.org/abs/2308.06668) #foundation model`
* Summary: <p>The past decade has witnessed the rapid development of ML and DL
methodologies in agricultural systems, showcased by great successes in variety
of agricultural applications. However, these conventional ML/DL models have
certain limitations: They heavily rely on large, costly-to-acquire labeled
datasets for training, require specialized expertise for development and
maintenance, and are mostly tailored for specific tasks, thus lacking
generalizability. Recently, foundation models have demonstrated remarkable
successes in language and vision tasks across various domains. These models are
trained on a vast amount of data from multiple domains and modalities. Once
trained, they can accomplish versatile tasks with just minor fine-tuning and
minimal task-specific labeled data. Despite their proven effectiveness and huge
potential, there has been little exploration of applying FMs to agriculture
fields. Therefore, this study aims to explore the potential of FMs in the field
of smart agriculture. In particular, we present conceptual tools and technical
background to facilitate the understanding of the problem space and uncover new
research directions in this field. To this end, we first review recent FMs in
the general computer science domain and categorize them into four categories:
language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs.
Subsequently, we outline the process of developing agriculture FMs and discuss
their potential applications in smart agriculture. We also discuss the unique
challenges associated with developing AFMs, including model training,
validation, and deployment. Through this study, we contribute to the
advancement of AI in agriculture by introducing AFMs as a promising paradigm
that can significantly mitigate the reliance on extensive labeled datasets and
enhance the efficiency, effectiveness, and generalization of agricultural AI
systems.
</p>

## generative
### Title: Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.06701](http://arxiv.org/abs/2308.06701)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06701] Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection](http://arxiv.org/abs/2308.06701) #generative`
* Summary: <p>Camouflaged objects that blend into natural scenes pose significant
challenges for deep-learning models to detect and synthesize. While camouflaged
object detection is a crucial task in computer vision with diverse real-world
applications, this research topic has been constrained by limited data
availability. We propose a framework for synthesizing camouflage data to
enhance the detection of camouflaged objects in natural scenes. Our approach
employs a generative model to produce realistic camouflage images, which can be
used to train existing object detection models. Specifically, we use a
camouflage environment generator supervised by a camouflage distribution
classifier to synthesize the camouflage images, which are then fed into our
generator to expand the dataset. Our framework outperforms the current
state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON),
demonstrating its effectiveness in improving camouflaged object detection. This
approach can serve as a plug-and-play data generation and augmentation module
for existing camouflaged object detection tasks and provides a novel way to
introduce more diversity and distributions into current camouflage datasets.
</p>

## anomaly
### Title: ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06663](http://arxiv.org/abs/2308.06663)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06663] ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN](http://arxiv.org/abs/2308.06663) #anomaly`
* Summary: <p>Anomaly detection in time series data, to identify points that deviate from
normal behaviour, is a common problem in various domains such as manufacturing,
medical imaging, and cybersecurity. Recently, Generative Adversarial Networks
(GANs) are shown to be effective in detecting anomalies in time series data.
The neural network architecture of GANs (i.e. Generator and Discriminator) can
significantly improve anomaly detection accuracy. In this paper, we propose a
new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an
LSTM network for improved anomaly detection in both univariate and multivariate
time series data in an unsupervised setting. We evaluate the performance of
ALGAN on 46 real-world univariate time series datasets and a large multivariate
dataset that spans multiple domains. Our experiments demonstrate that ALGAN
outperforms traditional, neural network-based, and other GAN-based methods for
anomaly detection in time series data.
</p>

## in-context
## memory
### Title: Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest. (arXiv:2308.06471v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.06471](http://arxiv.org/abs/2308.06471)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06471] Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest](http://arxiv.org/abs/2308.06471) #memory`
* Summary: <p>Intelligent automation supports us against cyclones, droughts, and seismic
events with recent technology advancements. Algorithmic learning has advanced
fields like neuroscience, genetics, and human-computer interaction. Time-series
data boosts progress. Challenges persist in adopting these approaches in
traditional fields. Neural networks face comprehension and bias issues. AI's
expansion across scientific areas is due to adaptable descriptors and
combinatorial argumentation. This article focuses on modeling Forest loss using
the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest
cover, demonstrated on Amazon Rainforest data against other forecasters like
Long Short-Term Memory, N-BEATS, RCN.
</p>

## few-shot
### Title: Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.06354](http://arxiv.org/abs/2308.06354)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06354] Large Language Models to Identify Social Determinants of Health in Electronic Health Records](http://arxiv.org/abs/2308.06354) #few-shot`
* Summary: <p>Social determinants of health (SDoH) have an important impact on patient
outcomes but are incompletely collected from the electronic health records
(EHR). This study researched the ability of large language models to extract
SDoH from free text in EHRs, where they are most commonly documented, and
explored the role of synthetic clinical text for improving the extraction of
these scarcely documented, yet extremely valuable, clinical data. 800 patient
notes were annotated for SDoH categories, and several transformer-based models
were evaluated. The study also experimented with synthetic data generation and
assessed for algorithmic bias. Our best-performing models were fine-tuned
Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The
benefit of augmenting fine-tuning with synthetic data varied across model
architecture and size, with smaller Flan-T5 models (base and large) showing the
greatest improvements in performance (delta F1 +0.12 to +0.23). Model
performance was similar on the in-hospital system dataset but worse on the
MIMIC-III dataset. Our best-performing fine-tuned models outperformed zero- and
few-shot performance of ChatGPT-family models for both tasks. These fine-tuned
models were less likely than ChatGPT to change their prediction when
race/ethnicity and gender descriptors were added to the text, suggesting less
algorithmic bias (p&lt;0.05). At the patient-level, our models identified 93.8% of
patients with adverse SDoH, while ICD-10 codes captured 2.0%. Our method can
effectively extracted SDoH information from clinic notes, performing better
compare to GPT zero- and few-shot settings. These models could enhance
real-world evidence on SDoH and aid in identifying patients needing social
support.
</p>

### Title: Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension. (arXiv:2308.06454v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.06454](http://arxiv.org/abs/2308.06454)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06454] Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension](http://arxiv.org/abs/2308.06454) #few-shot`
* Summary: <p>Although deep learning techniques have shown significant achievements, they
frequently depend on extensive amounts of hand-labeled data and tend to perform
inadequately in few-shot scenarios. The objective of this study is to devise a
strategy that can improve the model's capability to recognize biomedical
entities in scenarios of few-shot learning. By redefining biomedical named
entity recognition (BioNER) as a machine reading comprehension (MRC) problem,
we propose a demonstration-based learning method to address few-shot BioNER,
which involves constructing appropriate task demonstrations. In assessing our
proposed method, we compared the proposed method with existing advanced methods
using six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical,
BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models'
efficacy by reporting F1 scores from both the 25-shot and 50-shot learning
experiments. In 25-shot learning, we observed 1.1% improvements in the average
F1 scores compared to the baseline method, reaching 61.7%, 84.1%, 69.1%, 70.1%,
50.6%, and 59.9% on six datasets, respectively. In 50-shot learning, we further
improved the average F1 scores by 1.0% compared to the baseline method,
reaching 73.1%, 86.8%, 76.1%, 75.6%, 61.7%, and 65.4%, respectively. We
reported that in the realm of few-shot learning BioNER, MRC-based language
models are much more proficient in recognizing biomedical entities compared to
the sequence labeling approach. Furthermore, our MRC-language models can
compete successfully with fully-supervised learning methodologies that rely
heavily on the availability of abundant annotated data. These results highlight
possible pathways for future advancements in few-shot BioNER methodologies.
</p>

### Title: GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. (arXiv:2308.06463v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.06463](http://arxiv.org/abs/2308.06463)
* Code URL: [https://github.com/robustnlp/cipherchat](https://github.com/robustnlp/cipherchat)
* Copy Paste: `<input type="checkbox">[[2308.06463] GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](http://arxiv.org/abs/2308.06463) #few-shot`
* Summary: <p>Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</p>

### Title: Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.06502](http://arxiv.org/abs/2308.06502)
* Code URL: [https://github.com/oplatek/chateval-llm](https://github.com/oplatek/chateval-llm)
* Copy Paste: `<input type="checkbox">[[2308.06502] Three Ways of Using Large Language Models to Evaluate Chat](http://arxiv.org/abs/2308.06502) #few-shot`
* Summary: <p>This paper describes the systems submitted by team6 for ChatEval, the DSTC 11
Track 4 competition. We present three different approaches to predicting
turn-level qualities of chatbot responses based on large language models
(LLMs). We report improvement over the baseline using dynamic few-shot examples
from a vector store for the prompts for ChatGPT. We also analyze the
performance of the other two approaches and report needed improvements for
future work. We developed the three systems over just two weeks, showing the
potential of LLMs for this task. An ablation study conducted after the
challenge deadline shows that the new Llama 2 models are closing the
performance gap between ChatGPT and open-source LLMs. However, we find that the
Llama 2 models do not benefit from few-shot examples in the same way as
ChatGPT.
</p>

### Title: AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models. (arXiv:2308.06507v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.06507](http://arxiv.org/abs/2308.06507)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.06507] AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models](http://arxiv.org/abs/2308.06507) #few-shot`
* Summary: <p>Information-seeking conversation, which aims to help users gather information
through conversation, has achieved great progress in recent years. However, the
research is still stymied by the scarcity of training data. To alleviate this
problem, we propose AutoConv for synthetic conversation generation, which takes
advantage of the few-shot learning ability and generation capacity of large
language models (LLM). Specifically, we formulate the conversation generation
problem as a language modeling task, then finetune an LLM with a few human
conversations to capture the characteristics of the information-seeking process
and use it for generating synthetic conversations with high quality.
Experimental results on two frequently-used datasets verify that AutoConv has
substantial improvements over strong baselines and alleviates the dependence on
human annotation. In addition, we also provide several analysis studies to
promote future research.
</p>

