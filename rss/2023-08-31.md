## diffusion
### Title: Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models. (arXiv:2308.15692v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15692](http://arxiv.org/abs/2308.15692)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15692] Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models](http://arxiv.org/abs/2308.15692) #diffusion`
* Summary: <p>Denoising probabilistic diffusion models have shown breakthrough performance
that can generate more photo-realistic images or human-level illustrations than
the prior models such as GANs. This high image-generation capability has
stimulated the creation of many downstream applications in various areas.
However, we find that this technology is indeed a double-edged sword: We
identify a new type of attack, called the Natural Denoising Diffusion (NDD)
attack based on the finding that state-of-the-art deep neural network (DNN)
models still hold their prediction even if we intentionally remove their robust
features, which are essential to the human visual system (HVS), by text
prompts. The NDD attack can generate low-cost, model-agnostic, and
transferrable adversarial attacks by exploiting the natural attack capability
in diffusion models. Motivated by the finding, we construct a large-scale
dataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically
evaluate the risk of the natural attack capability of diffusion models with
state-of-the-art text-to-image diffusion models. We evaluate the natural attack
capability by answering 6 research questions. Through a user study to confirm
the validity of the NDD attack, we find that the NDD attack can achieve an 88%
detection rate while being stealthy to 93% of human subjects. We also find that
the non-robust features embedded by diffusion models contribute to the natural
attack capability. To confirm the model-agnostic and transferrable attack
capability, we perform the NDD attack against an AD vehicle and find that 73%
of the physically printed attacks can be detected as a stop sign. We hope that
our study and dataset can help our community to be aware of the risk of
diffusion models and facilitate further research toward robust DNN models.
</p>

### Title: Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15854](http://arxiv.org/abs/2308.15854)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15854] Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models](http://arxiv.org/abs/2308.15854) #diffusion`
* Summary: <p>Denoising diffusion models have shown outstanding performance in image
editing. Existing works tend to use either image-guided methods, which provide
a visual reference but lack control over semantic coherence, or text-guided
methods, which ensure faithfulness to text guidance but lack visual quality. To
address the problem, we propose the Zero-shot Inversion Process (ZIP), a
framework that injects a fusion of generated visual reference and text guidance
into the semantic latent space of a \textit{frozen} pre-trained diffusion
model. Only using a tiny neural network, the proposed ZIP produces diverse
content and attributes under the intuitive control of the text prompt.
Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain
attribute manipulation on real images. We perform detailed experiments on
various benchmark datasets. Compared to state-of-the-art methods, ZIP produces
images of equivalent quality while providing a realistic editing effect.
</p>

### Title: Feature Attention Network (FA-Net): A Deep-Learning Based Approach for Underwater Single Image Enhancement. (arXiv:2308.15868v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15868](http://arxiv.org/abs/2308.15868)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15868] Feature Attention Network (FA-Net): A Deep-Learning Based Approach for Underwater Single Image Enhancement](http://arxiv.org/abs/2308.15868) #diffusion`
* Summary: <p>Underwater image processing and analysis have been a hotspot of study in
recent years, as more emphasis has been focused to underwater monitoring and
usage of marine resources. Compared with the open environment, underwater image
encountered with more complicated conditions such as light abortion,
scattering, turbulence, nonuniform illumination and color diffusion. Although
considerable advances and enhancement techniques achieved in resolving these
issues, they treat low-frequency information equally across the entire channel,
which results in limiting the network's representativeness. We propose a deep
learning and feature-attention-based end-to-end network (FA-Net) to solve this
problem. In particular, we propose a Residual Feature Attention Block (RFAB),
containing the channel attention, pixel attention, and residual learning
mechanism with long and short skip connections. RFAB allows the network to
focus on learning high-frequency information while skipping low-frequency
information on multi-hop connections. The channel and pixel attention mechanism
considers each channel's different features and the uneven distribution of haze
over different pixels in the image. The experimental results shows that the
FA-Net propose by us provides higher accuracy, quantitatively and qualitatively
and superiority to previous state-of-the-art methods.
</p>

### Title: Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation. (arXiv:2308.15918v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15918](http://arxiv.org/abs/2308.15918)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15918] Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation](http://arxiv.org/abs/2308.15918) #diffusion`
* Summary: <p>In the field of parallel imaging (PI), alongside image-domain regularization
methods, substantial research has been dedicated to exploring $k$-space
interpolation. However, the interpretability of these methods remains an
unresolved issue. Furthermore, these approaches currently face acceleration
limitations that are comparable to those experienced by image-domain methods.
In order to enhance interpretability and overcome the acceleration limitations,
this paper introduces an interpretable framework that unifies both $k$-space
interpolation techniques and image-domain methods, grounded in the physical
principles of heat diffusion equations. Building upon this foundational
framework, a novel $k$-space interpolation method is proposed. Specifically, we
model the process of high-frequency information attenuation in $k$-space as a
heat diffusion equation, while the effort to reconstruct high-frequency
information from low-frequency regions can be conceptualized as a reverse heat
equation. However, solving the reverse heat equation poses a challenging
inverse problem. To tackle this challenge, we modify the heat equation to align
with the principles of magnetic resonance PI physics and employ the score-based
generative method to precisely execute the modified reverse heat diffusion.
Finally, experimental validation conducted on publicly available datasets
demonstrates the superiority of the proposed approach over traditional
$k$-space interpolation methods, deep learning-based $k$-space interpolation
methods, and conventional diffusion models in terms of reconstruction accuracy,
particularly in high-frequency regions.
</p>

### Title: DiffuVolume: Diffusion Model for Volume based Stereo Matching. (arXiv:2308.15989v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15989](http://arxiv.org/abs/2308.15989)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15989] DiffuVolume: Diffusion Model for Volume based Stereo Matching](http://arxiv.org/abs/2308.15989) #diffusion`
* Summary: <p>Stereo matching is a significant part in many computer vision tasks and
driving-based applications. Recently cost volume-based methods have achieved
great success benefiting from the rich geometry information in paired images.
However, the redundancy of cost volume also interferes with the model training
and limits the performance. To construct a more precise cost volume, we
pioneeringly apply the diffusion model to stereo matching. Our method, termed
DiffuVolume, considers the diffusion model as a cost volume filter, which will
recurrently remove the redundant information from the cost volume. Two main
designs make our method not trivial. Firstly, to make the diffusion model more
adaptive to stereo matching, we eschew the traditional manner of directly
adding noise into the image but embed the diffusion model into a task-specific
module. In this way, we outperform the traditional diffusion stereo matching
method by 22% EPE improvement and 240 times inference acceleration. Secondly,
DiffuVolume can be easily embedded into any volume-based stereo matching
network with boost performance but slight parameters rise (only 2%). By adding
the DiffuVolume into well-performed methods, we outperform all the published
methods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shot
generalization setting. It is worth mentioning that the proposed model ranks
1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July
2023.
</p>

### Title: SignDiff: Learning Diffusion Models for American Sign Language Production. (arXiv:2308.16082v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.16082](http://arxiv.org/abs/2308.16082)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.16082] SignDiff: Learning Diffusion Models for American Sign Language Production](http://arxiv.org/abs/2308.16082) #diffusion`
* Summary: <p>The field of Sign Language Production (SLP) lacked a large-scale, pre-trained
model based on deep learning for continuous American Sign Language (ASL)
production in the past decade. This limitation hampers communication for all
individuals with disabilities relying on ASL. To address this issue, we
undertook the secondary development and utilization of How2Sign, one of the
largest publicly available ASL datasets. Despite its significance, prior
researchers in the field of sign language have not effectively employed this
corpus due to the intricacies involved in American Sign Language Production
(ASLP).
</p>
<p>To conduct large-scale ASLP, we propose SignDiff based on the latest work in
related fields, which is a dual-condition diffusion pre-training model that can
generate human sign language speakers from a skeleton pose. SignDiff has a
novel Frame Reinforcement Network called FR-Net, similar to dense human pose
estimation work, which enhances the correspondence between text lexical symbols
and sign language dense pose frames reduce the occurrence of multiple fingers
in the diffusion model. In addition, our ASLP method proposes two new improved
modules and a new loss function to improve the accuracy and quality of sign
language skeletal posture and enhance the ability of the model to train on
large-scale data.
</p>
<p>We propose the first baseline for ASL production and report the scores of
17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We also evaluated our
model on the previous mainstream dataset called PHOENIX14T, and the main
experiments achieved the results of SOTA. In addition, our image quality far
exceeds all previous results by 10 percentage points on the SSIM indicator.
Finally, we conducted ablation studies and qualitative evaluations for
discussion.
</p>

## self-supervised
## foundation model
### Title: Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15670](http://arxiv.org/abs/2308.15670)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15670] Multimodal Foundation Models For Echocardiogram Interpretation](http://arxiv.org/abs/2308.15670) #foundation model`
* Summary: <p>Multimodal deep learning foundation models can learn the relationship between
images and text. In the context of medical imaging, mapping images to language
concepts reflects the clinical task of diagnostic image interpretation, however
current general-purpose foundation models do not perform well in this context
because their training corpus have limited medical text and images. To address
this challenge and account for the range of cardiac physiology, we leverage
1,032,975 cardiac ultrasound videos and corresponding expert interpretations to
develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP
displays strong zero-shot (not explicitly trained) performance in cardiac
function assessment (external validation left ventricular ejection fraction
mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac
devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and
artificial heart valves). We also developed a long-context variant (EchoCLIP-R)
with a custom echocardiography report text tokenizer which can accurately
identify unique patients across multiple videos (AUC of 0.86), identify
clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac
surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal
retrieval rank in the top 1% of candidate text reports). These emergent
capabilities can be used for preliminary assessment and summarization of
echocardiographic findings.
</p>

## generative
### Title: Unveiling Camouflage: A Learnable Fourier-based Augmentation for Camouflaged Object Detection and Instance Segmentation. (arXiv:2308.15660v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15660](http://arxiv.org/abs/2308.15660)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15660] Unveiling Camouflage: A Learnable Fourier-based Augmentation for Camouflaged Object Detection and Instance Segmentation](http://arxiv.org/abs/2308.15660) #generative`
* Summary: <p>Camouflaged object detection (COD) and camouflaged instance segmentation
(CIS) aim to recognize and segment objects that are blended into their
surroundings, respectively. While several deep neural network models have been
proposed to tackle those tasks, augmentation methods for COD and CIS have not
been thoroughly explored. Augmentation strategies can help improve the
performance of models by increasing the size and diversity of the training data
and exposing the model to a wider range of variations in the data. Besides, we
aim to automatically learn transformations that help to reveal the underlying
structure of camouflaged objects and allow the model to learn to better
identify and segment camouflaged objects. To achieve this, we propose a
learnable augmentation method in the frequency domain for COD and CIS via
Fourier transform approach, dubbed CamoFourier. Our method leverages a
conditional generative adversarial network and cross-attention mechanism to
generate a reference image and an adaptive hybrid swapping with parameters to
mix the low-frequency component of the reference image and the high-frequency
component of the input image. This approach aims to make camouflaged objects
more visible for detection and segmentation models. Without bells and whistles,
our proposed augmentation method boosts the performance of camouflaged object
detectors and camouflaged instance segmenters by large margins.
</p>

### Title: DTrOCR: Decoder-only Transformer for Optical Character Recognition. (arXiv:2308.15996v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15996](http://arxiv.org/abs/2308.15996)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15996] DTrOCR: Decoder-only Transformer for Optical Character Recognition](http://arxiv.org/abs/2308.15996) #generative`
* Summary: <p>Typical text recognition methods rely on an encoder-decoder structure, in
which the encoder extracts features from an image, and the decoder produces
recognized text from these features. In this study, we propose a simpler and
more effective method for text recognition, known as the Decoder-only
Transformer for Optical Character Recognition (DTrOCR). This method uses a
decoder-only Transformer to take advantage of a generative language model that
is pre-trained on a large corpus. We examined whether a generative language
model that has been successful in natural language processing can also be
effective for text recognition in computer vision. Our experiments demonstrated
that DTrOCR outperforms current state-of-the-art methods by a large margin in
the recognition of printed, handwritten, and scene text in both English and
Chinese.
</p>

### Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.16149] Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models](http://arxiv.org/abs/2308.16149) #generative`
* Summary: <p>We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric
foundation and instruction-tuned open generative large language models (LLMs).
The models are based on the GPT-3 decoder-only architecture and are pretrained
on a mixture of Arabic and English texts, including source code in various
programming languages. With 13 billion parameters, they demonstrate better
knowledge and reasoning capabilities in Arabic than any existing open Arabic
and multilingual models by a sizable margin, based on extensive evaluation.
Moreover, the models are competitive in English compared to English-centric
open models of similar size, despite being trained on much less English data.
We provide a detailed description of the training, the tuning, the safety
alignment, and the evaluation of the models. We release two open versions of
the model -- the foundation Jais model, and an instruction-tuned Jais-chat
variant -- with the aim of promoting research on Arabic LLMs. Available at
https://huggingface.co/inception-mbzuai/jais-13b-chat
</p>

### Title: On the Steganographic Capacity of Selected Learning Models. (arXiv:2308.15502v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.15502](http://arxiv.org/abs/2308.15502)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15502] On the Steganographic Capacity of Selected Learning Models](http://arxiv.org/abs/2308.15502) #generative`
* Summary: <p>Machine learning and deep learning models are potential vectors for various
attack scenarios. For example, previous research has shown that malware can be
hidden in deep learning models. Hiding information in a learning model can be
viewed as a form of steganography. In this research, we consider the general
question of the steganographic capacity of learning models. Specifically, for a
wide range of models, we determine the number of low-order bits of the trained
parameters that can be overwritten, without adversely affecting model
performance. For each model considered, we graph the accuracy as a function of
the number of low-order bits that have been overwritten, and for selected
models, we also analyze the steganographic capacity of individual layers. The
models that we test include the classic machine learning techniques of Linear
Regression (LR) and Support Vector Machine (SVM); the popular general deep
learning models of Multilayer Perceptron (MLP) and Convolutional Neural Network
(CNN); the highly-successful Recurrent Neural Network (RNN) architecture of
Long Short-Term Memory (LSTM); the pre-trained transfer learning-based models
VGG16, DenseNet121, InceptionV3, and Xception; and, finally, an Auxiliary
Classifier Generative Adversarial Network (ACGAN). In all cases, we find that a
majority of the bits of each trained parameter can be overwritten before the
accuracy degrades. Of the models tested, the steganographic capacity ranges
from 7.04 KB for our LR experiments, to 44.74 MB for InceptionV3. We discuss
the implications of our results and consider possible avenues for further
research.
</p>

### Title: Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.15730](http://arxiv.org/abs/2308.15730)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15730] Fully Embedded Time-Series Generative Adversarial Networks](http://arxiv.org/abs/2308.15730) #generative`
* Summary: <p>Generative Adversarial Networks (GANs) should produce synthetic data that
fits the underlying distribution of the data being modeled. For real valued
time-series data, this implies the need to simultaneously capture the static
distribution of the data, but also the full temporal distribution of the data
for any potential time horizon. This temporal element produces a more complex
problem that can potentially leave current solutions under-constrained,
unstable during training, or prone to varying degrees of mode collapse. In
FETSGAN, entire sequences are translated directly to the generator's sampling
space using a seq2seq style adversarial auto encoder (AAE), where adversarial
training is used to match the training distribution in both the feature space
and the lower dimensional sampling space. This additional constraint provides a
loose assurance that the temporal distribution of the synthetic samples will
not collapse. In addition, the First Above Threshold (FAT) operator is
introduced to supplement the reconstruction of encoded sequences, which
improves training stability and the overall quality of the synthetic data being
generated. These novel contributions demonstrate a significant improvement to
the current state of the art for adversarial learners in qualitative measures
of temporal similarity and quantitative predictive ability of data generated
through FETSGAN.
</p>

## anomaly
### Title: AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization. (arXiv:2308.15939v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15939](http://arxiv.org/abs/2308.15939)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15939] AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization](http://arxiv.org/abs/2308.15939) #anomaly`
* Summary: <p>Contrastive Language-Image Pre-training (CLIP) models have shown promising
performance on zero-shot visual recognition tasks by learning visual
representations under natural language supervision. Recent studies attempt the
use of CLIP to tackle zero-shot anomaly detection by matching images with
normal and abnormal state prompts. However, since CLIP focuses on building
correspondence between paired text prompts and global image-level
representations, the lack of patch-level vision to text alignment limits its
capability on precise visual anomaly localization. In this work, we introduce a
training-free adaptation (TFA) framework of CLIP for zero-shot anomaly
localization. In the visual encoder, we innovate a training-free value-wise
attention mechanism to extract intrinsic local tokens of CLIP for patch-level
local description. From the perspective of text supervision, we particularly
design a unified domain-aware contrastive state prompting template. On top of
the proposed TFA, we further introduce a test-time adaptation (TTA) mechanism
to refine anomaly localization results, where a layer of trainable parameters
in the adapter is optimized using TFA's pseudo-labels and synthetic
noise-corrupted tokens. With both TFA and TTA adaptation, we significantly
exploit the potential of CLIP for zero-shot anomaly localization and
demonstrate the effectiveness of our proposed methods on various datasets.
</p>

## in-context
### Title: Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.16060](http://arxiv.org/abs/2308.16060)
* Code URL: [https://github.com/raphael-sch/overpassnl](https://github.com/raphael-sch/overpassnl)
* Copy Paste: `<input type="checkbox">[[2308.16060] Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap](http://arxiv.org/abs/2308.16060) #in-context`
* Summary: <p>We present Text-to-OverpassQL, a task designed to facilitate a natural
language interface for querying geodata from OpenStreetMap (OSM). The Overpass
Query Language (OverpassQL) allows users to formulate complex database queries
and is widely adopted in the OSM ecosystem. Generating Overpass queries from
natural language input serves multiple use-cases. It enables novice users to
utilize OverpassQL without prior knowledge, assists experienced users with
crafting advanced queries, and enables tool-augmented large language models to
access information stored in the OSM database. In order to assess the
performance of current sequence generation models on this task, we propose
OverpassNL, a dataset of 8,352 queries with corresponding natural language
inputs. We further introduce task specific evaluation metrics and ground the
evaluation of the Text-to-OverpassQL task by executing the queries against the
OSM database. We establish strong baselines by finetuning sequence-to-sequence
models and adapting large language models with in-context examples. The
detailed evaluation reveals strengths and weaknesses of the considered learning
strategies, laying the foundations for further research into the
Text-to-OverpassQL task.
</p>

## memory
### Title: Towards Earlier Detection of Oral Diseases On Smartphones Using Oral and Dental RGB Images. (arXiv:2308.15705v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.15705](http://arxiv.org/abs/2308.15705)
* Code URL: [https://github.com/megargayu/dentalclassification](https://github.com/megargayu/dentalclassification)
* Copy Paste: `<input type="checkbox">[[2308.15705] Towards Earlier Detection of Oral Diseases On Smartphones Using Oral and Dental RGB Images](http://arxiv.org/abs/2308.15705) #memory`
* Summary: <p>Oral diseases such as periodontal (gum) diseases and dental caries (cavities)
affect billions of people across the world today. However, previous
state-of-the-art models have relied on X-ray images to detect oral diseases,
making them inaccessible to remote monitoring, developing countries, and
telemedicine. To combat this overuse of X-ray imagery, we propose a lightweight
machine learning model capable of detecting calculus (also known as hardened
plaque or tartar) in RGB images while running efficiently on low-end devices.
The model, a modified MobileNetV3-Small neural network transfer learned from
ImageNet, achieved an accuracy of 72.73% (which is comparable to
state-of-the-art solutions) while still being able to run on mobile devices due
to its reduced memory requirements and processing times. A ResNet34-based model
was also constructed and achieved an accuracy of 81.82%. Both of these models
were tested on a mobile app, demonstrating their potential to limit the number
of serious oral disease cases as their predictions can help patients schedule
appointments earlier without the need to go to the clinic.
</p>

### Title: Everything Perturbed All at Once: Enabling Differentiable Graph Attacks. (arXiv:2308.15614v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.15614](http://arxiv.org/abs/2308.15614)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15614] Everything Perturbed All at Once: Enabling Differentiable Graph Attacks](http://arxiv.org/abs/2308.15614) #memory`
* Summary: <p>As powerful tools for representation learning on graphs, graph neural
networks (GNNs) have played an important role in applications including social
networks, recommendation systems, and online web services. However, GNNs have
been shown to be vulnerable to adversarial attacks, which can significantly
degrade their effectiveness. Recent state-of-the-art approaches in adversarial
attacks rely on gradient-based meta-learning to selectively perturb a single
edge with the highest attack score until they reach the budget constraint.
While effective in identifying vulnerable links, these methods are plagued by
high computational costs. By leveraging continuous relaxation and
parameterization of the graph structure, we propose a novel attack method
called Differentiable Graph Attack (DGA) to efficiently generate effective
attacks and meanwhile eliminate the need for costly retraining. Compared to the
state-of-the-art, DGA achieves nearly equivalent attack performance with 6
times less training time and 11 times smaller GPU memory footprint on different
benchmark datasets. Additionally, we provide extensive experimental analyses of
the transferability of the DGA among different graph models, as well as its
robustness against widely-used defense mechanisms.
</p>

### Title: Advanced Deep Regression Models for Forecasting Time Series Oil Production. (arXiv:2308.16105v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.16105](http://arxiv.org/abs/2308.16105)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.16105] Advanced Deep Regression Models for Forecasting Time Series Oil Production](http://arxiv.org/abs/2308.16105) #memory`
* Summary: <p>Global oil demand is rapidly increasing and is expected to reach 106.3
million barrels per day by 2040. Thus, it is vital for hydrocarbon extraction
industries to forecast their production to optimize their operations and avoid
losses. Big companies have realized that exploiting the power of deep learning
(DL) and the massive amount of data from various oil wells for this purpose can
save a lot of operational costs and reduce unwanted environmental impacts. In
this direction, researchers have proposed models using conventional machine
learning (ML) techniques for oil production forecasting. However, these
techniques are inappropriate for this problem as they can not capture
historical patterns found in time series data, resulting in inaccurate
predictions. This research aims to overcome these issues by developing advanced
data-driven regression models using sequential convolutions and long short-term
memory (LSTM) units. Exhaustive analyses are conducted to select the optimal
sequence length, model hyperparameters, and cross-well dataset formation to
build highly generalized robust models. A comprehensive experimental study on
Volve oilfield data validates the proposed models. It reveals that the
LSTM-based sequence learning model can predict oil production better than the
1-D convolutional neural network (CNN) with mean absolute error (MAE) and R2
score of 111.16 and 0.98, respectively. It is also found that the LSTM-based
model performs better than all the existing state-of-the-art solutions and
achieves a 37% improvement compared to a standard linear regression, which is
considered the baseline model in this work.
</p>

## few-shot
### Title: Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation. (arXiv:2308.16110v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.16110](http://arxiv.org/abs/2308.16110)
* Code URL: [https://github.com/kobeshegu/sdtm-gan-acmmm-2023](https://github.com/kobeshegu/sdtm-gan-acmmm-2023)
* Copy Paste: `<input type="checkbox">[[2308.16110] Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation](http://arxiv.org/abs/2308.16110) #few-shot`
* Summary: <p>Few-shot image generation, which aims to produce plausible and diverse images
for one category given a few images from this category, has drawn extensive
attention. Existing approaches either globally interpolate different images or
fuse local representations with pre-defined coefficients. However, such an
intuitive combination of images/features only exploits the most relevant
information for generation, leading to poor diversity and coarse-grained
semantic fusion. To remedy this, this paper proposes a novel textural
modulation (TexMod) mechanism to inject external semantic signals into internal
local representations. Parameterized by the feedback from the discriminator,
our TexMod enables more fined-grained semantic injection while maintaining the
synthesis fidelity. Moreover, a global structural discriminator (StructD) is
developed to explicitly guide the model to generate images with reasonable
layout and outline. Furthermore, the frequency awareness of the model is
reinforced by encouraging the model to distinguish frequency signals. Together
with these techniques, we build a novel and effective model for few-shot image
generation. The effectiveness of our model is identified by extensive
experiments on three popular datasets and various settings. Besides achieving
state-of-the-art synthesis performance on these datasets, our proposed
techniques could be seamlessly integrated into existing models for a further
performance boost.
</p>

### Title: MerA: Merging Pretrained Adapters For Few-Shot Learning. (arXiv:2308.15982v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2308.15982](http://arxiv.org/abs/2308.15982)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.15982] MerA: Merging Pretrained Adapters For Few-Shot Learning](http://arxiv.org/abs/2308.15982) #few-shot`
* Summary: <p>Adapter tuning, which updates only a few parameters, has become a mainstream
method for fine-tuning pretrained language models to downstream tasks. However,
it often yields subpar results in few-shot learning. AdapterFusion, which
assembles pretrained adapters using composition layers tailored to specific
tasks, is a possible solution but significantly increases trainable parameters
and deployment costs. Despite this, our preliminary study reveals that even
single adapters can outperform Adapterfusion in few-shot learning, urging us to
propose \textbf{\texttt{Merging Pretrained Adapters}} (MerA) that efficiently
incorporates pretrained adapters to a single model through model fusion.
Extensive experiments on two PLMs demonstrate that MerA achieves substantial
improvements compared to both single adapters and AdapterFusion. To further
enhance the capacity of MerA, we also introduce a simple yet effective
technique, referred to as the "\textit{same-track}" setting, that merges
adapters from the same track of pretraining tasks. With the implementation of
the "\textit{same-track}" setting, we observe even more impressive gains,
surpassing the performance of both full fine-tuning and adapter tuning by a
substantial margin, e.g., 3.5\% in MRPC and 5.0\% in MNLI.
</p>

