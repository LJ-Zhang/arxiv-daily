## diffusion
### Title: Residual Denoising Diffusion Models. (arXiv:2308.13712v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13712](http://arxiv.org/abs/2308.13712)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13712] Residual Denoising Diffusion Models](http://arxiv.org/abs/2308.13712) #diffusion`
* Summary: <p>Current diffusion-based image restoration methods feed degraded input images
as conditions into the noise estimation network. However, interpreting this
diffusion process is challenging since it essentially generates the target
image from the noise. To establish a unified and more interpretable model for
image generation and restoration, we propose residual denoising diffusion
models (RDDM). In contrast to existing diffusion models (e.g., DDPM or DDIM)
that focus solely on noise estimation, our RDDM predicts residuals to represent
directional diffusion from the target domain to the input domain, while
concurrently estimating noise to account for random perturbations in the
diffusion process. The introduction of residuals allows us to redefine the
forward diffusion process, wherein the target image progressively diffuses into
a purely noisy image or a noise-carrying input image, thus unifying image
generation and restoration. We demonstrate that our sampling process is
consistent with that of DDPM and DDIM through coefficient transformation, and
propose a partially path-independent generation process to better understand
the reverse process. Notably, with native support for conditional inputs, our
RDDM enables a generic UNet, trained with only an $\ell _1$ loss and a batch
size of 1, to compete with state-of-the-art image restoration methods. We
provide code and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/nachifur/RDDM).
</p>

### Title: DiffI2I: Efficient Diffusion Model for Image-to-Image Translation. (arXiv:2308.13767v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13767](http://arxiv.org/abs/2308.13767)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13767] DiffI2I: Efficient Diffusion Model for Image-to-Image Translation](http://arxiv.org/abs/2308.13767) #diffusion`
* Summary: <p>The Diffusion Model (DM) has emerged as the SOTA approach for image
synthesis. However, the existing DM cannot perform well on some image-to-image
translation (I2I) tasks. Different from image synthesis, some I2I tasks, such
as super-resolution, require generating results in accordance with GT images.
Traditional DMs for image synthesis require extensive iterations and large
denoising models to estimate entire images, which gives their strong generative
ability but also leads to artifacts and inefficiency for I2I. To tackle this
challenge, we propose a simple, efficient, and powerful DM framework for I2I,
called DiffI2I. Specifically, DiffI2I comprises three key components: a compact
I2I prior extraction network (CPEN), a dynamic I2I transformer (DI2Iformer),
and a denoising network. We train DiffI2I in two stages: pretraining and DM
training. For pretraining, GT and input images are fed into CPEN$_{S1}$ to
capture a compact I2I prior representation (IPR) guiding DI2Iformer. In the
second stage, the DM is trained to only use the input images to estimate the
same IRP as CPEN$_{S1}$. Compared to traditional DMs, the compact IPR enables
DiffI2I to obtain more accurate outcomes and employ a lighter denoising network
and fewer iterations. Through extensive experiments on various I2I tasks, we
demonstrate that DiffI2I achieves SOTA performance while significantly reducing
computational burdens.
</p>

### Title: ORES: Open-vocabulary Responsible Visual Synthesis. (arXiv:2308.13785v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13785](http://arxiv.org/abs/2308.13785)
* Code URL: [https://github.com/kodenii/ores](https://github.com/kodenii/ores)
* Copy Paste: `<input type="checkbox">[[2308.13785] ORES: Open-vocabulary Responsible Visual Synthesis](http://arxiv.org/abs/2308.13785) #diffusion`
* Summary: <p>Avoiding synthesizing specific visual concepts is an essential challenge in
responsible visual synthesis. However, the visual concept that needs to be
avoided for responsible visual synthesis tends to be diverse, depending on the
region, context, and usage scenarios. In this work, we formalize a new task,
Open-vocabulary Responsible Visual Synthesis (ORES), where the synthesis model
is able to avoid forbidden visual concepts while allowing users to input any
desired content. To address this problem, we present a Two-stage Intervention
(TIN) framework. By introducing 1) rewriting with learnable instruction through
a large-scale language model (LLM) and 2) synthesizing with prompt intervention
on a diffusion synthesis model, it can effectively synthesize images avoiding
any concepts but following the user's query as much as possible. To evaluate on
ORES, we provide a publicly available dataset, baseline models, and benchmark.
Experimental results demonstrate the effectiveness of our method in reducing
risks of image generation. Our work highlights the potential of LLMs in
responsible visual synthesis. Our code and dataset is public available.
</p>

### Title: Unsupervised Domain Adaptation via Domain-Adaptive Diffusion. (arXiv:2308.13893v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13893](http://arxiv.org/abs/2308.13893)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13893] Unsupervised Domain Adaptation via Domain-Adaptive Diffusion](http://arxiv.org/abs/2308.13893) #diffusion`
* Summary: <p>Unsupervised Domain Adaptation (UDA) is quite challenging due to the large
distribution discrepancy between the source domain and the target domain.
Inspired by diffusion models which have strong capability to gradually convert
data distributions across a large gap, we consider to explore the diffusion
technique to handle the challenging UDA task. However, using diffusion models
to convert data distribution across different domains is a non-trivial problem
as the standard diffusion models generally perform conversion from the Gaussian
distribution instead of from a specific domain distribution. Besides, during
the conversion, the semantics of the source-domain data needs to be preserved
for classification in the target domain. To tackle these problems, we propose a
novel Domain-Adaptive Diffusion (DAD) module accompanied by a Mutual Learning
Strategy (MLS), which can gradually convert data distribution from the source
domain to the target domain while enabling the classification model to learn
along the domain transition process. Consequently, our method successfully
eases the challenge of UDA by decomposing the large domain gap into small ones
and gradually enhancing the capacity of classification model to finally adapt
to the target domain. Our method outperforms the current state-of-the-arts by a
large margin on three widely used UDA datasets.
</p>

### Title: Network Embedding Using Sparse Approximations of Random Walks. (arXiv:2308.13663v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13663](http://arxiv.org/abs/2308.13663)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13663] Network Embedding Using Sparse Approximations of Random Walks](http://arxiv.org/abs/2308.13663) #diffusion`
* Summary: <p>In this paper, we propose an efficient numerical implementation of Network
Embedding based on commute times, using sparse approximation of a diffusion
process on the network obtained by a modified version of the diffusion wavelet
algorithm. The node embeddings are computed by optimizing the cross entropy
loss via the stochastic gradient descent method with sampling of
low-dimensional representations of green functions. We demonstrate the efficacy
of this method for data clustering and multi-label classification through
several examples, and compare its performance over existing methods in terms of
efficiency and accuracy. Theoretical issues justifying the scheme are also
discussed.
</p>

## self-supervised
## foundation model
### Title: SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation. (arXiv:2308.13759v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13759](http://arxiv.org/abs/2308.13759)
* Code URL: [https://github.com/yizhezhang2000/samdsk](https://github.com/yizhezhang2000/samdsk)
* Copy Paste: `<input type="checkbox">[[2308.13759] SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation](http://arxiv.org/abs/2308.13759) #foundation model`
* Summary: <p>The Segment Anything Model (SAM) exhibits a capability to segment a wide
array of objects in natural images, serving as a versatile perceptual tool for
various downstream image segmentation tasks. In contrast, medical image
segmentation tasks often rely on domain-specific knowledge (DSK). In this
paper, we propose a novel method that combines the segmentation foundation
model (i.e., SAM) with domain-specific knowledge for reliable utilization of
unlabeled images in building a medical image segmentation model. Our new method
is iterative and consists of two main stages: (1) segmentation model training;
(2) expanding the labeled set by using the trained segmentation model, an
unlabeled set, SAM, and domain-specific knowledge. These two stages are
repeated until no more samples are added to the labeled set. A novel
optimal-matching-based method is developed for combining the SAM-generated
segmentation proposals and pixel-level and image-level DSK for constructing
annotations of unlabeled images in the iterative stage (2). In experiments, we
demonstrate the effectiveness of our proposed method for breast cancer
segmentation in ultrasound images, polyp segmentation in endoscopic images, and
skin lesion segmentation in dermoscopic images. Our work initiates a new
direction of semi-supervised learning for medical image segmentation: the
segmentation foundation model can be harnessed as a valuable tool for
label-efficient segmentation learning in medical image segmentation.
</p>

### Title: Zero-Shot Edge Detection with SCESAME: Spectral Clustering-based Ensemble for Segment Anything Model Estimation. (arXiv:2308.13779v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13779](http://arxiv.org/abs/2308.13779)
* Code URL: [https://github.com/ymgw55/scesame](https://github.com/ymgw55/scesame)
* Copy Paste: `<input type="checkbox">[[2308.13779] Zero-Shot Edge Detection with SCESAME: Spectral Clustering-based Ensemble for Segment Anything Model Estimation](http://arxiv.org/abs/2308.13779) #foundation model`
* Summary: <p>This paper proposes a novel zero-shot edge detection with SCESAME, which
stands for Spectral Clustering-based Ensemble for Segment Anything Model
Estimation, based on the recently proposed Segment Anything Model (SAM). SAM is
a foundation model for segmentation tasks, and one of the interesting
applications of SAM is Automatic Mask Generation (AMG), which generates
zero-shot segmentation masks of an entire image. AMG can be applied to edge
detection, but suffers from the problem of overdetecting edges. Edge detection
with SCESAME overcomes this problem by three steps: (1) eliminating small
generated masks, (2) combining masks by spectral clustering, taking into
account mask positions and overlaps, and (3) removing artifacts after edge
detection. We performed edge detection experiments on two datasets, BSDS500 and
NYUDv2. Although our zero-shot approach is simple, the experimental results on
BSDS500 showed almost identical performance to human performance and CNN-based
methods from seven years ago. In the NYUDv2 experiments, it performed almost as
well as recent CNN-based methods. These results indicate that our method has
the potential to be a strong baseline for future zero-shot edge detection
methods. Furthermore, SCESAME is not only applicable to edge detection, but
also to other downstream zero-shot tasks.
</p>

## generative
### Title: A Systematic Study on Quantifying Bias in GAN-Augmented Data. (arXiv:2308.13554v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13554](http://arxiv.org/abs/2308.13554)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13554] A Systematic Study on Quantifying Bias in GAN-Augmented Data](http://arxiv.org/abs/2308.13554) #generative`
* Summary: <p>Generative adversarial networks (GANs) have recently become a popular data
augmentation technique used by machine learning practitioners. However, they
have been shown to suffer from the so-called mode collapse failure mode, which
makes them vulnerable to exacerbating biases on already skewed datasets,
resulting in the generated data distribution being less diverse than the
training distribution. To this end, we address the problem of quantifying the
extent to which mode collapse occurs. This study is a systematic effort focused
on the evaluation of state-of-the-art metrics that can potentially quantify
biases in GAN-augmented data. We show that, while several such methods are
available, there is no single metric that quantifies bias exacerbation reliably
over the span of different image domains.
</p>

### Title: Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13792](http://arxiv.org/abs/2308.13792)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13792] Out-of-distribution detection using normalizing flows on the data manifold](http://arxiv.org/abs/2308.13792) #generative`
* Summary: <p>A common approach for out-of-distribution detection involves estimating an
underlying data distribution, which assigns a lower likelihood value to
out-of-distribution data. Normalizing flows are likelihood-based generative
models providing a tractable density estimation via dimension-preserving
invertible transformations. Conventional normalizing flows are prone to fail in
out-of-distribution detection, because of the well-known curse of
dimensionality problem of the likelihood-based models. According to the
manifold hypothesis, real-world data often lie on a low-dimensional manifold.
This study investigates the effect of manifold learning using normalizing flows
on out-of-distribution detection. We proceed by estimating the density on a
low-dimensional manifold, coupled with measuring the distance from the
manifold, as criteria for out-of-distribution detection. However, individually,
each of them is insufficient for this task. The extensive experimental results
show that manifold learning improves the out-of-distribution detection ability
of a class of likelihood-based models known as normalizing flows. This
improvement is achieved without modifying the model structure or using
auxiliary out-of-distribution data during training.
</p>

### Title: VIDES: Virtual Interior Design via Natural Language and Visual Guidance. (arXiv:2308.13795v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13795](http://arxiv.org/abs/2308.13795)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13795] VIDES: Virtual Interior Design via Natural Language and Visual Guidance](http://arxiv.org/abs/2308.13795) #generative`
* Summary: <p>Interior design is crucial in creating aesthetically pleasing and functional
indoor spaces. However, developing and editing interior design concepts
requires significant time and expertise. We propose Virtual Interior DESign
(VIDES) system in response to this challenge. Leveraging cutting-edge
technology in generative AI, our system can assist users in generating and
editing indoor scene concepts quickly, given user text description and visual
guidance. Using both visual guidance and language as the conditional inputs
significantly enhances the accuracy and coherence of the generated scenes,
resulting in visually appealing designs. Through extensive experimentation, we
demonstrate the effectiveness of VIDES in developing new indoor concepts,
changing indoor styles, and replacing and removing interior objects. The system
successfully captures the essence of users' descriptions while providing
flexibility for customization. Consequently, this system can potentially reduce
the entry barrier for indoor design, making it more accessible to users with
limited technical skills and reducing the time required to create high-quality
images. Individuals who have a background in design can now easily communicate
their ideas visually and effectively present their design concepts.
https://sites.google.com/view/ltnghia/research/VIDES
</p>

### Title: DM-VTON: Distilled Mobile Real-time Virtual Try-On. (arXiv:2308.13798v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13798](http://arxiv.org/abs/2308.13798)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13798] DM-VTON: Distilled Mobile Real-time Virtual Try-On](http://arxiv.org/abs/2308.13798) #generative`
* Summary: <p>The fashion e-commerce industry has witnessed significant growth in recent
years, prompting exploring image-based virtual try-on techniques to incorporate
Augmented Reality (AR) experiences into online shopping platforms. However,
existing research has primarily overlooked a crucial aspect - the runtime of
the underlying machine-learning model. While existing methods prioritize
enhancing output quality, they often disregard the execution time, which
restricts their applications on a limited range of devices. To address this
gap, we propose Distilled Mobile Real-time Virtual Try-On (DM-VTON), a novel
virtual try-on framework designed to achieve simplicity and efficiency. Our
approach is based on a knowledge distillation scheme that leverages a strong
Teacher network as supervision to guide a Student network without relying on
human parsing. Notably, we introduce an efficient Mobile Generative Module
within the Student network, significantly reducing the runtime while ensuring
high-quality output. Additionally, we propose Virtual Try-on-guided Pose for
Data Synthesis to address the limited pose variation observed in training
images. Experimental results show that the proposed method can achieve 40
frames per second on a single Nvidia Tesla T4 GPU and only take up 37 MB of
memory while producing almost the same output quality as other state-of-the-art
methods. DM-VTON stands poised to facilitate the advancement of real-time AR
applications, in addition to the generation of lifelike attired human figures
tailored for diverse specialized training tasks.
https://sites.google.com/view/ltnghia/research/DMVTON
</p>

### Title: Time-to-Pattern: Information-Theoretic Unsupervised Learning for Scalable Time Series Summarization. (arXiv:2308.13722v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13722](http://arxiv.org/abs/2308.13722)
* Code URL: [https://github.com/alirezaghods/t2p-time-to-pattern](https://github.com/alirezaghods/t2p-time-to-pattern)
* Copy Paste: `<input type="checkbox">[[2308.13722] Time-to-Pattern: Information-Theoretic Unsupervised Learning for Scalable Time Series Summarization](http://arxiv.org/abs/2308.13722) #generative`
* Summary: <p>Data summarization is the process of generating interpretable and
representative subsets from a dataset. Existing time series summarization
approaches often search for recurring subsequences using a set of manually
devised similarity functions to summarize the data. However, such approaches
are fraught with limitations stemming from an exhaustive search coupled with a
heuristic definition of series similarity. Such approaches affect the diversity
and comprehensiveness of the generated data summaries. To mitigate these
limitations, we introduce an approach to time series summarization, called
Time-to-Pattern (T2P), which aims to find a set of diverse patterns that
together encode the most salient information, following the notion of minimum
description length. T2P is implemented as a deep generative model that learns
informative embeddings of the discrete time series on a latent space
specifically designed to be interpretable. Our synthetic and real-world
experiments reveal that T2P discovers informative patterns, even in noisy and
complex settings. Furthermore, our results also showcase the improved
performance of T2P over previous work in pattern diversity and processing
scalability, which conclusively demonstrate the algorithm's effectiveness for
time series summarization.
</p>

### Title: SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13815](http://arxiv.org/abs/2308.13815)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13815] SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy](http://arxiv.org/abs/2308.13815) #generative`
* Summary: <p>Finding a transformation between two unknown probability distributions from
samples is crucial for modeling complex data distributions and perform tasks
such as density estimation, sample generation, and statistical inference. One
powerful framework for such transformations is normalizing flow, which
transforms an unknown distribution into a standard normal distribution using an
invertible network. In this paper, we introduce a novel model called SyMOT-Flow
that trains an invertible transformation by minimizing the symmetric maximum
mean discrepancy between samples from two unknown distributions, and we
incorporate an optimal transport cost as regularization to obtain a
short-distance and interpretable transformation. The resulted transformation
leads to more stable and accurate sample generation. We establish several
theoretical results for the proposed model and demonstrate its effectiveness
with low-dimensional illustrative examples as well as high-dimensional
generative samples obtained through the forward and reverse flows.
</p>

## anomaly
### Title: Exploring Human Crowd Patterns and Categorization in Video Footage for Enhanced Security and Surveillance using Computer Vision and Machine Learning. (arXiv:2308.13910v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13910](http://arxiv.org/abs/2308.13910)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13910] Exploring Human Crowd Patterns and Categorization in Video Footage for Enhanced Security and Surveillance using Computer Vision and Machine Learning](http://arxiv.org/abs/2308.13910) #anomaly`
* Summary: <p>Computer vision and machine learning have brought revolutionary shifts in
perception for researchers, scientists, and the general populace. Once thought
to be unattainable, these technologies have achieved the seemingly impossible.
Their exceptional applications in diverse fields like security, agriculture,
and education are a testament to their impact. However, the full potential of
computer vision remains untapped. This paper explores computer vision's
potential in security and surveillance, presenting a novel approach to track
motion in videos. By categorizing motion into Arcs, Lanes,
Converging/Diverging, and Random/Block motions using Motion Information Images
and Blockwise dominant motion data, the paper examines different optical flow
techniques, CNN models, and machine learning models. Successfully achieving its
objectives with promising accuracy, the results can train anomaly-detection
models, provide behavioral insights based on motion, and enhance scene
comprehension.
</p>

## in-context
## memory
### Title: GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13646](http://arxiv.org/abs/2308.13646)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13646] GRASP: A Rehearsal Policy for Efficient Online Continual Learning](http://arxiv.org/abs/2308.13646) #memory`
* Summary: <p>Continual learning (CL) in deep neural networks (DNNs) involves incrementally
accumulating knowledge in a DNN from a growing data stream. A major challenge
in CL is that non-stationary data streams cause catastrophic forgetting of
previously learned abilities. Rehearsal is a popular and effective way to
mitigate this problem, which is storing past observations in a buffer and
mixing them with new observations during learning. This leads to a question:
Which stored samples should be selected for rehearsal? Choosing samples that
are best for learning, rather than simply selecting them at random, could lead
to significantly faster learning. For class incremental learning, prior work
has shown that a simple class balanced random selection policy outperforms more
sophisticated methods. Here, we revisit this question by exploring a new sample
selection policy called GRASP. GRASP selects the most prototypical (class
representative) samples first and then gradually selects less prototypical
(harder) examples to update the DNN. GRASP has little additional compute or
memory overhead compared to uniform selection, enabling it to scale to large
datasets. We evaluate GRASP and other policies by conducting CL experiments on
the large-scale ImageNet-1K and Places-LT image classification datasets. GRASP
outperforms all other rehearsal policies. Beyond vision, we also demonstrate
that GRASP is effective for CL on five text classification datasets.
</p>

### Title: MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree. (arXiv:2308.13735v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13735](http://arxiv.org/abs/2308.13735)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13735] MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree](http://arxiv.org/abs/2308.13735) #memory`
* Summary: <p>Binary neural networks (BNNs) have been widely adopted to reduce the
computational cost and memory storage on edge-computing devices by using
one-bit representation for activations and weights. However, as neural networks
become wider/deeper to improve accuracy and meet practical requirements, the
computational burden remains a significant challenge even on the binary
version. To address these issues, this paper proposes a novel method called
Minimum Spanning Tree (MST) compression that learns to compress and accelerate
BNNs. The proposed architecture leverages an observation from previous works
that an output channel in a binary convolution can be computed using another
output channel and XNOR operations with weights that differ from the weights of
the reused channel. We first construct a fully connected graph with vertices
corresponding to output channels, where the distance between two vertices is
the number of different values between the weight sets used for these outputs.
Then, the MST of the graph with the minimum depth is proposed to reorder output
calculations, aiming to reduce computational cost and latency. Moreover, we
propose a new learning algorithm to reduce the total MST distance during
training. Experimental results on benchmark models demonstrate that our method
achieves significant compression ratios with negligible accuracy drops, making
it a promising approach for resource-constrained edge-computing devices.
</p>

### Title: Prior-guided Source-free Domain Adaptation for Human Pose Estimation. (arXiv:2308.13954v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13954](http://arxiv.org/abs/2308.13954)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13954] Prior-guided Source-free Domain Adaptation for Human Pose Estimation](http://arxiv.org/abs/2308.13954) #memory`
* Summary: <p>Domain adaptation methods for 2D human pose estimation typically require
continuous access to the source data during adaptation, which can be
challenging due to privacy, memory, or computational constraints. To address
this limitation, we focus on the task of source-free domain adaptation for pose
estimation, where a source model must adapt to a new target domain using only
unlabeled target data. Although recent advances have introduced source-free
methods for classification tasks, extending them to the regression task of pose
estimation is non-trivial. In this paper, we present Prior-guided Self-training
(POST), a pseudo-labeling approach that builds on the popular Mean Teacher
framework to compensate for the distribution shift. POST leverages
prediction-level and feature-level consistency between a student and teacher
model against certain image transformations. In the absence of source data,
POST utilizes a human pose prior that regularizes the adaptation process by
directing the model to generate more accurate and anatomically plausible pose
pseudo-labels. Despite being simple and intuitive, our framework can deliver
significant performance gains compared to applying the source model directly to
the target data, as demonstrated in our extensive experiments and ablation
studies. In fact, our approach achieves comparable performance to recent
state-of-the-art methods that use source data for adaptation.
</p>

### Title: Go Beyond Imagination: Maximizing Episodic Reachability with World Models. (arXiv:2308.13661v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13661](http://arxiv.org/abs/2308.13661)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13661] Go Beyond Imagination: Maximizing Episodic Reachability with World Models](http://arxiv.org/abs/2308.13661) #memory`
* Summary: <p>Efficient exploration is a challenging topic in reinforcement learning,
especially for sparse reward tasks. To deal with the reward sparsity, people
commonly apply intrinsic rewards to motivate agents to explore the state space
efficiently. In this paper, we introduce a new intrinsic reward design called
GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty
motivation with an episodic intrinsic reward that is designed to maximize the
stepwise reachability expansion. More specifically, we apply learned world
models to generate predicted future states with random actions. States with
more unique predictions that are not in episodic memory are assigned high
intrinsic rewards. Our method greatly outperforms previous state-of-the-art
methods on 12 of the most challenging Minigrid navigation tasks and improves
the sample efficiency on locomotion tasks from DeepMind Control Suite.
</p>

### Title: Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization. (arXiv:2308.13898v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2308.13898](http://arxiv.org/abs/2308.13898)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2308.13898] Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization](http://arxiv.org/abs/2308.13898) #memory`
* Summary: <p>Memory-aware network scheduling is becoming increasingly important for deep
neural network (DNN) inference on resource-constrained devices. However, due to
the complex cell-level and network-level topologies, memory-aware scheduling
becomes very challenging. While previous algorithms all suffer from poor
scalability, in this paper, we propose an efficient memory-aware scheduling
framework based on iterative computation graph optimization. Our framework
features an iterative graph fusion algorithm that simplifies the computation
graph while preserving the scheduling optimality. We further propose an integer
linear programming formulation together with topology-aware variable pruning to
schedule the simplified graph efficiently. We evaluate our method against
prior-art algorithms on different networks and demonstrate that our method
outperforms existing techniques in all the benchmarks, reducing the peak memory
footprint by 13.4%, and achieving better scalability for networks with complex
network-level topologies.
</p>

## few-shot
### Title: Transfer Learning for Microstructure Segmentation with CS-UNet: A Hybrid Algorithm with Transformer and CNN Encoders. (arXiv:2308.13917v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2308.13917](http://arxiv.org/abs/2308.13917)
* Code URL: [https://github.com/kalrfou/swint-pretrained-microscopy-models](https://github.com/kalrfou/swint-pretrained-microscopy-models)
* Copy Paste: `<input type="checkbox">[[2308.13917] Transfer Learning for Microstructure Segmentation with CS-UNet: A Hybrid Algorithm with Transformer and CNN Encoders](http://arxiv.org/abs/2308.13917) #few-shot`
* Summary: <p>Transfer learning improves the performance of deep learning models by
initializing them with parameters pre-trained on larger datasets. Intuitively,
transfer learning is more effective when pre-training is on the in-domain
datasets. A recent study by NASA has demonstrated that the microstructure
segmentation with encoder-decoder algorithms benefits more from CNN encoders
pre-trained on microscopy images than from those pre-trained on natural images.
However, CNN models only capture the local spatial relations in images. In
recent years, attention networks such as Transformers are increasingly used in
image analysis to capture the long-range relations between pixels. In this
study, we compare the segmentation performance of Transformer and CNN models
pre-trained on microscopy images with those pre-trained on natural images. Our
result partially confirms the NASA study that the segmentation performance of
out-of-distribution images (taken under different imaging and sample
conditions) is significantly improved when pre-training on microscopy images.
However, the performance gain for one-shot and few-shot learning is more modest
with Transformers. We also find that for image segmentation, the combination of
pre-trained Transformers and CNN encoders are consistently better than
pre-trained CNN encoders alone. Our dataset (of about 50,000 images) combines
the public portion of the NASA dataset with additional images we collected.
Even with much less training data, our pre-trained models have significantly
better performance for image segmentation. This result suggests that
Transformers and CNN complement each other and when pre-trained on microscopy
images, they are more beneficial to the downstream tasks.
</p>

