## diffusion
### Title: Patch-based Selection and Refinement for Early Object Detection. (arXiv:2311.02274v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2311.02274](http://arxiv.org/abs/2311.02274)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02274] Patch-based Selection and Refinement for Early Object Detection](http://arxiv.org/abs/2311.02274) #diffusion`
* Summary: <p>Early object detection (OD) is a crucial task for the safety of many dynamic
systems. Current OD algorithms have limited success for small objects at a long
distance. To improve the accuracy and efficiency of such a task, we propose a
novel set of algorithms that divide the image into patches, select patches with
objects at various scales, elaborate the details of a small object, and detect
it as early as possible. Our approach is built upon a transformer-based network
and integrates the diffusion model to improve the detection accuracy. As
demonstrated on BDD100K, our algorithms enhance the mAP for small objects from
1.03 to 8.93, and reduce the data volume in computation by more than 77\%. The
source code is available at
\href{https://github.com/destiny301/dpr}{https://github.com/destiny301/dpr}
</p>

### Title: Sparse Training of Discrete Diffusion Models for Graph Generation. (arXiv:2311.02142v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02142](http://arxiv.org/abs/2311.02142)
* Code URL: [https://github.com/qym7/sparsediff](https://github.com/qym7/sparsediff)
* Copy Paste: `<input type="checkbox">[[2311.02142] Sparse Training of Discrete Diffusion Models for Graph Generation](http://arxiv.org/abs/2311.02142) #diffusion`
* Summary: <p>Generative models for graphs often encounter scalability challenges due to
the inherent need to predict interactions for every node pair. Despite the
sparsity often exhibited by real-world graphs, the unpredictable sparsity
patterns of their adjacency matrices, stemming from their unordered nature,
leads to quadratic computational complexity. In this work, we introduce
SparseDiff, a denoising diffusion model for graph generation that is able to
exploit sparsity during its training phase. At the core of SparseDiff is a
message-passing neural network tailored to predict only a subset of edges
during each forward pass. When combined with a sparsity-preserving noise model,
this model can efficiently work with edge lists representations of graphs,
paving the way for scalability to much larger structures. During the sampling
phase, SparseDiff iteratively populates the adjacency matrix from its prior
state, ensuring prediction of the full graph while controlling memory
utilization. Experimental results show that SparseDiff simultaneously matches
state-of-the-art in generation performance on both small and large graphs,
highlighting the versatility of our method.
</p>

## self-supervised
### Title: Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells. (arXiv:2311.02316v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02316](http://arxiv.org/abs/2311.02316)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02316] Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells](http://arxiv.org/abs/2311.02316) #self-supervised`
* Summary: <p>To solve the spatial problems of mapping, localization and navigation, the
mammalian lineage has developed striking spatial representations. One important
spatial representation is the Nobel-prize winning grid cells: neurons that
represent self-location, a local and aperiodic quantity, with seemingly bizarre
non-local and spatially periodic activity patterns of a few discrete periods.
Why has the mammalian lineage learnt this peculiar grid representation?
Mathematical analysis suggests that this multi-periodic representation has
excellent properties as an algebraic code with high capacity and intrinsic
error-correction, but to date, there is no satisfactory synthesis of core
principles that lead to multi-modular grid cells in deep recurrent neural
networks. In this work, we begin by identifying key insights from four families
of approaches to answering the grid cell question: coding theory, dynamical
systems, function optimization and supervised deep learning. We then leverage
our insights to propose a new approach that combines the strengths of all four
approaches. Our approach is a self-supervised learning (SSL) framework -
including data, data augmentations, loss functions and a network architecture -
motivated from a normative perspective, without access to supervised position
information or engineering of particular readout representations as needed in
previous approaches. We show that multiple grid cell modules can emerge in
networks trained on our SSL framework and that the networks and emergent
representations generalize well outside their training distribution. This work
contains insights for neuroscientists interested in the origins of grid cells
as well as machine learning researchers interested in novel SSL frameworks.
</p>

## foundation model
### Title: Robust Fine-Tuning of Vision-Language Models for Domain Generalization. (arXiv:2311.02236v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2311.02236](http://arxiv.org/abs/2311.02236)
* Code URL: [https://github.com/mit-ll/robust-vision-language-finetuning](https://github.com/mit-ll/robust-vision-language-finetuning)
* Copy Paste: `<input type="checkbox">[[2311.02236] Robust Fine-Tuning of Vision-Language Models for Domain Generalization](http://arxiv.org/abs/2311.02236) #foundation model`
* Summary: <p>Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning
</p>

## generative
### Title: Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist. (arXiv:2311.02107v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02107](http://arxiv.org/abs/2311.02107)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02107] Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist](http://arxiv.org/abs/2311.02107) #generative`
* Summary: <p>The widespread use of ChatGPT and other emerging technology powered by
generative artificial intelligence (AI) has drawn much attention to potential
ethical issues, especially in high-stakes applications such as healthcare.
However, less clear is how to resolve such issues beyond following guidelines
and regulations that are still under discussion and development. On the other
hand, other types of generative AI have been used to synthesize images and
other types of data for research and practical purposes, which have resolved
some ethical issues and exposed other ethical issues, but such technology is
less often the focus of ongoing ethical discussions. Here we highlight gaps in
current ethical discussions of generative AI via a systematic scoping review of
relevant existing research in healthcare, and reduce the gaps by proposing an
ethics checklist for comprehensive assessment and transparent documentation of
ethical discussions in generative AI development. While the checklist can be
readily integrated into the current peer review and publication system to
enhance generative AI research, it may also be used in broader settings to
disclose ethics-related considerations in generative AI-powered products (or
real-life applications of such products) to help users establish reasonable
trust in their capabilities.
</p>

### Title: Resist Label Noise with PGM for Graph Neural Networks. (arXiv:2311.02116v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02116](http://arxiv.org/abs/2311.02116)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02116] Resist Label Noise with PGM for Graph Neural Networks](http://arxiv.org/abs/2311.02116) #generative`
* Summary: <p>While robust graph neural networks (GNNs) have been widely studied for graph
perturbation and attack, those for label noise have received significantly less
attention. Most existing methods heavily rely on the label smoothness
assumption to correct noisy labels, which adversely affects their performance
on heterophilous graphs. Further, they generally perform poorly in high
noise-rate scenarios. To address these problems, in this paper, we propose a
novel probabilistic graphical model (PGM) based framework LNP. Given a noisy
label set and a clean label set, our goal is to maximize the likelihood of
labels in the clean set. We first present LNP-v1, which generates clean labels
based on graphs only in the Bayesian network. To further leverage the
information of clean labels in the noisy label set, we put forward LNP-v2,
which incorporates the noisy label set into the Bayesian network to generate
clean labels. The generative process can then be used to predict labels for
unlabeled nodes. We conduct extensive experiments to show the robustness of LNP
on varying noise types and rates, and also on graphs with different
heterophilies. In particular, we show that LNP can lead to inspiring
performance in high noise-rate situations.
</p>

### Title: Joint Composite Latent Space Bayesian Optimization. (arXiv:2311.02213v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02213](http://arxiv.org/abs/2311.02213)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02213] Joint Composite Latent Space Bayesian Optimization](http://arxiv.org/abs/2311.02213) #generative`
* Summary: <p>Bayesian Optimization (BO) is a technique for sample-efficient black-box
optimization that employs probabilistic models to identify promising input
locations for evaluation. When dealing with composite-structured functions,
such as f=g o h, evaluating a specific location x yields observations of both
the final outcome f(x) = g(h(x)) as well as the intermediate output(s) h(x).
Previous research has shown that integrating information from these
intermediate outputs can enhance BO performance substantially. However,
existing methods struggle if the outputs h(x) are high-dimensional. Many
relevant problems fall into this setting, including in the context of
generative AI, molecular design, or robotics. To effectively tackle these
challenges, we introduce Joint Composite Latent Space Bayesian Optimization
(JoCo), a novel framework that jointly trains neural network encoders and
probabilistic models to adaptively compress high-dimensional input and output
spaces into manageable latent representations. This enables viable BO on these
compressed representations, allowing JoCo to outperform other state-of-the-art
methods in high-dimensional BO on a wide variety of simulated and real-world
problems.
</p>

### Title: Structured Neural Networks for Density Estimation and Causal Inference. (arXiv:2311.02221v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02221](http://arxiv.org/abs/2311.02221)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02221] Structured Neural Networks for Density Estimation and Causal Inference](http://arxiv.org/abs/2311.02221) #generative`
* Summary: <p>Injecting structure into neural networks enables learning functions that
satisfy invariances with respect to subsets of inputs. For instance, when
learning generative models using neural networks, it is advantageous to encode
the conditional independence structure of observed variables, often in the form
of Bayesian networks. We propose the Structured Neural Network (StrNN), which
injects structure through masking pathways in a neural network. The masks are
designed via a novel relationship we explore between neural network
architectures and binary matrix factorization, to ensure that the desired
independencies are respected. We devise and study practical algorithms for this
otherwise NP-hard design problem based on novel objectives that control the
model architecture. We demonstrate the utility of StrNN in three applications:
(1) binary and Gaussian density estimation with StrNN, (2) real-valued density
estimation with Structured Autoregressive Flows (StrAFs) and Structured
Continuous Normalizing Flows (StrCNF), and (3) interventional and
counterfactual analysis with StrAFs for causal inference. Our work opens up new
avenues for learning neural networks that enable data-efficient generative
modeling and the use of normalizing flows for causal effect estimation.
</p>

## anomaly
## in-context
### Title: COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning. (arXiv:2311.02248v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2311.02248](http://arxiv.org/abs/2311.02248)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02248] COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning](http://arxiv.org/abs/2311.02248) #in-context`
* Summary: <p>We present a data and cost efficient way of incorporating the speech modality
into a large language model (LLM). The resulting multi-modal LLM is a
COntextual Speech Model with Instruction-following/in-context-learning
Capabilities - COSMIC. Speech comprehension test question-answer (SQA) pairs
are generated using GPT-3.5 based on the speech transcriptions as a part of the
supervision for the instruction tuning. With fewer than 20M trainable
parameters and as little as 450 hours of English speech data for SQA
generation, COSMIC exhibits emergent instruction-following and in-context
learning capabilities in speech-to-text tasks. The model is able to follow the
given text instructions to generate text response even on the unseen EN$\to$X
speech-to-text translation (S2TT) task with zero-shot setting. We evaluate the
model's in-context learning via various tasks such as EN$\to$X S2TT and
few-shot domain adaptation. And instruction-following capabilities are
evaluated through a contextual biasing benchmark. Our results demonstrate the
efficacy of the proposed low cost recipe for building a speech LLM and that
with the new instruction-tuning data.
</p>

## memory
### Title: LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes. (arXiv:2311.02313v1 [cs.CV])
* Paper URL: [http://arxiv.org/abs/2311.02313](http://arxiv.org/abs/2311.02313)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02313] LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes](http://arxiv.org/abs/2311.02313) #memory`
* Summary: <p>Large-scale semantic mapping is crucial for outdoor autonomous agents to
fulfill high-level tasks such as planning and navigation. This paper proposes a
novel method for large-scale 3D semantic reconstruction through implicit
representations from LiDAR measurements alone. We firstly leverages an
octree-based and hierarchical structure to store implicit features, then these
implicit features are decoded to semantic information and signed distance value
through shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf
algorithms to predict the semantic labels and instance IDs of point cloud. Then
we jointly optimize the implicit features and MLPs parameters with
self-supervision paradigm for point cloud geometry and pseudo-supervision
pradigm for semantic and panoptic labels. Subsequently, Marching Cubes
algorithm is exploited to subdivide and visualize the scenes in the inferring
stage. For scenarios with memory constraints, a map stitching strategy is also
developed to merge sub-maps into a complete map. As far as we know, our method
is the first work to reconstruct semantic implicit scenes from LiDAR-only
input. Experiments on three real-world datasets, SemanticKITTI, SemanticPOSS
and nuScenes, demonstrate the effectiveness and efficiency of our framework
compared to current state-of-the-art 3D mapping methods.
</p>

## few-shot
### Title: Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data. (arXiv:2311.02216v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2311.02216](http://arxiv.org/abs/2311.02216)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02216] Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data](http://arxiv.org/abs/2311.02216) #few-shot`
* Summary: <p>Numbers are crucial for various real-world domains such as finance,
economics, and science. Thus, understanding and reasoning with numbers are
essential skills for language models to solve different tasks. While different
numerical benchmarks have been introduced in recent years, they are limited to
specific numerical aspects mostly. In this paper, we propose a hierarchical
taxonomy for numerical reasoning skills with more than ten reasoning types
across four levels: representation, number sense, manipulation, and complex
reasoning. We conduct a comprehensive evaluation of state-of-the-art models to
identify reasoning challenges specific to them. Henceforth, we develop a
diverse set of numerical probes employing a semi-automated approach. We focus
on the tabular Natural Language Inference (TNLI) task as a case study and
measure models' performance shifts. Our results show that no model consistently
excels across all numerical reasoning types. Among the probed models, FlanT5
(few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical
reasoning skills compared to other models. Label-flipping probes indicate that
models often exploit dataset artifacts to predict the correct labels.
</p>

### Title: Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles. (arXiv:2311.02310v1 [cs.CL])
* Paper URL: [http://arxiv.org/abs/2311.02310](http://arxiv.org/abs/2311.02310)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02310] Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles](http://arxiv.org/abs/2311.02310) #few-shot`
* Summary: <p>Large language models trained primarily in a monolingual setting have
demonstrated their ability to generalize to machine translation using zero- and
few-shot examples with in-context learning. However, even though zero-shot
translations are relatively good, there remains a discernible gap comparing
their performance with the few-shot setting. In this paper, we investigate the
factors contributing to this gap and find that this gap can largely be closed
(for about 70%) by matching the writing styles of the target corpus.
Additionally, we explore potential approaches to enhance zero-shot baselines
without the need for parallel demonstration examples, providing valuable
insights into how these methods contribute to improving translation metrics.
</p>

### Title: Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis. (arXiv:2311.02300v1 [cs.LG])
* Paper URL: [http://arxiv.org/abs/2311.02300](http://arxiv.org/abs/2311.02300)
* Code URL: null
* Copy Paste: `<input type="checkbox">[[2311.02300] Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis](http://arxiv.org/abs/2311.02300) #few-shot`
* Summary: <p>Meta learning is a promising technique for solving few-shot fault prediction
problems, which have attracted the attention of many researchers in recent
years. Existing meta-learning methods for time series prediction, which
predominantly rely on random and similarity matching-based task partitioning,
face three major limitations: (1) feature exploitation inefficiency; (2)
suboptimal task data allocation; and (3) limited robustness with small samples.
To overcome these limitations, we introduce a novel 'pseudo meta-task'
partitioning scheme that treats a continuous time period of a time series as a
meta-task, composed of multiple successive short time periods. Employing
continuous time series as pseudo meta-tasks allows our method to extract more
comprehensive features and relationships from the data, resulting in more
accurate predictions. Moreover, we introduce a differential algorithm to
enhance the robustness of our method across different datasets. Through
extensive experiments on several fault and time series prediction datasets, we
demonstrate that our approach substantially enhances prediction performance and
generalization capability under both few-shot and general conditions.
</p>

