### Title: On the Limitations of Stochastic Pre-processing Defenses
* Paper ID: 2206.09491v1
* Paper URL: [http://arxiv.org/abs/2206.09491v1](http://arxiv.org/abs/2206.09491v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Defending against adversarial examples remains an open problem. A common
belief is that randomness at inference increases the cost of finding
adversarial inputs. An example of such a defense is to apply a random
transformation to inputs prior to feeding them to the model. In this paper, we
empirically and theoretically investigate such stochastic pre-processing
defenses and demonstrate that they are flawed. First, we show that most
stochastic defenses are weaker than previously thought; they lack sufficient
randomness to withstand even standard attacks like projected gradient descent.
This casts doubt on a long-held assumption that stochastic defenses invalidate
attacks designed to evade deterministic defenses and force attackers to
integrate the Expectation over Transformation (EOT) concept. Second, we show
that stochastic defenses confront a trade-off between adversarial robustness
and model invariance; they become less effective as the defended model acquires
more invariance to their randomization. Future work will need to decouple these
two effects. Our code is available in the supplementary material.

### Title: Cyber Threats Jurisdiction and Authority
* Paper ID: 2206.09465v1
* Paper URL: [http://arxiv.org/abs/2206.09465v1](http://arxiv.org/abs/2206.09465v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Cybersecurity threats affect all aspects of society; critical infrastructures
(such as networks, corporate systems, water supply systems, and intelligent
transportation systems) are especially prone to attacks and can have tangible
negative consequences on society. However, these critical cyber systems are
generally governed by multiple jurisdictions, for instance the Metro in the
Washington, D.C. area is managed by the states of Virginia and Maryland, as
well as the District of Columbia (DC) through Washington Metropolitan Area
Transit Authority (WMATA). Additionally, the water treatment infrastructure
managed by DC Water consists of waste water input from Fairfax and Arlington
counties, and the district (i.e. DC). Additionally, cyber attacks usually
launch from unknown sources, through unknown switches and servers, and end up
at the destination without much knowledge on their source or path. Certain
infrastructures are shared amongst multiple countries, another idiosyncrasy
that exacerbates the issue of governance. This law paper however, is not
concerned with the general governance of these infrastructures, rather with the
ambiguity in the relevant laws or doctrines about which authority would prevail
in the context of a cyber threat or a cyber-attack, with a focus on federal vs.
state issues, international law involvement, federal preemption, technical
aspects that could affect lawmaking, and conflicting responsibilities in cases
of cyber crime. A legal analysis of previous cases is presented, as well as an
extended discussion addressing different sides of the argument.

### Title: A Universal Adversarial Policy for Text Classifiers
* Paper ID: 2206.09458v1
* Paper URL: [http://arxiv.org/abs/2206.09458v1](http://arxiv.org/abs/2206.09458v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Discovering the existence of universal adversarial perturbations had large
theoretical and practical impacts on the field of adversarial learning. In the
text domain, most universal studies focused on adversarial prefixes which are
added to all texts. However, unlike the vision domain, adding the same
perturbation to different inputs results in noticeably unnatural inputs.
Therefore, we introduce a new universal adversarial setup - a universal
adversarial policy, which has many advantages of other universal attacks but
also results in valid texts - thus making it relevant in practice. We achieve
this by learning a single search policy over a predefined set of semantics
preserving text alterations, on many texts. This formulation is universal in
that the policy is successful in finding adversarial examples on new texts
efficiently. Our approach uses text perturbations which were extensively shown
to produce natural attacks in the non-universal setup (specific synonym
replacements). We suggest a strong baseline approach for this formulation which
uses reinforcement learning. It's ability to generalise (from as few as 500
training texts) shows that universal adversarial patterns exist in the text
domain as well.

### Title: Reputation, Risk, and Trust on User Adoption of Internet Search Engines: The Case of DuckDuckGo
* Paper ID: 2206.09428v1
* Paper URL: [http://arxiv.org/abs/2206.09428v1](http://arxiv.org/abs/2206.09428v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: This paper investigates the determinants of end-user adoption of the
DuckDuckGo search engine coupling the standard UTAUT model with factors to
reflect reputation, risk, and trust. An experimental approach was taken to
validate our model, where participants were exposed to the DuckDuckGo product
using a vignette. Subsequently, answering questions on their perception of the
technology. The data was analyzed using the partial least squares-structural
equation modeling (PLS-SEM) approach. From the nine distinct factors studied,
we found that 'Performance Expectancy' played the greatest role in user
decisions on adoption, followed by 'Firm Reputation', 'Initial Trust in
Technology', 'Social Influence', and 'Individual Disposition to Trust'. We
conclude by exploring how these findings can explain DuckDuckGo's rising
prominence as a search engine.

### Title: Construction and Optimization of TRNG Based Substitution Boxes for Block Encryption Algorithms
* Paper ID: 2206.09424v1
* Paper URL: [http://arxiv.org/abs/2206.09424v1](http://arxiv.org/abs/2206.09424v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Internet of Things is an ecosystem of interconnected devices that are
accessible through the internet. The recent research focuses on adding more
smartness and intelligence to these edge devices. This makes them susceptible
to various kinds of security threats. These edge devices rely on cryptographic
techniques to encrypt the pre-processed data collected from the sensors
deployed in the field. In this regard, block cipher has been one of the most
reliable options through which data security is accomplished. The strength of
block encryption algorithms against different attacks is dependent on its
nonlinear primitive which is called Substitution Boxes. For the design of
S-boxes mainly algebraic and chaos-based techniques are used but researchers
also found various weaknesses in these techniques. On the other side,
literature endorse the true random numbers for information security due to the
reason that, true random numbers are purely non-deterministic. In this paper
firstly a natural dynamical phenomenon is utilized for the generation of true
random numbers based S-boxes. Secondly, a systematic literature review was
conducted to know which metaheuristic optimization technique is highly adopted
in the current decade for the optimization of S-boxes. Based on the outcome of
Systematic Literature Review (SLR), genetic algorithm is chosen for the
optimization of s-boxes. The results of our method validate that the proposed
dynamic S-boxes are effective for the block ciphers. Moreover, our results
showed that the proposed substitution boxes achieve better

### Title: Phantom Artifacts & Code Review Coverage in Dependency Updates
* Paper ID: 2206.09422v1
* Paper URL: [http://arxiv.org/abs/2206.09422v1](http://arxiv.org/abs/2206.09422v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: The goal of this study is to aid developers in securely accepting dependency
updates by measuring if the code changes in an update have passed through a
code review process. We implement DepDive, an update audit tool for packages in
Crates.io, npm, PyPI, and RubyGems registry. DepDive first (i) identifies the
files and the code changes in an update that cannot be traced back to the
package's source repository, i.e., phantom artifacts; and then (ii) measures
what portion of changes in the update, excluding the phantom artifacts, has
passed through a code review process, i.e., code review coverage.
  Using DepDive, we present an empirical study across the latest ten updates of
the most downloaded 1000 packages in each of the four registries. Our study
unveils interesting insights while also providing an evaluation of our proposed
approach. We find that phantom artifacts are not uncommon in the updates
(20.1\% of the analyzed updates had at least one phantom file). The phantoms
can appear either due to legitimate reasons, such as in the case of
programmatically generated files, or from accidental inclusion, such as in the
case of files that are ignored in the repository. However, without provenance
tracking, we cannot audit if the changes in these phantom artifacts were
code-reviewed or not.
  Regarding code review coverage (\textit{CRC)}, we find the updates are
typically only partially code-reviewed (52.5\% of the time). Further, only
9.0\% of the packages had all their updates in our data set fully
code-reviewed, indicating that even the most used packages can introduce
non-reviewed code in the software supply chain. We also observe that updates
either tend to have very high \textit{CRC} or very low \textit{CRC}, suggesting
that packages at the opposite end of the spectrum may require a separate set of
treatments.

### Title: Extended field-of-view speckle-correlation imaging by estimating autocorrelation
* Paper ID: 2206.09417v1
* Paper URL: [http://arxiv.org/abs/2206.09417v1](http://arxiv.org/abs/2206.09417v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Imaging through scattering media is a longstanding issue in a wide range of
applications, including biomedicine, security, and astronomy.
Speckle-correlation imaging is promising for non-invasively seeing through
scattering media by assuming shift-invariance of the scattering process called
the memory effect. However, the memory effect is known to be severely limited
when the medium is thick. Under such a scattering condition,
speckle-correlation imaging is not practical because the correlation of the
speckle decays, reducing the field of view. To address this problem, we present
a method for expanding the field of view of single-shot speckle-correlation
imaging through scattering media with a limited memory effect. We derive the
imaging model under this scattering condition and its inversion for
reconstructing the object. Our method simultaneously estimates both the object
and the decay of the speckle correlation based on the gradient descent method.
We numerically and experimentally demonstrate the proposed method by
reconstructing point sources behind scattering media with a limited memory
effect. In the demonstrations, our speckle-correlation imaging method with a
minimal lensless optical setup realized a larger field of view compared with
the conventional one. This study will make techniques for imaging through
scattering media more practical in various fields.

### Title: Privacy-Preserving Analytics on Decentralized Social Graphs: The Case of Eigendecomposition
* Paper ID: 2206.09388v1
* Paper URL: [http://arxiv.org/abs/2206.09388v1](http://arxiv.org/abs/2206.09388v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: Analytics over social graphs allows to extract valuable knowledge and
insights for many fields like community detection, fraud detection, and
interest mining. In practice, decentralized social graphs frequently arise,
where the social graph is not available to a single entity and is decentralized
among a large number of users, each holding only a limited local view about the
whole graph. Collecting the local views for analytics of decentralized social
graphs raises critical privacy concerns, as they encode private information
about the social interactions among individuals. In this paper, we design,
implement, and evaluate PrivGED, a new system aimed at privacy-preserving
analytics over decentralized social graphs. PrivGED focuses on the support for
eigendecomposition, one popular and fundamental graph analytics task producing
eigenvalues/eigenvectors over the adjacency matrix of a social graph and
benefits various practical applications. PrivGED is built from a delicate
synergy of insights on graph analytics, lightweight cryptography, and
differential privacy, allowing users to securely contribute their local views
on a decentralized social graph for a cloud-based eigendecomposition analytics
service while gaining strong privacy protection. Extensive experiments over
real-world social graph datasets demonstrate that PrivGED achieves accuracy
comparable to the plaintext domain, with practically affordable performance
superior to prior art.

### Title: Multi-period Optimal Control for Mobile Agents Considering State Unpredictability
* Paper ID: 2206.09330v1
* Paper URL: [http://arxiv.org/abs/2206.09330v1](http://arxiv.org/abs/2206.09330v1)
* Updated Date: 2022-06-19
* Code URL: null
* Summary: The optimal control for mobile agents is an important and challenging
research issue. Recent work shows that using randomized mechanism in agents'
control can make the state unpredictable, and thus ensure the security of
agents. However, the unpredictable design is only considered in single period,
which can lead to intolerable control performance in long time horizon. This
paper aims at the trade-off between the control performance and state
unpredictability of mobile agents in long time horizon. Utilizing random
perturbations consistent with uniform distributions to maximize the attackers'
prediction errors of future states, we formulate the problem as a multi-period
convex stochastic optimization problem and solve it through dynamic
programming. Specifically, we design the optimal control strategy considering
both unconstrained and input constrained systems. The analytical iterative
expressions of the control are further provided. Simulation illustrates that
the algorithm increases the prediction errors under Kalman filter while
achieving the control performance requirements successfully.

