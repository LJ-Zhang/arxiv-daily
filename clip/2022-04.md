# clip

## 04-17

### Title: No Token Left Behind: Explainability-Aided Image Classification and Generation
* Paper ID: 2204.04908v1
* Paper URL: [http://arxiv.org/abs/2204.04908v1](http://arxiv.org/abs/2204.04908v1)
* Updated Date: 2022-04-11
* Code URL: null
* Summary: The application of zero-shot learning in computer vision has been
revolutionized by the use of image-text matching models. The most notable
example, CLIP, has been widely used for both zero-shot classification and
guiding generative models with a text prompt. However, the zero-shot use of
CLIP is unstable with respect to the phrasing of the input text, making it
necessary to carefully engineer the prompts used. We find that this instability
stems from a selective similarity score, which is based only on a subset of the
semantically meaningful input tokens. To mitigate it, we present a novel
explainability-based approach, which adds a loss term to ensure that CLIP
focuses on all relevant semantic parts of the input, in addition to employing
the CLIP similarity loss used in previous works. When applied to one-shot
classification through prompt engineering, our method yields an improvement in
the recognition rate, without additional training or fine-tuning. Additionally,
we show that CLIP guidance of generative models using our method significantly
improves the generated images. Finally, we demonstrate a novel use of CLIP
guidance for text-based image generation with spatial conditioning on object
location, by requiring the image explainability heatmap for each object to be
confined to a pre-determined bounding box.

### Title: Settling the Sample Complexity of Model-Based Offline Reinforcement Learning
* Paper ID: 2204.05275v1
* Paper URL: [http://arxiv.org/abs/2204.05275v1](http://arxiv.org/abs/2204.05275v1)
* Updated Date: 2022-04-11
* Code URL: null
* Summary: This paper is concerned with offline reinforcement learning (RL), which
learns using pre-collected data without further exploration. Effective offline
RL would be able to accommodate distribution shift and limited data coverage.
However, prior algorithms or analyses either suffer from suboptimal sample
complexities or incur high burn-in cost to reach sample optimality, thus posing
an impediment to efficient offline RL in sample-starved applications.
  We demonstrate that the model-based (or "plug-in") approach achieves
minimax-optimal sample complexity without burn-in cost for tabular Markov
decision processes (MDPs). Concretely, consider a finite-horizon (resp.
$\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$
(resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution
shift of data is reflected by some single-policy clipped concentrability
coefficient $C^{\star}_{\text{clipped}}$. We prove that model-based offline RL
yields $\varepsilon$-accuracy with a sample complexity of \[ \begin{cases}
\frac{H^{4}SC_{\text{clipped}}^{\star}}{\varepsilon^{2}} &
(\text{finite-horizon MDPs})
\frac{SC_{\text{clipped}}^{\star}}{(1-\gamma)^{3}\varepsilon^{2}} &
(\text{infinite-horizon MDPs}) \end{cases} \] up to log factor, which is
minimax optimal for the entire $\varepsilon$-range. Our algorithms are
"pessimistic" variants of value iteration with Bernstein-style penalties, and
do not require sophisticated variance reduction.

### Title: How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image
* Paper ID: 2204.05533v1
* Paper URL: [http://arxiv.org/abs/2204.05533v1](http://arxiv.org/abs/2204.05533v1)
* Updated Date: 2022-04-12
* Code URL: [https://github.com/ssu-humane/fake-news-thumbnail](https://github.com/ssu-humane/fake-news-thumbnail)
* Summary: This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.

### Title: Text-Driven Separation of Arbitrary Sounds
* Paper ID: 2204.05738v1
* Paper URL: [http://arxiv.org/abs/2204.05738v1](http://arxiv.org/abs/2204.05738v1)
* Updated Date: 2022-04-12
* Code URL: null
* Summary: We propose a method of separating a desired sound source from a
single-channel mixture, based on either a textual description or a short audio
sample of the target source. This is achieved by combining two distinct models.
The first model, SoundWords, is trained to jointly embed both an audio clip and
its textual description to the same embedding in a shared representation. The
second model, SoundFilter, takes a mixed source audio clip as an input and
separates it based on a conditioning vector from the shared text-audio
representation defined by SoundWords, making the model agnostic to the
conditioning modality. Evaluating on multiple datasets, we show that our
approach can achieve an SI-SDR of 9.1 dB for mixtures of two arbitrary sounds
when conditioned on text and 10.1 dB when conditioned on audio. We also show
that SoundWords is effective at learning co-embeddings and that our multi-modal
training approach improves the performance of SoundFilter.

### Title: VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration
* Paper ID: 2204.05841v1
* Paper URL: [http://arxiv.org/abs/2204.05841v1](http://arxiv.org/abs/2204.05841v1)
* Updated Date: 2022-04-12
* Code URL: null
* Summary: Speech restoration aims to remove distortions in speech signals. Prior
methods mainly focus on a single type of distortion, such as speech denoising
or dereverberation. However, speech signals can be degraded by several
different distortions simultaneously in the real world. It is thus important to
extend speech restoration models to deal with multiple distortions. In this
paper, we introduce VoiceFixer, a unified framework for high-fidelity speech
restoration. VoiceFixer restores speech from multiple distortions (e.g., noise,
reverberation, and clipping) and can expand degraded speech (e.g., noisy
speech) with a low bandwidth to 44.1 kHz full-bandwidth high-fidelity speech.
We design VoiceFixer based on (1) an analysis stage that predicts
intermediate-level features from the degraded speech, and (2) a synthesis stage
that generates waveform using a neural vocoder. Both objective and subjective
evaluations show that VoiceFixer is effective on severely degraded speech, such
as real-world historical speech recordings. Samples of VoiceFixer are available
at https://haoheliu.github.io/voicefixer.

### Title: ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension
* Paper ID: 2204.05991v1
* Paper URL: [http://arxiv.org/abs/2204.05991v1](http://arxiv.org/abs/2204.05991v1)
* Updated Date: 2022-04-12
* Code URL: [https://github.com/allenai/reclip](https://github.com/allenai/reclip)
* Summary: Training a referring expression comprehension (ReC) model for a new visual
domain requires collecting referring expressions, and potentially corresponding
bounding boxes, for images in the domain. While large-scale pre-trained models
are useful for image classification across domains, it remains unclear if they
can be applied in a zero-shot manner to more complex tasks like ReC. We present
ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a
state-of-the-art large-scale model, for ReC. Motivated by the close connection
between ReC and CLIP's contrastive pre-training objective, the first component
of ReCLIP is a region-scoring method that isolates object proposals via
cropping and blurring, and passes them to CLIP. However, through controlled
experiments on a synthetic dataset, we find that CLIP is largely incapable of
performing spatial reasoning off-the-shelf. Thus, the second component of
ReCLIP is a spatial relation resolver that handles several types of spatial
relations. We reduce the gap between zero-shot baselines from prior work and
supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game
imagery), ReCLIP's relative improvement over supervised ReC models trained on
real images is 8%.

### Title: Hierarchical Text-Conditional Image Generation with CLIP Latents
* Paper ID: 2204.06125v1
* Paper URL: [http://arxiv.org/abs/2204.06125v1](http://arxiv.org/abs/2204.06125v1)
* Updated Date: 2022-04-13
* Code URL: null
* Summary: Contrastive models like CLIP have been shown to learn robust representations
of images that capture both semantics and style. To leverage these
representations for image generation, we propose a two-stage model: a prior
that generates a CLIP image embedding given a text caption, and a decoder that
generates an image conditioned on the image embedding. We show that explicitly
generating image representations improves image diversity with minimal loss in
photorealism and caption similarity. Our decoders conditioned on image
representations can also produce variations of an image that preserve both its
semantics and style, while varying the non-essential details absent from the
image representation. Moreover, the joint embedding space of CLIP enables
language-guided image manipulations in a zero-shot fashion. We use diffusion
models for the decoder and experiment with both autoregressive and diffusion
models for the prior, finding that the latter are computationally more
efficient and produce higher-quality samples.

### Title: CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU
* Paper ID: 2204.06240v1
* Paper URL: [http://arxiv.org/abs/2204.06240v1](http://arxiv.org/abs/2204.06240v1)
* Updated Date: 2022-04-13
* Code URL: [https://github.com/zhengzangw/largebatchctr](https://github.com/zhengzangw/largebatchctr)
* Summary: The click-through rate (CTR) prediction task is to predict whether a user
will click on the recommended item. As mind-boggling amounts of data are
produced online daily, accelerating CTR prediction model training is critical
to ensuring an up-to-date model and reducing the training cost. One approach to
increase the training speed is to apply large batch training. However, as shown
in computer vision and natural language processing tasks, training with a large
batch easily suffers from the loss of accuracy. Our experiments show that
previous scaling rules fail in the training of CTR prediction neural networks.
To tackle this problem, we first theoretically show that different frequencies
of ids make it challenging to scale hyperparameters when scaling the batch
size. To stabilize the training process in a large batch size setting, we
develop the adaptive Column-wise Clipping (CowClip). It enables an easy and
effective scaling rule for the embeddings, which keeps the learning rate
unchanged and scales the L2 loss. We conduct extensive experiments with four
CTR prediction networks on two real-world datasets and successfully scaled 128
times the original batch size without accuracy loss. In particular, for CTR
prediction model DeepFM training on the Criteo dataset, our optimization
framework enlarges the batch size from 1K to 128K with over 0.1% AUC
improvement and reduces training time from 12 hours to 10 minutes on a single
V100 GPU. Our code locates at https://github.com/zhengzangw/LargeBatchCTR.

### Title: Realistic Video Sequences for Subjective QoE Analysis
* Paper ID: 2204.06829v1
* Paper URL: [http://arxiv.org/abs/2204.06829v1](http://arxiv.org/abs/2204.06829v1)
* Updated Date: 2022-04-14
* Code URL: null
* Summary: Multimedia streaming over the Internet (live and on demand) is the
cornerstone of modern Internet carrying more than 60% of all traffic. With such
high demand, delivering outstanding user experience is a crucial and
challenging task. To evaluate user QoE many researchers deploy subjective
quality assessments where participants watch and rate videos artificially
infused with various temporal and spatial impairments. To aid current efforts
in bridging the gap between the mapping of objective video QoE metrics to user
experience, we developed DashReStreamer, an open-source framework for
re-creating adaptively streamed video in real networks. DashReStreamer utilises
a log created by a HAS algorithm run in an uncontrolled environment (i.e.,
wired or wireless networks), encoding visual changes and stall events in one
video file. These videos are applicable for subjective QoE evaluation mimicking
realistic network conditions.
  To supplement DashReStreamer, we re-create 234 realistic video clips, based
on video logs collected from real mobile and wireless networks. In addition our
dataset contains both video logs with all decisions made by the HASalgorithm
and network bandwidth profile illustrating throughput distribution. We believe
this dataset and framework will permit other researchers in their pursuit for
the final frontier in understanding the impact of video QoE dynamics.

## 04-10

