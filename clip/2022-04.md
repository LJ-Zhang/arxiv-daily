# clip

## 04-17

### Title: Hierarchical Text-Conditional Image Generation with CLIP Latents
* Paper ID: 2204.06125v1
* Paper URL: [http://arxiv.org/abs/2204.06125v1](http://arxiv.org/abs/2204.06125v1)
* Updated Date: 2022-04-13 01:10:33+00:00
* Code URL: null
* Summary: Contrastive models like CLIP have been shown to learn robust representations
of images that capture both semantics and style. To leverage these
representations for image generation, we propose a two-stage model: a prior
that generates a CLIP image embedding given a text caption, and a decoder that
generates an image conditioned on the image embedding. We show that explicitly
generating image representations improves image diversity with minimal loss in
photorealism and caption similarity. Our decoders conditioned on image
representations can also produce variations of an image that preserve both its
semantics and style, while varying the non-essential details absent from the
image representation. Moreover, the joint embedding space of CLIP enables
language-guided image manipulations in a zero-shot fashion. We use diffusion
models for the decoder and experiment with both autoregressive and diffusion
models for the prior, finding that the latter are computationally more
efficient and produce higher-quality samples.

### Title: CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU
* Paper ID: 2204.06240v1
* Paper URL: [http://arxiv.org/abs/2204.06240v1](http://arxiv.org/abs/2204.06240v1)
* Updated Date: 2022-04-13 08:17:15+00:00
* Code URL: [https://github.com/zhengzangw/largebatchctr](https://github.com/zhengzangw/largebatchctr)
* Summary: The click-through rate (CTR) prediction task is to predict whether a user
will click on the recommended item. As mind-boggling amounts of data are
produced online daily, accelerating CTR prediction model training is critical
to ensuring an up-to-date model and reducing the training cost. One approach to
increase the training speed is to apply large batch training. However, as shown
in computer vision and natural language processing tasks, training with a large
batch easily suffers from the loss of accuracy. Our experiments show that
previous scaling rules fail in the training of CTR prediction neural networks.
To tackle this problem, we first theoretically show that different frequencies
of ids make it challenging to scale hyperparameters when scaling the batch
size. To stabilize the training process in a large batch size setting, we
develop the adaptive Column-wise Clipping (CowClip). It enables an easy and
effective scaling rule for the embeddings, which keeps the learning rate
unchanged and scales the L2 loss. We conduct extensive experiments with four
CTR prediction networks on two real-world datasets and successfully scaled 128
times the original batch size without accuracy loss. In particular, for CTR
prediction model DeepFM training on the Criteo dataset, our optimization
framework enlarges the batch size from 1K to 128K with over 0.1% AUC
improvement and reduces training time from 12 hours to 10 minutes on a single
V100 GPU. Our code locates at https://github.com/zhengzangw/LargeBatchCTR.

### Title: Realistic Video Sequences for Subjective QoE Analysis
* Paper ID: 2204.06829v1
* Paper URL: [http://arxiv.org/abs/2204.06829v1](http://arxiv.org/abs/2204.06829v1)
* Updated Date: 2022-04-14 08:56:30+00:00
* Code URL: null
* Summary: Multimedia streaming over the Internet (live and on demand) is the
cornerstone of modern Internet carrying more than 60% of all traffic. With such
high demand, delivering outstanding user experience is a crucial and
challenging task. To evaluate user QoE many researchers deploy subjective
quality assessments where participants watch and rate videos artificially
infused with various temporal and spatial impairments. To aid current efforts
in bridging the gap between the mapping of objective video QoE metrics to user
experience, we developed DashReStreamer, an open-source framework for
re-creating adaptively streamed video in real networks. DashReStreamer utilises
a log created by a HAS algorithm run in an uncontrolled environment (i.e.,
wired or wireless networks), encoding visual changes and stall events in one
video file. These videos are applicable for subjective QoE evaluation mimicking
realistic network conditions.
  To supplement DashReStreamer, we re-create 234 realistic video clips, based
on video logs collected from real mobile and wireless networks. In addition our
dataset contains both video logs with all decisions made by the HASalgorithm
and network bandwidth profile illustrating throughput distribution. We believe
this dataset and framework will permit other researchers in their pursuit for
the final frontier in understanding the impact of video QoE dynamics.

